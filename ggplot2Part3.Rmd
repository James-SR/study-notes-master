# Advanced ggplot2
***
Notes taken during/inspired by the Datacamp course 'Data Visualization with ggplot2 (Part 3)' by Rick Scavetta.  This course builds on the first course and second courses, which looked at how to build plots, aesthetics, statistics and practical matters such as themes.

The focus of this course is on more specific plot types, including looking at handling large data plots.  We also look at maps and video frames aka animations.  The fourth chapter looks at the internals of GGPlot2 including the grid extra package.  Finally we will look at a case study, which includes looking at how we can use the extensions function to build our own plots from scratch.  

Course slides:
* [Part 1 - Statistical Plots](https://s3.amazonaws.com/assets.datacamp.com/production/course_862/slides/ggplot2_course_3_ch1.pdf)
* [Part 2 - Plots for Specific Data Part 1](https://s3.amazonaws.com/assets.datacamp.com/production/course_862/slides/ggplot2_course_3_ch2.pdf)
* [Part 3 - Plots for Specific Data Part 2](https://s3.amazonaws.com/assets.datacamp.com/production/course_862/slides/ggplot2_course_3_ch3.pdf)
* [Part 4 - GGPlot2 Internals](https://s3.amazonaws.com/assets.datacamp.com/production/course_862/slides/ggplot2_course_3_ch4.pdf)
* [Part 5 - Case Study](https://s3.amazonaws.com/assets.datacamp.com/production/course_862/slides/ggplot2_course_3_ch5.pdf)

## Refresher

As a refresher to statistical plots, let's build a scatter plot with an additional statistic layer.

A dataset called movies_small is coded in your workspace. It is a random sample of 1000 observations from the larger movies dataset, that's inside the ggplot2movies package. The dataset contains information on movies from IMDB. The variable votes is the number of IMDB users who have rated a movie and the rating (converted into a categorical variable) is the average rating for the movie.

```{r}
library(ggplot2)
#load the data
movies_small <- readRDS("D:/CloudStation/Documents/2017/RData/ch1_movies_small.RDS")

# Explore movies_small with str()
str(movies_small)

# Build a scatter plot with mean and 95% CI
ggplot(movies_small, aes(x = rating, y = votes)) +
  geom_point() +
  stat_summary(fun.data = "mean_cl_normal",
               geom = "crossbar",
               width = 0.2,
               col = "red") +
  scale_y_log10()

```

Next we are going to look at the diamons dataset.  Recall that there are a variety of scale_ functions. Here, data are transformed or filtered first, after which the plot and associated statistics are computed. For example, scale_y_continuous(limits = c(100, 1000) will remove values outside that range.

Contrast this to coord_cartesian(), which computes the statistics before plotting. That means that the plot and summary statistics are performed on the raw data. That's why we say that coord_cartesian(c(100, 1000)) "zooms in" a plot. This was discussed in the chapter on coordinates in course 2.

Here we're going to expand on this and introduce scale_x_log10() and scale_y_log10() which perform log10 transformations, and coord_equal(), which sets an aspect ratio of 1 (coord_fixed() is also an option).

```{r}
str(diamonds)

# Produce the plot - To get nice formatting we're using the expression() function for the labels
ggplot(diamonds, aes(x = carat, y = price, col = color)) +
  geom_point(alpha = 0.5, size = 0.5, shape = 16) +
  scale_x_log10(expression(log[10](Carat)), limits = c(0.1,10)) +
  scale_y_log10(expression(log[10](Price)), limits = c(100,100000)) +
  scale_color_brewer(palette = "YlOrRd") +
  coord_equal() + # sets the aspect ratio to 1
  theme_classic()
  

```

Or we can present the data as a smooth/linear model

```{r}
# Add smooth layer and facet the plot
ggplot(diamonds, aes(x = carat, y = price, col = color)) +
  stat_smooth(method = "lm") +
  scale_x_log10(expression(log[10](Carat)), limits = c(0.1,10)) +
  scale_y_log10(expression(log[10](Price)), limits = c(100,100000)) +
  scale_color_brewer(palette = "YlOrRd") +
  coord_equal() +
  theme_classic()
```

## Statistical plots

Whilst all the previous plots could be considered as statistical plots, we now concentrate on those more suited to a statistical or academic audienc - two examples are box plots and density plots.  In this exercise you'll return to the first plotting exercise and see how box plots compare to dot plots for representing high-density data.

### Box plots

Box plots are very useful, but they don't solve all your problems all the time, for example, when your data are heavily skewed, you will still need to transform it. You'll see that here, using the movies_small dataset, a subset of 10,000 observations of ggplot2movies::movies.

```{r}
# Add a boxplot geom
d <- ggplot(movies_small, aes(x = rating, y = votes)) +
  geom_point() +
  geom_boxplot() +
  stat_summary(fun.data = "mean_cl_normal",
               geom = "crossbar",
               width = 0.2,
               col = "red")

# Untransformed plot
d

# Transform the scale
d + scale_y_log10()

# Transform the coordinates
d + coord_trans(y = "log10")
```

Notice how different the normal distribution estimation (red boxes) and boxplots (less prone to outliers) are.

If you only have continuous variables, you can convert them into ordinal variables using any of the following functions:

* cut_interval(x, n) makes n groups from vector x with equal range.
* cut_number(x, n) makes n groups from vector x with (approximately) equal numbers of observations.
* cut_width(x, width) makes groups of width width from vector x.

This is useful when you want to summarize a complex scatter plot. By applying these functions to the carat variable and mapping that onto the group aesthetic, you can convert the scatter plot in the viewer into a series of box plots on the fly.

Going from a continuous to a categorical variable reduces the amount of information, but sometimes that helps us understand the data.

```{r}
# Plot object p
p <- ggplot(diamonds, aes(x = carat, y = price))

# Use cut_interval
p + geom_boxplot(aes(group = cut_interval(carat, n = 10)))

# Use cut_number
p + geom_boxplot(aes(group = cut_number(carat, n = 10)))

# Use cut_width
p + geom_boxplot(aes(group = cut_width(carat, width = 0.25)))
```

Be aware that there are many ways to calculate the IQR, short for inter-quartile range (that is Q3−Q1Q3−Q1). These are defined in the help pages for the quantile() function:

> ?quantile

Generally, the IQR becomes more consistent across methods as the sample size increases, you are likely to encounter spurious artefacts when drawing box plots with small sample sizes.

### Density Plots

Density plots are similar to histograms but less well used.  They are used to display the distribution of univariate data, such as probabilty density functions (PDFs).  One aspect you can set is the bandwidth, which helps to determine how 'lumpy' or how high the seperation is between each individual peak in a dataset.  

To make a straightforward density plot, add a geom_density() layer.

Before plotting, you will calculate the emperical density function, similar to how you can use the density() function in the stats package, available by default when you start R. The following default parameters are used (you can specify these arguments both in density() as well as geom_density()):

> bw = "nrd0", telling R which rule to use to choose an appropriate bandwidth.
> kernel = "gaussian", telling R to use the Gaussian kernel.

There is some test data, containing three columns: norm, bimodal and uniform. Each column represents 200 samples from a normal, bimodal and uniform distribution.

```{r}
# Load the test data
load("D:/CloudStation/Documents/2017/RData/test_datasets.RData")
test_data <- ch1_test_data

# Calculating density: d
d <- density(test_data$norm)

# Use which.max() to calculate mode
mode <- d$x[which.max(d$y)]

# Finish the ggplot call
ggplot(test_data, aes(x = norm)) +
  geom_rug() +
  geom_density() +
  geom_vline(xintercept = mode, col = "red")
```

Sometimes it is useful to compare a histogram with a density plot. However, the histogram's y-scale must first be converted to frequency instead of absolute count. After doing so, you can add an empirical PDF using geom_density() or a theoretical PDF using stat_function().

```{r}
# Arguments you'll need later on
fun_args <- list(mean = mean(test_data$norm), sd = sd(test_data$norm))

# Finish the ggplot
ggplot(test_data, aes(x = norm)) + 
  geom_histogram(aes(y = ..density..)) + 
  geom_density(col = "red") + 
  stat_function(fun = dnorm, args = fun_args, col = "blue")
```

There are three parameters that you may be tempted to adjust in a density plot:

* bw - the smoothing bandwidth to be used, see ?density for details
* adjust - adjustment of the bandwidth, see density for details
* kernel - kernel used for density estimation, defined as
* "g" = gaussian
* "r" = rectangular
* "t" = triangular
* "e" = epanechnikov
* "b" = biweight
* "c" = cosine
* "o" = optcosine

In this exercise you'll use a dataset containing only four points, small_data, so that you can see how these three arguments affect the shape of the density plot.

The vector get_bw contains the bandwidth that is used by default in geom_density(). p is a basic plotting object that you can start from.

```{r}
x <- c(-3.5, 0.0, 0.5, 6.0)
small_data <- data.frame(x)

# Get the bandwith
get_bw <- density(small_data$x)$bw

# Basic plotting object
p <- ggplot(small_data, aes(x = x)) +
  geom_rug() +
  coord_cartesian(ylim = c(0,0.5))

# Create three plots
p + geom_density()
p + geom_density(adjust = 0.25)
p + geom_density(bw = 0.25 * get_bw)

# Create two plots
p + geom_density(kernel = "r")
p + geom_density(kernel = "e")
```

Notice how the curve contained more features and their individual heights were increased as the bandwidth decreased.

## Multiple groups or variables

Groups = levels of a factor variable.  A drawback of showing a box plot per group, is that you don't have any indication of the sample size, n, in each group, that went into making the plot. One way of dealing with this is to use a variable width for the box, which reflects differences in n.

```{r}
# Diamond box plot sized according to the number of observations
ggplot(diamonds, aes(x = cut, y = price, col = color)) +
  geom_boxplot(varwidth = TRUE) +
  facet_grid(. ~ color)
  
```

This helps us see the differences in group size, but unfortunately there is no legend, so it's not a complete solution.

The next section of code combines multiple density plots. Here, you'll combine just two distributions, a normal and a bimodal.

The first thing to remember is that you can consider values as two separate variables, like in the test_data data frame, or as a single continuous variable with their ID as a separate categorical variable, like in the test_data2 data frame. test_data2 is more convenient for combining and comparing multiple distributions.

A small number of overlapping density plots are a fantastic way of comparing distinct distributions, for example, when descriptive statistics only (mean and sd) don't represent the data well enough.

```{r}
# Load the data
test_data  <- ch1_test_data
test_data2 <- ch1_test_data2

# check the structure
str(test_data)
str(test_data2)

# Plot with test_data
ggplot(test_data, aes(x = norm)) +
  geom_rug() +
  geom_density()

# Plot two distributions with test_data2
ggplot(test_data2, aes(x = value, fill = dist, col = dist)) +
  geom_rug(alpha = 0.6) +
  geom_density(alpha = 0.6)
```
When you looked at multiple box plots, you compared the total sleep time of various mammals, sorted according to their eating habits. One thing you noted is that for insectivores, box plots didn't really make sense, since there were only 5 observations to begin with. You decided that you could nonetheless use the width of a box plot to show the difference in sample size between the groups. Here, you'll see a similar thing with density plots.

A cleaned up version of the mammalian dataset is first loaded as mammals.

```{r}
mammals <- readRDS("D:/CloudStation/Documents/2017/RData/mammals.RDS")

# Individual densities
ggplot(mammals[mammals$vore == "Insectivore", ], aes(x = sleep_total, fill = vore)) +
  geom_density(col = NA, alpha = 0.35) +
  scale_x_continuous(limits = c(0, 24)) +
  coord_cartesian(ylim = c(0, 0.3))

# With faceting
ggplot(mammals, aes(x = sleep_total, fill = vore)) +
  geom_density(col = NA, alpha = 0.35) +
  scale_x_continuous(limits = c(0, 24)) +
  coord_cartesian(ylim = c(0, 0.3)) +
  facet_wrap( ~ vore, nrow = 2)

# Note that by default, the x ranges fill the scale
ggplot(mammals, aes(x = sleep_total, fill = vore)) +
  geom_density(col = NA, alpha = 0.35) +
  scale_x_continuous(limits = c(0, 24)) +
  coord_cartesian(ylim = c(0, 0.3))

# Trim each density plot individually
ggplot(mammals, aes(x = sleep_total, fill = vore)) +
  geom_density(col = NA, alpha = 0.35, trim = TRUE) +
  scale_x_continuous(limits=c(0,24)) +
  coord_cartesian(ylim = c(0, 0.3))
```
When plotting a single variable, the density plots (and their bandwidths) are calculated separate for each variable (see the first plot).

However, when you compare several variables (such as eating habits) it's useful to see the density of each subset in relation to the whole data set. This holds true for multiple density plots as well as for violin plots.

For this, we need to weight the density plots so that they're relative to each other. Each density plot is adjusted according to what proportion of the total data set each sub-group represents. We calculated this using the dplyr commands in the third section.

After executing the commnads, it will have the variable n, which we'll use for weighting.  To generate the weighted density plot, use aes() to map n onto the weight aesthetic inside geom_density().  The results will be more detailed and accurate.

```{r}
# Unweighted density plot from before
ggplot(mammals, aes(x = sleep_total, fill = vore)) +
  geom_density(col = NA, alpha = 0.35) +
  scale_x_continuous(limits = c(0, 24)) +
  coord_cartesian(ylim = c(0, 0.3))

# Unweighted violin plot
ggplot(mammals, aes(x = vore, y = sleep_total, fill = vore)) +
  geom_violin()

# Calculate weighting measure
library(dplyr)
mammals2 <- mammals %>%
  group_by(vore) %>%
  mutate(n = n() / nrow(mammals)) -> mammals

# Weighted density plot
ggplot(mammals2, aes(x = sleep_total, fill = vore)) +
  geom_density(aes(weight = n), col = NA, alpha = 0.35) +
  scale_x_continuous(limits = c(0, 24)) +
  coord_cartesian(ylim = c(0, 0.3))

# Weighted violin plot
ggplot(mammals2, aes(x = vore, y = sleep_total, fill = vore)) +
  geom_violin(aes(weight = n), col = NA)
```

We can also create 2D density plots.  You can consider two orthogonal density plots in the form of a 2D density plot. Just like with a 1D density plot, you can adjust the bandwidth of both axes independently.

The data is stored in the faithful data frame, available in the datasets package. The object p contains the base definitions of a plot.  Think about the message in your scatter plots, sometimes clusters of high-density are more intersting than linear models.

```{r}
# Base layers
p <- ggplot(faithful, aes(x = waiting, y = eruptions)) +
  scale_y_continuous(limits = c(1, 5.5), expand = c(0, 0)) +
  scale_x_continuous(limits = c(40, 100), expand = c(0, 0)) +
  coord_fixed(60 / 4.5)

# 1 - Use geom_density_2d()
p + geom_density_2d()

# 2 - Use stat_density_2d() with arguments
p + stat_density_2d(aes(col = ..level..), h = c(5, 0.5))
```

Next we use the viridis package. This package contains multi-hue color palettes suitable for continuous variables.

The advantage of these scales is that instead of providing an even color gradient for a continuous scale, they highlight the highest values by using an uneven color gradient on purpose. The high values are lighter colors (yellow versus blue), so they stand out more.

```{r}
# Load in the viridis package
library(viridis)

# Add viridis color scale
ggplot(faithful, aes(x = waiting, y = eruptions)) +
  scale_y_continuous(limits = c(1, 5.5), expand = c(0,0)) +
  scale_x_continuous(limits = c(40, 100), expand = c(0,0)) +
  coord_fixed(60/4.5) +
  stat_density_2d(geom = "tile", aes(fill = ..density..), h=c(5,.5), contour = FALSE) + 
  scale_fill_viridis()
```

## Plots for Specific Data 1

In this and the next section, we cover a lot of different plot types which are meant for specific use cases.  They give an overview of the different plots so you can remember them when you have appropriate data, even if that isn't very often.  

### Big data

This could be in the form of many observations, or it could be many variables (multidimensional data) or some combination thereof.  

In the case of many observations (big n) there are some techniques we can use:

* Reducing overplotting
* Reducing the amount of information that is plotted
* Aggregating data

In the case of multidimensional or hign  data (big p) there are other techniques we can use:

* Data Reduction methods (e.g. PCA)
* Use facets in plots
* Use a SPLOM - Scatter PLOt Matrix, a nice example is the chart.Correlation fun in PerformanceAnalytics package
* Use a parallel coordinate plot, which can be used for continous and discreet data, inluding those on different scales

First we will look at SPLOMs. Base R features two useful quick-and-dirty pairs plots functions. They both only take continuous variables.  There are two datasets - iris dataset and mtcars_fact, a version of mtcars where categorical variables have been converted into actual factor columns.

*S*catter *PLO*t *M*atrices.

```{r, warning = FALSE, message = FALSE}
# Convert nums to factors where needed for mtcars
mtcars_fact <- mtcars
mtcars_fact[c(2, 8:11)] <- lapply(mtcars_fact[c(2, 8:11)], as.factor)

# pairs
pairs(iris[1:4])

# chart.Correlation
library(PerformanceAnalytics)
chart.Correlation(iris[1:4])

# ggpairs
library(GGally)
ggpairs(mtcars_fact[1:3])
```

Instead of using an off-the-shelf correlation matrix function, you can of course create your own plot. For starters, a correlation matrix can be calculated using, for example, cor(dataframe) (if all variables are numerical). Before you can use your data frame to create your own correlation matrix plot, you'll need to get it in the right format.

There is a definition of cor_list(), a function that re-formats the data frame x. Here, L is used to add the points to the lower triangle of the matrix, and M is used to add the numerical values as text to the upper triangle of the matrix. With reshape2::melt(), the correlation matrices L and M are each converted into a three-column data frame: the x and y axes of the correlation matrix make up the first two columns and the corresponding correlation coefficient makes up the third column. These become the new variables "points" and "labels", which can be mapped onto the size aesthetic for the points in the lower triangle and onto the label aesthetic for the text in the upper triangle, respectively. Their values will be the same, but their positions on the plot will be symmetrical about the diagonal! Merging L and M, you have everything you need.

We use reshape2 instead of tidyr is that reshape2::melt() can handle a matrix, whereas tidyr::gather() requires a data frame. At this point you just need to understand how to use the output from cor_list().

First use dplyr to execute this function on the continuous variables in the iris data frame (the first four columns), but separately for each species. Next, you'll actually plot the resulting data frame with ggplot2 functions.

```{r}
library(reshape2)

cor_list <- function(x) {
  L <- M <- cor(x)
  
  M[lower.tri(M, diag = TRUE)] <- NA
  M <- melt(M)
  names(M)[3] <- "points"
  
  L[upper.tri(L, diag = TRUE)] <- NA
  L <- melt(L)
  names(L)[3] <- "labels"
  
  merge(M, L)
}

# Calculate xx with cor_list
library(dplyr)
xx <- iris %>%
  group_by(Species) %>%
  do(cor_list(.[1:4])) 

# Finish the plot
ggplot(xx, aes(x = Var1, y = Var2)) +
  geom_point(
    aes(col = points, size = abs(points)), 
    shape = 16
    ) +
  geom_text(
    aes(col = labels, size = abs(labels), label = round(labels, 2))
    ) +
  scale_size(range = c(0, 6)) +
  scale_color_gradient2("r", limits = c(-1, 1)) +
  scale_y_discrete("", limits = rev(levels(xx$Var1))) +
  scale_x_discrete("") +
  guides(size = FALSE) +
  geom_abline(slope = -1, intercept = nlevels(xx$Var1) + 1) +
  coord_fixed() +
  facet_grid(. ~ Species) +
  theme(axis.text.y = element_text(angle = 45, hjust = 1),
        axis.text.x = element_text(angle = 45, hjust = 1),
        strip.background = element_blank())
```

### Ternary Plots

Sometimes called triangle plots, these are useful for 'compositional trivariate data' - three variables that add up to 100%.  A classic example is soil - sand, silt and clay.  One typically option might be a stacked bar, but a ternary plot is an alternative.    

The data frame for the African Soil Profiles Database can be found in the GSIF package. It contains three columns: Sand, Silt and Clay. A smaller version, containing only 50 observations is stored in africa_sample.

In the first course we mentioned that in the data layer, the structure of the data should reflect how you wish to plot it. For a ternary plot, you need to have three separate variables, for example, Sand, Silt and Clay in africa. However, for a proportional/stacked bar plot, you just need two. The type should be defined as three levels within a single factor variable. That is, you want tidy data.

It's also useful to maintain the site IDs as a variable within the data frame, currently, they are stored at row names, which is poor style and not useful.
```{r}
#load the data
load("D:/CloudStation/Documents/2017/RData/africa.RData")
africa_sample <- africa[sample(1:nrow(africa), 50, replace=FALSE),]

# Explore africa
str(africa)
str(africa_sample)

# Add an ID column from the row.names
africa_sample$ID <- row.names(africa_sample)

# Gather africa_sample
library(tidyr)
africa_sample_tidy <- gather(africa_sample, key, value, -ID)
head(africa_sample_tidy)

# Finish the ggplot command
ggplot(africa_sample_tidy, aes(x = factor(ID), y = value, fill = key)) +
  geom_col() +
  coord_flip()

```

Here we wil use ternary plots.  For this you'll use the ggtern package, which provides the ggtern() function.

In contrast to what you just saw in africa_small_tidy, the three soil properties, Sand, Silt and Clay, are not going to be located in a single variable. The distinction between wide and tidy format is here in action. Sometimes you need to rearrange your data for the desired plot type.

Here, we use the complete dataset, africa, containing three separate variables for the measures of interest: that format is perfect for a ternary plot.  For this many points, other geoms might be more appropriate.

```{r}
# Load ggtern
library(ggtern)

# Build ternary plot
ggtern(africa, aes(x = Sand, y = Silt, z = Clay)) +
  geom_point(shape = 16, alpha = 0.2)
```

Ternary plots have been around for a while in R; you could achieve the same thing with the vcd package authored by Michael Friendly. If you just need a quick and dirty ternary plot, that may suit you just fine. However, since ggtern is built on ggplot2, you can take advantage of all the tools available therein.

ggtern is authored by Nicholas Hamilton, more information can be found on his package website: www.ggtern.com.

```{r}
# ggtern and ggplot2 are loaded
# Original plot:
ggtern(africa, aes(x = Sand, y = Silt, z = Clay)) +
  geom_point(shape = 16, alpha = 0.2)

# Plot 1 - contour plot
ggtern(africa, aes(x = Sand, y = Silt, z = Clay)) +
  geom_density_tern()
  
# Plot 2 - filled density plot
ggtern(africa, aes(x = Sand, y = Silt, z = Clay)) +
  stat_density_tern(geom = 'polygon', aes(fill = ..level.., alpha = ..level..)) +
  guides(fill = FALSE)
```

### Network Plots

Sometimes called relationship data.  In a network diagram each observation is a vertex, the location of the vertices can alter our perception of the relationship.  The connection between the observations is called an edge.  Network data may be stored in a variety of ways.

For this example, you'll use an undirected network of romantic relationships in the TV show Mad Men: geomnet::madmen.

```{r}
# Load geomnet & examine structure of madmen
library(geomnet)
str(madmen)

# Merge edges and vertices
mmnet <- merge(madmen$edges, madmen$vertices,
               by.x = "Name1", by.y = "label",
               all = TRUE)

# Examine structure of mmnet
str(mmnet)

```

Now the data is re-structured we can create our plot.

```{r}
# Merge edges and vertices
mmnet <- merge(madmen$edges, madmen$vertices,
               by.x = "Name1", by.y = "label",
               all = TRUE)

# Finish the ggplot command
ggplot(data = mmnet, aes(from_id = Name1, to_id = Name2)) +
  geom_net(
    aes(col = Gender),
    size = 6,
    linewidth = 1,
    labelon = T,
    fontsize = 3,
    labelcolour = "black")
```

Next we can clean it up, since this is in the ggplot2 framework, you can manually adjust the scales like you have always done.

```{r}
# Merge edges and vertices
mmnet <- merge(madmen$edges, madmen$vertices,
               by.x = "Name1", by.y = "label",
               all = TRUE)

# Tweak the network plot
ggplot(data = mmnet, aes(from_id = Name1, to_id = Name2)) +
  geom_net(
        aes(col = Gender),
        size = 6,
        linewidth = 1,
        labelon = TRUE,
        fontsize = 3,
        labelcolour = "black",
        directed = T) +
  scale_color_manual(values = c("#FF69B4", "#0099ff")) +
  xlim(c(-0.05, 1.05)) +
  theme(legend.key = element_blank())
```

### Diagnostic Plots

Such plots often use diagnostics from other outputs or models, for instance how well as OLS fits the data. R has several plotting methods for specific objects. For example using plot() on the results of an lm() call results in four plots that give you insight into how well the assigned model fits the data.

The ggfortify package is an all-purpose plot converter between base graphics and ggplot2 grid graphics.

```{r}
# Create linear model: res
res <- lm(Volume ~ Girth, data = trees)

# Plot res
plot(res)

# Import ggfortify and use autoplot()
library(ggfortify)
autoplot(res, ncol = 2)
```

Time series objects (class mts or ts) also have their own methods for plot(). The time-series and multiple time-series class objets are flexible and common formats.  ggfortify can also take advantage of this functionality.

Here we will look at the variable Canada (it comes from the vars package): an mts class object with four series: prod is a measure of labour productivity, e is employment, U is the unemployment rate, and rw the real wage. They are each plotted as separate series by default.

```{r}
library(vars)
data(Canada)

# Inspect structure of Canada
str(Canada)

# Call plot() on Canada
plot(Canada)

# Call autoplot() on Canada
autoplot(Canada)
```

Distance matrices (class dist) contain the measured distance between all pair-wise combinations of many points. For this xample, the eurodist dataset contains the distances between major European cities. dist objects lend themselves well to autoplot().

The cmdscale() function from the stats package performs Classical Multi-Dimensional Scaling and returns point coodinates as a matrix. Although autoplot() will work on this object, it will produce a heatmap, and not a scatter plot. However, if either eig = TRUE, add = TRUE or x.ret = TRUE is specified, cmdscale() will return a list instead of matrix. In these cases, the list method for autoplot() in the ggfortify package can deal with the output. Specifics on multi-dimensional scaling is beyond the scope of this course, however details on the method and these arguments can be found in the help pages ?cmdscale.

```{r}

# Autoplot + ggplot2 tweaking
autoplot(eurodist) + 
  coord_fixed()

# Autoplot of MDS
autoplot(cmdscale(eurodist, eig = TRUE), 
         label = TRUE, 
         label.size = 3, 
         size = 0)
```

### K means clustering

ggfortify also supports stats::kmeans class objects. You must explicitly pass the original data to the autoplot function via the data argument, since kmeans objects don't contain the original data. The result will be automatically colored according to cluster.

Here we use the iris dataset and just look at K-means clustering, although this works on many clustering methods, including cluster::clara(), cluster::fanny(), cluster::pam() and stats::prcomp(). 

```{r}
# Perform clustering
iris_k <- kmeans(iris[-5], 3)

# Autoplot: color according to cluster
autoplot(iris_k, data = iris, frame = TRUE)

# Autoplot: above, plus shape according to species
autoplot(iris_k, data = iris, frame = TRUE, shape = 'Species')
```

## Plots for Specific Data 2

R can be used as a GIS, here we will look at two main types; choropleths and cartographic maps.  

The easiest way to obtain map polygons is through the maps package. Unfortunately there are only a few locations available, but if your region of interest is included they are extremely convenient.

The available maps of political boundaries are:

* Global: world, world2
* Country: france, italy, nz, usa
* USA: county, state

The maps can be accessed via ggplot2::map_data(), which converts the map into a data frame containing the variables long and lat. To draw the map, you need to use geom_polygon() which will connect the points of latitude and longitude for you.

```{r}
library(ggmap)
usa <- map_data("usa")
str(usa)

# Build the map - you can also define your own CRS in the coord_map() layer.
ggplot(usa, aes(x = long, y = lat, group = group)) +
  geom_polygon() +
  coord_map() +
  theme_nothing()
```

Now that you have some polygons, there are a number of things you can do. Here you'll add some data points, namely the location of US cities with a population over 100,000 (population estimation as of 2015). Since you're only looking at the continental US, Honolulu, Hawaii and Anchorage, Alaska are not included.

The data is stored in the cities data frame. You'll begin by drawing points of varying sizes, relative to the estimated population. An alternative is to use color instead of size, and in this case a nice trick is to order the data frame, so that the largest cities are drawn on top of the smaller cities. This is so that they will stand out against the background, which is particularly effective when using the viridis color palette.

```{r}
library(ggthemes)
# load the data
cities <- read.delim("D:/CloudStation/Documents/2017/RData/US_Cities.txt")

# Plot 1
ggplot(usa, aes(x = long, y = lat, group = group)) +
  geom_polygon() +
  geom_point(data = cities, aes(group = State, size = Pop_est),
             col = "red", shape = 16, alpha = 0.6) +
  coord_map() +
  theme_map()

# Arrange cities
cities_arr <- arrange(cities, Pop_est)

# Adapt plot 1
ggplot(usa, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "grey90") +
  geom_point(data = cities_arr, aes(group = State, col = Pop_est),
             size = 2, shape = 16, alpha = 0.6) +
  coord_map() +
  theme_map() + 
  scale_color_viridis()

```

New York appears as a bright yellow anomly in a sea of darker points. If you didn't set the order of the data, this point would have been obscured. You can also see LA, Chicago and Houston as lighter blue points.

To make a choropleth (a map in which areas are shaded according to some measure) you'll need information on state boundaries, which you can find in the maps package. Once the map information is converted into a data frame, you can merge this with another data frame containing some quantitative information, like the estimated population, and use that variable in our aesthetic mappings.

```{r}
# load the data
pop <- read.delim("D:/CloudStation/Documents/2017/RData/US_States.txt")

# Use map_data() to create state
state <- map_data("state")

# Map of states
ggplot(state, aes(x = long, y = lat, fill = region, group = group)) +
  geom_polygon(col = "white") +
  coord_map() +
  theme_nothing() # removes the not useful legend

# Merge state and pop: state2
state2 <- merge(state, pop)

# Map of states with populations
ggplot(state2, aes(x = long, y = lat, fill = Pop_est, group = group)) +
  geom_polygon(col = "white") +
  coord_map() +
  theme_map() # includes the legend
```

Although the built-in maps from the maps package are very convenient, using shapefiles is a more flexible way of accessing geographic and political boundaries.

Shapefiles can be used to describe points, polylines or polygons - here you'll focus on polygons for drawing maps.

A single shapefile actually consists of several files, each describing a specific aspect of the overall geometry. The three core file types are:

* .shp: the shape, the feature geometry itself.
* .shx: the shape index, a positional index.
* .dbf: the attribute, attributes for each shape arranged in columns.

The prefix name of these files must be consistent and they must be kept in the same directory. The files you'll use are for Germany, and all begin with DEU. The suffix specifies the level of organization:

DEU_adm0 is the administrative (political) boundaries of the entire country (like the usa object before)
DEU_adm1 is the administrative boundaries for each of the 16 states (like the state object before)
Let's start by importing the shapefile and creating a map of Germany.

```{r}
library(rgdal)
# specify the location of the shapefiles
shapes <- "D:/CloudStation/Documents/2017/RData/Germany"

# Import shape information: germany
germany <- readOGR(dsn = shapes, layer = "DEU_adm1")

# fortify germany: bundes - changes it to a data frame
bundes <- fortify(germany)

# Plot map of germany
ggplot(bundes, aes(x = long, y = lat, group = group)) +
    geom_polygon(fill = "blue", col = "white") +
    coord_map() +
    theme_nothing()
```

Now that you have the shape data a neatly formatted data frame called bundes, you can easily merge it with state-level data. 

If you check out bundes, you'll see that you've lost the state names, so merging isn't going to work out of box so we will need to resolve that.  
```{r}
# Load the unemployment data
unemp <- read.delim("D:/CloudStation/Documents/2017/RData/germany_unemployment.txt")

# re-add state names to bundes
bundes$state <- factor(as.numeric(bundes$id))
levels(bundes$state) <- germany$NAME_1

# Merge bundes and unemp: bundes_unemp
bundes_unemp <- merge(bundes, unemp)

# Update the ggplot call
ggplot(bundes_unemp, aes(x = long, y = lat, group = group, fill = unemployment)) +
  geom_polygon() +
  coord_map() +
  theme_map()
```

There are only 14 states shown. The reason is that Berlin and Bremen are city-states located within the shape definitions of other states. You can solve this by reording the data (like you did before), or by adding two specific layers as shown below.

```{r}
# Update the ggplot call
ggplot(bundes_unemp, aes(x = long, y = lat, group = group, fill = unemployment)) +
  geom_polygon() +
  geom_polygon(data = subset(bundes_unemp, id == "Berlin")) +
  geom_polygon(data = subset(bundes_unemp, id == "Bremen")) +
  coord_map() +
  theme_map()

```

### Cartographic Maps

Whereas before we were interested in plotting variables, in cartographic maps we are interested in plotting some type of image, be it topographical, altitude, aerial imagery or some combination.  We can use the ggmpa library to create some standard map types. We can also add features and labels to our map, define the bounding boxes and make calls to things like the Google Api to geocode a list of places.  

ggmap provides the get_map() function to access google maps. Only one argument, location is essential. Other than that, there are a bunch of arguments you can set, such as zoom, maptype (how do you want your map to look?) and source (where to get your data from?).

There are several default map types you can use for cartographic maps. For example, "toner" is a black and white theme, but you can also ask for pure satellite images or a hybrid image, showing the major roads and cities.

```{r, message = FALSE}
# Make sure ggmap is loaded
library(ggmap)

# Create london_map_13 with get_map
london_map_13 <- get_map("London, England", zoom = 13)

# Create the map of london
ggmap(london_map_13)

# Experiment with get_map() and use ggmap() to plot it!
london_map_13 <- get_map("London, England", zoom = 13, source = "stamen", maptype = "toner")
# Create the map of london
ggmap(london_map_13)
```

```{r}
# List our sites of interest
london_sites <- c("Tower of London, London", "Buckingham Palace, London", "Tower Bridge, London", "Westminster Abbey, London", "Queen Elizabeth Olympic Park, London")

# Use geocode() to create xx
xx <- geocode(london_sites)

# Add a location column to xx
xx$location <- sub(", London", "", london_sites)

# Get map data
london_ton_13 <- get_map(location = "London, England", zoom = 13,
                         source = "stamen", maptype = "toner")

# Add a geom_points layer
ggmap(london_ton_13) + 
  geom_point(data = xx, aes(col = location), size = 6)


```

Notice the warning message; one location is missing: Queen Elizabeth Olympic Park.  That's because your map data was fetched before you knew what you wanted to plot.

Instead of defining your cartographic map based on a general location, you can define a bounding box around specific coordinates. You have these coordinates already since you used geocode() to find the latitude and longitude of the sites in London. Everything you need to build the bounding box is inside xx.

```{r, message=FALSE}
# Build xx
xx <- geocode(london_sites)
xx$location <- sub(", London", "", london_sites)
xx$location[5] <- "Queen Elizabeth\nOlympic Park"

# Create bounding box: bbox, f is the buffer
bbox <- make_bbox(lon = xx$lon, lat = xx$lat, f = 0.3)

# Re-run get_map to use bbox
london_ton_13 <- get_map(location = bbox, zoom = 13,
                         source = "stamen", maptype = "toner")

# Map from previous exercise
ggmap(london_ton_13) +
  geom_point(data = xx, aes(col = location), size = 6)

# New map with labels
ggmap(london_ton_13) +
  geom_label(data = xx, aes(label = location), size = 4, 
             fontface = "bold", fill = "grey90", 
             col = "#E41A1C")
```

Instead of plotting points on a cartographic map, you can also draw polygons, similar to what you saw earlier in this chapter.

The polygons come from the shapefile for the German states that you used previously. You'll use the data frame version, bundes, because it's more flexible. 

```{r, message = FALSE}
# Get the map data of "Germany"
germany_06 <- get_map(location = "Germany", zoom = 6)

# Plot map and polygon on top:
ggmap(germany_06) +
  geom_polygon(data = bundes,
               aes(x = long, y = lat, group = group),
               fill = NA, col = "red") +
  coord_map()
```

### Animations

This is good for exploring temporal data or as an explantory tool.  We could use a for loop then use an external piece of software to make an animated gif, but we can do this inside R using the animation package, and the gganimate which acts as a wrapper when using ggplot arguments.  To do this, we are going to use Hans Rosling's Gapminder data.  These images can then be embedded in a website, report, or in a viewer with some controls.  

Animations are particularly useful for temporal or geospatial data, and they are surprisingly easy to make! Here, you simply loop over the time variable in your dataset, composing a new plot for each subset in the data. These individual images are then cataloged in an animated GIF file.

To show this you'll use a great animated population pyramid that was presented on the [Revolutions blog](http://blog.revolutionanalytics.com/2016/02/japans-ageing-population-animated-with-r.html). There are many more adjustments you could have made to the plot, but we'll just make a barebones version here.

```{r}
library(animation)

# Load the population data
japan <- read.delim("D:/CloudStation/Documents/2017/RData/japanPOP.txt")

# Inspect structure of japan
str(japan)

# Finish the code inside saveGIF
saveGIF({

  # Loop through all time points
  for (i in unique(japan$time)) {

    # Subset japan: data
    data <- subset(japan, time == i)

    # Finish the ggplot command
    p <- ggplot(data, aes(x = AGE, y = POP, fill = SEX, width = 1)) +
      coord_flip() +
      geom_bar(data = data[data$SEX == "Female",], stat = "identity") +
      geom_bar(data = data[data$SEX == "Male",], stat = "identity") +
      ggtitle(i)

    print(p)

  }

}, movie.name = "pyramid.gif", interval = 0.1)

```

Another method of making animations is to use the gganimate package. This provides the gg_animate() function, which is basically a convenient wrapper for the functions in the animation package, so it's pretty useful for straight-forward plots. gg_animate() provides a new aesthetic argument, frame, that allows you to side-step having to establish a for loop for each time point, like you did in the previous example.

Here you'll see another example of making an animation, to cycle through years and look at changing linear models. The dataset is the Vocab data frame from the car package.

```{r}
library(gganimate)
library(car)

# Update the static plot
p <- ggplot(Vocab, aes(x = education, y = vocabulary,
                       color = year, group = year,
                       frame = year, cumulative = TRUE)) +
  stat_smooth(method = "lm", se = FALSE, size = 3)

# Call gganimate on p
"vocab.gif" <- gganimate(p, interval = 1.0)
```

## ggplot2 Internals

We will look at how graphics work not just in ggplot2 but also how they work in R.  There are two plotting systems in R, base graphics and grid graphics.  Grid graphics was developed to overcome some of the problems in base graphic, ggplot2 is built on top of grid graphics, which is more like a set of low level elements that have to be assembled together.  There are two main parts

* Create graphic outputs
* Layer and composition outputs with viewports - a viewport is a window on to which we draw a graphics output

With grid graphics you are also able to save your item and then later edit or customise that item.  

To get familiar with grid graphics, you'll begin with using some grid. functions. 

```{r}
library(grid)

#Sample viewport
grid.show.viewport(viewport(x=0.6, y=0.6,
                   w=unit(1, "inches"), h=unit(1, "inches")))

# Draw rectangle
grid.rect(gp = gpar(fill = "grey90"))


# Write text in null viewport
grid.text("null viewport")

# Draw a line
grid.lines(x = c(0, 0.75), y = c(0.25, 1),
          gp = gpar(lty = 2, col = "red"))
```

Or we can stack viewports in top of each other.

```{r}
# Populate null viewport
grid.rect(gp = gpar(fill = "grey90"))
grid.text("null viewport")
grid.lines(x = c(0,0.75), y = c(0.25, 1),
           gp = gpar(lty = 2, col = "red"))

# Create new viewport: vp
vp <- viewport(x = 0.5, y = 0.5, width = 0.5, height = 0.5, just = "center")

# Push vp
pushViewport(vp)

# Populate new viewport with rectangle
grid.rect(gp = gpar(fill = "blue"))
```

Using the viewports, you can create plots, manipulating the space as needed.

Here we establish a grid viewport and in the following exercise you'll populate it with values.

```{r}
# 1 - Create plot viewport: pvp
mar <- c(5, 4, 2, 2)
pvp <- plotViewport(mar)

# 2 - Push pvp
pushViewport(pvp)

# 3 - Add rectangle
grid.rect(gp = gpar(fill = "grey80"))

# Create data viewport: dvp
dvp <- dataViewport(xData = mtcars$wt, yData = mtcars$mpg)

# 4 - Push dvp
pushViewport(dvp)

# Add two axes
grid.xaxis()
grid.yaxis()
```

Now you're ready to add the points and the appropriate labels.

```{r}
# Work from before
pushViewport(plotViewport(c(5, 4, 2, 2)))
grid.rect(gp = gpar())
pushViewport(dataViewport(xData = mtcars$wt, yData = mtcars$mpg))
grid.xaxis()
grid.yaxis()

# 1 - Add text to x axis 
grid.text("Weight", y = unit(-3, "lines"))

# 2 - Add text to y axis
grid.text("MPG", x = unit(-3, "lines"), rot = 90)

# 3 - Add points
grid.points(x = mtcars$wt, y = mtcars$mpg, pch = 16)
```

The great thing about grid, in comparison to base, is that you can name the different plot elements, so that you can access them and change them later on. You can do this with the grid.edit() function. 

```{r}
# Work from before
pushViewport(plotViewport(c(5, 4, 2, 2)))
grid.rect(gp = gpar())
pushViewport(dataViewport(xData = mtcars$wt, yData = mtcars$mpg))
grid.xaxis()
grid.yaxis()

# Work from before - add names
grid.text("Weight", y = unit(-3, "lines"), name = "xaxis")
grid.text("MPG", x = unit(-3, "lines"), rot = 90, name = "yaxis")
grid.points(x = mtcars$wt, y = mtcars$mpg, pch = 16, name = "datapoints")

# Edit "xaxis"
grid.edit("xaxis", label = "Weight (1000 lbs)")

# Edit "yaxis"
grid.edit("yaxis", label = "Miles/(US) gallon")

# Edit "datapoints"
grid.edit("datapoints", gp = gpar(col = "#C3212766", cex = 2))
```

### Grid and ggplot2

In addition to Graphic outputs, we can also produce Graphic Objects or 'grobs'. Using a standard ggplot plot, we can access the internal grobs using ggplotgrob() from the grid package.  We can use the dollar notation to retrieve the list of rows e.g. >g$grob if we only wish to view and individual plot we can use g$grob[[8]] or view it using grid.draw(g$grob[[8]]). 

Graphical Objects, aka Grobs, are the object form of these items and can be found in your ggplot2 plots. Let's take a look at how these grobs are stored in ggplot objects.

```{r}
# A simple plot p
p <- ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point()

# Create gtab with ggplotGrob()
gtab <- ggplotGrob(p)

# Print out gtab
gtab

# Extract the grobs from gtab: gtab
g <- gtab$grobs

# Draw only the legend
legend_index <- which(vapply(g, inherits, what = "gtable", logical(1)))
grid.draw(g[[legend_index]])
```

You can visualize the layout of a gTable object with gtable_show_layout(). In the layout plot, each segment is labelled with its position.

The legend, that you can access with g[[8]], is a gTable itself, so you can also show its layout. It's perfectly possible to update this layout by adding new graphical objects.

```{r}
# Code from before
p <- ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point()
gtab <- ggplotGrob(p)
g <- gtab$grobs
legend_index <- which(vapply(g, inherits, what = "gtable", logical(1)))
grid.draw(g[[legend_index]])

# 1 - Show layout of legend grob
gtable_show_layout(g[[legend_index]])

# Create text grob
my_text <- textGrob(label = "Motor Trend, 1974", gp = gpar(fontsize = 7, col = "gray25"))

# 2 - Use gtable_add_grob to modify original gtab
new_legend <- gtable_add_grob(gtab$grobs[[legend_index]], my_text, 3, 2)

# 3 - Update in gtab
gtab$grobs[[legend_index]] <- new_legend

# 4 - Draw gtab
grid.draw(gtab)
```

### ggplot Objects

We can access other objects such as the panel, which is a grobtree which can become complicated.  When you create a ggplot, there are 9 objects created that hold all the different parts of the plot.  Some objects contain functions rather than data, when you run a ggplot, it will go through those functions using the ggplot_build() command.  This object will only contain three items;

* data - a list of data frames, one for each layer
* panel - all information about the axis, such as limits and breaks
* plot - items which are not found in the original data, but are used in the plots, such as histograms and other visual elements

If we have a grid table (gtab) we can use the function grid.draw(gtab).

It is important to remeber that ggplot objects are basically just a named list that contains the information to make the actual plot.

```{r}
# Simple plot p
p <- ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point()

# Examine class() and names()
class(p)
names(p)

# Print the scales sub-list
p$scales$scales

# Update p
p <- p +
  scale_x_continuous("Length", limits = c(4, 8), expand = c(0, 0)) +
  scale_y_continuous("Width", limits = c(2, 4.5), expand = c(0, 0))

# Print the scales sub-list
p$scales$scales
```

This is pretty detailed, but the more we can access out objects the more custom our visualisations become.

```{r}
# Box plot of mtcars: p
p <- ggplot(mtcars, aes(x = factor(cyl), y = wt)) + geom_boxplot()

# Create pbuild
pbuild <- ggplot_build(p)

# a list of 3 elements
names(pbuild)

# Print out each element in pbuild
pbuild$data
pbuild$panel
pbuild$plot

# Create gtab from pbuild
gtab <- ggplot_gtable(pbuild)

# Draw gtab
grid.draw(gtab)
```

Many geoms are associated with underlying descriptive statistics which are calculated and then plotted. In these cases you actually don't have the actual values that were plotted. Of course, these values are stored under the hood and you can access them in the results from ggplot_build(). This can be particularly useful for box plots. For example, since there are many methods for calculating Q1 and Q3, if you calculate your IQR and outliers outside of ggplot2 you may end up using a different method and get different results. Sometimes you want to have exactly the values that were plotted.  Sometimes you want to get specific information that was used to create a plot.

```{r}
# Box plot of mtcars: p
p <- ggplot(mtcars, aes(x = factor(cyl), y = wt)) + geom_boxplot()

# Build pdata -  a list with one element, which is a data frame
pdata <- ggplot_build(p)$data

# confirm that the first element of the list is a data frame
class(pdata[[1]])

# Isolate this data frame
my_df <- pdata[[1]]

# The x labels - changes from simply a numeric group to represent the data, the # of cylinders instead
my_df$group <- c("4", "6", "8")

# Print out specific variables - which selects the first through the sixth and the 11th variable. 
# This contains everything you need to know: the 5 quartiles (Q0 up to Q4) and the outliers for each separate group
my_df[c(1:6, 11)]
```

### gridExtra

This allows us to manage multiple plotting objects - perhaps to avoid a large facet plot or to specfically arrange our plots in a layout, or make a mutiple page PDF of our plots.  We can use the plyr package and dlply or the apply family of functions to first create our list of plots. The result becomes a list, which you can then just reference as you would any other list[[n]] or list[["object_name""]].  We can use the grid.arrange function to arrange thse plots together, perhaps with just some of the list rather than all of it.  You also have the ability to then combine diferent types of objects together, like different plots, tables, different source data etc.  

You can use gird arrange for automated reports, websites/pages or dashboard type displays.  




