# Importing data - Part 2
***
Notes taken during/inspired by the Datacamp course 'Importing Data in R (Part 2)' by Filip Schouwenaars.

## Importing from Databases - 1

In a professional or commercial setting, you often deal with more complicated file structures and source systems that simple flat files.  Often the data is stored in a DBMS or Database Management System and SQL is the usual way of quering the DBMS.  As there can be slight differences, you are likely to need different packages, some include:

* **MySQL**: Use the RMySQL package
* **PostgresSQL**: Use the RPostgresSQL package
* **Oracle**: Use the ROracle (etc...)

Conventions are specified in the DBI - another R package, DBI is the interface and the other packages are the implentation.  Some of the packages will automaticlaly install the DBI package as well. To connect to a database we would so something like the following.

```{r}
# Load the DBI package
library(DBI)

# Edit dbConnect() call - the first part specifies how connections are map to the database
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "tweater", 
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                 port = 3306,
                 user = "student",
                 password = "datacamp")

# Build a vector of table names: tables
tables <- dbListTables(con)

# Display structure of tables
str(tables)

# Import the users table from tweater: users
users <- dbReadTable(con, "users")

# Print users
users

# Or we can import all tables using lapply
tables <- lapply(tables, dbReadTable, conn = con)

# Print out tables
tables

```

## SQL Queries Inside R

OFten you don't want an entire tabel from a database, but a selection from the table.  You can use SQL queries from inside R to extract only what you are interested in.  You can alternatively use subset on the imported table, but often it is easier to extract only what you need first, particularly when working with large databases.  The SQL goes inside e.g. dbGetQuery(con, "SQL QUERY").

```{r}
# Connect to the database
library(DBI)
con <- dbConnect(RMySQL::MySQL(),
                 dbname = "tweater",
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com",
                 port = 3306,
                 user = "student",
                 password = "datacamp")

# Import tweat_id column of comments where user_id is 1: elisabeth
elisabeth <- dbGetQuery(con, "SELECT tweat_id FROM comments WHERE user_id = 1")

# Print elisabeth
elisabeth


# Import post column of tweats where date is higher than '2015-09-21': latest
latest <- dbGetQuery(con, "SELECT post FROM tweats WHERE date > '2015-09-21'")

# Print latest
latest


# Create data frame specific using boolean
specific <- dbGetQuery(con, "SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4")

# Print specific
specific


# Create data frame short selecting two columns
short <- dbGetQuery(con, "SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5")

# Print short
short


# We can also join elements from different tables using the same id/key

dbGetQuery(con, "SELECT post, message
  FROM tweats INNER JOIN comments on tweats.id = tweat_id
    WHERE tweat_id = 77")

```

You've used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed:

* Sending the specified query with dbSendQuery();
* Fetching the result of executing the query on the database with dbFetch();
* Clearing the result with dbClearResult().

Let's not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query's result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch().

**It is important to close the connection to the database once complete using the dbDisconnect() function**

```{r}
# Send query to the database
res <- dbSendQuery(con, "SELECT * FROM comments WHERE user_id > 4")

# Use dbFetch() twice
dbFetch(res, n = 2)
dbFetch(res) # imports all

# Clear res
dbClearResult(res)

# Create the data frame  long_tweats
long_tweats <- dbGetQuery(con, "SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40")

# Print long_tweats
print(long_tweats)

# Disconnect from the database
dbDisconnect(con)

```

```{r}

```

## Web Data

HyperText Transfer Protocol (HTTP) is the 'language of the web' and consists of a set of rules about data exchange between computers.  If the file is a csv file, we can use functions like read.csv() and add in the url in quotations marks, read.csv will recognise this is a URL and will issue a HTTP GET command to download the file.  This will also work on https sites on newer versions of R.  We can also use the readr package and other packages.

```{r}
# Load the readr package
library(readr)

# Import the csv file: pools
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
pools <- read_csv(url_csv)

# Import the txt file: potatoes
url_delim <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt"
potatoes <- read_tsv(url_delim)

# Print pools and potatoes
pools
potatoes

# https URL to the swimming_pools csv file.
url_csv <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"

# Import the file using read.csv(): pools1
pools1 <- read.csv(url_csv)

str(pools1)
```

Some packages, like the readxl package, do not currently recognise urls.  However, we can use the donwload.file() or other command to download the file and then read it in locally.  This process can be much quicker that browsing the internet then downloading the file.

```{r, eval = FALSE}
library(readxl)

# Specification of url: url_xls
url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# Download file behind URL, name it local_latitude.xls
download.file(url_xls, destfile = "local_latitude.xls")

# Import the local .xls file with readxl: excel_readxl
excel_readxl <- read_excel("local_latitude.xls")

```

```{r}
# https URL to the wine RData file.
url_rdata <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData"

# Download the wine file to your working directory
download.file(url_rdata, destfile = "wine_local.RData")

# Load the wine data into your workspace using load()
load("wine_local.RData")

# Print out the summary of the wine data
summary(wine)
```

We can also read http content using the httr package.  This includes JSON formatted text, which httr will convert to a named list.

```{r}
# Load the httr package
library(httr)

# Get the url, save response to resp
url <- "http://www.example.com/"
resp <- GET(url)

# Print resp
resp

# Get the raw content of resp: raw_content
raw_content <- content(resp, as = "raw")

# Print the head of raw_content
head(raw_content)

# JSON formatted

# Get the url
url <- "http://www.omdbapi.com/?apikey=ff21610b&t=Annie+Hall&y=&plot=short&r=json"
resp <- GET(url)

# Print resp
resp

# Print content of resp as text
content(resp, as = "text")

# Print content of resp
content(resp)

```

## JSON and APIs

JSON is both easy for machines to parse and generate and is human readable.  APIs are programtical ways of getting data, consisting of a set of protocols to interact with some other system or database.  JSON can be useful since it is often well structured and can save time over, say, parsing a html page.  So for instance, you can use the OMDb API to return JSON formatted text about a movie, rather than parse an IMDB html page entry.  One package for handling JSON in R is jsonlite.

```{r}
library(jsonlite)

# wine_json is a JSON
wine_json <- '{"name":"Chateau Migraine", "year":1997, "alcohol_pct":12.4, "color":"red", "awarded":false}'

# Convert wine_json into a list: wine
wine <- fromJSON(wine_json)

# Print structure of wine
str(wine)

```

There are two types of JSON structures

* JSON objects - has key value pairs e.g. name:James, age:21 etc
* JSON arrays - a sequence of values, numbers, nulls e.g. 4, "a", 10, false, null etc

You can also nest JSON objects or arrays within each other.  Some examples are below.  YOu can also use the minify and prettify functions to convert a JSON string to a more compact of easier to read version.  Similar functions can also be used inside the toJSON() function e.g. toJSON(x, pretty = TRUE)

```{r}
# Challenge 1
json1 <- '[1, 2, 3, 4, 5, 6]'
fromJSON(json1)

# Challenge 2
json2 <- '{"a": [1, 2, 3], "b": [4, 5, 6]}'
fromJSON(json2)

# You can also convert data to JSON from other formats.  Here we take a csv and format it into a JSON array

# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"

# Import the .csv file located at url_csv
water <- read.csv(url_csv, stringsAsFactors = FALSE)

# Convert the data file according to the requirements
water_json <- toJSON(water)

# Print out water_json
water_json

# Convert mtcars to a pretty JSON: pretty_json
pretty_json <- toJSON(mtcars, pretty = TRUE)

# Print pretty_json
pretty_json

# Minify pretty_json: mini_json
mini_json <- minify(pretty_json)

# Print mini_json
mini_json


```

## Importing from other statistical software

Common software packages include SAS, STATA and SPSS.  Two packages useful for importing data from these packages are:

* **haven**: by Hadley Wickham and is under active development.  It aims to be more consistent, easier and faster than foreign.  It can read SAS, Stata and SPSS and will read in the file as an D dataframe.
* **foreign**: is an older package by the R Core Team.  Foreign support more data formats than haven including Weka and Systat

```{r, eval = FALSE}
# Load the haven package
library(haven)

# Import sales.sas7bdat: sales
sales <- read_sas("sales.sas7bdat")

# Display the structure of sales
str(sales)

# Import the data from the URL: sugar
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")

# Structure of sugar
str(sugar)

# Convert values in Date column to dates
sugar$Date <- as.Date(as_factor(sugar$Date))

# Structure of sugar again
str(sugar)

# Import person.sav: traits
traits <- read_sav("person.sav")

# Summarize traits
summary(traits)

# Print out a subset
subset(traits, Extroversion > 40 & Agreeableness > 40)

```

When using SPSS files, it is often the case that the variable labels are also imported, it is best to change these in to standard R factors.

```{r, eval = FALSE}
# Import SPSS data from the URL: work
work <- read_sav("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav")

# Display summary of work$GENDER
summary(work$GENDER)


# Convert work$GENDER to a factor
work$GENDER <- as_factor(work$GENDER)


# Display summary of work$GENDER again
summary(work$GENDER)
```

Foreign cannot use single SAS datafiles like haven, it works with SAS library files .xport.  Foreign tends to use dots in the function names rather than underscores in haven e.g. read.dta() vs read_dta().  Foreign does not provide consistency with it's functions i.e. read.dta() has different arguments than read.spss(), however foreign provides more control over the data importing, such as dealing with multiple types of missing data which are often present in survey data, more comprehensively than haven.  Although haven is still being developed. 

```{r, eval = FALSE}
# Load the foreign package
library(foreign)

# Specify the file path using file.path(): path
path <- file.path("worldbank", "edequality.dta")

# Create and print structure of edu_equal_1
edu_equal_1 <- read.dta(path)
str(edu_equal_1)

# Create and print structure of edu_equal_2
edu_equal_2 <- read.dta(path, convert.factors = FALSE)
str(edu_equal_2)

# Create and print structure of edu_equal_3
edu_equal_3 <- read.dta(path, convert.underscore = TRUE)
str(edu_equal_3)


# Import international.sav as a data frame: demo
demo <- read.spss("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/international.sav", to.data.frame = TRUE)

# Create boxplot of gdp variable of demo
boxplot(demo$gdp)

```

`r if (knitr:::is_html_output()) '# References {-}'`