<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Study notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Study notes taken from courses and self learning.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Study notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/James-SR/study-notes-master" />
  
  <meta property="og:description" content="Study notes taken from courses and self learning." />
  <meta name="github-repo" content="James-SR/study-notes-master" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Study notes" />
  
  <meta name="twitter:description" content="Study notes taken from courses and self learning." />
  

<meta name="author" content="James Solomon-Rounce">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="references.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Study Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html"><i class="fa fa-check"></i><b>1</b> Foundations of Inference</a><ul>
<li class="chapter" data-level="1.1" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#introduction-to-inference"><i class="fa fa-check"></i><b>1.1</b> Introduction to Inference</a></li>
<li class="chapter" data-level="1.2" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#home-ownership-by-gender"><i class="fa fa-check"></i><b>1.2</b> Home Ownership by Gender</a></li>
<li class="chapter" data-level="1.3" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#density-plots"><i class="fa fa-check"></i><b>1.3</b> Density Plots</a></li>
<li class="chapter" data-level="1.4" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#gender-discrimination-p-values"><i class="fa fa-check"></i><b>1.4</b> Gender Discrimination (p-values)</a></li>
<li class="chapter" data-level="1.5" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#opportunity-cost"><i class="fa fa-check"></i><b>1.5</b> Opportunity Cost</a></li>
<li class="chapter" data-level="1.6" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>1.6</b> Type I and Type II errors</a></li>
<li class="chapter" data-level="1.7" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#bootstrapping"><i class="fa fa-check"></i><b>1.7</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Study notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="foundations-of-inference" class="section level1">
<h1><span class="header-section-number">1</span> Foundations of Inference</h1>
<hr />
<p>Notes taken during/inspired by the Datacamp course ‘Foundations of Inference’ by Jo Hardin, collaborators; Nick Carchedi and Tom Jeon.</p>
<div id="introduction-to-inference" class="section level2">
<h2><span class="header-section-number">1.1</span> Introduction to Inference</h2>
<p>Classical statistical inference is the process of making claims about a population based on a sample of information. We are making an inference from a small group (sample) to a much larger one (population). We typically have:</p>
<ul>
<li><strong>Null Hypothesis <span class="math inline">\(H_{0}\)</span></strong>: What we are researching has no effect</li>
<li><strong>Alternate Hypothesis <span class="math inline">\(H_{A}\)</span></strong>: What we are researching does have an effect</li>
</ul>
<p>Under the null hypothesis, chance alone is responsible for the results. Under the alternate hypothesis, we reject the null hypothesis, by using statistical techniques that indicate that chance is not responsible for our findings. Hypothesis or statistical testing goes back over 300 years, with the first recorded use by John Arbuthnot:</p>
<table>
<caption><span id="tab:simple-table">Table 1.1: </span> Statistical Testing Applications</caption>
<thead>
<tr class="header">
<th align="center">Year</th>
<th align="left">Person</th>
<th align="left">Context</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1710</td>
<td align="left">Arbuthnot</td>
<td align="left">Sex ratio at birth</td>
</tr>
<tr class="even">
<td align="center">1767</td>
<td align="left">Michelle</td>
<td align="left">Distribution of stars</td>
</tr>
<tr class="odd">
<td align="center">1823</td>
<td align="left">Laplace</td>
<td align="left">Moon phase and barometric changes</td>
</tr>
<tr class="even">
<td align="center">1900</td>
<td align="left">K. Pearson</td>
<td align="left">Goodness of fit</td>
</tr>
<tr class="odd">
<td align="center">1908</td>
<td align="left">Gosset</td>
<td align="left">A single mean</td>
</tr>
</tbody>
</table>
<p>Source: <span class="citation">(Huberty <a href="#ref-Huberty1993">1993</a>, pg 318)</span></p>
<p>Contemporary statistical testing is a usually that of either Fisher or Neyman-Pearson approaches. Fisher tends to use a single hypothesis test and a p-value strength of evidence test, where as the Neyman-Pearson test will set a critical alpha value and compare the null hypothesis against an alternative hypothesis, rejecting the null if the test statistic is high enough <span class="citation">(Huberty <a href="#ref-Huberty1993">1993</a>, pg 318)</span>.</p>
<p>The course goes on to say that idea behind statistical inference is to understand samples from a hypothetical population, where the null hypothesis is true - there is no difference between two groups. We can do this by calculating one statistic - for instance the proportion (mean) of a test group who show a positive response when testing a new drug, compared to a placebo control group - for each repeated sample from a population, then work out the difference between these two groups means. With each sample, the mean will change, resulting in a changing difference for each sample.</p>
<p>We can then generate a distribution (histogram) of differences, assuming the null hypothesis - that there is no link between drug effectiveness between a test group and a control group - is true. <em>“Generating a distribution of the statistic from the null population gives information about whether the observed data are inconsistent with the null hypothesis”</em>. That is to say, by taking repeated samples and creating a distribution, we can then say whether our observed difference is consistent (within an acceptable value range due to chance) to the null hypothesis. The null samples consist of randomly shuffled drug effectiveness variables (permuted samples from the population), so that the samples don’t have any dependency between the two groups and effectiveness.</p>
</div>
<div id="home-ownership-by-gender" class="section level2">
<h2><span class="header-section-number">1.2</span> Home Ownership by Gender</h2>
<p>Data used in the exercises are from NHANES 2009-2012 With Adjusted Weighting.</p>
<p>This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC).</p>
<p>The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see <a href="http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf" class="uri">http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf</a>) that oversample certain subpopulations like racial minorities.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load packages</span>
<span class="kw">library</span>(<span class="st">&quot;dplyr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;ggplot2&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;NHANES&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;oilabs&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create bar plot for Home Ownership by Gender</span>
<span class="kw">ggplot</span>(NHANES, <span class="kw">aes</span>(<span class="dt">x =</span> Gender, <span class="dt">fill =</span> HomeOwn)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position =</span> <span class="st">&quot;fill&quot;</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Relative frequencies&quot;</span>)</code></pre></div>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Density for SleepHrsNight coloured by SleepTrouble, faceted by HealthGen</span>
<span class="kw">ggplot</span>(NHANES, <span class="kw">aes</span>(<span class="dt">x =</span> SleepHrsNight, <span class="dt">col =</span> SleepTrouble)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">adjust =</span> <span class="dv">2</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>HealthGen)</code></pre></div>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Next we want to create a selection for just our variables of interest - rent and owner occupation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Subset the data: homes</span>
homes &lt;-<span class="st"> </span>NHANES %&gt;%
<span class="st">  </span><span class="kw">select</span>(Gender, HomeOwn) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(HomeOwn %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Own&quot;</span>, <span class="st">&quot;Rent&quot;</span>))</code></pre></div>
<p>We build a distribution of differences assuming the null hypothesis - that there is no link between gender and home ownership - is true.</p>
<p>In this first step, we just do a single iteration, or permutation from the true values. The null (permuted) version here will create a randomly shuffled home ownership variable, so that the permuted version does not have any dependency between gender and homeownership. We effectively have the same gender split variables as per the original, with the same owned and rented proportions, but disassociated from the gender variable - just randomly shuffled.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Perform one permutation </span>
homes %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HomeOwn_perm =</span> <span class="kw">sample</span>(HomeOwn)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(Gender) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_own_perm =</span> <span class="kw">mean</span>(HomeOwn_perm ==<span class="st"> &quot;Own&quot;</span>), 
            <span class="dt">prop_own =</span> <span class="kw">mean</span>(HomeOwn ==<span class="st"> &quot;Own&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">diff_perm =</span> <span class="kw">diff</span>(prop_own),
            <span class="dt">diff_orig =</span> <span class="kw">diff</span>(prop_own_perm))</code></pre></div>
<pre><code>## # A tibble: 1 × 2
##      diff_perm     diff_orig
##          &lt;dbl&gt;         &lt;dbl&gt;
## 1 -0.007828723 -0.0008267323</code></pre>
<p>It is easier to see what is going on by breaking the results down iteratively. Our selected and filtered homes dataset looks like.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(homes)</code></pre></div>
<pre><code>## # A tibble: 6 × 2
##   Gender HomeOwn
##   &lt;fctr&gt;  &lt;fctr&gt;
## 1   male     Own
## 2   male     Own
## 3   male     Own
## 4   male     Own
## 5 female    Rent
## 6   male    Rent</code></pre>
<p>Next we shuffle this data, let’s call it homes 2. we can then check the total number of owns and rents are the same using the summary function, which confirms the data is just randomly shuffled.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">homes2 &lt;-<span class="st"> </span>homes %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HomeOwn_perm =</span> <span class="kw">sample</span>(HomeOwn)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(Gender)
<span class="kw">tail</span>(homes2)</code></pre></div>
<pre><code>## Source: local data frame [6 x 3]
## Groups: Gender [2]
## 
##   Gender HomeOwn HomeOwn_perm
##   &lt;fctr&gt;  &lt;fctr&gt;       &lt;fctr&gt;
## 1   male    Rent          Own
## 2   male    Rent          Own
## 3 female     Own         Rent
## 4   male     Own         Rent
## 5   male     Own         Rent
## 6   male     Own          Own</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(homes2)</code></pre></div>
<pre><code>##     Gender      HomeOwn     HomeOwn_perm
##  female:4890   Own  :6425   Own  :6425  
##  male  :4822   Rent :3287   Rent :3287  
##                Other:   0   Other:   0</code></pre>
<p>Then we calculate the mean value of home ownership (Own) across our original and shuffled (permutated) data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">homes3 &lt;-<span class="st"> </span>homes2 %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_own_perm =</span> <span class="kw">mean</span>(HomeOwn_perm ==<span class="st"> &quot;Own&quot;</span>), 
             <span class="dt">prop_own =</span> <span class="kw">mean</span>(HomeOwn ==<span class="st"> &quot;Own&quot;</span>))
homes3</code></pre></div>
<pre><code>## # A tibble: 2 × 3
##   Gender prop_own_perm  prop_own
##   &lt;fctr&gt;         &lt;dbl&gt;     &lt;dbl&gt;
## 1 female     0.6621677 0.6654397
## 2   male     0.6609291 0.6576109</code></pre>
<p>FFinally we calculate the differences in ownership - note that the difference for the permuted value here may be different from the full code above, as it a new random permutation and we have used the set.seed() function which would create an identical permutation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">homes4 &lt;-<span class="st"> </span>homes3 %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">diff_perm =</span> <span class="kw">diff</span>(prop_own),
  <span class="dt">diff_orig =</span> <span class="kw">diff</span>(prop_own_perm))
homes4</code></pre></div>
<pre><code>## # A tibble: 1 × 2
##      diff_perm    diff_orig
##          &lt;dbl&gt;        &lt;dbl&gt;
## 1 -0.007828723 -0.001238614</code></pre>
</div>
<div id="density-plots" class="section level2">
<h2><span class="header-section-number">1.3</span> Density Plots</h2>
<p>Next we can make multiple permutations using the rep_sample_n from the oilabs package. We specify the data (tbl), the sample size, the number of samples to take (reps), and whether sampling should be done with or without replacement (replace). The output includes a new column, replicate, which indicates the sample number. We can create 100 permutations and create a dot plot of the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Perform 100 permutations</span>
homeown_perm &lt;-<span class="st"> </span>homes %&gt;%
<span class="st">  </span><span class="kw">rep_sample_n</span>(<span class="dt">size =</span> <span class="kw">nrow</span>(homes), <span class="dt">reps =</span> <span class="dv">100</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HomeOwn_perm =</span> <span class="kw">sample</span>(HomeOwn)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(replicate, Gender) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_own_perm =</span> <span class="kw">mean</span>(HomeOwn_perm ==<span class="st"> &quot;Own&quot;</span>), 
            <span class="dt">prop_own =</span> <span class="kw">mean</span>(HomeOwn ==<span class="st"> &quot;Own&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">diff_perm =</span> <span class="kw">diff</span>(prop_own_perm),
            <span class="dt">diff_orig =</span> <span class="kw">diff</span>(prop_own)) <span class="co"># male - female</span>

<span class="co"># Dotplot of 100 permuted differences in proportions</span>
<span class="kw">ggplot</span>(homeown_perm, <span class="kw">aes</span>(<span class="dt">x =</span> diff_perm)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_dotplot</span>(<span class="dt">binwidth =</span> .<span class="dv">001</span>)</code></pre></div>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We can go further and run 1000 permutations and create a density chart.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">666</span>)
<span class="co"># Perform 1000 permutations</span>
homeown_perm &lt;-<span class="st"> </span>homes %&gt;%
<span class="st">  </span><span class="kw">rep_sample_n</span>(<span class="dt">size =</span> <span class="kw">nrow</span>(homes), <span class="dt">reps =</span> <span class="dv">1000</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HomeOwn_perm =</span> <span class="kw">sample</span>(HomeOwn)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(replicate, Gender) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_own_perm =</span> <span class="kw">mean</span>(HomeOwn_perm ==<span class="st"> &quot;Own&quot;</span>), 
            <span class="dt">prop_own =</span> <span class="kw">mean</span>(HomeOwn ==<span class="st"> &quot;Own&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">diff_perm =</span> <span class="kw">diff</span>(prop_own_perm),
            <span class="dt">diff_orig =</span> <span class="kw">diff</span>(prop_own)) <span class="co"># male - female</span>

<span class="co"># Density plot of 1000 permuted differences in proportions</span>
<span class="kw">ggplot</span>(homeown_perm, <span class="kw">aes</span>(<span class="dt">x =</span> diff_perm)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>()</code></pre></div>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Now we have our density plot of the null hypothesis - randomly permuted samples - we can see where our actual observed difference lies, plus how many randomly permuted differences were less than the observed difference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="co"># Plot permuted differences</span>
<span class="kw">ggplot</span>(homeown_perm, <span class="kw">aes</span>(<span class="dt">x =</span> diff_perm)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>() +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> diff_orig),
          <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compare permuted differences to observed difference and calculate the percent of differences</span>
homeown_perm %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">sum</span>(diff_orig &gt;=<span class="st"> </span>diff_perm)) /<span class="dv">1000</span> *<span class="st"> </span><span class="dv">100</span></code></pre></div>
<pre><code>##   sum(diff_orig &gt;= diff_perm)
## 1                        20.5</code></pre>
<p>So in this instance, when we set the seed of 666 we end up with 20.5% of randomly shuffled (permuted) differences being greater than the observed difference, so the observed difference is consistent with the null hypothesis. That it to say it is within the range we may expect by chance alone, were we to repeat the exercise, although we should specify a distribtion we are comparing against, in this which is inferred as being the normal distribution in this instance. <strong>We can therefore say that there is no statistically significant difference between gender and home ownership</strong>. Or put more formally</p>
<blockquote>
<p><strong>We fail to reject the null hypothesis:</strong> There is no evidence that our data are inconsistent with the null hypothesis</p>
</blockquote>
</div>
<div id="gender-discrimination-p-values" class="section level2">
<h2><span class="header-section-number">1.4</span> Gender Discrimination (p-values)</h2>
<p>In this section we use data from <span class="citation">Rosen and Jerdee (<a href="#ref-Rosen1974">1974</a>)</span>, where 48 male bank supervisors were given personnel files and asked if they should be promoted to Branch Manager. All files were identical, but half (24) were named as female, and the other half (24) were named male. The results showed 21 males were promoted and 14 females, meaning 35 of the total 48 were promoted. In <span class="citation">Rosen and Jerdee (<a href="#ref-Rosen1974">1974</a>)</span> sex was given along with an indication of the difficulty - routine or complex - here we only look at the routine promotion candidates. Do we know if gender is a statistically significant factor?</p>
<ul>
<li><strong>Null Hypothesis <span class="math inline">\(H_{0}\)</span></strong>: Gender and promotion are unrelated variables</li>
<li><strong>Alternate Hypothesis <span class="math inline">\(H_{A}\)</span></strong>: Men are more likely to be promoted</li>
</ul>
<p>First, we create the data frame disc</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">disc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">promote =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;promoted&quot;</span>, <span class="dv">35</span>), <span class="kw">rep</span>(<span class="st">&quot;not_promoted&quot;</span>, <span class="dv">13</span>)),
  <span class="dt">sex =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;male&quot;</span>, <span class="dv">21</span>), <span class="kw">rep</span>(<span class="st">&quot;female&quot;</span>, <span class="dv">14</span>), <span class="kw">rep</span>(<span class="st">&quot;male&quot;</span>, <span class="dv">3</span>), <span class="kw">rep</span>(<span class="st">&quot;female&quot;</span>, <span class="dv">10</span>))
)</code></pre></div>
<p>Then let’s see the resulting table and proportion who were promoted</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(disc)</code></pre></div>
<pre><code>##               sex
## promote        female male
##   not_promoted     10    3
##   promoted         14   21</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">disc %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(sex) %&gt;%
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">promoted_prop =</span> <span class="kw">mean</span>(promote ==<span class="st"> &quot;promoted&quot;</span>))</code></pre></div>
<pre><code>## # A tibble: 2 × 2
##      sex promoted_prop
##   &lt;fctr&gt;         &lt;dbl&gt;
## 1 female     0.5833333
## 2   male     0.8750000</code></pre>
<p>So there difference in promotions by gender is around 0.3 or around 30%, but could this be due to chance? We can create 1000 permutations and compare our observed diffrence to the distribution, plus how many randomly permuted differences were less than the observed difference.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a data frame of differences in promotion rates</span>
<span class="kw">set.seed</span>(<span class="dv">42</span>)
disc_perm &lt;-<span class="st"> </span>disc %&gt;%
<span class="st">  </span><span class="kw">rep_sample_n</span>(<span class="dt">size =</span> <span class="kw">nrow</span>(disc), <span class="dt">reps =</span> <span class="dv">1000</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prom_perm =</span> <span class="kw">sample</span>(promote)) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(replicate, sex) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_prom_perm =</span> <span class="kw">mean</span>(prom_perm ==<span class="st"> &quot;promoted&quot;</span>),
            <span class="dt">prop_prom =</span> <span class="kw">mean</span>(promote ==<span class="st"> &quot;promoted&quot;</span>))   %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">diff_perm =</span> <span class="kw">diff</span>(prop_prom_perm),
            <span class="dt">diff_orig =</span> <span class="kw">diff</span>(prop_prom))  <span class="co"># male - female</span>

<span class="co"># Histogram of permuted differences</span>
<span class="kw">ggplot</span>(disc_perm, <span class="kw">aes</span>(<span class="dt">x =</span> diff_perm)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>() +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept =</span> diff_orig), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compare permuted differences to observed difference and calculate the percent of differences</span>
disc_perm %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">sum</span>(diff_orig &gt;=<span class="st"> </span>diff_perm)) /<span class="dv">1000</span> *<span class="st"> </span><span class="dv">100</span></code></pre></div>
<pre><code>##   sum(diff_orig &gt;= diff_perm)
## 1                        99.3</code></pre>
<p>So here, just 0.5% of the randomly permuted/shuffled results are greater than our observed promotion differences, or 99.5% are lower, so our results are definitely quite extreme. We typically use a 5% cut off, which the course mentions is arbitrary and historic, being attributed to Fisher. So we can say at 0.5% our value is within this critical region, meaning the results are statistically significant - we should not ignore them. We can calculate quantiles of the null statistic using our randomly generated shuffles.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">disc_perm %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">q.90 =</span> <span class="kw">quantile</span>(diff_perm, <span class="dt">p =</span> <span class="fl">0.90</span>),
            <span class="dt">q.95 =</span> <span class="kw">quantile</span>(diff_perm, <span class="dt">p =</span> <span class="fl">0.95</span>),
            <span class="dt">q.99 =</span> <span class="kw">quantile</span>(diff_perm, <span class="dt">p =</span> <span class="fl">0.99</span>))</code></pre></div>
<pre><code>## # A tibble: 1 × 3
##    q.90      q.95      q.99
##   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 0.125 0.2083333 0.2916667</code></pre>
<p>So here, 95% of our null differences are 0.208 or lower, indeed 99% are 0.292 or lower, so our observed difference of 0.3 is quite extreme - it is in the critical region of the distribution. We can go one step further by calculating the p-value.</p>
<blockquote>
<p><strong>The p-value is</strong>: the probability of observing data as or more extreme than what we actually got given that the null hypothesis is true.</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">disc_perm %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">mean</span>(diff_orig &lt;=<span class="st"> </span>diff_perm))</code></pre></div>
<pre><code>## # A tibble: 1 × 1
##   `mean(diff_orig &lt;= diff_perm)`
##                            &lt;dbl&gt;
## 1                          0.028</code></pre>
<p>So the p-value here is 0.028 (less than 3 %). If repeat the exercise with smaller and larger number of shuffles we would get different p-values.</p>
<pre><code>## # A tibble: 1 × 1
##   `mean(diff_orig &lt;= diff_perm)`
##                            &lt;dbl&gt;
## 1                           0.03</code></pre>
<pre><code>## # A tibble: 1 × 1
##   `mean(diff_orig &lt;= diff_perm)`
##                            &lt;dbl&gt;
## 1                         0.0235</code></pre>
<p>With 100 shuffles our p-value is 0.03, and with 10,000 shuffles our p-value is 0.0235. If we had a two-tailed test - for instance if we said the original research hypothesis had focused on any difference in promotion rates between men and women instead of focusing on whether men are more likely to be promoted than women - we could simple double the p-value.</p>
<blockquote>
<p><strong>In both cases, the p-value is below or close to the 0.05 (5%) critical value, meaning we can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. However, as both values are close to the critical value, we should indicate that more work should be done</strong>.</p>
</blockquote>
<p>Indeed since the <span class="citation">Rosen and Jerdee (<a href="#ref-Rosen1974">1974</a>)</span> study, many further studies have been undertaken and found a similar pattern of discrimination.</p>
</div>
<div id="opportunity-cost" class="section level2">
<h2><span class="header-section-number">1.5</span> Opportunity Cost</h2>
<p>In <span class="citation">Frederick et al. (<a href="#ref-Frederick2009">2009</a>)</span> their study showed that when potential purchasers were reminded that if they did not buy a particular DVD they could instead save the money, when compared to a control group who were just told they could not buy the DVD, those being reminded of the saving appeared to be more inclined not to make the purchase - 34 in the treatment group did not buy compared to 19 in the control. So our test is setup as:</p>
<ul>
<li><strong>Null Hypothesis <span class="math inline">\(H_{0}\)</span></strong>: Reminding students will have no impact on their spending decisions</li>
<li><strong>Alternate Hypothesis <span class="math inline">\(H_{A}\)</span></strong>: Reminding students will reduce the chance they continue with a purchase</li>
</ul>
<p>We can create a data frame containing the results and find the initial proportions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#create the data frame</span>
opportunity &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">decision =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;buyDVD&quot;</span>, <span class="dv">97</span>), <span class="kw">rep</span>(<span class="st">&quot;nobuyDVD&quot;</span>, <span class="dv">53</span>)),
  <span class="dt">group =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;control&quot;</span>, <span class="dv">56</span>), <span class="kw">rep</span>(<span class="st">&quot;treatment&quot;</span>, <span class="dv">41</span>), <span class="kw">rep</span>(<span class="st">&quot;control&quot;</span>, <span class="dv">19</span>), <span class="kw">rep</span>(<span class="st">&quot;treatment&quot;</span>, <span class="dv">34</span>))
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Tabulate the data</span>
opportunity %&gt;%
<span class="st">  </span><span class="kw">select</span>(decision, group) %&gt;%
<span class="st">  </span><span class="kw">table</span>()</code></pre></div>
<pre><code>##           group
## decision   control treatment
##   buyDVD        56        41
##   nobuyDVD      19        34</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Find the proportion who bought the DVD in each group</span>
opportunity %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(group) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">buy_prop =</span> <span class="kw">mean</span>(decision ==<span class="st"> &quot;buyDVD&quot;</span>))</code></pre></div>
<pre><code>## # A tibble: 2 × 2
##       group  buy_prop
##      &lt;fctr&gt;     &lt;dbl&gt;
## 1   control 0.7466667
## 2 treatment 0.5466667</code></pre>
<p>So around 55% of the treatment group - those who were reminded they could save the money - bought the DVD, comapred to 75% of the control group. We can represent this with a bar plot.</p>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>As before, we can calculate 1000 random shuffles and then compare our difference in proportions, to the distribution of those 1000 samples.</p>
<p><img src="Foundations-of-Inference_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>And finally, we can calculate the p-value</p>
<pre><code>## # A tibble: 1 × 1
##   `mean(diff_perm &lt;= diff_orig)`
##                            &lt;dbl&gt;
## 1                          0.008</code></pre>
<p>In this instance, of p-value is substantially less than the usual critical value - 0.8% versus the usual value of 5% - so we can can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. Our results would only occur 8 times in 1000 by chance. We can therefore accept the alternative hypothesis (<span class="math inline">\(H_{A}\)</span>) that reminding students does cause them to be less likely to buy a DVD, as they were randomly assigned to the treatment and control groups, therefore any difference is due to the reminder to save. Who can we therefore make the inference to? Our sample was drawn from the student population for the <span class="citation">Frederick et al. (<a href="#ref-Frederick2009">2009</a>)</span> study, so we would be able to generalise to that student population however defined, but not to another wider population.</p>
</div>
<div id="type-i-and-type-ii-errors" class="section level2">
<h2><span class="header-section-number">1.6</span> Type I and Type II errors</h2>
<p>In our research and conslusions there is a risk that we will be incorrect, we will make an error. The two errors are:</p>
<blockquote>
<p><strong>Type I error</strong> : The null hypothesis (<span class="math inline">\(H_{0}\)</span>) is true, but is rejected. On the basis of the evidence, we have decided to erroneously accept the alternative hypothesis (<span class="math inline">\(H_{A}\)</span>) when in fact the null hypothesis is correct. It is sometimes called a false positive.</p>
</blockquote>
<blockquote>
<p><strong>Type II error</strong> : the null hypothesis is false, but erroneously fails to be rejected. On the basis of the evidence, we have failed to accept the alternative hypothesis despite it being correct - an effect that exists in the population. It is sometimes called a false negative.</p>
</blockquote>
<p>If we return to our previous example, our associated errors would be</p>
<p>Type I: There is not a difference in proportions, but the observed difference is big enough to indicate that the proportions are different.</p>
<p>Type II: There is a difference in proportions, but the observed difference is not large enough to indicate that the proportions are different.</p>
</div>
<div id="bootstrapping" class="section level2">
<h2><span class="header-section-number">1.7</span> Bootstrapping</h2>
<p>Sometimes we are not neccessarily interested in testing a hypothesis, we are instead interested in making a claim about how our sample can be inferred to a large population. To do so we use confidece intervals. When calculating confidence intervals there is no null hypothesis like in hypothesis testing. We need to understand how samples from our population vary around the parameter of interest. In an ideal world we would take many samples from the population or know what the true value is in the population, but realistically this is not possible, so we use booststrapping.</p>
<p>Bootstrapping is the process of taking repeated samples from the same sample, to estimate the variability. As our population parameters are not known, we can use our sample to estimate a simulated population parameter (<span class="math inline">\(\hat{p}*\)</span>) by repeated sampling. We can then estimate other parameters such as the standard deviation, s.e. and the confidence interval. Instead of taking repeated samples from our population, we take repeated samples from our data, with replacement, each bootstrap sample is the same size as the original sample.</p>
<div class="figure"><span id="fig:bootstrap"></span>
<img src="images/bootstrap.png" alt="Illustration of the bootstrap approach on a small sample containing n = 3 observations [@ISLR2013, pg 190]"  />
<p class="caption">
Figure 1.1: Illustration of the bootstrap approach on a small sample containing n = 3 observations <span class="citation">(James et al. <a href="#ref-ISLR2013">2013</a>, pg 190)</span>
</p>
</div>
<p>Firstly we setup our single poll, where 70% (21/30) are intended to vote for a particular candidate</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Setup our single poll example</span>
one_poll &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">times =</span> <span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">21</span>)))
one_poll &lt;-<span class="st"> </span><span class="kw">tbl_df</span>(one_poll)
<span class="kw">colnames</span>(one_poll) &lt;-<span class="st"> &quot;vote&quot;</span></code></pre></div>
<p>Next we can create 1000 bootstrap samples from this original poll, then calculate the variability</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Generate 1000 resamples of one_poll: one_poll_boot_30</span>
one_poll_boot_30 &lt;-<span class="st"> </span>one_poll %&gt;%
<span class="st">  </span><span class="kw">rep_sample_n</span>(<span class="dt">size =</span> <span class="dv">30</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">reps =</span> <span class="dv">1000</span>)

<span class="co"># Compute p-hat* for each resampled poll</span>
ex1_props &lt;-<span class="st"> </span>one_poll_boot_30 %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop_yes =</span> <span class="kw">mean</span>(vote)) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="kw">sd</span>(prop_yes))</code></pre></div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Huberty1993">
<p>Huberty, Carl J. 1993. “Historical Origins of Statistical Testing Practices.” <em>The Journal of Experimental Education</em> 61 (4): 317–33.</p>
</div>
<div id="ref-Rosen1974">
<p>Rosen, B., and T. Jerdee. 1974. “Influence of Sex Role Stereotypes on Personnel Decisions.” <em>Journal of Applied Psychology</em> 59 (1): 9–14.</p>
</div>
<div id="ref-Frederick2009">
<p>Frederick, S., N. Novemsky, J. Wang, R. Dhar, and S. Nowlis. 2009. “Opportunity Cost Neglect.” <em>Journal of Consumer Research</em> 36 (4): 553–61.</p>
</div>
<div id="ref-ISLR2013">
<p>James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. 6th Printing. London: Springer. <a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Study-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
