[
["index.html", "Study notes Preface", " Study notes James Solomon-Rounce Last updated:2017-07-24 Preface The following notes were taken by me for educational, non-commercial, purposes. If you find the information useful, buy the material/take the course. Thank you to the original content providers. Additional ramblings are my own. "],
["importing-data-part-1.html", "1 Importing data - Part 1 1.1 Introduction 1.2 Reading CSV files 1.3 Reading tab deliminated files or other table formats 1.4 Readr and data.table 1.5 Reading Excel files 1.6 XLConnect - read and write to excel", " 1 Importing data - Part 1 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 1)’ by Filip Schouwenaars. 1.1 Introduction Data often comes from many different sources and formats, including Flat files - simple tables e.g. csv Excel Databases - MySQL, Postgres Websites - APIs, JSON, scraping Other statistical software - SPSS, STATA, SAS 1.2 Reading CSV files Reading csv files can be achived with simple code like read.csv(&quot;file.csv&quot;, stringsAsFactors = FALSE) We may want to import strings as categorical variables, in which case we would set stringsAsFactors = TRUE which is also the default option, if not stated. When working across different machines or operating systems, problems can arise due to different ways of addressing file locations and differing file locations. Therefore, it can be easier to set a relative path to the users home directory, which would be achieved with the following code. path &lt;- file.path(&quot;~&quot;, &quot;datasets&quot;, &quot;file.csv&quot;) path ## [1] &quot;~/datasets/file.csv&quot; Then use the file path as before, assigning to a dataframe. df &lt;- read.csv(path, stringsAsFactors = FALSE) 1.3 Reading tab deliminated files or other table formats In a similar way to before, we add the path to the file and if we want strings as strings, for instance read.delim(&quot;file.csv&quot;, stringsAsFactors = FALSE) However, if the file comes in another format perhaps due to the system encoding or setup, it is still possible to try and read the file as a tabular formatting converting it to a data frame. To do so, we use the read.table() command which has a lot of arguments that can be customised. You can specify column names and types for instance. If for instance we have a file format where the objects are separated by a / rather than a comma or tab as before, we could use read.table(&quot;file.txt&quot;, header = TRUE, sep = &quot;/&quot;, stringsAsFactors = FALSE) Or, if you have a file which has no column/variable names and tabs as spaces, you would read the file as: # Path to the file.txt file: path path &lt;- file.path(&quot;data&quot;, &quot;file.txt&quot;) # Import the file.txt file: hotdogs file &lt;- read.table(path, sep = &quot;\\t&quot;, # specify seperator - tab in this instance col.names = c(&quot;VarName1&quot;, &quot;VarName2&quot;, &quot;VarName3&quot;), # specifiy variable names colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # specify the column/variable classes Both read.csv and read.delim are wrapper functions of read.table(), both use read.table but have different default options depending on the file type. There are two further wrapper functions - read.csv2 and read.delim2 - which deal with regional differences in formatting, notably that some areas use full stops as decimal places, whereas other areas use commas for decimal places. 1.4 Readr and data.table These two packages are other ways of reading in files. Readr uses the tibble, so will be compatible with other tidyverse packages such as dplyr. It is faster than utils, the r default and also prints out the column classes, depending on what other packages are loaded. It is not necessary to specify stringsAsFactors = FALSE. library(readr) read_csv(&quot;file.csv&quot;) #read comma seperated read_tsv(&quot;file2.txt&quot;) #read tab seperated files #If there are no row heads, you can create a vector then read it in using the col_names argument #specify the vector for column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) #read in the vector df &lt;- read_tsv(&quot;file3.txt&quot;, col_names = properties) Like the utils package, these are wrapper functions, with the base function being read_delim(). Unlike the utils package, read_delim() expects the first row to contain headers, so this doesn’t need to be explicit. As mentioned previously, it is also not necessary to specify the we don’t want strings as factors. You can specify col_names using a vector as before, or we can read them directly at the time. If we also want to explicitly state the column types, perahps because the automatically assigned variable is not correct, we can do so with col_type using abbreviations: c = character d = double i = integer n = number l = logical D = date T = date time t = time ? = guess _ = skip column (underscore) Finally, we can use skip and n_max to specify how many rows to skip at the beginning of a file, perhaps due to a large header, and the maximum now of rows to read, perhaps due to a very large file with many rows. read_delim(&quot;file4.txt&quot;, delim = &quot;/&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;)) read_delim(&quot;file5.txt&quot;, delim = &quot;/&quot;, col_types = &quot;ccid&quot;) read_delim(&quot;file6.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;), skip = 12, n_max = 50000) Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column. Look at the collector documentation for more details. Two examples are shown below, one for columns to be interpreted as integers and one for a column with factors. # The collectors needed for importing fac &lt;- col_factor(levels = c(&quot;Beef&quot;, &quot;Meat&quot;, &quot;Poultry&quot;)) int &lt;- col_integer() # Edit the col_types argument with the specified collectors hotdogs_factor &lt;- read_tsv(&quot;hotdogs.txt&quot;, col_names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), col_types = list(fac, int, int)) 1.4.1 data.table fread data.table is a tool for doing fast data analysis, particularly on large datasets. It also has a function to read data using the fread() command. It can automatically infer column names, types and separators. You can also drop or select columns at read time. df &lt;- fread(&quot;file7.csv&quot;, select = c(&quot;colname1&quot;, &quot;colname2&quot;)) The readr package fill create different dataframe types or object classes - ‘tbl_df’, ‘tbl’ and ‘data.frame’ - which can be useful for different purposes, such as for use in dplyr. Fread creates a data.table object class. 1.5 Reading Excel files There are many packages for reading Excel files, one package is the readxl package by Hadley Wickham. There are to main functions excel_sheets(): lists the sheets within an excel file or workbook read_excel(): import the data, unless specified the first sheet is read, this can either be done with sheet = 7, or sheet = “name”. So to read an urbanpop.xlsx file containing three sheets of urban populations, for different time frames, our code would look similar to that below. library(readxl) #list the sheerts in the file excel_sheets(&quot;urbanpop.xlsx&quot;) # Read the sheets, one by one pop_1 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 1) pop_2 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2) pop_3 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 3) # Put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) # IF we want to read all the files, a more efficient way to read all the files in the file uses lapply pop_list &lt;- lapply(excel_sheets(&quot;urbanpop.xlsx&quot;), read_excel, path = &quot;urbanpop.xlsx&quot;) There are other arguments that can be used with the read_excel() function: col_names: If true, the first row is read, if false R will assign it’s own names or you specify a charecter vector manually col_types: If NULL, R gueses the data types of the columns. Alternatively, they can be specified e.g. text, numeric, date, blank (which ignores the col) skip: Speficies the number of rows to ignore # Some examples # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = FALSE) # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = cols) # Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel urbanpop_sel &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2, col_names = FALSE, skip = 21) # Print out the first observation from urbanpop_sel urbanpop_sel[1,] 1.5.1 Alternatives for importing Excel files One alternative is the gdata package, which is a suite of tools for data. There is a read.xls() function which only, currently, supports XLS files although xlsx could be supported with a driver. The data is interpreted by the read.xls file using perl into a csv file, which is then read using the read.csv function - itself a offshoot of read.table, in to an R data frame. Hadley’s readxl package is faster, but is quite early in it’s development so some of the functions may change. For gdata, as it is an offshoot of read.table(), all of the same arguments can be used by read.xls(). 1.6 XLConnect - read and write to excel Most of the Excel tools can become accessible but inside R, using XLConnect. It is possible to use XLS and XLSX and it will create a ‘workbook’ object in R, but it does require Java to work. library(XLConnect) #create a connect to a file and list the sheets book &lt;- loadWorkbook(&quot;file.xlsx&quot;) getSheets(book) #read in the specific sheet but only the columns we are interested in wardData &lt;- readWorksheet(book, sheet = &quot;sheet_1&quot;, startCol = 3, endCol = 5) # read in the names column, previoulsy excluded wardNames &lt;- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1) #cbind the data and names together selection &lt;- cbind(wardNames, wardData) XLConnect has more features than simply reading sheets. It is possible to write data back to the Excel file also. We can add sheets, write or add data to sheets, rename and remove sheets. # Add a worksheet to my_book, named &quot;summary&quot; createSheet(my_book, &quot;summary&quot;) # Add data in summ to &quot;data_summary&quot; sheet writeWorksheet(my_book, summ, &quot;summary&quot;) # Save workbook as summary.xlsx saveWorkbook(my_book, &quot;summary.xlsx&quot;) # Rename &quot;summary&quot; sheet to &quot;data_summary&quot; renameSheet(my_book, sheet = 4, &quot;data_summary&quot;) # Remove the third sheet removeSheet(my_book, sheet = 3) "],
["introduction-to-data.html", "2 Introduction to Data 2.1 Language of Data", " 2 Introduction to Data Notes taken during/inspired by the Datacamp course ‘Introduction to Data’ by Mine Cetinkaya-Rundel. The supporting textbook is Diez, Barr, and Cetinkaya-Rundel (2015). 2.1 Language of Data The course makes use of the openintro package, accompanying the textbook. LEt’s load the package and our first dataset, email50. # Load packages library(&quot;openintro&quot;) # Load data data(email50) # View its structure str(email50) ## &#39;data.frame&#39;: 50 obs. of 21 variables: ## $ spam : num 0 0 1 0 0 0 0 0 0 0 ... ## $ to_multiple : num 0 0 0 0 0 0 0 0 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 4 0 0 0 0 0 1 0 ... ## $ sent_email : num 1 0 0 0 0 0 0 1 1 0 ... ## $ time : POSIXct, format: &quot;2012-01-04 13:19:16&quot; &quot;2012-02-16 20:10:06&quot; ... ## $ image : num 0 0 0 0 0 0 0 0 0 0 ... ## $ attach : num 0 0 2 0 0 0 0 0 0 0 ... ## $ dollar : num 0 0 0 0 9 0 0 0 0 23 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 0 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 1 0 0 0 0 0 ... ## $ num_char : num 21.705 7.011 0.631 2.454 41.623 ... ## $ line_breaks : int 551 183 28 61 1088 5 17 88 242 578 ... ## $ format : num 1 1 0 0 1 0 0 1 1 1 ... ## $ re_subj : num 1 0 0 0 0 0 0 1 1 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 1 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 8 1 2 1 43 0 0 2 22 3 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 2 3 1 2 2 2 2 2 2 2 ... References "],
["references.html", "References", " References "],
["foundations-of-inference.html", "3 Foundations of Inference 3.1 Introduction to Inference 3.2 Home Ownership by Gender 3.3 Density Plots 3.4 Gender Discrimination (p-values) 3.5 Opportunity Cost 3.6 Type I and Type II errors 3.7 Bootstrapping", " 3 Foundations of Inference Notes taken during/inspired by the Datacamp course ‘Foundations of Inference’ by Jo Hardin, collaborators; Nick Carchedi and Tom Jeon. 3.1 Introduction to Inference Classical statistical inference is the process of making claims about a population based on a sample of information. We are making an inference from a small group (sample) to a much larger one (population). We typically have: Null Hypothesis \\(H_{0}\\): What we are researching has no effect Alternate Hypothesis \\(H_{A}\\): What we are researching does have an effect Under the null hypothesis, chance alone is responsible for the results. Under the alternate hypothesis, we reject the null hypothesis, by using statistical techniques that indicate that chance is not responsible for our findings. Hypothesis or statistical testing goes back over 300 years, with the first recorded use by John Arbuthnot: Table 3.1: Statistical Testing Applications Year Person Context 1710 Arbuthnot Sex ratio at birth 1767 Michelle Distribution of stars 1823 Laplace Moon phase and barometric changes 1900 K. Pearson Goodness of fit 1908 Gosset A single mean Source: (Huberty 1993, pg 318) Contemporary statistical testing is a usually that of either Fisher or Neyman-Pearson approaches. Fisher tends to use a single hypothesis test and a p-value strength of evidence test, where as the Neyman-Pearson test will set a critical alpha value and compare the null hypothesis against an alternative hypothesis, rejecting the null if the test statistic is high enough (Huberty 1993, pg 318). The course goes on to say that idea behind statistical inference is to understand samples from a hypothetical population, where the null hypothesis is true - there is no difference between two groups. We can do this by calculating one statistic - for instance the proportion (mean) of a test group who show a positive response when testing a new drug, compared to a placebo control group - for each repeated sample from a population, then work out the difference between these two groups means. With each sample, the mean will change, resulting in a changing difference for each sample. We can then generate a distribution (histogram) of differences, assuming the null hypothesis - that there is no link between drug effectiveness between a test group and a control group - is true. “Generating a distribution of the statistic from the null population gives information about whether the observed data are inconsistent with the null hypothesis”. That is to say, by taking repeated samples and creating a distribution, we can then say whether our observed difference is consistent (within an acceptable value range due to chance) to the null hypothesis. The null samples consist of randomly shuffled drug effectiveness variables (permuted samples from the population), so that the samples don’t have any dependency between the two groups and effectiveness. 3.2 Home Ownership by Gender Data used in the exercises are from NHANES 2009-2012 With Adjusted Weighting. This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. # Load packages library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) library(&quot;NHANES&quot;) library(&quot;oilabs&quot;) # Create bar plot for Home Ownership by Gender ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Relative frequencies&quot;) # Density for SleepHrsNight coloured by SleepTrouble, faceted by HealthGen ggplot(NHANES, aes(x = SleepHrsNight, col = SleepTrouble)) + geom_density(adjust = 2) + facet_wrap(~ HealthGen) Next we want to create a selection for just our variables of interest - rent and owner occupation. # Subset the data: homes homes &lt;- NHANES %&gt;% select(Gender, HomeOwn) %&gt;% filter(HomeOwn %in% c(&quot;Own&quot;, &quot;Rent&quot;)) We build a distribution of differences assuming the null hypothesis - that there is no link between gender and home ownership - is true. In this first step, we just do a single iteration, or permutation from the true values. The null (permuted) version here will create a randomly shuffled home ownership variable, so that the permuted version does not have any dependency between gender and homeownership. We effectively have the same gender split variables as per the original, with the same owned and rented proportions, but disassociated from the gender variable - just randomly shuffled. # Perform one permutation homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) ## # A tibble: 1 × 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 0.004527731 It is easier to see what is going on by breaking the results down iteratively. Our selected and filtered homes dataset looks like. head(homes) ## # A tibble: 6 × 2 ## Gender HomeOwn ## &lt;fctr&gt; &lt;fctr&gt; ## 1 male Own ## 2 male Own ## 3 male Own ## 4 male Own ## 5 female Rent ## 6 male Rent Next we shuffle this data, let’s call it homes 2. we can then check the total number of owns and rents are the same using the summary function, which confirms the data is just randomly shuffled. homes2 &lt;- homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) tail(homes2) ## Source: local data frame [6 x 3] ## Groups: Gender [2] ## ## Gender HomeOwn HomeOwn_perm ## &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; ## 1 male Rent Rent ## 2 male Rent Own ## 3 female Own Rent ## 4 male Own Own ## 5 male Own Rent ## 6 male Own Rent summary(homes2) ## Gender HomeOwn HomeOwn_perm ## female:4890 Own :6425 Own :6425 ## male :4822 Rent :3287 Rent :3287 ## Other: 0 Other: 0 Then we calculate the mean value of home ownership (Own) across our original and shuffled (permutated) data homes3 &lt;- homes2 %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) homes3 ## # A tibble: 2 × 3 ## Gender prop_own_perm prop_own ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 0.6652352 0.6654397 ## 2 male 0.6578183 0.6576109 FFinally we calculate the differences in ownership - note that the difference for the permuted value here may be different from the full code above, as it a new random permutation and we have used the set.seed() function which would create an identical permutation. homes4 &lt;- homes3 %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) homes4 ## # A tibble: 1 × 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 -0.007416841 3.3 Density Plots Next we can make multiple permutations using the rep_sample_n from the oilabs package. We specify the data (tbl), the sample size, the number of samples to take (reps), and whether sampling should be done with or without replacement (replace). The output includes a new column, replicate, which indicates the sample number. We can create 100 permutations and create a dot plot of the results. # Perform 100 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 100) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Dotplot of 100 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_dotplot(binwidth = .001) We can go further and run 1000 permutations and create a density chart. set.seed(666) # Perform 1000 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 1000) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Density plot of 1000 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() Now we have our density plot of the null hypothesis - randomly permuted samples - we can see where our actual observed difference lies, plus how many randomly permuted differences were less than the observed difference. # Plot permuted differences ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences homeown_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 20.5 So in this instance, when we set the seed of 666 we end up with 20.5% of randomly shuffled (permuted) differences being greater than the observed difference, so the observed difference is consistent with the null hypothesis. That it to say it is within the range we may expect by chance alone, were we to repeat the exercise, although we should specify a distribtion we are comparing against, in this which is inferred as being the normal distribution in this instance. We can therefore say that there is no statistically significant difference between gender and home ownership. Or put more formally We fail to reject the null hypothesis: There is no evidence that our data are inconsistent with the null hypothesis 3.4 Gender Discrimination (p-values) In this section we use data from Rosen and Jerdee (1974), where 48 male bank supervisors were given personnel files and asked if they should be promoted to Branch Manager. All files were identical, but half (24) were named as female, and the other half (24) were named male. The results showed 21 males were promoted and 14 females, meaning 35 of the total 48 were promoted. In Rosen and Jerdee (1974) sex was given along with an indication of the difficulty - routine or complex - here we only look at the routine promotion candidates. Do we know if gender is a statistically significant factor? Null Hypothesis \\(H_{0}\\): Gender and promotion are unrelated variables Alternate Hypothesis \\(H_{A}\\): Men are more likely to be promoted First, we create the data frame disc disc &lt;- data.frame( promote = c(rep(&quot;promoted&quot;, 35), rep(&quot;not_promoted&quot;, 13)), sex = c(rep(&quot;male&quot;, 21), rep(&quot;female&quot;, 14), rep(&quot;male&quot;, 3), rep(&quot;female&quot;, 10)) ) Then let’s see the resulting table and proportion who were promoted table(disc) ## sex ## promote female male ## not_promoted 10 3 ## promoted 14 21 disc %&gt;% group_by(sex) %&gt;% summarise(promoted_prop = mean(promote == &quot;promoted&quot;)) ## # A tibble: 2 × 2 ## sex promoted_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 female 0.5833333 ## 2 male 0.8750000 So there difference in promotions by gender is around 0.3 or around 30%, but could this be due to chance? We can create 1000 permutations and compare our observed diffrence to the distribution, plus how many randomly permuted differences were less than the observed difference. # Create a data frame of differences in promotion rates set.seed(42) disc_perm &lt;- disc %&gt;% rep_sample_n(size = nrow(disc), reps = 1000) %&gt;% mutate(prom_perm = sample(promote)) %&gt;% group_by(replicate, sex) %&gt;% summarize(prop_prom_perm = mean(prom_perm == &quot;promoted&quot;), prop_prom = mean(promote == &quot;promoted&quot;)) %&gt;% summarize(diff_perm = diff(prop_prom_perm), diff_orig = diff(prop_prom)) # male - female # Histogram of permuted differences ggplot(disc_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences disc_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 99.3 So here, just 0.5% of the randomly permuted/shuffled results are greater than our observed promotion differences, or 99.5% are lower, so our results are definitely quite extreme. We typically use a 5% cut off, which the course mentions is arbitrary and historic, being attributed to Fisher. So we can say at 0.5% our value is within this critical region, meaning the results are statistically significant - we should not ignore them. We can calculate quantiles of the null statistic using our randomly generated shuffles. disc_perm %&gt;% summarize(q.90 = quantile(diff_perm, p = 0.90), q.95 = quantile(diff_perm, p = 0.95), q.99 = quantile(diff_perm, p = 0.99)) ## # A tibble: 1 × 3 ## q.90 q.95 q.99 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.2083333 0.2916667 So here, 95% of our null differences are 0.208 or lower, indeed 99% are 0.292 or lower, so our observed difference of 0.3 is quite extreme - it is in the critical region of the distribution. We can go one step further by calculating the p-value. The p-value is: the probability of observing data as or more extreme than what we actually got given that the null hypothesis is true. disc_perm %&gt;% summarize(mean(diff_orig &lt;= diff_perm)) ## # A tibble: 1 × 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.028 So the p-value here is 0.028 (less than 3 %). If repeat the exercise with smaller and larger number of shuffles we would get different p-values. ## # A tibble: 1 × 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.03 ## # A tibble: 1 × 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.0235 With 100 shuffles our p-value is 0.03, and with 10,000 shuffles our p-value is 0.0235. If we had a two-tailed test - for instance if we said the original research hypothesis had focused on any difference in promotion rates between men and women instead of focusing on whether men are more likely to be promoted than women - we could simple double the p-value. In both cases, the p-value is below or close to the 0.05 (5%) critical value, meaning we can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. However, as both values are close to the critical value, we should indicate that more work should be done. Indeed since the Rosen and Jerdee (1974) study, many further studies have been undertaken and found a similar pattern of discrimination. 3.5 Opportunity Cost In Frederick et al. (2009) their study showed that when potential purchasers were reminded that if they did not buy a particular DVD they could instead save the money, when compared to a control group who were just told they could not buy the DVD, those being reminded of the saving appeared to be more inclined not to make the purchase - 34 in the treatment group did not buy compared to 19 in the control. So our test is setup as: Null Hypothesis \\(H_{0}\\): Reminding students will have no impact on their spending decisions Alternate Hypothesis \\(H_{A}\\): Reminding students will reduce the chance they continue with a purchase We can create a data frame containing the results and find the initial proportions. #create the data frame opportunity &lt;- data.frame( decision = c(rep(&quot;buyDVD&quot;, 97), rep(&quot;nobuyDVD&quot;, 53)), group = c(rep(&quot;control&quot;, 56), rep(&quot;treatment&quot;, 41), rep(&quot;control&quot;, 19), rep(&quot;treatment&quot;, 34)) ) # Tabulate the data opportunity %&gt;% select(decision, group) %&gt;% table() ## group ## decision control treatment ## buyDVD 56 41 ## nobuyDVD 19 34 # Find the proportion who bought the DVD in each group opportunity %&gt;% group_by(group) %&gt;% summarize(buy_prop = mean(decision == &quot;buyDVD&quot;)) ## # A tibble: 2 × 2 ## group buy_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 control 0.7466667 ## 2 treatment 0.5466667 So around 55% of the treatment group - those who were reminded they could save the money - bought the DVD, comapred to 75% of the control group. We can represent this with a bar plot. As before, we can calculate 1000 random shuffles and then compare our difference in proportions, to the distribution of those 1000 samples. And finally, we can calculate the p-value ## # A tibble: 1 × 1 ## `mean(diff_perm &lt;= diff_orig)` ## &lt;dbl&gt; ## 1 0.008 In this instance, of p-value is substantially less than the usual critical value - 0.8% versus the usual value of 5% - so we can can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. Our results would only occur 8 times in 1000 by chance. We can therefore accept the alternative hypothesis (\\(H_{A}\\)) that reminding students does cause them to be less likely to buy a DVD, as they were randomly assigned to the treatment and control groups, therefore any difference is due to the reminder to save. Who can we therefore make the inference to? Our sample was drawn from the student population for the Frederick et al. (2009) study, so we would be able to generalise to that student population however defined, but not to another wider population. 3.6 Type I and Type II errors In our research and conslusions there is a risk that we will be incorrect, we will make an error. The two errors are: Type I error : The null hypothesis (\\(H_{0}\\)) is true, but is rejected. On the basis of the evidence, we have decided to erroneously accept the alternative hypothesis (\\(H_{A}\\)) when in fact the null hypothesis is correct. It is sometimes called a false positive. Type II error : the null hypothesis is false, but erroneously fails to be rejected. On the basis of the evidence, we have failed to accept the alternative hypothesis despite it being correct - an effect that exists in the population. It is sometimes called a false negative. If we return to our previous example, our associated errors would be Type I: There is not a difference in proportions, but the observed difference is big enough to indicate that the proportions are different. Type II: There is a difference in proportions, but the observed difference is not large enough to indicate that the proportions are different. 3.7 Bootstrapping Sometimes we are not neccessarily interested in testing a hypothesis, we are instead interested in making a claim about how our sample can be inferred to a large population. To do so we use confidece intervals. When calculating confidence intervals there is no null hypothesis like in hypothesis testing. We need to understand how samples from our population vary around the parameter of interest. In an ideal world we would take many samples from the population or know what the true value is in the population, but realistically this is not possible, so we use booststrapping. Bootstrapping is the process of taking repeated samples from the same sample, to estimate the variability. As our population parameters are not known, we can use our sample to estimate a simulated population parameter (\\(\\hat{p}*\\)) by repeated sampling. We can then estimate other parameters such as the standard deviation, s.e. and the confidence interval. Instead of taking repeated samples from our population, we take repeated samples from our data, with replacement, each bootstrap sample is the same size as the original sample. Figure 3.1: Illustration of the bootstrap approach on a small sample containing n = 3 observations (James et al. 2013, pg 190) Firstly we setup our single poll, where 70% (21/30) are intended to vote for a particular candidate # Setup our single poll example one_poll &lt;- sample(rep(c(0, 1), times = c(9,21))) one_poll &lt;- tbl_df(one_poll) colnames(one_poll) &lt;- &quot;vote&quot; Next we can create 1000 bootstrap samples from this original poll, then calculate the variability set.seed(42) # Generate 1000 resamples of one_poll: one_poll_boot_30 one_poll_boot_30 &lt;- one_poll %&gt;% rep_sample_n(size = 30, replace = TRUE, reps = 1000) # Compute p-hat* for each resampled poll ex1_props &lt;- one_poll_boot_30 %&gt;% summarize(prop_yes = mean(vote)) %&gt;% summarize(sd(prop_yes)) #compute variability p-hat* ex1_props ## # A tibble: 1 × 1 ## `sd(prop_yes)` ## &lt;dbl&gt; ## 1 0.085741 So the variability - the standard error or SE - of \\(\\hat{p}*\\) is 0.0841. We can now use this SE to calculate a confidence interval, since 95% of samples will be within +/- 1.96 standard errors of the centre of the distribution assuming a normal distribution \\(N(\\mu, \\sigma ^2)\\). We also use the bootstrap to calculate our bootstrap confidence interval, to give a range of possible values. # Compute p-hat for one poll p_hat &lt;- mean(one_poll$vote) set.seed(42) # Bootstrap to find the SE of p-hat: one_poll_boot one_poll_boot &lt;- one_poll %&gt;% rep_sample_n(30, replace = TRUE, reps = 1000) %&gt;% summarize(prop_yes_boot = mean(vote)) # Create an interval of possible values one_poll_boot %&gt;% summarize(lower = p_hat - 1.96 * sd(prop_yes_boot), upper = p_hat + 1.96 * sd(prop_yes_boot)) ## # A tibble: 1 × 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5319476 0.8680524 So our possible range of values, using the bootstrap at 95%, is between 53.2% and 86.8%. Going back to our original statement, we had a single poll where 70% of those polled intended to vote for a particular candidate. We can now say, using the bootstrap t-confidence interval, we are 95% confident that the true proportion planning to vote for the candidate is between 53% and 87%. We are assuming that the distribution is normally distributed \\(N(\\mu, \\sigma ^2)\\). References "],
["references-1.html", "References", " References "],
["dimensional-modelling.html", "4 Dimensional Modelling 4.1 Introduction to Dimensional Data 4.2 Architecture considerations 4.3 Graphical Representations 4.4 Kimball Approach 4.5 Four-Step Dimensional Design Process 4.6 Tips", " 4 Dimensional Modelling 4.1 Introduction to Dimensional Data Dimensional modelling helps to build the ability for users to query the information, for instance analysing results by a geographic region. Multi-dimensional modelling is an extension to allowing multiple ways to analsye the information, by geographic region but also over time, by product or service, by store or office and so on. It provides a way for a system user, manager or analyst to navigate what information - the ‘information space’ - is available in a database or data warehouse, but at a more intuitive level (see Meridith 2017, lecture 4). The goal is to help understanding, exploration and to make better decisions. A dimension is simply a direction, usually query or analytically based, in which you can move. Dimensional modelling is different from Entity Relationship diagrams which are more typically used for database design, however they do share some similarities and are sometimes used for dimensional modelling particuarly by those from a database or IT background. The dimensions used therefore become the ways in which the end user wants to query the information. Typical terms used in the BI arena for helping to navigate this ‘information space’ include; ‘slice and dice’ meaning to make a large data space in to a smaller one (you are making a selection or subset of all the available data), ‘drill down’ meaning to go in to a lower level of a hierachy (moving from a geographic region to a particular store), ‘drill up’ meaning to go in to a higher level (sometimes called rolling-up) and ‘drill across’ meaning adding more data (or facts) about something, typically from another source (a different fact table). There are two slightly different interpretations of a dimensional model (Meridith 2017, lecture 4): OLAP: A dimension is a structural attribute of a data cube. A dimension acts as an index for identifying values in a multi-dimensional array Kimball: A dimension table are where the textual descriptions which relate to aspects of the business are stored In both instances however, they provide ways to interact and understand our information. There are two things we are typically trying to map: Facts: Data itself, values, sales and so on e.g. a sales transaction number and the products sold Dimensions: Different ways of presenting or quering the information, this is often in the form of attributes about the fact e.g. product specific and store details 4.1.1 Data Modelling levels There are three aspects of information with a Business Intelligence system - conceptual, logical and physical - which exist on a spectrum. Conceptual: The business needs are usually the high level conceptual solution, what things we want to include at a more abstract level Logical: We start thinking about what data to include in the model and what data is available, it starts giving something which can be implemented in to a warehouse Physical: The final solution which is usually then what is implemented in the data warehouse. It is the more technical/IT solution and may include normalisation (3NF or higher) and perhaps other database optimisations to improve performance of the system. In some instances, the conceptual and logical can become one and the same thing. Table 3.1: The three levels of data modelling Feature Conceptual Logical Logical Entity Names Y Y Entity Relationships Y Y Attributes Y Primary Keys Y Y Foreign Keys Y Y Table Names Y Column Names Y Column Data Types 4.2 Architecture considerations There are a number of different approaches to implementing a data warehouse, or Enterprise Data Warehouse (EDW) from the IT or technical perspective. However, all approaches use the dimensional data modelling technique. A full detailed explanation of all possible architecture approaches, including hybrid approaches, is not included here. Instead we discuss at a high level the three main approaches - Kimball Inmon and Data Valut - and touch on a couple of others. Where they differ in terms of data modelling in part depends on the location of the dimensional model. Kimball - as the last part of the Extract Transform and Load (ETL) process the data is structured and loaded in to the desired dimensional model(s). There is no EDW in the Kimball approach, instead the presentation area is where data is organized, stored, and made available for direct querying by users, report writers, and other analytical BI applications. Data is stored in the multi-dimensional views as different data marts, which are typically subsets of all the data originally extracted, perhaps for different business users or services Inmon - suggests that the data should be relationally designed. The data is stored in an EDW in third normal form (3NF). The dimensional model then transates the data from the EDW in to something for an end user, visualisation tool or other such BI tool, potentially including data marts. A Hub and Spoke system is often used to describe the approach, with the EDW being the hub and the spokes being the depdendent data marts. This helps to ensure a ‘single verison of the truth’ Data Vault - Centralised approach - similar to Inmon but without the dependent data marts (spokes). Users directly target the EDW and there may be many different dimensional data models Hybrid - there are various different ways this could be setup, however one way would be that data is still stored in the EDW, but the dimensional model is used to help structure the data in the EDW. Therefore the extra translation required from the EDW to a BI tool is reduced. In the Kimball approach when attributes in separate dimension tables have the same column names and domain contents. After validating the data for conformance with the defined one-to-one and many-to-one business rules [as part of the ETL processs], it may be pointless to take the final step of building a 3NF physical database, just before transforming the data once again into denormalized structures for the BI presentation area. (Kimball and Ross 2013, pg 20) For the kimball approach to work, so called ‘conformed dimensions’ must be developed which are said to conform when attributes in separate dimension tables have the same column names and domain contents (Kimball and Ross 2013, pg 51). Inmon sees that the dimensional modelling technique can cause problems when teams need different star schemas - dimensional models - which then lead to a need to combine the different joins together, or lead to issues of duplication and inconsistencies. simply doing dimensional modeling as a basis for data warehouse design leads down a dark path when multiple star joins are considered. It is never apparent that there is a problem with star joins when you are looking at just one star join. But when you look at multiple star joins, the limitations of dimensional modeling become apparent. (Inmon 2000) Inmon concludes that dimensional modelling is only really suitable for data marts (ibid). 4.3 Graphical Representations Figure 4.1: High Level Overview of a Data Warehouse (Schnider, Martino, and Eschermann 2014, pg 3) Figure 4.2: Star schema versus OLAP cube (Kimball and Ross 2013, pg 9) Figure 4.3: Star schema example (Kimball and Ross 2013, pg 16) Figure 4.4: Star and Snowflake Schemas (Sharda, Delan, and Turban 2014, pg 139) Figure 4.5: Example slices from a OLAP data cube (Sharda, Delan, and Turban 2014, pg 141) Figure 4.6: Star schema reporting (Kimball and Ross 2013, pg 17) 4.4 Kimball Approach Before work begins of the data modelling, it is neccessary to understand the needs of the business and the underlying data (Kimball and Ross 2013, pg 37). The business needs arise out of meetings with manangers, decision makers and other representatives of the business. Kimball also recommends meetings with ‘source system experts and doing high-level data profiling to assess data feasibilities’ (Kimball and Ross 2013, pg 38). Whilst the data modeller is ‘in charge’ the actual model should unfold via a series of interactive workshops with those business representatives. Data governance reps should also be involved to obtain buy-in. In this sense, the Kimball approach covers both the conceptual and physical, it may also include some considerations of physical level at initiation. 4.5 Four-Step Dimensional Design Process Kimball outlines four key decisions that are to be made during the design of a dimensional model include: Select the business process - the operational activities done by the business, these activities create the facts Declare the grain - what a single row represents. The atomic grain is the lowest data captured by the business, which is the ideal and can be aggregared (rolled-up) to other levels. Different grains must not be mixed in the same fact table Identify the dimensions - the descriptive attributes about the facts, to be used for analysis. Provide the “who, what, where, when, why, and how” (6W) context Identify the facts - the measurements (how many) from the business process, it should relate to a physical observable event, rather than reporting needs Typically the output of this process is a star schema, with a fact table at the centre supported by the associated dimension tables, with primary/forenigh key relationships. This is often then structured into a online analytical processing (OLAP) cube, which contains the facts and dimensions appropriate to the analysis, but allows for more detailed analytical capabilities than SQL. Sometimes aggregated fact tables are built to speed up query performance, as are aggregated OLAP cubes which are typically designed for users. A key advantage of the dimensional model approach is that new dimensions can be added to an existing fact table by adding a new foreign key column. Discussion then of * Thomsen diagrams OLAP Solutions (2nd ed) 2002, an abstract, but can be a little simple * ADAPT Diagrams, White Paper Bulos and Foresman - included in some Microsoft products such as Visio and SQL Server, a bit too technical but good for communicating to IT * BEAM/Agile approach 8 mins 4.6 Tips Think about the types of analysis or questions that the user or manager may want to ask. This will help structure the data and help to ensure nothing is missing At the same time, just because something exists in the organisation or in a data source does not mean it has to be included. You need to think about that to include and what to exclude Equally, there may be instances where there is a desire to add something in to the model but it does not currently exist. This should be flagged and discussed with those intending to use the BI tool / output What are the end uses of the system or systems? If there are potentially multiple systems, multiple teams and multiple views on the data, it may make sense to store the data in its original state (3NF) in the EDW or similar store, then do the dimensional mapping in the BI tool, so it can be customised to the audience (Meridith 2017, lecture 4). This can lead to some duplication, however an option might be to share the dimensional models in some central repository, allowing users to customise for their use, whilst still being able to share the same source data and the benefits this brings. Evidently this lends itself to an Inmon or other such approach and less so the Kimball approach References "],
["references-2.html", "References", " References "]
]
