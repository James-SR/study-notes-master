[
["index.html", "Study notes Preface", " Study notes James Solomon-Rounce Last updated:2017-08-23 Preface The following notes were taken by me for educational, non-commercial, purposes. If you find the information useful, buy the material/take the course. Thank you to the original content providers. Additional ramblings are my own. "],
["importing-data-part-1.html", "1 Importing data - Part 1 1.1 Introduction 1.2 Reading CSV files 1.3 Reading tab deliminated files or other table formats 1.4 Readr and data.table 1.5 Reading Excel files 1.6 XLConnect - read and write to excel", " 1 Importing data - Part 1 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 1)’ by Filip Schouwenaars. 1.1 Introduction Data often comes from many different sources and formats, including Flat files - simple tables e.g. csv Excel Databases - MySQL, Postgres Websites - APIs, JSON, scraping Other statistical software - SPSS, STATA, SAS 1.2 Reading CSV files Reading csv files can be achived with simple code like read.csv(&quot;file.csv&quot;, stringsAsFactors = FALSE) We may want to import strings as categorical variables, in which case we would set stringsAsFactors = TRUE which is also the default option, if not stated. When working across different machines or operating systems, problems can arise due to different ways of addressing file locations and differing file locations. Therefore, it can be easier to set a relative path to the users home directory, which would be achieved with the following code. path &lt;- file.path(&quot;~&quot;, &quot;datasets&quot;, &quot;file.csv&quot;) path ## [1] &quot;~/datasets/file.csv&quot; Then use the file path as before, assigning to a dataframe. df &lt;- read.csv(path, stringsAsFactors = FALSE) 1.3 Reading tab deliminated files or other table formats In a similar way to before, we add the path to the file and if we want strings as strings, for instance read.delim(&quot;file.csv&quot;, stringsAsFactors = FALSE) However, if the file comes in another format perhaps due to the system encoding or setup, it is still possible to try and read the file as a tabular formatting converting it to a data frame. To do so, we use the read.table() command which has a lot of arguments that can be customised. You can specify column names and types for instance. If for instance we have a file format where the objects are separated by a / rather than a comma or tab as before, we could use read.table(&quot;file.txt&quot;, header = TRUE, sep = &quot;/&quot;, stringsAsFactors = FALSE) Or, if you have a file which has no column/variable names and tabs as spaces, you would read the file as: # Path to the file.txt file: path path &lt;- file.path(&quot;data&quot;, &quot;file.txt&quot;) # Import the file.txt file: hotdogs file &lt;- read.table(path, sep = &quot;\\t&quot;, # specify seperator - tab in this instance col.names = c(&quot;VarName1&quot;, &quot;VarName2&quot;, &quot;VarName3&quot;), # specifiy variable names colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # specify the column/variable classes Both read.csv and read.delim are wrapper functions of read.table(), both use read.table but have different default options depending on the file type. There are two further wrapper functions - read.csv2 and read.delim2 - which deal with regional differences in formatting, notably that some areas use full stops as decimal places, whereas other areas use commas for decimal places. 1.4 Readr and data.table These two packages are other ways of reading in files. Readr uses the tibble, so will be compatible with other tidyverse packages such as dplyr. It is faster than utils, the r default and also prints out the column classes, depending on what other packages are loaded. It is not necessary to specify stringsAsFactors = FALSE. library(readr) read_csv(&quot;file.csv&quot;) #read comma seperated read_tsv(&quot;file2.txt&quot;) #read tab seperated files #If there are no row heads, you can create a vector then read it in using the col_names argument #specify the vector for column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) #read in the vector df &lt;- read_tsv(&quot;file3.txt&quot;, col_names = properties) Like the utils package, these are wrapper functions, with the base function being read_delim(). Unlike the utils package, read_delim() expects the first row to contain headers, so this doesn’t need to be explicit. As mentioned previously, it is also not necessary to specify the we don’t want strings as factors. You can specify col_names using a vector as before, or we can read them directly at the time. If we also want to explicitly state the column types, perahps because the automatically assigned variable is not correct, we can do so with col_type using abbreviations: c = character d = double i = integer n = number l = logical D = date T = date time t = time ? = guess _ = skip column (underscore) Finally, we can use skip and n_max to specify how many rows to skip at the beginning of a file, perhaps due to a large header, and the maximum now of rows to read, perhaps due to a very large file with many rows. read_delim(&quot;file4.txt&quot;, delim = &quot;/&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;)) read_delim(&quot;file5.txt&quot;, delim = &quot;/&quot;, col_types = &quot;ccid&quot;) read_delim(&quot;file6.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;), skip = 12, n_max = 50000) Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column. Look at the collector documentation for more details. Two examples are shown below, one for columns to be interpreted as integers and one for a column with factors. # The collectors needed for importing fac &lt;- col_factor(levels = c(&quot;Beef&quot;, &quot;Meat&quot;, &quot;Poultry&quot;)) int &lt;- col_integer() # Edit the col_types argument with the specified collectors hotdogs_factor &lt;- read_tsv(&quot;hotdogs.txt&quot;, col_names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), col_types = list(fac, int, int)) 1.4.1 data.table fread data.table is a tool for doing fast data analysis, particularly on large datasets. It also has a function to read data using the fread() command. It can automatically infer column names, types and separators. You can also drop or select columns at read time. df &lt;- fread(&quot;file7.csv&quot;, select = c(&quot;colname1&quot;, &quot;colname2&quot;)) The readr package fill create different dataframe types or object classes - ‘tbl_df’, ‘tbl’ and ‘data.frame’ - which can be useful for different purposes, such as for use in dplyr. Fread creates a data.table object class. 1.5 Reading Excel files There are many packages for reading Excel files, one package is the readxl package by Hadley Wickham. There are to main functions excel_sheets(): lists the sheets within an excel file or workbook read_excel(): import the data, unless specified the first sheet is read, this can either be done with sheet = 7, or sheet = “name”. So to read an urbanpop.xlsx file containing three sheets of urban populations, for different time frames, our code would look similar to that below. library(readxl) #list the sheerts in the file excel_sheets(&quot;urbanpop.xlsx&quot;) # Read the sheets, one by one pop_1 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 1) pop_2 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2) pop_3 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 3) # Put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) # IF we want to read all the files, a more efficient way to read all the files in the file uses lapply pop_list &lt;- lapply(excel_sheets(&quot;urbanpop.xlsx&quot;), read_excel, path = &quot;urbanpop.xlsx&quot;) There are other arguments that can be used with the read_excel() function: col_names: If true, the first row is read, if false R will assign it’s own names or you specify a charecter vector manually col_types: If NULL, R gueses the data types of the columns. Alternatively, they can be specified e.g. text, numeric, date, blank (which ignores the col) skip: Speficies the number of rows to ignore # Some examples # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = FALSE) # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = cols) # Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel urbanpop_sel &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2, col_names = FALSE, skip = 21) # Print out the first observation from urbanpop_sel urbanpop_sel[1,] 1.5.1 Alternatives for importing Excel files One alternative is the gdata package, which is a suite of tools for data. There is a read.xls() function which only, currently, supports XLS files although xlsx could be supported with a driver. The data is interpreted by the read.xls file using perl into a csv file, which is then read using the read.csv function - itself a offshoot of read.table, in to an R data frame. Hadley’s readxl package is faster, but is quite early in it’s development so some of the functions may change. For gdata, as it is an offshoot of read.table(), all of the same arguments can be used by read.xls(). 1.6 XLConnect - read and write to excel Most of the Excel tools can become accessible but inside R, using XLConnect. It is possible to use XLS and XLSX and it will create a ‘workbook’ object in R, but it does require Java to work. library(XLConnect) #create a connect to a file and list the sheets book &lt;- loadWorkbook(&quot;file.xlsx&quot;) getSheets(book) #read in the specific sheet but only the columns we are interested in wardData &lt;- readWorksheet(book, sheet = &quot;sheet_1&quot;, startCol = 3, endCol = 5) # read in the names column, previoulsy excluded wardNames &lt;- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1) #cbind the data and names together selection &lt;- cbind(wardNames, wardData) XLConnect has more features than simply reading sheets. It is possible to write data back to the Excel file also. We can add sheets, write or add data to sheets, rename and remove sheets. # Add a worksheet to my_book, named &quot;summary&quot; createSheet(my_book, &quot;summary&quot;) # Add data in summ to &quot;data_summary&quot; sheet writeWorksheet(my_book, summ, &quot;summary&quot;) # Save workbook as summary.xlsx saveWorkbook(my_book, &quot;summary.xlsx&quot;) # Rename &quot;summary&quot; sheet to &quot;data_summary&quot; renameSheet(my_book, sheet = 4, &quot;data_summary&quot;) # Remove the third sheet removeSheet(my_book, sheet = 3) "],
["importing-data-part-2.html", "2 Importing data - Part 2 2.1 Importing from Databases - 1 2.2 SQL Queries Inside R 2.3 Web Data 2.4 JSON and APIs 2.5 Importing from other statistical software", " 2 Importing data - Part 2 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 2)’ by Filip Schouwenaars. 2.1 Importing from Databases - 1 In a professional or commercial setting, you often deal with more complicated file structures and source systems that simple flat files. Often the data is stored in a DBMS or Database Management System and SQL is the usual way of quering the DBMS. As there can be slight differences, you are likely to need different packages, some include: MySQL: Use the RMySQL package PostgresSQL: Use the RPostgresSQL package Oracle: Use the ROracle (etc…) Conventions are specified in the DBI - another R package, DBI is the interface and the other packages are the implentation. Some of the packages will automaticlaly install the DBI package as well. To connect to a database we would so something like the following. # Load the DBI package library(DBI) ## Loading required package: methods # Edit dbConnect() call - the first part specifies how connections are map to the database con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Build a vector of table names: tables tables &lt;- dbListTables(con) # Display structure of tables str(tables) ## chr [1:3] &quot;comments&quot; &quot;tweats&quot; &quot;users&quot; # Import the users table from tweater: users users &lt;- dbReadTable(con, &quot;users&quot;) # Print users users ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja # Or we can import all tables using lapply tables &lt;- lapply(tables, dbReadTable, conn = con) # Print out tables tables ## [[1]] ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! ## 3 1011 49 5 love it ## 4 1012 87 1 awesome! thanks! ## 5 1010 88 6 yuck! ## 6 1026 77 4 not my thing! ## 7 1004 49 1 this is fabulous! ## 8 1030 75 6 so easy! ## 9 1025 88 2 oh yes ## 10 1007 49 3 serious? ## 11 1020 77 1 couldn&#39;t be better ## 12 1014 77 1 saved my day ## ## [[2]] ## id user_id ## 1 75 3 ## 2 88 4 ## 3 77 6 ## 4 87 5 ## 5 49 1 ## 6 24 7 ## post ## 1 break egg. bake egg. eat egg. ## 2 wash strawberries. add ice. blend. enjoy. ## 3 2 slices of bread. add cheese. grill. heaven. ## 4 open and crush avocado. add shrimps. perfect starter. ## 5 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 6 just eat an apple. simply and healthy. ## date ## 1 2015-09-05 ## 2 2015-09-14 ## 3 2015-09-21 ## 4 2015-09-22 ## 5 2015-09-22 ## 6 2015-09-24 ## ## [[3]] ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja 2.2 SQL Queries Inside R OFten you don’t want an entire tabel from a database, but a selection from the table. You can use SQL queries from inside R to extract only what you are interested in. You can alternatively use subset on the imported table, but often it is easier to extract only what you need first, particularly when working with large databases. The SQL goes inside e.g. dbGetQuery(con, “SQL QUERY”). # Connect to the database library(DBI) con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth &lt;- dbGetQuery(con, &quot;SELECT tweat_id FROM comments WHERE user_id = 1&quot;) # Print elisabeth elisabeth ## tweat_id ## 1 87 ## 2 49 ## 3 77 ## 4 77 # Import post column of tweats where date is higher than &#39;2015-09-21&#39;: latest latest &lt;- dbGetQuery(con, &quot;SELECT post FROM tweats WHERE date &gt; &#39;2015-09-21&#39;&quot;) # Print latest latest ## post ## 1 open and crush avocado. add shrimps. perfect starter. ## 2 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 3 just eat an apple. simply and healthy. # Create data frame specific using boolean specific &lt;- dbGetQuery(con, &quot;SELECT message FROM comments WHERE tweat_id = 77 AND user_id &gt; 4&quot;) # Print specific specific ## message ## 1 great! # Create data frame short selecting two columns short &lt;- dbGetQuery(con, &quot;SELECT id, name FROM users WHERE CHAR_LENGTH(name) &lt; 5&quot;) # Print short short ## id name ## 1 2 mike ## 2 3 thea ## 3 6 kate # We can also join elements from different tables using the same id/key dbGetQuery(con, &quot;SELECT post, message FROM tweats INNER JOIN comments on tweats.id = tweat_id WHERE tweat_id = 77&quot;) ## post message ## 1 2 slices of bread. add cheese. grill. heaven. great! ## 2 2 slices of bread. add cheese. grill. heaven. not my thing! ## 3 2 slices of bread. add cheese. grill. heaven. couldn&#39;t be better ## 4 2 slices of bread. add cheese. grill. heaven. saved my day You’ve used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed: Sending the specified query with dbSendQuery(); Fetching the result of executing the query on the database with dbFetch(); Clearing the result with dbClearResult(). Let’s not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query’s result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch(). It is important to close the connection to the database once complete using the dbDisconnect() function # Send query to the database res &lt;- dbSendQuery(con, &quot;SELECT * FROM comments WHERE user_id &gt; 4&quot;) # Use dbFetch() twice dbFetch(res, n = 2) ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! dbFetch(res) # imports all ## id tweat_id user_id message ## 1 1011 49 5 love it ## 2 1010 88 6 yuck! ## 3 1030 75 6 so easy! # Clear res dbClearResult(res) ## [1] TRUE # Create the data frame long_tweats long_tweats &lt;- dbGetQuery(con, &quot;SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) &gt; 40&quot;) # Print long_tweats print(long_tweats) ## post ## 1 wash strawberries. add ice. blend. enjoy. ## 2 2 slices of bread. add cheese. grill. heaven. ## 3 open and crush avocado. add shrimps. perfect starter. ## 4 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## date ## 1 2015-09-14 ## 2 2015-09-21 ## 3 2015-09-22 ## 4 2015-09-22 # Disconnect from the database dbDisconnect(con) ## [1] TRUE 2.3 Web Data HyperText Transfer Protocol (HTTP) is the ‘language of the web’ and consists of a set of rules about data exchange between computers. If the file is a csv file, we can use functions like read.csv() and add in the url in quotations marks, read.csv will recognise this is a URL and will issue a HTTP GET command to download the file. This will also work on https sites on newer versions of R. We can also use the readr package and other packages. # Load the readr package library(readr) # Import the csv file: pools url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; pools &lt;- read_csv(url_csv) ## Parsed with column specification: ## cols( ## Name = col_character(), ## Address = col_character(), ## Latitude = col_double(), ## Longitude = col_double() ## ) # Import the txt file: potatoes url_delim &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt&quot; potatoes &lt;- read_tsv(url_delim) ## Parsed with column specification: ## cols( ## area = col_integer(), ## temp = col_integer(), ## size = col_integer(), ## storage = col_integer(), ## method = col_integer(), ## texture = col_double(), ## flavor = col_double(), ## moistness = col_double() ## ) # Print pools and potatoes pools ## # A tibble: 20 x 4 ## Name ## &lt;chr&gt; ## 1 Acacia Ridge Leisure Centre ## 2 Bellbowrie Pool ## 3 Carole Park ## 4 Centenary Pool (inner City) ## 5 Chermside Pool ## 6 Colmslie Pool (Morningside) ## 7 Spring Hill Baths (inner City) ## 8 Dunlop Park Pool (Corinda) ## 9 Fortitude Valley Pool ## 10 Hibiscus Sports Complex (upper MtGravatt) ## 11 Ithaca Pool ( Paddington) ## 12 Jindalee Pool ## 13 Manly Pool ## 14 Mt Gravatt East Aquatic Centre ## 15 Musgrave Park Pool (South Brisbane) ## 16 Newmarket Pool ## 17 Runcorn Pool ## 18 Sandgate Pool ## 19 Langlands Parks Pool (Stones Corner) ## 20 Yeronga Park Pool ## # ... with 3 more variables: Address &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt; potatoes ## # A tibble: 160 x 8 ## area temp size storage method texture flavor moistness ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3.0 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3.0 1.7 ## 7 1 1 1 2 2 2.6 3.1 2.4 ## 8 1 1 1 2 3 3.0 3.0 2.9 ## 9 1 1 1 2 4 2.2 3.2 2.5 ## 10 1 1 1 2 5 2.0 2.8 1.9 ## # ... with 150 more rows # https URL to the swimming_pools csv file. url_csv &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; # Import the file using read.csv(): pools1 pools1 &lt;- read.csv(url_csv) str(pools1) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : Factor w/ 20 levels &quot;Acacia Ridge Leisure Centre&quot;,..: 1 2 3 4 5 6 19 7 8 9 ... ## $ Address : Factor w/ 20 levels &quot;1 Fairlead Crescent, Manly&quot;,..: 5 20 18 10 9 11 6 15 12 17 ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... Some packages, like the readxl package, do not currently recognise urls. However, we can use the donwload.file() or other command to download the file and then read it in locally. This process can be much quicker that browsing the internet then downloading the file. library(readxl) # Specification of url: url_xls url_xls &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls&quot; # Download file behind URL, name it local_latitude.xls download.file(url_xls, destfile = &quot;local_latitude.xls&quot;) # Import the local .xls file with readxl: excel_readxl excel_readxl &lt;- read_excel(&quot;local_latitude.xls&quot;) # https URL to the wine RData file. url_rdata &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData&quot; # Download the wine file to your working directory download.file(url_rdata, destfile = &quot;wine_local.RData&quot;) # Load the wine data into your workspace using load() load(&quot;wine_local.RData&quot;) # Print out the summary of the wine data summary(wine) ## Alcohol Malic acid Ash Alcalinity of ash ## Min. :11.03 Min. :0.74 Min. :1.360 Min. :10.60 ## 1st Qu.:12.36 1st Qu.:1.60 1st Qu.:2.210 1st Qu.:17.20 ## Median :13.05 Median :1.87 Median :2.360 Median :19.50 ## Mean :12.99 Mean :2.34 Mean :2.366 Mean :19.52 ## 3rd Qu.:13.67 3rd Qu.:3.10 3rd Qu.:2.560 3rd Qu.:21.50 ## Max. :14.83 Max. :5.80 Max. :3.230 Max. :30.00 ## Magnesium Total phenols Flavanoids Nonflavanoid phenols ## Min. : 70.00 Min. :0.980 Min. :0.340 Min. :0.1300 ## 1st Qu.: 88.00 1st Qu.:1.740 1st Qu.:1.200 1st Qu.:0.2700 ## Median : 98.00 Median :2.350 Median :2.130 Median :0.3400 ## Mean : 99.59 Mean :2.292 Mean :2.023 Mean :0.3623 ## 3rd Qu.:107.00 3rd Qu.:2.800 3rd Qu.:2.860 3rd Qu.:0.4400 ## Max. :162.00 Max. :3.880 Max. :5.080 Max. :0.6600 ## Proanthocyanins Color intensity Hue Proline ## Min. :0.410 Min. : 1.280 Min. :1.270 Min. : 278.0 ## 1st Qu.:1.250 1st Qu.: 3.210 1st Qu.:1.930 1st Qu.: 500.0 ## Median :1.550 Median : 4.680 Median :2.780 Median : 672.0 ## Mean :1.587 Mean : 5.055 Mean :2.604 Mean : 745.1 ## 3rd Qu.:1.950 3rd Qu.: 6.200 3rd Qu.:3.170 3rd Qu.: 985.0 ## Max. :3.580 Max. :13.000 Max. :4.000 Max. :1680.0 We can also read http content using the httr package. This includes JSON formatted text, which httr will convert to a named list. # Load the httr package library(httr) # Get the url, save response to resp url &lt;- &quot;http://www.example.com/&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.example.com/] ## Date: 2017-08-23 15:17 ## Status: 200 ## Content-Type: text/html ## Size: 1.27 kB ## &lt;!doctype html&gt; ## &lt;html&gt; ## &lt;head&gt; ## &lt;title&gt;Example Domain&lt;/title&gt; ## ## &lt;meta charset=&quot;utf-8&quot; /&gt; ## &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; ## &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt; ## &lt;style type=&quot;text/css&quot;&gt; ## body { ## ... # Get the raw content of resp: raw_content raw_content &lt;- content(resp, as = &quot;raw&quot;) # Print the head of raw_content head(raw_content) ## [1] 3c 21 64 6f 63 74 # JSON formatted # Get the url url &lt;- &quot;http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json] ## Date: 2017-08-23 15:17 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 942 B # Print content of resp as text content(resp, as = &quot;text&quot;) ## [1] &quot;{\\&quot;Title\\&quot;:\\&quot;Annie Hall\\&quot;,\\&quot;Year\\&quot;:\\&quot;1977\\&quot;,\\&quot;Rated\\&quot;:\\&quot;PG\\&quot;,\\&quot;Released\\&quot;:\\&quot;20 Apr 1977\\&quot;,\\&quot;Runtime\\&quot;:\\&quot;93 min\\&quot;,\\&quot;Genre\\&quot;:\\&quot;Comedy, Romance\\&quot;,\\&quot;Director\\&quot;:\\&quot;Woody Allen\\&quot;,\\&quot;Writer\\&quot;:\\&quot;Woody Allen, Marshall Brickman\\&quot;,\\&quot;Actors\\&quot;:\\&quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane\\&quot;,\\&quot;Plot\\&quot;:\\&quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.\\&quot;,\\&quot;Language\\&quot;:\\&quot;English, German\\&quot;,\\&quot;Country\\&quot;:\\&quot;USA\\&quot;,\\&quot;Awards\\&quot;:\\&quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.\\&quot;,\\&quot;Poster\\&quot;:\\&quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg\\&quot;,\\&quot;Ratings\\&quot;:[{\\&quot;Source\\&quot;:\\&quot;Internet Movie Database\\&quot;,\\&quot;Value\\&quot;:\\&quot;8.1/10\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Rotten Tomatoes\\&quot;,\\&quot;Value\\&quot;:\\&quot;97%\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Metacritic\\&quot;,\\&quot;Value\\&quot;:\\&quot;92/100\\&quot;}],\\&quot;Metascore\\&quot;:\\&quot;92\\&quot;,\\&quot;imdbRating\\&quot;:\\&quot;8.1\\&quot;,\\&quot;imdbVotes\\&quot;:\\&quot;211,761\\&quot;,\\&quot;imdbID\\&quot;:\\&quot;tt0075686\\&quot;,\\&quot;Type\\&quot;:\\&quot;movie\\&quot;,\\&quot;DVD\\&quot;:\\&quot;28 Apr 1998\\&quot;,\\&quot;BoxOffice\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Production\\&quot;:\\&quot;United Artists\\&quot;,\\&quot;Website\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Response\\&quot;:\\&quot;True\\&quot;}&quot; # Print content of resp content(resp) ## $Title ## [1] &quot;Annie Hall&quot; ## ## $Year ## [1] &quot;1977&quot; ## ## $Rated ## [1] &quot;PG&quot; ## ## $Released ## [1] &quot;20 Apr 1977&quot; ## ## $Runtime ## [1] &quot;93 min&quot; ## ## $Genre ## [1] &quot;Comedy, Romance&quot; ## ## $Director ## [1] &quot;Woody Allen&quot; ## ## $Writer ## [1] &quot;Woody Allen, Marshall Brickman&quot; ## ## $Actors ## [1] &quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane&quot; ## ## $Plot ## [1] &quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.&quot; ## ## $Language ## [1] &quot;English, German&quot; ## ## $Country ## [1] &quot;USA&quot; ## ## $Awards ## [1] &quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.&quot; ## ## $Poster ## [1] &quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg&quot; ## ## $Ratings ## $Ratings[[1]] ## $Ratings[[1]]$Source ## [1] &quot;Internet Movie Database&quot; ## ## $Ratings[[1]]$Value ## [1] &quot;8.1/10&quot; ## ## ## $Ratings[[2]] ## $Ratings[[2]]$Source ## [1] &quot;Rotten Tomatoes&quot; ## ## $Ratings[[2]]$Value ## [1] &quot;97%&quot; ## ## ## $Ratings[[3]] ## $Ratings[[3]]$Source ## [1] &quot;Metacritic&quot; ## ## $Ratings[[3]]$Value ## [1] &quot;92/100&quot; ## ## ## ## $Metascore ## [1] &quot;92&quot; ## ## $imdbRating ## [1] &quot;8.1&quot; ## ## $imdbVotes ## [1] &quot;211,761&quot; ## ## $imdbID ## [1] &quot;tt0075686&quot; ## ## $Type ## [1] &quot;movie&quot; ## ## $DVD ## [1] &quot;28 Apr 1998&quot; ## ## $BoxOffice ## [1] &quot;N/A&quot; ## ## $Production ## [1] &quot;United Artists&quot; ## ## $Website ## [1] &quot;N/A&quot; ## ## $Response ## [1] &quot;True&quot; 2.4 JSON and APIs JSON is both easy for machines to parse and generate and is human readable. APIs are programtical ways of getting data, consisting of a set of protocols to interact with some other system or database. JSON can be useful since it is often well structured and can save time over, say, parsing a html page. So for instance, you can use the OMDb API to return JSON formatted text about a movie, rather than parse an IMDB html page entry. One package for handling JSON in R is jsonlite. library(jsonlite) # wine_json is a JSON wine_json &lt;- &#39;{&quot;name&quot;:&quot;Chateau Migraine&quot;, &quot;year&quot;:1997, &quot;alcohol_pct&quot;:12.4, &quot;color&quot;:&quot;red&quot;, &quot;awarded&quot;:false}&#39; # Convert wine_json into a list: wine wine &lt;- fromJSON(wine_json) # Print structure of wine str(wine) ## List of 5 ## $ name : chr &quot;Chateau Migraine&quot; ## $ year : int 1997 ## $ alcohol_pct: num 12.4 ## $ color : chr &quot;red&quot; ## $ awarded : logi FALSE There are two types of JSON structures JSON objects - has key value pairs e.g. name:James, age:21 etc JSON arrays - a sequence of values, numbers, nulls e.g. 4, “a”, 10, false, null etc You can also nest JSON objects or arrays within each other. Some examples are below. YOu can also use the minify and prettify functions to convert a JSON string to a more compact of easier to read version. Similar functions can also be used inside the toJSON() function e.g. toJSON(x, pretty = TRUE) # Challenge 1 json1 &lt;- &#39;[1, 2, 3, 4, 5, 6]&#39; fromJSON(json1) ## [1] 1 2 3 4 5 6 # Challenge 2 json2 &lt;- &#39;{&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]}&#39; fromJSON(json2) ## $a ## [1] 1 2 3 ## ## $b ## [1] 4 5 6 # You can also convert data to JSON from other formats. Here we take a csv and format it into a JSON array # URL pointing to the .csv file url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv&quot; # Import the .csv file located at url_csv water &lt;- read.csv(url_csv, stringsAsFactors = FALSE) # Convert the data file according to the requirements water_json &lt;- toJSON(water) # Print out water_json water_json ## [{&quot;water&quot;:&quot;Algeria&quot;,&quot;X1992&quot;:0.064,&quot;X2002&quot;:0.017},{&quot;water&quot;:&quot;American Samoa&quot;},{&quot;water&quot;:&quot;Angola&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Antigua and Barbuda&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Argentina&quot;,&quot;X1992&quot;:0.0007,&quot;X1997&quot;:0.0007,&quot;X2002&quot;:0.0007},{&quot;water&quot;:&quot;Australia&quot;,&quot;X1992&quot;:0.0298,&quot;X2002&quot;:0.0298},{&quot;water&quot;:&quot;Austria&quot;,&quot;X1992&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Bahamas&quot;,&quot;X1992&quot;:0.0013,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Bahrain&quot;,&quot;X1992&quot;:0.0441,&quot;X2002&quot;:0.0441,&quot;X2007&quot;:0.1024},{&quot;water&quot;:&quot;Barbados&quot;,&quot;X2007&quot;:0.0146},{&quot;water&quot;:&quot;British Virgin Islands&quot;,&quot;X2007&quot;:0.0042},{&quot;water&quot;:&quot;Canada&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cape Verde&quot;,&quot;X1992&quot;:0.002,&quot;X1997&quot;:0.0017},{&quot;water&quot;:&quot;Cayman Islands&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Central African Rep.&quot;},{&quot;water&quot;:&quot;Chile&quot;,&quot;X1992&quot;:0.0048,&quot;X2002&quot;:0.0048},{&quot;water&quot;:&quot;Colombia&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cuba&quot;,&quot;X1992&quot;:0.0069,&quot;X1997&quot;:0.0069,&quot;X2002&quot;:0.0069},{&quot;water&quot;:&quot;Cyprus&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.0335},{&quot;water&quot;:&quot;Czech Rep.&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Denmark&quot;,&quot;X1992&quot;:0.015,&quot;X2002&quot;:0.015},{&quot;water&quot;:&quot;Djibouti&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Ecuador&quot;,&quot;X1992&quot;:0.0022,&quot;X1997&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Egypt&quot;,&quot;X1992&quot;:0.025,&quot;X1997&quot;:0.025,&quot;X2002&quot;:0.1},{&quot;water&quot;:&quot;El Salvador&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Finland&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;France&quot;,&quot;X1992&quot;:0.0117,&quot;X2002&quot;:0.0117},{&quot;water&quot;:&quot;Gibraltar&quot;,&quot;X1992&quot;:0.0077},{&quot;water&quot;:&quot;Greece&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01},{&quot;water&quot;:&quot;Honduras&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Hungary&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;India&quot;,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Indonesia&quot;,&quot;X1992&quot;:0.0187,&quot;X2002&quot;:0.0187},{&quot;water&quot;:&quot;Iran&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.003,&quot;X2007&quot;:0.2},{&quot;water&quot;:&quot;Iraq&quot;,&quot;X1997&quot;:0.0074,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Ireland&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Israel&quot;,&quot;X1992&quot;:0.0256,&quot;X2002&quot;:0.0256,&quot;X2007&quot;:0.14},{&quot;water&quot;:&quot;Italy&quot;,&quot;X1992&quot;:0.0973,&quot;X2002&quot;:0.0973},{&quot;water&quot;:&quot;Jamaica&quot;,&quot;X1992&quot;:0.0005,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Japan&quot;,&quot;X1997&quot;:0.04,&quot;X2002&quot;:0.04},{&quot;water&quot;:&quot;Jordan&quot;,&quot;X1997&quot;:0.002,&quot;X2007&quot;:0.0098},{&quot;water&quot;:&quot;Kazakhstan&quot;,&quot;X1997&quot;:1.328,&quot;X2002&quot;:1.328},{&quot;water&quot;:&quot;Kuwait&quot;,&quot;X1992&quot;:0.507,&quot;X1997&quot;:0.231,&quot;X2002&quot;:0.4202},{&quot;water&quot;:&quot;Lebanon&quot;,&quot;X2007&quot;:0.0473},{&quot;water&quot;:&quot;Libya&quot;,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Malaysia&quot;,&quot;X1992&quot;:0.0043,&quot;X2002&quot;:0.0043},{&quot;water&quot;:&quot;Maldives&quot;,&quot;X1992&quot;:0.0004},{&quot;water&quot;:&quot;Malta&quot;,&quot;X1992&quot;:0.024,&quot;X1997&quot;:0.031,&quot;X2002&quot;:0.031},{&quot;water&quot;:&quot;Marshall Islands&quot;,&quot;X1992&quot;:0.0007},{&quot;water&quot;:&quot;Mauritania&quot;,&quot;X1992&quot;:0.002,&quot;X2002&quot;:0.002},{&quot;water&quot;:&quot;Mexico&quot;,&quot;X1992&quot;:0.0307,&quot;X2002&quot;:0.0307},{&quot;water&quot;:&quot;Morocco&quot;,&quot;X1992&quot;:0.0034,&quot;X1997&quot;:0.0034,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Namibia&quot;,&quot;X1992&quot;:0.0003,&quot;X2002&quot;:0.0003},{&quot;water&quot;:&quot;Netherlands Antilles&quot;,&quot;X1992&quot;:0.063},{&quot;water&quot;:&quot;Nicaragua&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Nigeria&quot;,&quot;X1992&quot;:0.003,&quot;X2002&quot;:0.003},{&quot;water&quot;:&quot;Norway&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Oman&quot;,&quot;X1997&quot;:0.034,&quot;X2002&quot;:0.034,&quot;X2007&quot;:0.109},{&quot;water&quot;:&quot;Peru&quot;,&quot;X1992&quot;:0.0054,&quot;X2002&quot;:0.0054},{&quot;water&quot;:&quot;Poland&quot;,&quot;X1992&quot;:0.007,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Portugal&quot;,&quot;X1992&quot;:0.0016,&quot;X2002&quot;:0.0016},{&quot;water&quot;:&quot;Qatar&quot;,&quot;X1992&quot;:0.065,&quot;X1997&quot;:0.099,&quot;X2002&quot;:0.099,&quot;X2007&quot;:0.18},{&quot;water&quot;:&quot;Saudi Arabia&quot;,&quot;X1992&quot;:0.683,&quot;X1997&quot;:0.727,&quot;X2002&quot;:0.863,&quot;X2007&quot;:1.033},{&quot;water&quot;:&quot;Senegal&quot;,&quot;X1992&quot;:0,&quot;X2002&quot;:0},{&quot;water&quot;:&quot;Somalia&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;South Africa&quot;,&quot;X1992&quot;:0.018,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Spain&quot;,&quot;X1992&quot;:0.1002,&quot;X2002&quot;:0.1002},{&quot;water&quot;:&quot;Sudan&quot;,&quot;X1992&quot;:0.0004,&quot;X1997&quot;:0.0004,&quot;X2002&quot;:0.0004},{&quot;water&quot;:&quot;Sweden&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Trinidad and Tobago&quot;,&quot;X2007&quot;:0.036},{&quot;water&quot;:&quot;Tunisia&quot;,&quot;X1992&quot;:0.008,&quot;X2002&quot;:0.013},{&quot;water&quot;:&quot;Turkey&quot;,&quot;X1992&quot;:0.0005,&quot;X2002&quot;:0.0005,&quot;X2007&quot;:0.0005},{&quot;water&quot;:&quot;United Arab Emirates&quot;,&quot;X1992&quot;:0.163,&quot;X1997&quot;:0.385,&quot;X2007&quot;:0.95},{&quot;water&quot;:&quot;United Kingdom&quot;,&quot;X1992&quot;:0.0333,&quot;X2002&quot;:0.0333},{&quot;water&quot;:&quot;United States&quot;,&quot;X1992&quot;:0.58,&quot;X2002&quot;:0.58},{&quot;water&quot;:&quot;Venezuela&quot;,&quot;X1992&quot;:0.0052,&quot;X2002&quot;:0.0052},{&quot;water&quot;:&quot;Yemen, Rep.&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01}] # Convert mtcars to a pretty JSON: pretty_json pretty_json &lt;- toJSON(mtcars, pretty = TRUE) # Print pretty_json pretty_json ## [ ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.62, ## &quot;qsec&quot;: 16.46, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4&quot; ## }, ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.875, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4 Wag&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 108, ## &quot;hp&quot;: 93, ## &quot;drat&quot;: 3.85, ## &quot;wt&quot;: 2.32, ## &quot;qsec&quot;: 18.61, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Datsun 710&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 258, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.215, ## &quot;qsec&quot;: 19.44, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Hornet 4 Drive&quot; ## }, ## { ## &quot;mpg&quot;: 18.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Hornet Sportabout&quot; ## }, ## { ## &quot;mpg&quot;: 18.1, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 225, ## &quot;hp&quot;: 105, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.46, ## &quot;qsec&quot;: 20.22, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Valiant&quot; ## }, ## { ## &quot;mpg&quot;: 14.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.21, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 15.84, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Duster 360&quot; ## }, ## { ## &quot;mpg&quot;: 24.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 146.7, ## &quot;hp&quot;: 62, ## &quot;drat&quot;: 3.69, ## &quot;wt&quot;: 3.19, ## &quot;qsec&quot;: 20, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 240D&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 140.8, ## &quot;hp&quot;: 95, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.15, ## &quot;qsec&quot;: 22.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 230&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.3, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280&quot; ## }, ## { ## &quot;mpg&quot;: 17.8, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280C&quot; ## }, ## { ## &quot;mpg&quot;: 16.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 4.07, ## &quot;qsec&quot;: 17.4, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SE&quot; ## }, ## { ## &quot;mpg&quot;: 17.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.73, ## &quot;qsec&quot;: 17.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SL&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.78, ## &quot;qsec&quot;: 18, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SLC&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 472, ## &quot;hp&quot;: 205, ## &quot;drat&quot;: 2.93, ## &quot;wt&quot;: 5.25, ## &quot;qsec&quot;: 17.98, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Cadillac Fleetwood&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 460, ## &quot;hp&quot;: 215, ## &quot;drat&quot;: 3, ## &quot;wt&quot;: 5.424, ## &quot;qsec&quot;: 17.82, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Lincoln Continental&quot; ## }, ## { ## &quot;mpg&quot;: 14.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 440, ## &quot;hp&quot;: 230, ## &quot;drat&quot;: 3.23, ## &quot;wt&quot;: 5.345, ## &quot;qsec&quot;: 17.42, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Chrysler Imperial&quot; ## }, ## { ## &quot;mpg&quot;: 32.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 78.7, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 2.2, ## &quot;qsec&quot;: 19.47, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat 128&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 75.7, ## &quot;hp&quot;: 52, ## &quot;drat&quot;: 4.93, ## &quot;wt&quot;: 1.615, ## &quot;qsec&quot;: 18.52, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Honda Civic&quot; ## }, ## { ## &quot;mpg&quot;: 33.9, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 71.1, ## &quot;hp&quot;: 65, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 1.835, ## &quot;qsec&quot;: 19.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corolla&quot; ## }, ## { ## &quot;mpg&quot;: 21.5, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.1, ## &quot;hp&quot;: 97, ## &quot;drat&quot;: 3.7, ## &quot;wt&quot;: 2.465, ## &quot;qsec&quot;: 20.01, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corona&quot; ## }, ## { ## &quot;mpg&quot;: 15.5, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 318, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.52, ## &quot;qsec&quot;: 16.87, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Dodge Challenger&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 304, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.435, ## &quot;qsec&quot;: 17.3, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;AMC Javelin&quot; ## }, ## { ## &quot;mpg&quot;: 13.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 350, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.73, ## &quot;wt&quot;: 3.84, ## &quot;qsec&quot;: 15.41, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Camaro Z28&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 400, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.845, ## &quot;qsec&quot;: 17.05, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Pontiac Firebird&quot; ## }, ## { ## &quot;mpg&quot;: 27.3, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 79, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 1.935, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat X1-9&quot; ## }, ## { ## &quot;mpg&quot;: 26, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.3, ## &quot;hp&quot;: 91, ## &quot;drat&quot;: 4.43, ## &quot;wt&quot;: 2.14, ## &quot;qsec&quot;: 16.7, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Porsche 914-2&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 95.1, ## &quot;hp&quot;: 113, ## &quot;drat&quot;: 3.77, ## &quot;wt&quot;: 1.513, ## &quot;qsec&quot;: 16.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Lotus Europa&quot; ## }, ## { ## &quot;mpg&quot;: 15.8, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 351, ## &quot;hp&quot;: 264, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 3.17, ## &quot;qsec&quot;: 14.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Ford Pantera L&quot; ## }, ## { ## &quot;mpg&quot;: 19.7, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 145, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.62, ## &quot;wt&quot;: 2.77, ## &quot;qsec&quot;: 15.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 6, ## &quot;_row&quot;: &quot;Ferrari Dino&quot; ## }, ## { ## &quot;mpg&quot;: 15, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 301, ## &quot;hp&quot;: 335, ## &quot;drat&quot;: 3.54, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 14.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 8, ## &quot;_row&quot;: &quot;Maserati Bora&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 121, ## &quot;hp&quot;: 109, ## &quot;drat&quot;: 4.11, ## &quot;wt&quot;: 2.78, ## &quot;qsec&quot;: 18.6, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Volvo 142E&quot; ## } ## ] # Minify pretty_json: mini_json mini_json &lt;- minify(pretty_json) # Print mini_json mini_json ## [{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.62,&quot;qsec&quot;:16.46,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4&quot;},{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.875,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4 Wag&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:108,&quot;hp&quot;:93,&quot;drat&quot;:3.85,&quot;wt&quot;:2.32,&quot;qsec&quot;:18.61,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Datsun 710&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:6,&quot;disp&quot;:258,&quot;hp&quot;:110,&quot;drat&quot;:3.08,&quot;wt&quot;:3.215,&quot;qsec&quot;:19.44,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Hornet 4 Drive&quot;},{&quot;mpg&quot;:18.7,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:175,&quot;drat&quot;:3.15,&quot;wt&quot;:3.44,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Hornet Sportabout&quot;},{&quot;mpg&quot;:18.1,&quot;cyl&quot;:6,&quot;disp&quot;:225,&quot;hp&quot;:105,&quot;drat&quot;:2.76,&quot;wt&quot;:3.46,&quot;qsec&quot;:20.22,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Valiant&quot;},{&quot;mpg&quot;:14.3,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:245,&quot;drat&quot;:3.21,&quot;wt&quot;:3.57,&quot;qsec&quot;:15.84,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Duster 360&quot;},{&quot;mpg&quot;:24.4,&quot;cyl&quot;:4,&quot;disp&quot;:146.7,&quot;hp&quot;:62,&quot;drat&quot;:3.69,&quot;wt&quot;:3.19,&quot;qsec&quot;:20,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 240D&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:140.8,&quot;hp&quot;:95,&quot;drat&quot;:3.92,&quot;wt&quot;:3.15,&quot;qsec&quot;:22.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 230&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.3,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280&quot;},{&quot;mpg&quot;:17.8,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280C&quot;},{&quot;mpg&quot;:16.4,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:4.07,&quot;qsec&quot;:17.4,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SE&quot;},{&quot;mpg&quot;:17.3,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.73,&quot;qsec&quot;:17.6,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SL&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.78,&quot;qsec&quot;:18,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SLC&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:472,&quot;hp&quot;:205,&quot;drat&quot;:2.93,&quot;wt&quot;:5.25,&quot;qsec&quot;:17.98,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Cadillac Fleetwood&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:460,&quot;hp&quot;:215,&quot;drat&quot;:3,&quot;wt&quot;:5.424,&quot;qsec&quot;:17.82,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Lincoln Continental&quot;},{&quot;mpg&quot;:14.7,&quot;cyl&quot;:8,&quot;disp&quot;:440,&quot;hp&quot;:230,&quot;drat&quot;:3.23,&quot;wt&quot;:5.345,&quot;qsec&quot;:17.42,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Chrysler Imperial&quot;},{&quot;mpg&quot;:32.4,&quot;cyl&quot;:4,&quot;disp&quot;:78.7,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:2.2,&quot;qsec&quot;:19.47,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat 128&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:75.7,&quot;hp&quot;:52,&quot;drat&quot;:4.93,&quot;wt&quot;:1.615,&quot;qsec&quot;:18.52,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Honda Civic&quot;},{&quot;mpg&quot;:33.9,&quot;cyl&quot;:4,&quot;disp&quot;:71.1,&quot;hp&quot;:65,&quot;drat&quot;:4.22,&quot;wt&quot;:1.835,&quot;qsec&quot;:19.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corolla&quot;},{&quot;mpg&quot;:21.5,&quot;cyl&quot;:4,&quot;disp&quot;:120.1,&quot;hp&quot;:97,&quot;drat&quot;:3.7,&quot;wt&quot;:2.465,&quot;qsec&quot;:20.01,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corona&quot;},{&quot;mpg&quot;:15.5,&quot;cyl&quot;:8,&quot;disp&quot;:318,&quot;hp&quot;:150,&quot;drat&quot;:2.76,&quot;wt&quot;:3.52,&quot;qsec&quot;:16.87,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Dodge Challenger&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:304,&quot;hp&quot;:150,&quot;drat&quot;:3.15,&quot;wt&quot;:3.435,&quot;qsec&quot;:17.3,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;AMC Javelin&quot;},{&quot;mpg&quot;:13.3,&quot;cyl&quot;:8,&quot;disp&quot;:350,&quot;hp&quot;:245,&quot;drat&quot;:3.73,&quot;wt&quot;:3.84,&quot;qsec&quot;:15.41,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Camaro Z28&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:8,&quot;disp&quot;:400,&quot;hp&quot;:175,&quot;drat&quot;:3.08,&quot;wt&quot;:3.845,&quot;qsec&quot;:17.05,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Pontiac Firebird&quot;},{&quot;mpg&quot;:27.3,&quot;cyl&quot;:4,&quot;disp&quot;:79,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:1.935,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat X1-9&quot;},{&quot;mpg&quot;:26,&quot;cyl&quot;:4,&quot;disp&quot;:120.3,&quot;hp&quot;:91,&quot;drat&quot;:4.43,&quot;wt&quot;:2.14,&quot;qsec&quot;:16.7,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Porsche 914-2&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:95.1,&quot;hp&quot;:113,&quot;drat&quot;:3.77,&quot;wt&quot;:1.513,&quot;qsec&quot;:16.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Lotus Europa&quot;},{&quot;mpg&quot;:15.8,&quot;cyl&quot;:8,&quot;disp&quot;:351,&quot;hp&quot;:264,&quot;drat&quot;:4.22,&quot;wt&quot;:3.17,&quot;qsec&quot;:14.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Ford Pantera L&quot;},{&quot;mpg&quot;:19.7,&quot;cyl&quot;:6,&quot;disp&quot;:145,&quot;hp&quot;:175,&quot;drat&quot;:3.62,&quot;wt&quot;:2.77,&quot;qsec&quot;:15.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:6,&quot;_row&quot;:&quot;Ferrari Dino&quot;},{&quot;mpg&quot;:15,&quot;cyl&quot;:8,&quot;disp&quot;:301,&quot;hp&quot;:335,&quot;drat&quot;:3.54,&quot;wt&quot;:3.57,&quot;qsec&quot;:14.6,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:8,&quot;_row&quot;:&quot;Maserati Bora&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:4,&quot;disp&quot;:121,&quot;hp&quot;:109,&quot;drat&quot;:4.11,&quot;wt&quot;:2.78,&quot;qsec&quot;:18.6,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Volvo 142E&quot;}] 2.5 Importing from other statistical software Common software packages include SAS, STATA and SPSS. Two packages useful for importing data from these packages are: haven: by Hadley Wickham and is under active development. It aims to be more consistent, easier and faster than foreign. It can read SAS, Stata and SPSS and will read in the file as an D dataframe. foreign: is an older package by the R Core Team. Foreign support more data formats than haven including Weka and Systat # Load the haven package library(haven) # Import sales.sas7bdat: sales sales &lt;- read_sas(&quot;sales.sas7bdat&quot;) # Display the structure of sales str(sales) # Import the data from the URL: sugar sugar &lt;- read_dta(&quot;http://assets.datacamp.com/production/course_1478/datasets/trade.dta&quot;) # Structure of sugar str(sugar) # Convert values in Date column to dates sugar$Date &lt;- as.Date(as_factor(sugar$Date)) # Structure of sugar again str(sugar) # Import person.sav: traits traits &lt;- read_sav(&quot;person.sav&quot;) # Summarize traits summary(traits) # Print out a subset subset(traits, Extroversion &gt; 40 &amp; Agreeableness &gt; 40) When using SPSS files, it is often the case that the variable labels are also imported, it is best to change these in to standard R factors. # Import SPSS data from the URL: work work &lt;- read_sav(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav&quot;) # Display summary of work$GENDER summary(work$GENDER) # Convert work$GENDER to a factor work$GENDER &lt;- as_factor(work$GENDER) # Display summary of work$GENDER again summary(work$GENDER) Foreign cannot use single SAS datafiles like haven, it works with SAS library files .xport. Foreign tends to use dots in the function names rather than underscores in haven e.g. read.dta() vs read_dta(). Foreign does not provide consistency with it’s functions i.e. read.dta() has different arguments than read.spss(), however foreign provides more control over the data importing, such as dealing with multiple types of missing data which are often present in survey data, more comprehensively than haven. Although haven is still being developed. # Load the foreign package library(foreign) # Specify the file path using file.path(): path path &lt;- file.path(&quot;worldbank&quot;, &quot;edequality.dta&quot;) # Create and print structure of edu_equal_1 edu_equal_1 &lt;- read.dta(path) str(edu_equal_1) # Create and print structure of edu_equal_2 edu_equal_2 &lt;- read.dta(path, convert.factors = FALSE) str(edu_equal_2) # Create and print structure of edu_equal_3 edu_equal_3 &lt;- read.dta(path, convert.underscore = TRUE) str(edu_equal_3) # Import international.sav as a data frame: demo demo &lt;- read.spss(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/international.sav&quot;, to.data.frame = TRUE) # Create boxplot of gdp variable of demo boxplot(demo$gdp) "],
["references.html", "References", " References "],
["cleaning-data.html", "3 Cleaning Data 3.1 Tidying data 3.2 Preparing data for analysis 3.3 String manipulation 3.4 Missing, Specials and Outliers 3.5 Examples", " 3 Cleaning Data Notes taken during/inspired by the Datacamp course ‘Cleaning Data in R’ by Nick Carchedi. 3.1 Tidying data In Hadley’s paper on tidy data, he talked about how columns in a data frame should be variables or attributes and rows should be observations - this somestimes does not happen if there are things like dummy variables as columns, that could be collpased in to a single column. The entire data table (data frame) should be about one particular set of data i.e. we have countries without an embedded table about cats. Hadley introduced the tidyr package to try and help clean some data. There are two fundamental verbs of data tidying: gather() takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer spread() takes two columns (key &amp; value) and spreads in to multiple columns, it makes “long” data wider gather(data, key, value …) data: is a data frame key: the name of the new key column value: the name of the new value column …: names of columns to gather or not (if not, state -col e.g. -time to not include the time column in the gathered table) spread(data, key, value) data: is a data frame key: the name containing the key column value: the name containing the value column # Apply gather() to bmi and save the result as bmi_long bmi_long &lt;- gather(bmi, year, bmi_value, -Country) # Apply spread() to bmi_long bmi_wide &lt;- spread(bmi_long, year, bmi_val) Another useful feature is separate(). This takes a single variable and separates it into two separate columns or variable, for instance converting a year-month (2015-10) into a separate column for year and month. separate(data, col, into, sep = “”) data: a data frame col: bare name of column to separate into: charecter vector of new column names Optional sep = “”: in the separate command you can designate on what item (/, @ etc) to break the data by. This is optional and can depend on the column type (numeric vs char) # separate year-mo into two columns separate(treatments, year_mo, c(&quot;year&quot;, &quot;month&quot;)) We can also use the unite function to combine two columns together unite(data, col, …) data: a data frame col: name of the new column …: columns to unite The default seperator within the new column is an underscore, however we can specify something different Optional sep = “-”: would add the seperator as a hyphen head(bmi_cc) Country_ISO year bmi_val 1 Afghanistan/AF Y1980 21.48678 2 Albania/AL Y1980 25.22533 3 Algeria/DZ Y1980 22.25703 4 Andorra/AD Y1980 25.66652 5 Angola/AO Y1980 20.94876 6 Antigua and Barbuda/AG Y1980 23.31424 So to separate Country_ISO into two columns # Apply separate() to bmi_cc bmi_cc_clean &lt;- separate(bmi_cc, col = Country_ISO, into = c(&quot;Country&quot;, &quot;ISO&quot;), sep = &quot;/&quot;) # Apply unite() to bmi_cc_clean aand reverse bmi_cc &lt;- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = &quot;-&quot;) 3.2 Preparing data for analysis Often we need to convert, or in the case of raw data, create the appropriate data type for each variable prior to analysis. Some common data types include character: “treatment”, “123”, “A” numeric: 23.44, 120, NaN, Inf integer: 4L, 1123L factor: factor(“Hello”), factor(8) logical: TRUE, FALSE, NA We can use the class() function to detmine the variable type, or we can also include a value to determine the appropriate type e.g. class(77L) will return [1] “integer”. We can also use the coercion functions to change the types, such as as.numeric, as.factor() and as.character(). For dates and times, we can use the lubridate package. # Load the lubridate package library(lubridate) # Parse as date dmy(&quot;17 Sep 2015&quot;) # Parse as date and time (with no seconds!) mdy_hm(&quot;July 15, 2012 12:56&quot;) # Coerce dob to a date (with no time) students2$dob &lt;- ymd(students2$dob) # Coerce nurse_visit to a date and time students2$nurse_visit &lt;- ymd_hms(students2$nurse_visit) 3.3 String manipulation Another useful package is stringr, which like lubridate and other Hadley packages has a consistent interface, providing a range of functions for dealing with strings. Some functions include str_trim() - Trim leading and trailing white space str_pad() - Pad with additional characters str_detect() - Detect a pattern str_replace() - Find and replace a pattern # Load the stringr package library(stringr) # Trim all leading and trailing whitespace str_trim(c(&quot; Filip &quot;, &quot;Nick &quot;, &quot; Jonathan&quot;)) # Pad these strings with leading zeros str_pad(c(&quot;23485W&quot;, &quot;8823453Q&quot;, &quot;994Z&quot;), width = 9, side = &quot;left&quot;, pad = 0) # Detect all dates of birth (dob) in 1997 str_detect(students2$dob, &quot;1997&quot;) # In the sex column, replace &quot;F&quot; with &quot;Female&quot;... students2$sex &lt;- str_replace(students2$sex, &quot;F&quot;, &quot;Female&quot;) # ...And &quot;M&quot; with &quot;Male&quot; students2$sex &lt;- str_replace(students2$sex, &quot;M&quot;, &quot;Male&quot;) R {base} also has some handy features for strings, including toupper() and tolower(). 3.4 Missing, Specials and Outliers Generally missing values in R are represented by NA. However, if the data has been imported from other systems, the values can be different, such as a . (dot) if imported from SPSS. We can use the is.na(df) to return a TRUE/FALSE array of where there are NA values in a data frame. Or, for large datasets, we can use the any(is.na(df)) to return a true or false if there is an NA anywhere in the data frame. Alternatively we can use the sum(is.na(df)) to count how many NAs are in the dataframe. Use complete.cases() to see which rows have no missing values. Special values include inf for infinite value, NaN for Not a number. Outliers are best detected by measures such as the IQR or other nuemrical measures (see the EDA section), by using a boxplot or a histogram/density plot. There are a number of likely reasons for an outlier: Valid measurements Variability in measurement Experimental error Data entry error May be discarded or retained depending on cause. In some instances we may want to cap, or put a limit on, the maximum number the outlier can. Looking at the actual values and considering possible values can help, for instance negative age values or a perons age above 200 are not plausible values. However, they may be data entry errors or in the case of negative numbers, represent a deliberately coded missing value. 3.5 Examples The weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. head(weather) X year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 1 2014 12 Max.TemperatureF 64 42 51 43 42 45 38 29 49 48 39 39 42 45 2 2014 12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39 43 36 35 37 39 3 2014 12 Min.TemperatureF 39 33 37 30 26 38 21 18 29 38 32 31 32 33 4 2014 12 Max.Dew.PointF 46 40 49 24 37 45 36 28 49 45 37 28 28 29 # Load the tidyr package library(tidyr) # Gather the columns weather2 &lt;- gather(weather, day, value, X1:X31, na.rm = TRUE) becomes X year month measure day value 1 2014 12 Max.TemperatureF X1 64 2 2014 12 Mean.TemperatureF X1 52 3 2014 12 Min.TemperatureF X1 39 4 2014 12 Max.Dew.PointF X1 46 5 2014 12 MeanDew.PointF X1 40 6 2014 12 Min.DewpointF X1 26 Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. WE also have an additional column (X) which is not needed as it is just the row number. # First remove column of row names weather2 &lt;- weather2[, -1] # Spread the data weather3 &lt;- spread(weather2, measure, value) Table 3.1: year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH 2014 12 X1 6 Rain 46 29 2014 12 X2 7 Rain-Snow 40 29 2014 12 X3 8 Rain 49 38 2014 12 X4 3 24 33 2014 12 X5 5 Rain 37 26 2014 12 X6 8 Rain 45 25 … Now that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We’ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. # Remove X&#39;s from day column weather3$day &lt;- str_replace(weather3$day, &quot;X&quot;, &quot;&quot;) # Unite the year, month, and day columns weather4 &lt;- unite(weather3, date, year, month, day, sep = &quot;-&quot;) # Convert date column to proper date format using lubridates&#39;s ymd() weather4$date &lt;- ymd(weather4$date) # Rearrange columns using dplyr&#39;s select() weather5 &lt;- select(weather4, date, Events, CloudCover:WindDirDegrees) It’s important for analysis that variables are coded appropriately. This is not yet the case with our weather data. Recall that functions such as as.numeric() and as.character() can be used to coerce variables into different types. It’s important to keep in mind that coercions are not always successful, particularly if there’s some data in a column that you don’t expect. For example, the following will cause problems: as.numeric(c(4, 6.44, “some string”, 222)) So you can use the str_replace function to change character values to something else. If we have missing data, we can use indices and is.na function to identify then only see those rows with NA values on a variable of interest. # Count missing values sum(is.na(weather6)) # Find missing values summary(weather6) # Find indices of NAs in Max.Gust.SpeedMPH ind &lt;- which(is.na(weather6$Max.Gust.SpeedMPH)) # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind, ] Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary(). Once implausible values are identified, they must be dealt with in an intelligent and informed way. Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data. # Find row with Max.Humidity of 1000 ind &lt;- which(weather6$Max.Humidity == 1000) # Look at the data for that day weather6[ind, ] # Change 1000 to 100 weather6$Max.Humidity[ind] &lt;- 100 Before officially calling our weather data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we generally recommend using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. "],
["introduction-to-data.html", "4 Introduction to Data 4.1 Language of Data 4.2 Observational Studies and Experiments 4.3 Sampling strategies and experimental design", " 4 Introduction to Data Notes taken during/inspired by the Datacamp course ‘Introduction to Data’ by Mine Cetinkaya-Rundel. The supporting textbook is Diez, Barr, and Cetinkaya-Rundel (2015). 4.1 Language of Data The course makes use of the openintro package, accompanying the textbook. Let’s load the package and our first dataset, email50. # Load packages library(&quot;openintro&quot;) library(&quot;dplyr&quot;) # Load data data(email50) # View its structure str(email50) ## &#39;data.frame&#39;: 50 obs. of 21 variables: ## $ spam : num 0 0 1 0 0 0 0 0 0 0 ... ## $ to_multiple : num 0 0 0 0 0 0 0 0 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 4 0 0 0 0 0 1 0 ... ## $ sent_email : num 1 0 0 0 0 0 0 1 1 0 ... ## $ time : POSIXct, format: &quot;2012-01-04 13:19:16&quot; &quot;2012-02-16 20:10:06&quot; ... ## $ image : num 0 0 0 0 0 0 0 0 0 0 ... ## $ attach : num 0 0 2 0 0 0 0 0 0 0 ... ## $ dollar : num 0 0 0 0 9 0 0 0 0 23 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 0 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 1 0 0 0 0 0 ... ## $ num_char : num 21.705 7.011 0.631 2.454 41.623 ... ## $ line_breaks : int 551 183 28 61 1088 5 17 88 242 578 ... ## $ format : num 1 1 0 0 1 0 0 1 1 1 ... ## $ re_subj : num 1 0 0 0 0 0 0 1 1 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 1 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 8 1 2 1 43 0 0 2 22 3 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 2 3 1 2 2 2 2 2 2 2 ... #glimpse the first few items using dplyr glimpse(email50) ## Observations: 50 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0... ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0... ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ cc &lt;int&gt; 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ sent_email &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ time &lt;dttm&gt; 2012-01-04 13:19:16, 2012-02-16 20:10:06, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ attach &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0... ## $ dollar &lt;dbl&gt; 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, ... ## $ winner &lt;fctr&gt; no, no, no, no, no, no, no, no, no, no, no, no, ... ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ password &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0... ## $ num_char &lt;dbl&gt; 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809... ## $ line_breaks &lt;int&gt; 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167... ## $ format &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1... ## $ re_subj &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ exclaim_mess &lt;dbl&gt; 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, ... ## $ number &lt;fctr&gt; small, big, none, small, small, small, small, sm... When using certain functions, such as filters on categorical variables, the way R handles the filtered out variables is to leave the items in as place holders (empty containers), even though the place holder is empty. This can have undesirable effects, particularly if using the filtered object for modelling. We then end up with zero values which are actually filtered out factors. # Subset of emails with big numbers: email50_big email50_big &lt;- email50 %&gt;% filter(number == &quot;big&quot;) # Glimpse the subset glimpse(email50_big) ## Observations: 7 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0 ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ cc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0 ## $ sent_email &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0 ## $ time &lt;dttm&gt; 2012-02-16 20:10:06, 2012-02-04 23:26:09, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ attach &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ dollar &lt;dbl&gt; 0, 0, 3, 2, 0, 0, 0 ## $ winner &lt;fctr&gt; no, no, yes, no, no, no, no ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ password &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 8 ## $ num_char &lt;dbl&gt; 7.011, 10.368, 42.793, 26.520, 6.563, 11.223, 10.613 ## $ line_breaks &lt;int&gt; 183, 198, 712, 692, 140, 512, 225 ## $ format &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ re_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0 ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_mess &lt;dbl&gt; 1, 1, 2, 7, 2, 9, 9 ## $ number &lt;fctr&gt; big, big, big, big, big, big, big # Table of number variable - now we have just 7 values table(email50_big$number) ## ## none small big ## 0 0 7 # Drop levels email50_big$number &lt;- droplevels(email50_big$number) # Another table of number variable table(email50_big$number) ## ## big ## 7 In some instance you want to create a discreet function from a numeric value. That is to say we want to create a categorical value based on some groups of numbers. This can be achived as shown below. Note that when calculating a function R will typically either: Assign a value e.g. med_num_char &lt;- median(email50$num_char) Print a result e.g. median(email50$num_char) But we can do both by adding brackets (med_num_char &lt;- median(email50$num_char)) # Calculate median number of characters: med_num_char (med_num_char &lt;- median(email50$num_char)) ## [1] 6.8895 # Create num_char_cat variable in email50 email50 &lt;- email50 %&gt;% mutate(num_char_cat = ifelse(num_char &lt; med_num_char, &quot;below median&quot;, &quot;at or above median&quot;)) # Count emails in each category table(email50$num_char_cat) ## ## at or above median below median ## 25 25 We can also use the mutate function from dplyr to create a new variable from categorical variables # Create number_yn column in email50 email50 &lt;- email50 %&gt;% mutate(number_yn, ifelse(number == &quot;none&quot;, &quot;no&quot;, &quot;yes&quot;)) # Visualize number_yn ggplot(email50, aes(x = number_yn)) + geom_bar() We often want to compare two or three variables, which is most easily done using the ggplot package # Load ggplot2 library(ggplot2) # Scatterplot of exclaim_mess vs. num_char ggplot(email50, aes(x = num_char, y = exclaim_mess, color = factor(spam))) + geom_point() 4.2 Observational Studies and Experiments Typically there are two types of study, if we are interested in whether variable Y is caused by some factors (X) we could have two types of studies. Observational Study: We are observing, rather than specifically interfere or direct how the data is collected - only correlation can be inferred. In this case, we might survey people and look for patterns in their characteristics (X) and the outcome variable (Y) Experimental Study: We randomly assign subjects to various treatments - causation can be inferred. In this case, we would get a group of individuals together then randomly assign them to a group of interest (X), removing the decision from the subjects of the study, we often have a control group also. Another differentiation to be aware of is between Random sampling: We select our subjects at random in order that we can make inferences from our sample, to the wider population Random assignment: Subjects are randomly assigned to various treatments and helps us to make causal conclusions We can therefore combine random sampling with random assignment, to allow causal and generalisable conclusions, however in practice we typically have one or the other - random sampling only (not causal but generalisable), or random assignment (causal but not generalisable) - the negation of both leads to results that are neither causal nor generalisable, but may highlight a need for further research. Sometimes when there are looking for associations between variables, it is possible to omit variables of interest, which may be confounding variables. For instance, we may have two variables (x) that appear to show a relationship with another (y) but the inclusion of a third variable (x’) causes the apparent relationship to breakdown. If we fail to consider other associated variables, we may fall in to a Simpsons Paradox in which a trend appears in different groups, but disappears when the groups are combined together. Simpsons paradox is a form of Ecological Fallacy. One of the best known examples of Simpsons Paradox comes from admissions data for University of California, Berkeley. library(tidyr) data(&quot;UCBAdmissions&quot;) ucb_admit &lt;- as.data.frame(UCBAdmissions) # Restrucutre data - this is to follow the example provided, it takes the aggregated data from the original data frame and disaggregates # it using indexing by repeating the row indices Freq times for each row - see https://stackoverflow.com/questions/45445919/convert-wide-to-long-with-frequency-column ucb_admit_disagg = ucb_admit[rep(1:nrow(ucb_admit), ucb_admit$Freq), -grep(&quot;Freq&quot;, names(ucb_admit))] # Count number of male and female applicants admitted ucb_counts &lt;- ucb_admit_disagg %&gt;% count(Gender, Admit) # View result ucb_counts ## # A tibble: 4 x 3 ## Gender Admit n ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 Male Admitted 1198 ## 2 Male Rejected 1493 ## 3 Female Admitted 557 ## 4 Female Rejected 1278 # Spread the output across columns and calculate percentages ucb_counts %&gt;% spread(Admit, n) %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 2 x 4 ## Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Male 1198 1493 0.4451877 ## 2 Female 557 1278 0.3035422 So far, it seems that the results suggest females are less likely to be admitted, but what if we look at the results by department? # Table of counts of admission status and gender for each department admit_by_dept &lt;- ucb_admit_disagg %&gt;% count(Dept, Gender, Admit) %&gt;% spread(Admit, n) # View result admit_by_dept ## # A tibble: 12 x 4 ## Dept Gender Admitted Rejected ## * &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 A Male 512 313 ## 2 A Female 89 19 ## 3 B Male 353 207 ## 4 B Female 17 8 ## 5 C Male 120 205 ## 6 C Female 202 391 ## 7 D Male 138 279 ## 8 D Female 131 244 ## 9 E Male 53 138 ## 10 E Female 94 299 ## 11 F Male 22 351 ## 12 F Female 24 317 # Percentage of those admitted to each department admit_by_dept %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 12 x 5 ## Dept Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A Male 512 313 0.62060606 ## 2 A Female 89 19 0.82407407 ## 3 B Male 353 207 0.63035714 ## 4 B Female 17 8 0.68000000 ## 5 C Male 120 205 0.36923077 ## 6 C Female 202 391 0.34064081 ## 7 D Male 138 279 0.33093525 ## 8 D Female 131 244 0.34933333 ## 9 E Male 53 138 0.27748691 ## 10 E Female 94 299 0.23918575 ## 11 F Male 22 351 0.05898123 ## 12 F Female 24 317 0.07038123 Now we begin to see that for some departments, there is a higher proportion of females being accpeted than males. Equally for some departments, the rejection rate is very high for both males and females e.g. Dept F. In 4 of the 6 departments, females have a higher proportion of applications being admitted than males. Males tended to apply to less competitive departments than females, the less competitive departments had higher admission rates. 4.3 Sampling strategies and experimental design We use sampling when we do not want, for whatever reason, to conduct a full Census. A simple random sample is usually the most basic method. We can also use stratified sampling to ensure representation from certain groups. Or we use cluster sampling usually for economic reasons. Or some combination in multistage sampling. data(county) # Simple random sample: county_srs county_srs &lt;- county %&gt;% sample_n(size = 20) # Count counties by state county_srs %&gt;% group_by(state) %&gt;% count() ## # A tibble: 18 x 2 ## # Groups: state [18] ## state n ## &lt;fctr&gt; &lt;int&gt; ## 1 California 2 ## 2 Colorado 1 ## 3 Florida 1 ## 4 Georgia 1 ## 5 Idaho 1 ## 6 Indiana 1 ## 7 Iowa 1 ## 8 Louisiana 1 ## 9 Maryland 1 ## 10 Missouri 1 ## 11 Montana 2 ## 12 Nevada 1 ## 13 North Carolina 1 ## 14 North Dakota 1 ## 15 Oklahoma 1 ## 16 South Dakota 1 ## 17 Texas 1 ## 18 Virginia 1 For a stratified sample we would do something similar. # Stratified sample states_str &lt;- us_regions %&gt;% group_by(region) %&gt;% sample_n(size = 2) # Count states by region states_str %&gt;% group_by(region) %&gt;% count() The principles of experimental design include 4 key components: Control: compare treatment of interest to a control group Randomise: randomly assign subjects to treatments Replicate: collect a sufficiently large sample within a study, or replicate the entire study Block: account for the potential effect of confounding variables We group subjects into blocks based on these confounding variables, then randomise within each block to treatment groups. So for instance, if we were testing whether an online or classroom R course was more effective using an experiment, one possible confounding variable would be previous programming experience. Therefore we would seperate out - block - those with and those without previous programming experience, ensuring we have an equal number in each treatment group (online vs classroom) of those with and without previous experience. In random sampling, you use stratifying to control for a variable. In random assignment, you use blocking to achieve the same goal. References "],
["references-1.html", "References", " References "],
["foundations-of-inference.html", "5 Foundations of Inference 5.1 Introduction to Inference 5.2 Home Ownership by Gender 5.3 Density Plots 5.4 Gender Discrimination (p-values) 5.5 Opportunity Cost 5.6 Type I and Type II errors 5.7 Bootstrapping", " 5 Foundations of Inference Notes taken during/inspired by the Datacamp course ‘Foundations of Inference’ by Jo Hardin, collaborators; Nick Carchedi and Tom Jeon. 5.1 Introduction to Inference Classical statistical inference is the process of making claims about a population based on a sample of information. We are making an inference from a small group (sample) to a much larger one (population). We typically have: Null Hypothesis \\(H_{0}\\): What we are researching has no effect Alternate Hypothesis \\(H_{A}\\): What we are researching does have an effect Under the null hypothesis, chance alone is responsible for the results. Under the alternate hypothesis, we reject the null hypothesis, by using statistical techniques that indicate that chance is not responsible for our findings. Hypothesis or statistical testing goes back over 300 years, with the first recorded use by John Arbuthnot: Table 3.1: Statistical Testing Applications Year Person Context 1710 Arbuthnot Sex ratio at birth 1767 Michelle Distribution of stars 1823 Laplace Moon phase and barometric changes 1900 K. Pearson Goodness of fit 1908 Gosset A single mean Source: (Huberty 1993, pg 318) Contemporary statistical testing is a usually that of either Fisher or Neyman-Pearson approaches. Fisher tends to use a single hypothesis test and a p-value strength of evidence test, where as the Neyman-Pearson test will set a critical alpha value and compare the null hypothesis against an alternative hypothesis, rejecting the null if the test statistic is high enough (Huberty 1993, pg 318). The course goes on to say that idea behind statistical inference is to understand samples from a hypothetical population, where the null hypothesis is true - there is no difference between two groups. We can do this by calculating one statistic - for instance the proportion (mean) of a test group who show a positive response when testing a new drug, compared to a placebo control group - for each repeated sample from a population, then work out the difference between these two groups means. With each sample, the mean will change, resulting in a changing difference for each sample. We can then generate a distribution (histogram) of differences, assuming the null hypothesis - that there is no link between drug effectiveness between a test group and a control group - is true. “Generating a distribution of the statistic from the null population gives information about whether the observed data are inconsistent with the null hypothesis”. That is to say, by taking repeated samples and creating a distribution, we can then say whether our observed difference is consistent (within an acceptable value range due to chance) to the null hypothesis. The null samples consist of randomly shuffled drug effectiveness variables (permuted samples from the population), so that the samples don’t have any dependency between the two groups and effectiveness. 5.2 Home Ownership by Gender Data used in the exercises are from NHANES 2009-2012 With Adjusted Weighting. This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. # Load packages library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) library(&quot;NHANES&quot;) library(&quot;oilabs&quot;) # Create bar plot for Home Ownership by Gender ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Relative frequencies&quot;) # Density for SleepHrsNight coloured by SleepTrouble, faceted by HealthGen ggplot(NHANES, aes(x = SleepHrsNight, col = SleepTrouble)) + geom_density(adjust = 2) + facet_wrap(~ HealthGen) Next we want to create a selection for just our variables of interest - rent and owner occupation. # Subset the data: homes homes &lt;- NHANES %&gt;% select(Gender, HomeOwn) %&gt;% filter(HomeOwn %in% c(&quot;Own&quot;, &quot;Rent&quot;)) We build a distribution of differences assuming the null hypothesis - that there is no link between gender and home ownership - is true. In this first step, we just do a single iteration, or permutation from the true values. The null (permuted) version here will create a randomly shuffled home ownership variable, so that the permuted version does not have any dependency between gender and homeownership. We effectively have the same gender split variables as per the original, with the same owned and rented proportions, but disassociated from the gender variable - just randomly shuffled. # Perform one permutation homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 0.007822786 It is easier to see what is going on by breaking the results down iteratively. Our selected and filtered homes dataset looks like. head(homes) ## # A tibble: 6 x 2 ## Gender HomeOwn ## &lt;fctr&gt; &lt;fctr&gt; ## 1 male Own ## 2 male Own ## 3 male Own ## 4 male Own ## 5 female Rent ## 6 male Rent Next we shuffle this data, let’s call it homes 2. we can then check the total number of owns and rents are the same using the summary function, which confirms the data is just randomly shuffled. homes2 &lt;- homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) tail(homes2) ## # A tibble: 6 x 3 ## # Groups: Gender [2] ## Gender HomeOwn HomeOwn_perm ## &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; ## 1 male Rent Rent ## 2 male Rent Own ## 3 female Own Own ## 4 male Own Own ## 5 male Own Own ## 6 male Own Own summary(homes2) ## Gender HomeOwn HomeOwn_perm ## female:4890 Own :6425 Own :6425 ## male :4822 Rent :3287 Rent :3287 ## Other: 0 Other: 0 Then we calculate the mean value of home ownership (Own) across our original and shuffled (permutated) data homes3 &lt;- homes2 %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) homes3 ## # A tibble: 2 x 3 ## Gender prop_own_perm prop_own ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 0.6582822 0.6654397 ## 2 male 0.6648693 0.6576109 FFinally we calculate the differences in ownership - note that the difference for the permuted value here may be different from the full code above, as it a new random permutation and we have used the set.seed() function which would create an identical permutation. homes4 &lt;- homes3 %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) homes4 ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 0.00658714 5.3 Density Plots Next we can make multiple permutations using the rep_sample_n from the oilabs package. We specify the data (tbl), the sample size, the number of samples to take (reps), and whether sampling should be done with or without replacement (replace). The output includes a new column, replicate, which indicates the sample number. We can create 100 permutations and create a dot plot of the results. # Perform 100 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 100) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Dotplot of 100 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_dotplot(binwidth = .001) We can go further and run 1000 permutations and create a density chart. set.seed(666) # Perform 1000 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 1000) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Density plot of 1000 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() Now we have our density plot of the null hypothesis - randomly permuted samples - we can see where our actual observed difference lies, plus how many randomly permuted differences were less than the observed difference. # Plot permuted differences ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences homeown_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 21.5 So in this instance, when we set the seed of 666 we end up with 20.5% of randomly shuffled (permuted) differences being greater than the observed difference, so the observed difference is consistent with the null hypothesis. That it to say it is within the range we may expect by chance alone, were we to repeat the exercise, although we should specify a distribtion we are comparing against, in this which is inferred as being the normal distribution in this instance. We can therefore say that there is no statistically significant difference between gender and home ownership. Or put more formally We fail to reject the null hypothesis: There is no evidence that our data are inconsistent with the null hypothesis 5.4 Gender Discrimination (p-values) In this section we use data from Rosen and Jerdee (1974), where 48 male bank supervisors were given personnel files and asked if they should be promoted to Branch Manager. All files were identical, but half (24) were named as female, and the other half (24) were named male. The results showed 21 males were promoted and 14 females, meaning 35 of the total 48 were promoted. In Rosen and Jerdee (1974) sex was given along with an indication of the difficulty - routine or complex - here we only look at the routine promotion candidates. Do we know if gender is a statistically significant factor? Null Hypothesis \\(H_{0}\\): Gender and promotion are unrelated variables Alternate Hypothesis \\(H_{A}\\): Men are more likely to be promoted First, we create the data frame disc disc &lt;- data.frame( promote = c(rep(&quot;promoted&quot;, 35), rep(&quot;not_promoted&quot;, 13)), sex = c(rep(&quot;male&quot;, 21), rep(&quot;female&quot;, 14), rep(&quot;male&quot;, 3), rep(&quot;female&quot;, 10)) ) Then let’s see the resulting table and proportion who were promoted table(disc) ## sex ## promote female male ## not_promoted 10 3 ## promoted 14 21 disc %&gt;% group_by(sex) %&gt;% summarise(promoted_prop = mean(promote == &quot;promoted&quot;)) ## # A tibble: 2 x 2 ## sex promoted_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 female 0.5833333 ## 2 male 0.8750000 So there difference in promotions by gender is around 0.3 or around 30%, but could this be due to chance? We can create 1000 permutations and compare our observed diffrence to the distribution, plus how many randomly permuted differences were less than the observed difference. # Create a data frame of differences in promotion rates set.seed(42) disc_perm &lt;- disc %&gt;% rep_sample_n(size = nrow(disc), reps = 1000) %&gt;% mutate(prom_perm = sample(promote)) %&gt;% group_by(replicate, sex) %&gt;% summarize(prop_prom_perm = mean(prom_perm == &quot;promoted&quot;), prop_prom = mean(promote == &quot;promoted&quot;)) %&gt;% summarize(diff_perm = diff(prop_prom_perm), diff_orig = diff(prop_prom)) # male - female # Histogram of permuted differences ggplot(disc_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences disc_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 99.3 So here, just 0.5% of the randomly permuted/shuffled results are greater than our observed promotion differences, or 99.5% are lower, so our results are definitely quite extreme. We typically use a 5% cut off, which the course mentions is arbitrary and historic, being attributed to Fisher. So we can say at 0.5% our value is within this critical region, meaning the results are statistically significant - we should not ignore them. We can calculate quantiles of the null statistic using our randomly generated shuffles. disc_perm %&gt;% summarize(q.90 = quantile(diff_perm, p = 0.90), q.95 = quantile(diff_perm, p = 0.95), q.99 = quantile(diff_perm, p = 0.99)) ## # A tibble: 1 x 3 ## q.90 q.95 q.99 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2083333 0.2083333 0.2916667 So here, 95% of our null differences are 0.208 or lower, indeed 99% are 0.292 or lower, so our observed difference of 0.3 is quite extreme - it is in the critical region of the distribution. We can go one step further by calculating the p-value. The p-value is: the probability of observing data as or more extreme than what we actually got given that the null hypothesis is true. disc_perm %&gt;% summarize(mean(diff_orig &lt;= diff_perm)) ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.025 So the p-value here is 0.028 (less than 3 %). If repeat the exercise with smaller and larger number of shuffles we would get different p-values. ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.01 ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.0227 With 100 shuffles our p-value is 0.03, and with 10,000 shuffles our p-value is 0.0235. If we had a two-tailed test - for instance if we said the original research hypothesis had focused on any difference in promotion rates between men and women instead of focusing on whether men are more likely to be promoted than women - we could simple double the p-value. In both cases, the p-value is below or close to the 0.05 (5%) critical value, meaning we can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. However, as both values are close to the critical value, we should indicate that more work should be done. Indeed since the Rosen and Jerdee (1974) study, many further studies have been undertaken and found a similar pattern of discrimination. 5.5 Opportunity Cost In Frederick et al. (2009) their study showed that when potential purchasers were reminded that if they did not buy a particular DVD they could instead save the money, when compared to a control group who were just told they could not buy the DVD, those being reminded of the saving appeared to be more inclined not to make the purchase - 34 in the treatment group did not buy compared to 19 in the control. So our test is setup as: Null Hypothesis \\(H_{0}\\): Reminding students will have no impact on their spending decisions Alternate Hypothesis \\(H_{A}\\): Reminding students will reduce the chance they continue with a purchase We can create a data frame containing the results and find the initial proportions. #create the data frame opportunity &lt;- data.frame( decision = c(rep(&quot;buyDVD&quot;, 97), rep(&quot;nobuyDVD&quot;, 53)), group = c(rep(&quot;control&quot;, 56), rep(&quot;treatment&quot;, 41), rep(&quot;control&quot;, 19), rep(&quot;treatment&quot;, 34)) ) # Tabulate the data opportunity %&gt;% select(decision, group) %&gt;% table() ## group ## decision control treatment ## buyDVD 56 41 ## nobuyDVD 19 34 # Find the proportion who bought the DVD in each group opportunity %&gt;% group_by(group) %&gt;% summarize(buy_prop = mean(decision == &quot;buyDVD&quot;)) ## # A tibble: 2 x 2 ## group buy_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 control 0.7466667 ## 2 treatment 0.5466667 So around 55% of the treatment group - those who were reminded they could save the money - bought the DVD, comapred to 75% of the control group. We can represent this with a bar plot. As before, we can calculate 1000 random shuffles and then compare our difference in proportions, to the distribution of those 1000 samples. And finally, we can calculate the p-value ## # A tibble: 1 x 1 ## `mean(diff_perm &lt;= diff_orig)` ## &lt;dbl&gt; ## 1 0.012 In this instance, of p-value is substantially less than the usual critical value - 0.8% versus the usual value of 5% - so we can can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. Our results would only occur 8 times in 1000 by chance. We can therefore accept the alternative hypothesis (\\(H_{A}\\)) that reminding students does cause them to be less likely to buy a DVD, as they were randomly assigned to the treatment and control groups, therefore any difference is due to the reminder to save. Who can we therefore make the inference to? Our sample was drawn from the student population for the Frederick et al. (2009) study, so we would be able to generalise to that student population however defined, but not to another wider population. 5.6 Type I and Type II errors In our research and conslusions there is a risk that we will be incorrect, we will make an error. The two errors are: Type I error : The null hypothesis (\\(H_{0}\\)) is true, but is rejected. On the basis of the evidence, we have decided to erroneously accept the alternative hypothesis (\\(H_{A}\\)) when in fact the null hypothesis is correct. It is sometimes called a false positive. Type II error : the null hypothesis is false, but erroneously fails to be rejected. On the basis of the evidence, we have failed to accept the alternative hypothesis despite it being correct - an effect that exists in the population. It is sometimes called a false negative. If we return to our previous example, our associated errors would be Type I: There is not a difference in proportions, but the observed difference is big enough to indicate that the proportions are different. Type II: There is a difference in proportions, but the observed difference is not large enough to indicate that the proportions are different. 5.7 Bootstrapping Sometimes we are not neccessarily interested in testing a hypothesis, we are instead interested in making a claim about how our sample can be inferred to a large population. To do so we use confidece intervals. When calculating confidence intervals there is no null hypothesis like in hypothesis testing. We need to understand how samples from our population vary around the parameter of interest. In an ideal world we would take many samples from the population or know what the true value is in the population, but realistically this is not possible, so we use booststrapping. Bootstrapping is the process of taking repeated samples from the same sample, to estimate the variability. As our population parameters are not known, we can use our sample to estimate a simulated population parameter (\\(\\hat{p}*\\)) by repeated sampling. We can then estimate other parameters such as the standard deviation, s.e. and the confidence interval. Instead of taking repeated samples from our population, we take repeated samples from our data, with replacement, each bootstrap sample is the same size as the original sample. Figure 5.1: Illustration of the bootstrap approach on a small sample containing n = 3 observations (James et al. 2013, pg 190) Firstly we setup our single poll, where 70% (21/30) are intended to vote for a particular candidate # Setup our single poll example one_poll &lt;- sample(rep(c(0, 1), times = c(9,21))) one_poll &lt;- tbl_df(one_poll) colnames(one_poll) &lt;- &quot;vote&quot; Next we can create 1000 bootstrap samples from this original poll, then calculate the variability set.seed(42) # Generate 1000 resamples of one_poll: one_poll_boot_30 one_poll_boot_30 &lt;- one_poll %&gt;% rep_sample_n(size = 30, replace = TRUE, reps = 1000) # Compute p-hat* for each resampled poll ex1_props &lt;- one_poll_boot_30 %&gt;% summarize(prop_yes = mean(vote)) %&gt;% summarize(sd(prop_yes)) #compute variability p-hat* ex1_props ## # A tibble: 1 x 1 ## `sd(prop_yes)` ## &lt;dbl&gt; ## 1 0.08624387 So the variability - the standard error or SE - of \\(\\hat{p}*\\) is 0.0841. We can now use this SE to calculate a confidence interval, since 95% of samples will be within +/- 1.96 standard errors of the centre of the distribution assuming a normal distribution \\(N(\\mu, \\sigma ^2)\\). We also use the bootstrap to calculate our bootstrap confidence interval, to give a range of possible values. # Compute p-hat for one poll p_hat &lt;- mean(one_poll$vote) set.seed(42) # Bootstrap to find the SE of p-hat: one_poll_boot one_poll_boot &lt;- one_poll %&gt;% rep_sample_n(30, replace = TRUE, reps = 1000) %&gt;% summarize(prop_yes_boot = mean(vote)) # Create an interval of possible values one_poll_boot %&gt;% summarize(lower = p_hat - 1.96 * sd(prop_yes_boot), upper = p_hat + 1.96 * sd(prop_yes_boot)) ## # A tibble: 1 x 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.530962 0.869038 So our possible range of values, using the bootstrap at 95%, is between 53.2% and 86.8%. Going back to our original statement, we had a single poll where 70% of those polled intended to vote for a particular candidate. We can now say, using the bootstrap t-confidence interval, we are 95% confident that the true proportion planning to vote for the candidate is between 53% and 87%. We are assuming that the distribution is normally distributed \\(N(\\mu, \\sigma ^2)\\). References "],
["references-2.html", "References", " References "],
["exploratory-data-analysis.html", "6 Exploratory Data Analysis 6.1 Categorical Data 6.2 Numerical Data 6.3 Numerical Summaries 6.4 Email Case Study", " 6 Exploratory Data Analysis Notes taken during/inspired by the Datacamp course “Exploratory Data Analysis” by Andrew Bray. 6.1 Categorical Data Common functions when looking at categorical, aka factors variables, are levels(df\\(var) and to get a contigency or xtab table the table(df\\)var1, df$var2). We can also create bar charts to visually represent the data using ggplot. # Read in our dataset thanks to fivethirtyeight https://github.com/fivethirtyeight/data/tree/master/comic-characters comics &lt;- read.csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/comic-characters/dc-wikia-data.csv&quot;, stringsAsFactors = TRUE) comics$name &lt;- as.character(comics$name) # Check levels of align levels(comics$ALIGN) ## [1] &quot;&quot; &quot;Bad Characters&quot; &quot;Good Characters&quot; ## [4] &quot;Neutral Characters&quot; &quot;Reformed Criminals&quot; # Check the levels of gender levels(comics$SEX) ## [1] &quot;&quot; &quot;Female Characters&quot; ## [3] &quot;Genderless Characters&quot; &quot;Male Characters&quot; ## [5] &quot;Transgender Characters&quot; # Create a 2-way contingency table table(comics$ALIGN, comics$SEX) ## ## Female Characters Genderless Characters ## 25 220 0 ## Bad Characters 63 597 11 ## Good Characters 30 953 6 ## Neutral Characters 7 196 3 ## Reformed Criminals 0 1 0 ## ## Male Characters Transgender Characters ## 356 0 ## Bad Characters 2223 1 ## Good Characters 1843 0 ## Neutral Characters 359 0 ## Reformed Criminals 2 0 To simplify an analysis, it often helps to drop levels with small amounts of data. In R, this requires two steps: first filtering out any rows with the levels that have very low counts, then removing these levels from the factor variable with droplevels(). This is because the droplevels() function would keep levels that have just 1 or 2 counts; it only drops levels that don“t exist in a dataset. # Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Remove align level comics &lt;- comics %&gt;% filter(ALIGN != &quot;Reformed Criminals&quot;) %&gt;% droplevels() While a contingency table represents the counts numerically, it“s often more useful to represent them graphically. Here you“ll construct two side-by-side barcharts of the comics data. This shows that there can often be two or more options for presenting the same data. Passing the argument position =”dodge&quot; to geom_bar() says that you want a side-by-side (i.e. not stacked) barchart. # Load ggplot2 library(ggplot2) # Create side-by-side barchart of gender by alignment ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;dodge&quot;) # Create side-by-side barchart of alignment by gender ggplot(comics, aes(x = SEX, fill = ALIGN)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 90)) When creatign tables, it is often easier to look at proportions for patterns rather than counts. We can do this using conditional proportions, by using the prop.table(df_counts, n) where n is the number we want to condition our frequency/count table by, 1 = rows and 2 = columns. tab &lt;- table(comics$ALIGN, comics$SEX) options(scipen = 999, digits = 2) # Print fewer digits prop.table(tab) # Joint proportions (totals in the entire table) ## ## Female Characters Genderless Characters ## 0.00363 0.03192 0.00000 ## Bad Characters 0.00914 0.08661 0.00160 ## Good Characters 0.00435 0.13826 0.00087 ## Neutral Characters 0.00102 0.02843 0.00044 ## ## Male Characters Transgender Characters ## 0.05165 0.00000 ## Bad Characters 0.32250 0.00015 ## Good Characters 0.26737 0.00000 ## Neutral Characters 0.05208 0.00000 prop.table(tab, 2) # Conditional on columns (column totals) ## ## Female Characters Genderless Characters ## 0.200 0.112 0.000 ## Bad Characters 0.504 0.304 0.550 ## Good Characters 0.240 0.485 0.300 ## Neutral Characters 0.056 0.100 0.150 ## ## Male Characters Transgender Characters ## 0.074 0.000 ## Bad Characters 0.465 1.000 ## Good Characters 0.385 0.000 ## Neutral Characters 0.075 0.000 Here we see that approx. 49% of female characters are good, compared to 39% for males. Bar charts can tell dramatically different stories depending on whether they represent counts or proportions and, if proportions, what the proportions are conditioned on. To demonstrate this difference, you“ll construct two barcharts in this exercise: one of counts and one of proportions. # Plot of gender by align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar() # Plot proportion of gender, conditional on align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;fill&quot;) Conditional barchart Now, if you want to break down the distribution of alignment based on gender, you“re looking for conditional distributions. You could make these by creating multiple filtered datasets (one for each gender) or by faceting the plot of alignment based on gender. As a point of comparison, we“ve provided your plot of the marginal distribution of alignment from the last exercise. # Plot of alignment broken down by gender ggplot(comics, aes(x = ALIGN)) + geom_bar() + facet_wrap(~ SEX) 6.2 Numerical Data # Data courtesy of http://www.idvbook.com/teaching-aid/data-sets/ with some variable name modifictions to match those in the exercise library(readxl) cars &lt;- read_excel(&quot;04cars data.xls&quot;, sheet = 1) cars &lt;- cars[-2] # remove variable 2 # Rename vars names(cars) &lt;- c(&quot;name&quot;, &quot;sports_car&quot;, &quot;suv&quot;, &quot;wagon&quot;, &quot;minivan&quot;, &quot;pickup&quot;, &quot;all_wheel&quot;, &quot;rear_wheel&quot;, &quot;msrp&quot;, &quot;dealer_cost&quot;, &quot;eng_size&quot;, &quot;ncyl&quot;, &quot;horsepwr&quot;, &quot;city_mpg&quot;,&quot;hwy_mpg&quot;, &quot;weight&quot;, &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;) # Change data tpyes as needed cars[2:7] &lt;- sapply(cars[2:7],as.logical) cars[c(8:10,12:19)] &lt;- sapply(cars[c(8:10,12:19)],as.integer) ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion # Learn data structure str(cars) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 428 obs. of 19 variables: ## $ name : chr &quot;Acura 3.5 RL 4dr&quot; &quot;Acura 3.5 RL w/Navigation 4dr&quot; &quot;Acura MDX&quot; &quot;Acura NSX coupe 2dr manual S&quot; ... ## $ sports_car : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ suv : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ wagon : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ rear_wheel : int 0 0 0 1 0 0 0 0 0 0 ... ## $ msrp : int 43755 46100 36945 89765 23820 33195 26990 25940 31840 42490 ... ## $ dealer_cost: int 39014 41100 33337 79978 21761 30299 24647 23508 28846 38325 ... ## $ eng_size : num 3.5 3.5 3.5 3.2 2 3.2 2.4 1.8 3 3 ... ## $ ncyl : int 6 6 6 6 4 6 4 4 6 6 ... ## $ horsepwr : int 225 225 265 290 200 270 200 170 220 220 ... ## $ city_mpg : int 18 18 17 17 24 20 22 22 20 20 ... ## $ hwy_mpg : int 24 24 23 24 31 28 29 31 28 27 ... ## $ weight : int 3880 3893 4451 3153 2778 3575 3230 3252 3462 3814 ... ## $ wheel_base : int 115 115 106 100 101 108 105 104 104 105 ... ## $ length : int 197 197 189 174 172 186 183 179 179 180 ... ## $ width : int 72 72 77 71 68 72 69 70 70 70 ... # Create faceted histogram ggplot(cars, aes(x = city_mpg)) + geom_histogram() + facet_wrap(~ suv) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). The mileage of a car tends to be associated with the size of its engine (as measured by the number of cylinders). To explore the relationship between these two variables, you could stick to using histograms, but in this exercise you“ll try your hand at two alternatives: the box plot and the density plot. # Filter cars with 4, 6, 8 cylinders common_cyl &lt;- filter(cars, ncyl %in% c(4,6,8)) # Create box plots of city mpg by ncyl ggplot(common_cyl, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 11 rows containing non-finite values (stat_boxplot). # Create overlaid density plots for same data ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) + geom_density(alpha = .3) ## Warning: Removed 11 rows containing non-finite values (stat_density). Now, turn your attention to a new variable: horsepwr. The goal is to get a sense of the marginal distribution of this variable and then compare it to the distribution of horsepower conditional on the price of the car being less than $25,000. You“ll be making two plots using the”data pipeline&quot; paradigm, where you start with the raw data and end with the plot. In addition to indicating the center and spread of a distribution, a box plot provides a graphical means to detect outliers. You can apply this method to the msrp column (manufacturer“s suggested retail price) to detect if there are unusually expensive or cheap cars. # Create hist cars %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + ggtitle(&quot;ALL Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Create hist of horsepwr for affordable cars cars %&gt;% filter(msrp &lt; 25000) %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ggtitle(&quot;Affordable Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 1 rows containing missing values (geom_bar). # Construct box plot of msrp cars %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() # Exclude outliers from data cars_no_out &lt;- cars %&gt;% filter(msrp &lt; 100000) # Construct box plot of msrp using the reduced dataset cars_no_out %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() Consider two other columns in the cars dataset: city_mpg and width. Which is the most appropriate plot for displaying the important features of their distributions? Remember, both density plots and box plots display the central tendency and spread of the data, but the box plot is more robust to outliers. # Create plot of city_mpg cars %&gt;% ggplot(aes(x = width)) + geom_density() ## Warning: Removed 28 rows containing non-finite values (stat_density). # Create plot of width cars %&gt;% ggplot(aes(x = 1, y = city_mpg)) + geom_boxplot() ## Warning: Removed 14 rows containing non-finite values (stat_boxplot). Faceting is a valuable technique for looking at several conditional distributions at the same time. If the faceted distributions are laid out in a grid, you can consider the association between a variable and two others, one on the rows of the grid and the other on the columns. # Facet hists using hwy mileage and ncyl common_cyl %&gt;% ggplot(aes(x = hwy_mpg)) + geom_histogram() + facet_grid(ncyl ~ suv) + ggtitle(&quot;Faceted heavy mpg histograms by No. of Cyl and Suv&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 11 rows containing non-finite values (stat_bin). 6.3 Numerical Summaries Throughout this chapter, you will use data from gapminder, which tracks demographic data in countries of the world over time. To learn more about it, you can bring up the help file with ?gapminder. For this exercise, focus on how the life expectancy differs from continent to continent. This requires that you conduct your analysis not at the country level, but aggregated up to the continent level. This is made possible by the one-two punch of group_by() and summarize(), a very powerful syntax for carrying out the same analysis on different subsets of the full dataset. library(gapminder) # Create dataset of 2007 data gap2007 &lt;- filter(gapminder, year == 2007) # Compute groupwise mean and median lifeExp gap2007 %&gt;% group_by(continent) %&gt;% summarize(mean(lifeExp), median(lifeExp)) ## # A tibble: 5 x 3 ## continent `mean(lifeExp)` `median(lifeExp)` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 55 53 ## 2 Americas 74 73 ## 3 Asia 71 72 ## 4 Europe 78 79 ## 5 Oceania 81 81 # Generate box plots of lifeExp for each continent gap2007 %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() Let“s extend the powerful group_by() and summarize() syntax to measures of spread. If you”re unsure whether you“re working with symmetric or skewed distributions, it”s a good idea to consider a robust measure like IQR in addition to the usual measures of variance or standard deviation. # Compute groupwise measures of spread gap2007 %&gt;% group_by(continent) %&gt;% summarize(sd(lifeExp), IQR(lifeExp), n()) ## # A tibble: 5 x 4 ## continent `sd(lifeExp)` `IQR(lifeExp)` `n()` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 9.63 11.61 52 ## 2 Americas 4.44 4.63 25 ## 3 Asia 7.96 10.15 33 ## 4 Europe 2.98 4.78 30 ## 5 Oceania 0.73 0.52 2 # Generate overlaid density plots gap2007 %&gt;% ggplot(aes(x = lifeExp, fill = continent)) + geom_density(alpha = 0.3) # Compute stats for lifeExp in Americas gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% summarize(mean(lifeExp), sd(lifeExp)) ## # A tibble: 1 x 2 ## `mean(lifeExp)` `sd(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 74 4.4 # Compute stats for population gap2007 %&gt;% summarize(median(pop), IQR(pop)) ## # A tibble: 1 x 2 ## `median(pop)` `IQR(pop)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10517531 26702008 6.3.1 Transformations In some data, there are different ‘humps’ in the distribution, which are calls the modes or the modality of the dataset Unimode - single mean, this is a normal distribtion Bimodal - two common distributions Multimodal - three modes or more We also should consider whether the distribution is skewed. Right skewed data has a long tail to the right, with the majority of the distribution to the left - we often see this with income distributions. Left skewed has a small number of observations to the left and the majoirty of the distribution to the right. A normal distribution is typically smyterical. Highly skewed distributions can make it very difficult to learn anything from a visualization. Transformations can be helpful in revealing the more subtle structure. Here you’ll focus on the population variable, which exhibits strong right skew, and transform it with the natural logarithm function (log() in R). # Create density plot of old variable gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() # Transform the skewed pop variable gap2007 &lt;- gap2007 %&gt;% mutate(log_pop = log(pop)) # Create density plot of new variable gap2007 %&gt;% ggplot(aes(x = log_pop)) + geom_density() 6.3.2 Outliers It is often useful within a dataset to identify, using a column, whether the data is an outlier, this can be done by using the mutate function e.g. df &lt;- df %&gt;% mutate(is_outlier &gt; value), then filtering and arranging the resulting table e.g. df %&gt;% filter(is_outlier) %&gt;% arrange(desc(value)). We can also use this outlier column to remove the values from a plot e.g. df %&gt;% filter(!is_outlier) %&gt;% ggplot … The determination of the outlier value might be arbitary, or you could use a percentile value (say top or bottom 2%). # Filter for Asia, add column indicating outliers gap_asia &lt;- gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% mutate(is_outlier = lifeExp &lt;50) # Remove outliers, create box plot of lifeExp gap_asia %&gt;% filter(!is_outlier) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() 6.4 Email Case Study The example EDA comes from manually classified 3,900+ emails from the openintro package. Is there an association between spam and the length of an email? You could imagine a story either way: Spam is more likely to be a short message tempting me to click on a link, or *My normal email is likely shorter since I exchange brief emails with my friends all the time. Here, you’ll use the email dataset to settle that question. Begin by bringing up the help file and learning about all the variables with ?email. As you explore the association between spam and the length of an email, use this opportunity to try out linking a dplyr chain with the layers in a ggplot2 object. library(openintro) ## Please visit openintro.org for free statistics materials ## ## Attaching package: &#39;openintro&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## cars ## The following object is masked from &#39;package:datasets&#39;: ## ## cars # Compute summary statistics email %&gt;% group_by(spam) %&gt;% summarise(median(num_char), IQR(num_char)) ## # A tibble: 2 x 3 ## spam `median(num_char)` `IQR(num_char)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.8 13.6 ## 2 1 1.0 2.8 # Create plot email %&gt;% mutate(log_num_char = log(num_char)) %&gt;% ggplot(aes(x = factor(spam), y = log_num_char)) + geom_boxplot() Let’s look at a more obvious indicator of spam: exclamation marks. exclaim_mess contains the number of exclamation marks in each message. Using summary statistics and visualization, see if there is a relationship between this variable and whether or not a message is spam. Note: when computing the log(0) is -Inf in R, which isn’t a very useful value! You can get around this by adding a small number (like .01) to the quantity inside the log() function. This way, your value is never zero. This small shift to the right won’t affect your results. # Compute center and spread for exclaim_mess by spam email %&gt;% group_by(spam) %&gt;% summarise(mean(exclaim_mess), sd(exclaim_mess)) ## # A tibble: 2 x 3 ## spam `mean(exclaim_mess)` `sd(exclaim_mess)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.5 48 ## 2 1 7.3 80 # Create plot for spam and exclaim_mess email %&gt;% ggplot(aes(log(exclaim_mess)+0.1)) + geom_histogram() + facet_wrap( ~ spam) + ggtitle(&quot;Number of exclamation marks by not-spam vs spam&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1435 rows containing non-finite values (stat_bin). If it was difficult to work with the heavy skew of exclaim_mess, the number of images attached to each email (image) poses even more of a challenge. table(email$image) ## ## 0 1 2 3 4 5 9 20 ## 3811 76 17 11 2 2 1 1 Recall that this tabulates the number of cases in each category (so there were 3811 emails with 0 images, for example). Given the very low counts at the higher number of images, let’s collapse image into a categorical variable that indicates whether or not the email had at least one image. In this exercise, you’ll create this new variable and explore its association with spam. ** Here we deal with zero inflation** by converting the many zero values and the non zeros in to a categorical variable. There are other strategies, such as doing analysis on these two groups seperatley. # Create plot of proportion of spam by image email %&gt;% mutate(has_image = image &gt; 0) %&gt;% ggplot(aes(x = has_image, fill = factor(spam))) + geom_bar(position = &quot;fill&quot;) Sometimes it is neccessary to check if our understanding of the data and how it has been created is correct and if the values we expect are in fact true. In this instance, we check first if the number of charecters in the email is greater than zero (which it should be), then secondly whether images count as attachments using a boolean operator. If image is never greater than attach, we can infer that images are counted as attachments. # Verify that all emails have non-negative values for num_char sum(email$num_char &lt; 0) ## [1] 0 # Test if images count as attachments sum(email$images &gt;= email$attach) ## [1] 0 When you have a specific question about a dataset, you can find your way to an answer by carefully constructing the appropriate chain of R code. For example, consider the following question: “Within non-spam emails, is the typical length of emails shorter for those that were sent to multiple people?” This can be answered with the following chain: email %&gt;% filter(spam == &quot;not-spam&quot;) %&gt;% group_by(to_multiple) %&gt;% summarize(median(num_char)) ## # A tibble: 0 x 2 ## # ... with 2 variables: to_multiple &lt;dbl&gt;, median(num_char) &lt;lgl&gt; The code makes it clear that you are using num_char to measure the length of an email and median() as the measure of what is typical. If you run this code, you’ll learn that the answer to the question is “yes”: the typical length of non-spam sent to multiple people is a bit lower than those sent to only one person. This chain concluded with summary statistics, but others might end in a plot; it all depends on the question that you’re trying to answer. For emails containing the word “dollar”, does the typical spam email contain a greater number of occurrences of the word than the typical non-spam email? Create a summary statistic that answers this question. If you encounter an email with greater than 10 occurrences of the word “dollar”, is it more likely to be spam or not-spam? Create a barchart that answers this question. # Question 1 email %&gt;% filter(dollar &gt; 0) %&gt;% group_by(spam) %&gt;% summarize(median(dollar)) ## # A tibble: 2 x 2 ## spam `median(dollar)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 4 ## 2 1 2 # Question 2 email %&gt;% filter(dollar &gt; 10) %&gt;% ggplot(aes(x = spam)) + geom_bar() Turn your attention to the variable called number. To explore the association between this variable and spam, select and construct an informative plot. For illustrating relationships between categorical variables, you’ve seen # Reorder levels email$number &lt;- factor(email$number, levels = c(&quot;none&quot;, &quot;small&quot;, &quot;big&quot;)) # Construct plot of number ggplot(email, aes(x = number)) + geom_bar() + facet_wrap(~ spam) "],
["correlation-and-regression.html", "7 Correlation and Regression 7.1 Visualizing two variables 7.2 Correlation 7.3 Linear Regression 7.4 Model fit", " 7 Correlation and Regression Notes taken during/inspired by the Datacamp course ‘Correlation and Regression’ by Ben Baumer. 7.1 Visualizing two variables Some common terminology of data includes Response variable a.k.a. y, dependent (usually on the vertical axis if using a scatter plot) Explanatory variable, something you think might be related to the response a.k.a. x, independent, predictor (usually on the horizontal axis) library(openintro) library(ggplot2) library(dplyr) library(tidyr) # load the data data(ncbirths) # Scatterplot of weight vs. weeks ggplot(ncbirths, aes(weeks, weight)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). If it is helpful, you can think of boxplots as scatterplots for which the variable on the x-axis has been discretized. The cut() function takes two arguments: the continuous variable you want to discretize and the number of breaks that you want to make in that continuous variable in order to discretize it. # Boxplot of weight vs. weeks ggplot(data = ncbirths, aes(x = cut(weeks, breaks = 5), y = weight)) + geom_boxplot() 7.1.1 Transformations Here the relationship is hard to see. data(mammals) # Mammals scatterplot ggplot(mammals, aes(BodyWt, BrainWt)) + geom_point() The relationship between two variables may not be linear. In these cases we can sometimes see strange and even inscrutable patterns in a scatterplot of the data. Sometimes there really is no meaningful relationship between the two variables. Other times, a careful transformation of one or both of the variables can reveal a clear relationship. ggplot2 provides several different mechanisms for viewing transformed relationships. The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes. # Scatterplot with coord_trans() ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + geom_point() + coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) # Scatterplot with scale_x_log10() and scale_y_log10() ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + geom_point() + scale_x_log10() + scale_y_log10() 7.1.2 Identifying Outliers It is clear here, using the Using the mlbBat10 dataset, a scatterplot illustrates how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP). data(&quot;mlbBat10&quot;) # Baseball player scatterplot ggplot(mlbBat10, aes(OBP, SLG)) + geom_point() Most of the points are clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty is caused by a few outlying players whose on-base percentages (OBPs) were exceptionally high. These values are present in our dataset only because these players had very few batting opportunities. Both OBP and SLG are known as rate statistics, since they measure the frequency of certain events (as opposed to their count). In order to compare these rates sensibly, it makes sense to include only players with a reasonable number of opportunities, so that these observed rates have the chance to approach their long-run frequencies. In Major League Baseball, batters qualify for the batting title only if they have 3.1 plate appearances per game. This translates into roughly 502 plate appearances in a 162-game season. The mlbBat10 dataset does not include plate appearances as a variable, but we can use at-bats (AB) – which constitute a subset of plate appearances – as a proxy. # Scatterplot of SLG vs. OBP mlbBat10 %&gt;% filter(AB &gt;= 200) %&gt;% ggplot(aes(x = OBP, y = SLG)) + geom_point() # Identify the outlying player mlbBat10 %&gt;% filter(AB &gt;= 200, OBP &lt; 0.2) ## name team position G AB R H 2B 3B HR RBI TB BB SO SB CS OBP ## 1 B Wood LAA 3B 81 226 20 33 2 0 4 14 47 6 71 1 0 0.174 ## SLG AVG ## 1 0.208 0.146 7.2 Correlation We typically calculate the Pearsons aka Pearson product-moment correlation. The cor(x, y) function will compute the Pearson product-moment correlation between variables, x and y. Since this quantity is symmetric with respect to x and y, it doesn’t matter in which order you put the variables. At the same time, the cor() function is very conservative when it encounters missing data (e.g. NAs). The use argument allows you to override the default behavior of returning NA whenever any of the values encountered is NA. Setting the use argument to “pairwise.complete.obs” allows cor() to compute the correlation coefficient for those observations where the values of x and y are both not missing. data(ncbirths) # Compute correlation between the birthweight and mother&#39;s age ncbirths %&gt;% summarize(N = n(), r = cor(mage, weight)) ## N r ## 1 1000 0.05506589 # Compute correlation for all non-missing pairs ncbirths %&gt;% summarize(N = n(), r = cor(weight, weeks, use = &quot;pairwise.complete.obs&quot;)) ## N r ## 1 1000 0.6701013 7.2.1 Anscombe Dataset In 1973, Francis Anscombe famously created four synthetic datasets with remarkably similar numerical properties, but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable, set, that distinguishes the quartet. data(&quot;anscombe&quot;) # Tidy the data for plotting Anscombe &lt;- anscombe %&gt;% mutate(id = seq_len(n())) %&gt;% gather(key, value, -id) %&gt;% separate(key, c(&quot;variable&quot;, &quot;set&quot;), 1, convert = TRUE) %&gt;% mutate(set = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)[set]) %&gt;% spread(variable, value) # Plot the four variants ggplot(data = Anscombe, aes(x = x, y = y)) + geom_point() + facet_wrap(~ set) # Compute statistics for the sets Anscombe %&gt;% group_by(set) %&gt;% summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x,y)) ## # A tibble: 4 x 7 ## set N `mean(x)` `sd(x)` `mean(y)` `sd(y)` `cor(x, y)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11 9 3.316625 7.500909 2.031568 0.8164205 ## 2 2 11 9 3.316625 7.500909 2.031657 0.8162365 ## 3 3 11 9 3.316625 7.500000 2.030424 0.8162867 ## 4 4 11 9 3.316625 7.500909 2.030579 0.8165214 7.3 Linear Regression The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a “best fit” line that cuts through the data in a way that minimizes the distance between the line and the data points. We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we’ll set this argument to the value “lm”. data(bdims) # Scatterplot with regression line ggplot(data = bdims, aes(x = hgt, y = wgt)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) Sometimes it is better to think of the model error as ‘noise’ which we might try to better incorporate by creating a better model. Two facts enable you to compute the slope b1 and intercept b0 of a simple linear regression model from some basic summary statistics. First, the slope can be defined as: b1=rX,Y⋅sYsX where rX,Y represents the correlation (cor()) of X and Y and sX and sY represent the standard deviation (sd()) of X and Y, respectively. Second, the point (x¯,y¯)is always on the least squares regression line, where x¯and y¯denote the average of x and y, respectively. The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (Y) as a function of height (X). N &lt;- c(1507) r &lt;- c(0.7173011) mean_hgt &lt;- c(171.1438) sd_hgt &lt;- c(9.407205) mean_wgt &lt;- c(69.14753) sd_wgt &lt;- c(13.34576) bdims_summary &lt;- data.frame(N, r, mean_hgt, sd_hgt, mean_wgt, sd_wgt) # Print bdims_summary bdims_summary ## N r mean_hgt sd_hgt mean_wgt sd_wgt ## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 # Add slope and intercept bdims_summary %&gt;% mutate(slope = r * sd_wgt / sd_hgt, intercept = mean_wgt - slope * mean_hgt) ## N r mean_hgt sd_hgt mean_wgt sd_wgt slope intercept ## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 1.017617 -105.0112 7.3.1 Regression to the Mean Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While “regression to the mean” and “linear regression” are not the same thing, we will examine them together in this exercise. One way to see the effects of regression to the mean is to compare the heights of parents to their children’s heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches. # Galton data from http://www.math.uah.edu/stat/data/Galton.html Galton &lt;- read.csv(&quot;Galton.csv&quot;) # Height of children vs. height of father Galton %&gt;% filter(Gender == &quot;M&quot;) %&gt;% ggplot(aes(x = Father, y = Height)) + geom_point() + geom_abline(slope = 1, intercept = 0) + geom_smooth(method = &quot;lm&quot;, se = FALSE) # Height of children vs. height of mother Galton %&gt;% filter(Gender == &quot;F&quot;) %&gt;% ggplot(aes(x = Mother, y = Height)) + geom_point() + geom_abline(slope = 1, intercept = 0) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 7.3.2 Fitting linear models While the geom_smooth(method = “lm”) function is useful for drawing linear models on a scatterplot, it doesn’t actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is lm(). This function generally takes two arguments: A formula that specifies the model A data argument for the data frame that contains the data you want to use to fit the model The lm() function return a model object having class “lm”. This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc. # Linear model for weight as a function of height lm(wgt ~ hgt, data = bdims) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Coefficients: ## (Intercept) hgt ## -105.011 1.018 # Linear model for SLG as a function of OBP lm(SLG ~ OBP, data = mlbBat10) ## ## Call: ## lm(formula = SLG ~ OBP, data = mlbBat10) ## ## Coefficients: ## (Intercept) OBP ## 0.009407 1.110323 # Log-linear model for body weight as a function of brain weight lm(log(BodyWt) ~ log(BrainWt), data = mammals) ## ## Call: ## lm(formula = log(BodyWt) ~ log(BrainWt), data = mammals) ## ## Coefficients: ## (Intercept) log(BrainWt) ## -2.509 1.225 An “lm” object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information. The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2R2, adjusted R2R2, and the residual standard error. The summary of an “lm” object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.). Once you have fit a regression model, you are often interested in the fitted values (y^i) and the residuals (ei), where i indexes the observations. The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable. In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the fitted.values() and residuals() functions mod &lt;- lm(wgt ~ hgt, data = bdims) # Show the coefficients coef(mod) ## (Intercept) hgt ## -105.011254 1.017617 # Show the full output summary(mod) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## hgt 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 # Mean of weights equal to mean of fitted values? mean(bdims$wgt) == mean(fitted.values(mod)) ## [1] TRUE # Mean of the residuals mean(residuals(mod)) ## [1] -1.266971e-15 As you fit a regression model, there are some quantities (e.g. R2) that apply to the model as a whole, while others apply to each observation (e.g. y^i). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables. The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals. # Load broom library(broom) # Create bdims_tidy bdims_tidy &lt;- augment(mod) ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead # Glimpse the resulting data frame glimpse(bdims_tidy) ## Observations: 507 ## Variables: 9 ## $ wgt &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.... ## $ hgt &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 18... ## $ .fitted &lt;dbl&gt; 72.05406, 73.37697, 91.89759, 84.77427, 85.48661, 7... ## $ .se.fit &lt;dbl&gt; 0.4320546, 0.4520060, 1.0667332, 0.7919264, 0.81834... ## $ .resid &lt;dbl&gt; -6.4540648, -1.5769666, -11.1975919, -12.1742745, -... ## $ .hat &lt;dbl&gt; 0.002154570, 0.002358152, 0.013133942, 0.007238576,... ## $ .sigma &lt;dbl&gt; 9.312824, 9.317005, 9.303732, 9.301360, 9.312471, 9... ## $ .cooksd &lt;dbl&gt; 5.201807e-04, 3.400330e-05, 9.758463e-03, 6.282074e... ## $ .std.resid &lt;dbl&gt; -0.69413418, -0.16961994, -1.21098084, -1.31269063,... 7.4 Model fit One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are. However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or root mean squared error (RMSERMSE). R calls this quantity the residual standard error. To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. # View summary of model summary(mod) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## hgt 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 # Compute the mean of the residuals mean(residuals(mod)) ## [1] -1.266971e-15 # Compute RMSE sqrt(sum(residuals(mod)^2) / df.residual(mod)) ## [1] 9.30804 Another measure we can use is R squared, whihc is the he coefficient of determination. This gives us the interpretation of R2 as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model. In the example above, our model has an r-squared value of 51.5%. We can also calculate the R-squared value manually if desired. # Compute R-squared bdims_tidy %&gt;% summarize(var_y = var(wgt), var_e = var(.resid)) %&gt;% mutate(R_squared = 1 - var_e / var_y) ## var_y var_e R_squared ## 1 178.1094 86.46839 0.5145208 7.4.1 Unusual points As the model tries to fit the data on average, some extreme values can overly influence the model. We can quantify how much influence a particular point has by using the leverage, which is a measure for each observation as a function of the value of the explanatory variable and the mean of the explanatory variable. Therefore points to the centre line have a low leverage score, whilst points far from the line have a higher leverage. The explanatory variable y does not come in to effect. This can be calculated as the .hat value using augment() from broom. It is possible to have a value with a high leverage but a low overall impact on the model, if the point lies close to the line of the model. In this case, the residual is small for the point. Conversely, a point with a high leverage score and a high residual - a point laying a distance a way from other meaures and not predicted well by the model - does have an impact.We say such a point is influential. Numerically we can use cooks distance (.cooksd)to quantify this influence, which can also be calculated using the augment() function from broom. # Rank points of high leverage mod %&gt;% augment() %&gt;% arrange(desc(.hat)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## wgt hgt .fitted .se.fit .resid .hat .sigma .cooksd ## 1 85.5 198.1 96.57863 1.255712 -11.078629 0.01819968 9.303950 0.0133734319 ## 2 90.9 197.1 95.56101 1.214264 -4.661012 0.01701803 9.314916 0.0022081690 ## 3 49.8 147.2 44.78194 1.131432 5.018065 0.01477545 9.314548 0.0022120570 ## 4 80.7 193.5 91.89759 1.066733 -11.197592 0.01313394 9.303732 0.0097584634 ## 5 95.9 193.0 91.38878 1.046493 4.511216 0.01264027 9.315075 0.0015228117 ## 6 44.8 149.5 47.12245 1.037916 -2.322454 0.01243391 9.316688 0.0003968468 ## .std.resid ## 1 -1.2012024 ## 2 -0.5050673 ## 3 0.5431383 ## 4 -1.2109808 ## 5 0.4877505 ## 6 -0.2510763 # Rank influential points mod %&gt;% augment() %&gt;% arrange(desc(.cooksd)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## wgt hgt .fitted .se.fit .resid .hat .sigma .cooksd ## 1 73.2 151.1 48.75064 0.9737632 24.44936 0.010944356 9.252694 0.03859555 ## 2 116.4 177.8 75.92101 0.5065670 40.47899 0.002961811 9.140611 0.02817388 ## 3 104.1 165.1 62.99728 0.4914889 41.10272 0.002788117 9.135102 0.02733574 ## 4 108.6 190.5 88.84474 0.9464667 19.75526 0.010339372 9.275186 0.02377609 ## 5 67.3 152.4 50.07354 0.9223084 17.22646 0.009818289 9.285305 0.01714950 ## 6 76.8 157.5 55.26339 0.7287405 21.53661 0.006129560 9.267446 0.01661032 ## .std.resid ## 1 2.641185 ## 2 4.355274 ## 3 4.421999 ## 4 2.133444 ## 5 1.859860 ## 6 2.320888 When you have such outlying variables, you need to decide what to do. The main thing is to remove the variables from the model, but you need to consider the implications. There are other statistical techniques (see the EDA Chapter) for removing outliers. Think about whether the scope of the inference changes if you remove those values. Observations can be outliers for a number of different reasons. Statisticians must always be careful—and more importantly, transparent—when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher R2R2 is not a good enough reason! In the mlbBat10 data, the outlier with an OBP of 0.550 is Bobby Scales, an infielder who had four hits in 13 at-bats for the Chicago Cubs. Scales also walked seven times, resulting in his unusually high OBP. The justification for removing Scales here is weak. While his performance was unusual, there is nothing to suggest that it is not a valid data point, nor is there a good reason to think that somehow we will learn more about Major League Baseball players by excluding him. Nevertheless, we can demonstrate how removing him will affect our model. # Create nontrivial_players nontrivial_players &lt;- mlbBat10 %&gt;% filter(AB &gt;= 10 &amp; OBP &lt; 0.5) # Fit model to new data mod_cleaner &lt;- lm(SLG ~ OBP, data = nontrivial_players) # View model summary summary(mod_cleaner) ## ## Call: ## lm(formula = SLG ~ OBP, data = nontrivial_players) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31383 -0.04165 -0.00261 0.03992 0.35819 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.043326 0.009823 -4.411 1.18e-05 *** ## OBP 1.345816 0.033012 40.768 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07011 on 734 degrees of freedom ## Multiple R-squared: 0.6937, Adjusted R-squared: 0.6932 ## F-statistic: 1662 on 1 and 734 DF, p-value: &lt; 2.2e-16 # Visualize new model ggplot(data = nontrivial_players, aes(x = OBP, y = SLG)) + geom_point() + geom_smooth(method = &quot;lm&quot;) 7.4.2 High leverage Points Not all points of high leverage are influential. While the high leverage observation corresponding to Bobby Scales in the previous exercise is influential, the three observations for players with OBP and SLG values of 0 are not influential. This is because they happen to lie right near the regression anyway. Thus, while their extremely low OBP gives them the power to exert influence over the slope of the regression line, their low SLG prevents them from using it. mod &lt;- lm(formula = SLG ~ OBP, data = filter(mlbBat10, AB &gt;= 10)) # Rank high leverage points mod %&gt;% augment() %&gt;% arrange(desc(.hat),desc(.cooksd)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## SLG OBP .fitted .se.fit .resid .hat .sigma ## 1 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 2 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 3 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 4 0.308 0.550 0.69049108 0.009158810 -0.38249108 0.01641049 0.07011360 ## 5 0.000 0.037 0.01152451 0.008770891 -0.01152451 0.01504981 0.07154283 ## 6 0.038 0.038 0.01284803 0.008739031 0.02515197 0.01494067 0.07153800 ## .cooksd .std.resid ## 1 0.0027664282 0.5289049 ## 2 0.0027664282 0.5289049 ## 3 0.0027664282 0.5289049 ## 4 0.2427446800 -5.3943121 ## 5 0.0002015398 -0.1624191 ## 6 0.0009528017 0.3544561 "],
["supervised-learning.html", "8 Supervised Learning 8.1 Tree Based Models 8.2 Gradient Boosting Machines", " 8 Supervised Learning Notes taken during/inspired by the Datacamp course ‘Supervised Learning in R: Regression’ by Nina Zumel and John Mount. 8.1 Tree Based Models Tree based models can be used for both regression and classification models. Decision Trees say ‘if a AND b AND c THEN y’. We can therefore model non-linear models and multiplicative relationships - what is the affect of this AND that when combined together. We can use RMSE as a measure of accuracy of the model. The challenge with tree models is that they are interested in the model space as a whole, splitting this in to regions. Linear models can be better for linear relationships. We can adjust the tree depth, but there is a risk of overfitting (too deep/complex) or underfitting (to shallow/coarse). An ensemble model can be built combining different trees or indeed different models together, which will usually have the outcome of being better than a sinlge tree and less prone to overfitting, but at the loss of interpretability. 8.1.1 Random Forests One example of an ensemble approach is a random forest, building multiple trees from the training data. We can average the results of multiple models together to reduce the degree of overfitting. To build a random forest we perform the following Draw bootstrapped sample from training data For each sample grow a tree At each node, pick best variable to split on (from a random subset of all variables) Continue until tree is grown To score a datum, evaluate it with all the trees and average the results. We can use the ranger package to fit random forests. If the outcome is numeric, ranger will automatically do regression rather than classification. The default is for 500 trees, a minimum approach is 200. The value respect.unordered.factors will handle categorical values, set it to “order” if using cateogrical values, which will convert the values to numeric values. The measures of accuracy are R squared and OOB (Out of Bag or out of sample performance). You should still evaluate the model further using test data. In this exercise you will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July. You will use the ranger package to fit the random forest model. For this exercise, the key arguments to the ranger() call are: formula data num.trees: the number of trees in the forest. respect.unordered.factors : Specifies how to treat unordered factor variables. We recommend setting this to “order” for regression. seed: because this is a random algorithm, you will set the seed to get reproducible results Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in the variables outcome and vars, and use paste() to assemble a string representing the model formula. bikes &lt;- load(url(&quot;https://assets.datacamp.com/production/course_3851/datasets/Bikes.RData&quot;)) # bikesJuly is in the workspace str(bikesJuly) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 ... ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 ... ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ... ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... # Random seed to reproduce results seed &lt;- 423563 # The outcome column (outcome &lt;- &quot;cnt&quot;) ## [1] &quot;cnt&quot; # The input variables (vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;)) ## [1] &quot;hr&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot; ## [6] &quot;atemp&quot; &quot;hum&quot; &quot;windspeed&quot; # Create the formula string for bikes rented as a function of the inputs (fmla &lt;- paste(&quot;cnt&quot;, &quot;~&quot;, paste(vars, collapse = &quot; + &quot;))) ## [1] &quot;cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed&quot; # Load the package ranger library(ranger) # Fit and print the random forest model (bike_model_rf &lt;- ranger(fmla, # formula bikesJuly, # data num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed)) ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 In this exercise you will use the model that you fit in the previous exercise to predict bike rentals for the month of August. The predict() function for a ranger model produces a list. One of the elements of this list is predictions, a vector of predicted values. You can access predictions with the $ notation for accessing named elements of a list: predict(model, data)$predictions library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) # bikesAugust is in the workspace str(bikesAugust) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... ## $ cnt : int 47 33 13 7 4 49 185 487 681 350 ... ## $ instant : int 13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ... ## $ mnth : int 8 8 8 8 8 8 8 8 8 8 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... # bike_model_rf is in the workspace bike_model_rf ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 # Make predictions on the August data bikesAugust$pred &lt;- predict(bike_model_rf, bikesAugust)$predictions # Calculate the RMSE of the predictions bikesAugust %&gt;% mutate(residual = cnt - pred) %&gt;% # calculate the residual summarize(rmse = sqrt(mean(residual^2))) # calculate rmse ## rmse ## 1 97.18347 # Plot actual outcome vs predictions (predictions on x-axis) ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() In the previous exercise, you saw that the random forest bike model did better on the August data than the quasiposson model, in terms of RMSE. In this exercise you will visualize the random forest model’s August predictions as a function of time. The corresponding plot from the quasipoisson model that you built in a previous exercise is in the workspace for you to compare. Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares. library(tidyr) # Plot predictions and cnt by date/time randomforest_plot &lt;- bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% # set start to 0, convert unit to days gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Random Forest plot&quot;) randomforest_plot The random forest model captured the day-to-day variations in peak demand better than the quasipoisson model, but it still underestmates peak demand, and also overestimates minimum demand. So there is still room for improvement. 8.1.2 One-Hot-Encoding Categorical Variables For modelling purposes, we need to convert categorical variables to indicator variables. Some R packages do this automatically, but some non-native R packages, such as the xgboost package does not. So, these categorical variables need to be converted to numeric ones. We can use the vtreat package. DesignTreatmentsZ() to design a treatment plan from the training data, then prepare() to created “clean” data all numerical no missing values use prepare() with treatment plan for all future data In this exercise you will use vtreat to one-hot-encode a categorical variable on a small example. vtreat creates a treatment plan to transform categorical variables into indicator variables (coded “lev”), and to clean bad values out of numerical variables (coded “clean”). To design a treatment plan use the function designTreatmentsZ() treatplan &lt;- designTreatmentsZ(data, varlist) data: the original training data frame varlist: a vector of input variables to be treated (as strings). designTreatmentsZ() returns a list with an element scoreFrame: a data frame that includes the names and types of the new variables: scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code) varName: the name of the new treated variable origName: the name of the original variable that the treated variable comes from code: the type of the new variable. “clean”: a numerical variable with no NAs or NaNs “lev”: an indicator variable for a specific level of the original categorical variable. (magrittr::use_series() is an alias for $ that you can use in pipes.) For these exercises, we want varName where code is either “clean” or “lev”: (newvarlist &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;) %&gt;% magrittr::use_series(varName)) To transform the data set into all numerical and one-hot-encoded variables, use prepare(): data.treat &lt;- prepare(treatplan, data, varRestrictions = newvarlist) treatplan: the treatment plan data: the data frame to be treated varRestrictions: the variables desired in the treated data # Create the dataframe for cleaning color &lt;- c(&quot;b&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;b&quot;, &quot;r&quot;, &quot;g&quot;, &quot;b&quot;, &quot;b&quot;) size &lt;- c(13, 11, 15, 14, 13, 11, 9, 12, 7, 12) popularity &lt;- c(1.0785088, 1.3956245, 0.9217988, 1.2025453, 1.0838662, 0.8043527, 1.1035440, 0.8746332, 0.6947058, 0.8832502) dframe &lt;- cbind(color, size, popularity) dframe &lt;- as.data.frame((dframe)) # dframe is in the workspace dframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 # Create and print a vector of variable names (vars &lt;- c(&quot;color&quot;, &quot;size&quot;)) ## [1] &quot;color&quot; &quot;size&quot; # Load the package vtreat library(vtreat) # Create the treatment plan treatplan &lt;- designTreatmentsZ(dframe, vars) ## [1] &quot;desigining treatments Wed Aug 23 16:18:27 2017&quot; ## [1] &quot;designing treatments Wed Aug 23 16:18:27 2017&quot; ## [1] &quot; have level statistics Wed Aug 23 16:18:27 2017&quot; ## [1] &quot;design var color Wed Aug 23 16:18:27 2017&quot; ## [1] &quot;design var size Wed Aug 23 16:18:27 2017&quot; ## [1] &quot; scoring treatments Wed Aug 23 16:18:27 2017&quot; ## [1] &quot;have treatment plan Wed Aug 23 16:18:27 2017&quot; # Examine the scoreFrame (scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code)) ## varName origName code ## 1 color_lev_x.b color lev ## 2 color_lev_x.g color lev ## 3 color_lev_x.r color lev ## 4 color_catP color catP ## 5 size_lev_x.11 size lev ## 6 size_lev_x.12 size lev ## 7 size_lev_x.13 size lev ## 8 size_lev_x.14 size lev ## 9 size_lev_x.15 size lev ## 10 size_lev_x.7 size lev ## 11 size_lev_x.9 size lev ## 12 size_catP size catP # We only want the rows with codes &quot;clean&quot; or &quot;lev&quot; (newvars &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% magrittr::use_series(varName)) ## [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot; ## [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot; ## [9] &quot;size_lev_x.7&quot; &quot;size_lev_x.9&quot; # Create the treated training data (dframe.treat &lt;- prepare(treatplan, dframe, varRestriction = newvars)) ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 The new indicator variables have ‘lev’ in their names, and the new cleaned continuous variables have ’_clean’ in their names. The treated data is all numerical, with no missing values, and is suitable for use with xgboost and other R modeling functions. When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such novel levels appear, using model.matrix or caret::dummyVars to one-hot-encode will not work correctly. vtreat is a “safer” alternative to model.matrix for one-hot-encoding, because it can manage novel levels safely. vtreat also manages missing values in the data (both categorical and continuous). # Create the testframe for testing new vars color &lt;- c(&quot;g&quot;, &quot;g&quot;, &quot;y&quot;, &quot;g&quot;, &quot;g&quot;, &quot;y&quot;, &quot;b&quot;, &quot;g&quot;, &quot;g&quot;, &quot;r&quot;) size &lt;- c(7, 8, 10, 12, 6, 8, 12, 12, 12, 8) popularity &lt;- c(0.9733920, 0.9122529, 1.4217153, 1.1905828, 0.9866464, 1.3697515, 1.0959387, 0.9161547, 1.0000460, 1.3137360) testframe &lt;- cbind(color, size, popularity) testframe &lt;- as.data.frame((dframe)) # treatplan is in the workspace summary(treatplan) ## Length Class Mode ## treatments 4 -none- list ## scoreFrame 8 data.frame list ## outcomename 1 -none- character ## vtreatVersion 1 package_version list ## outcomeType 1 -none- character ## outcomeTarget 1 -none- character ## meanY 1 -none- logical ## splitmethod 1 -none- character # newvars is in the workspace newvars ## [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot; ## [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot; ## [9] &quot;size_lev_x.7&quot; &quot;size_lev_x.9&quot; # Print dframe and testframe dframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 testframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 # Use prepare() to one-hot-encode testframe (testframe.treat &lt;- prepare(treatplan, testframe, varRestriction = newvars)) ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 vtreat encodes novel colors like yellow that were not present in the data as all zeros: ‘none of the known colors’. This allows downstream models to accept these novel values without crashing. In this exercise you will create one-hot-encoded data frames of the July/August bike data, for use with xgboost later on. vars defines the variable vars with the list of variable columns for the model. # The outcome column (outcome &lt;- &quot;cnt&quot;) ## [1] &quot;cnt&quot; # The input columns (vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;)) ## [1] &quot;hr&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot; ## [6] &quot;atemp&quot; &quot;hum&quot; &quot;windspeed&quot; # Load the package vtreat library(vtreat) # Create the treatment plan from bikesJuly (the training data) treatplan &lt;- designTreatmentsZ(bikesJuly, vars, verbose = FALSE) # Get the &quot;clean&quot; and &quot;lev&quot; variables from the scoreFrame (newvars &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% # get the rows you care about magrittr::use_series(varName)) # get the varName column ## [1] &quot;hr_lev_x.0&quot; ## [2] &quot;hr_lev_x.1&quot; ## [3] &quot;hr_lev_x.10&quot; ## [4] &quot;hr_lev_x.11&quot; ## [5] &quot;hr_lev_x.12&quot; ## [6] &quot;hr_lev_x.13&quot; ## [7] &quot;hr_lev_x.14&quot; ## [8] &quot;hr_lev_x.15&quot; ## [9] &quot;hr_lev_x.16&quot; ## [10] &quot;hr_lev_x.17&quot; ## [11] &quot;hr_lev_x.18&quot; ## [12] &quot;hr_lev_x.19&quot; ## [13] &quot;hr_lev_x.2&quot; ## [14] &quot;hr_lev_x.20&quot; ## [15] &quot;hr_lev_x.21&quot; ## [16] &quot;hr_lev_x.22&quot; ## [17] &quot;hr_lev_x.23&quot; ## [18] &quot;hr_lev_x.3&quot; ## [19] &quot;hr_lev_x.4&quot; ## [20] &quot;hr_lev_x.5&quot; ## [21] &quot;hr_lev_x.6&quot; ## [22] &quot;hr_lev_x.7&quot; ## [23] &quot;hr_lev_x.8&quot; ## [24] &quot;hr_lev_x.9&quot; ## [25] &quot;holiday_clean&quot; ## [26] &quot;workingday_clean&quot; ## [27] &quot;weathersit_lev_x.Clear.to.partly.cloudy&quot; ## [28] &quot;weathersit_lev_x.Light.Precipitation&quot; ## [29] &quot;weathersit_lev_x.Misty&quot; ## [30] &quot;temp_clean&quot; ## [31] &quot;atemp_clean&quot; ## [32] &quot;hum_clean&quot; ## [33] &quot;windspeed_clean&quot; # Prepare the training data bikesJuly.treat &lt;- prepare(treatplan, bikesJuly, varRestriction = newvars) # Prepare the test data bikesAugust.treat &lt;- prepare(treatplan, bikesAugust, varRestriction = newvars) # Call str() on the treated data str(bikesAugust.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday_clean : num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 0 0 1 0 0 0 ... ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Misty : num 0 0 0 0 1 1 0 1 1 1 ... ## $ temp_clean : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp_clean : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum_clean : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed_clean : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... str(bikesJuly.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Misty : num 0 0 0 0 0 0 0 0 0 0 ... ## $ temp_clean : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp_clean : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum_clean : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed_clean : num 0 0.1343 0.0896 0.1343 0.194 ... 8.2 Gradient Boosting Machines Gradient boosting is an interative ensemble method, by improving the model each time. We start the model with a usually shallow tree. Next, we fit another model to the residuals ofd the model, then find the weighted sum of the second and first models that give the best fit. We can regualrise the learning by the factor eta, eta = 1 gives fast learning but with overfitting risk, smaller eta reduces speed of learning but reduces the risk of overfitting. We then repeat this process until the stopping condition is met. Gradient boosting works on the training data, so it can be easy to overfit. The best approach then is to use OOB and cross validation (CV) for each model, then determine how many trees to use. xgb.cv() is the function we use and has a number of diagnostic measures. One such measure is the xgb.cv()$evaluation_log: records estimated RMSE for each round - find the number that minimises the RMSE Inputs to xgb.cv() and xgboost() are: data: input data as matrix ; label: outcome label: vector of outcomes (also numeric) objective: for regression - “reg:linear” nrounds: maximum number of trees to fit eta: learning rate max_depth: depth of trees early_stopping_rounds: after this many rounds without improvement, stop nfold (xgb.cv() only): number of folds for cross validation. 5 is a good number verbose: 0 to stay silent. Then we use elog &lt;- as.data.frame(cv\\(evaluation_log) nrounds &lt;- which.min(elog\\)test_rmse_mean) With the resulting number being the best number of trees. We then use xbgoost with this number (nrounds &lt;- n) to get the final model. In this exercise you will get ready to build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July. The July data is loaded into your workspace. Remember that bikesJuly.treat no longer has the outcome column, so you must get it from the untreated data: bikesJuly$cnt. You will use the xgboost package to fit the random forest model. The function xgb.cv() uses cross-validation to estimate the out-of-sample learning error as each new tree is added to the model. The appropriate number of trees to use in the final model is the number that minimizes the holdout RMSE. # The July data is in the workspace ls() ## [1] &quot;bike_model_rf&quot; &quot;bikes&quot; &quot;bikesAugust&quot; ## [4] &quot;bikesAugust.treat&quot; &quot;bikesJuly&quot; &quot;bikesJuly.treat&quot; ## [7] &quot;color&quot; &quot;dframe&quot; &quot;dframe.treat&quot; ## [10] &quot;fmla&quot; &quot;newvars&quot; &quot;outcome&quot; ## [13] &quot;popularity&quot; &quot;randomforest_plot&quot; &quot;scoreFrame&quot; ## [16] &quot;seed&quot; &quot;size&quot; &quot;testframe&quot; ## [19] &quot;testframe.treat&quot; &quot;treatplan&quot; &quot;vars&quot; # Load the package xgboost library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice # Run xgb.cv cv &lt;- xgb.cv(data = as.matrix(bikesJuly.treat), label = bikesJuly$cnt, nrounds = 100, nfold = 5, objective = &quot;reg:linear&quot;, eta = 0.3, max_depth = 6, early_stopping_rounds = 10, verbose = 0 # silent ) # Get the evaluation log elog &lt;- as.data.frame(cv$evaluation_log) # Determine and print how many trees minimize training and test error elog %&gt;% summarize(ntrees.train = which.min(elog$train_rmse_mean), # find the index of min(train_rmse_mean) ntrees.test = which.min(elog$test_rmse_mean)) # find the index of min(test_rmse_mean) ## ntrees.train ntrees.test ## 1 97 87 In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test error starts to increase. It’s important to use cross-validation to find the right number of trees (as determined by ntrees.test) and avoid an overfit model. # The number of trees to use, as determined by xgb.cv ntrees &lt;- 84 # Run xgboost bike_model_xgb &lt;- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix label = bikesJuly$cnt, # column of outcomes nrounds = ntrees, # number of trees to build objective = &quot;reg:linear&quot;, # objective eta = 0.3, depth = 6, verbose = 0 # silent ) # Make predictions bikesAugust$pred &lt;- predict(bike_model_xgb, as.matrix(bikesAugust.treat)) # Plot predictions (on x axis) vs actual bike rental count ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() Overall, the scatterplot looked pretty good, but did you notice that the model made some negative predictions? In the next exercise, you’ll compare this model’s RMSE to the previous bike models that you’ve built. Finally we can calculate the RMSE # Calculate RMSE bikesAugust %&gt;% mutate(residuals = cnt - pred) %&gt;% summarize(rmse = sqrt(mean(residuals ^ 2))) ## rmse ## 1 76.36407 Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous model. Perhaps rounding negative predictions up to zero is a reasonable tradeoff. Finally we can compare the results graphically. randomforest_plot # Plot predictions and actual bike rentals as a function of time (days) bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% # set start to 0, convert unit to days gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Gradient Boosting model&quot;) We can also plot the importance of the top factors names &lt;- dimnames(data.matrix(bikesJuly.treat[,-1]))[[2]] importance_matrix &lt;- xgb.importance(names, model = bike_model_xgb) xgb.plot.importance(importance_matrix[1:10,]) Looking at the results indicates that the temperature and clear/partly cloudy and the two most important factors, followed by the windspeed. The other factors relate to the time of day - higher at commuting times (9-10 am and 6-7 pm) and lower at night (2 and 4 am). "],
["dimensional-modelling.html", "9 Dimensional Modelling 9.1 Introduction to Dimensional Data 9.2 Architecture considerations 9.3 Graphical Representations 9.4 Kimball Approach 9.5 Four-Step Dimensional Design Process 9.6 Tips", " 9 Dimensional Modelling 9.1 Introduction to Dimensional Data Dimensional modelling helps to build the ability for users to query the information, for instance analysing results by a geographic region. Multi-dimensional modelling is an extension to allowing multiple ways to analsye the information, by geographic region but also over time, by product or service, by store or office and so on. It provides a way for a system user, manager or analyst to navigate what information - the ‘information space’ - is available in a database or data warehouse, but at a more intuitive level (see Meridith 2017, lecture 4). The goal is to help understanding, exploration and to make better decisions. A dimension is simply a direction, usually query or analytically based, in which you can move. Dimensional modelling is different from Entity Relationship diagrams which are more typically used for database design, however they do share some similarities and are sometimes used for dimensional modelling particuarly by those from a database or IT background. The dimensions used therefore become the ways in which the end user wants to query the information. Typical terms used in the BI arena for helping to navigate this ‘information space’ include; ‘slice and dice’ meaning to make a large data space in to a smaller one (you are making a selection or subset of all the available data), ‘drill down’ meaning to go in to a lower level of a hierachy (moving from a geographic region to a particular store), ‘drill up’ meaning to go in to a higher level (sometimes called rolling-up) and ‘drill across’ meaning adding more data (or facts) about something, typically from another source (a different fact table). There are two slightly different interpretations of a dimensional model (Meridith 2017, lecture 4): OLAP: A dimension is a structural attribute of a data cube. A dimension acts as an index for identifying values in a multi-dimensional array Kimball: A dimension table are where the textual descriptions which relate to aspects of the business are stored In both instances however, they provide ways to interact and understand our information. There are two things we are typically trying to map: Facts: Data itself, values, sales and so on e.g. a sales transaction number and the products sold Dimensions: Different ways of presenting or quering the information, this is often in the form of attributes about the fact e.g. product specific and store details 9.1.1 Data Modelling levels There are three aspects of information with a Business Intelligence system - conceptual, logical and physical - which exist on a spectrum. Conceptual: The business needs are usually the high level conceptual solution, what things we want to include at a more abstract level Logical: We start thinking about what data to include in the model and what data is available, it starts giving something which can be implemented in to a warehouse Physical: The final solution which is usually then what is implemented in the data warehouse. It is the more technical/IT solution and may include normalisation (3NF or higher) and perhaps other database optimisations to improve performance of the system. In some instances, the conceptual and logical can become one and the same thing. Table 3.1: The three levels of data modelling Feature Conceptual Logical Logical Entity Names Y Y Entity Relationships Y Y Attributes Y Primary Keys Y Y Foreign Keys Y Y Table Names Y Column Names Y Column Data Types 9.2 Architecture considerations There are a number of different approaches to implementing a data warehouse, or Enterprise Data Warehouse (EDW) from the IT or technical perspective. However, all approaches use the dimensional data modelling technique. A full detailed explanation of all possible architecture approaches, including hybrid approaches, is not included here. Instead we discuss at a high level the three main approaches - Kimball Inmon and Data Valut - and touch on a couple of others. Where they differ in terms of data modelling in part depends on the location of the dimensional model. Kimball - as the last part of the Extract Transform and Load (ETL) process the data is structured and loaded in to the desired dimensional model(s). There is no EDW in the Kimball approach, instead the presentation area is where data is organized, stored, and made available for direct querying by users, report writers, and other analytical BI applications. Data is stored in the multi-dimensional views as different data marts, which are typically subsets of all the data originally extracted, perhaps for different business users or services Inmon - suggests that the data should be relationally designed. The data is stored in an EDW in third normal form (3NF). The dimensional model then transates the data from the EDW in to something for an end user, visualisation tool or other such BI tool, potentially including data marts. A Hub and Spoke system is often used to describe the approach, with the EDW being the hub and the spokes being the depdendent data marts. This helps to ensure a ‘single verison of the truth’ Data Vault - Centralised approach - similar to Inmon but without the dependent data marts (spokes). Users directly target the EDW and there may be many different dimensional data models Hybrid - there are various different ways this could be setup, however one way would be that data is still stored in the EDW, but the dimensional model is used to help structure the data in the EDW. Therefore the extra translation required from the EDW to a BI tool is reduced. In the Kimball approach when attributes in separate dimension tables have the same column names and domain contents. After validating the data for conformance with the defined one-to-one and many-to-one business rules [as part of the ETL processs], it may be pointless to take the final step of building a 3NF physical database, just before transforming the data once again into denormalized structures for the BI presentation area. (Kimball and Ross 2013, pg 20) For the kimball approach to work, so called ‘conformed dimensions’ must be developed which are said to conform when attributes in separate dimension tables have the same column names and domain contents (Kimball and Ross 2013, pg 51). Inmon sees that the dimensional modelling technique can cause problems when teams need different star schemas - dimensional models - which then lead to a need to combine the different joins together, or lead to issues of duplication and inconsistencies. simply doing dimensional modeling as a basis for data warehouse design leads down a dark path when multiple star joins are considered. It is never apparent that there is a problem with star joins when you are looking at just one star join. But when you look at multiple star joins, the limitations of dimensional modeling become apparent. (Inmon 2000) Inmon concludes that dimensional modelling is only really suitable for data marts (ibid). 9.3 Graphical Representations Figure 9.1: High Level Overview of a Data Warehouse (Schnider, Martino, and Eschermann 2014, pg 3) Figure 9.2: Star schema versus OLAP cube (Kimball and Ross 2013, pg 9) Figure 9.3: Star schema example (Kimball and Ross 2013, pg 16) Figure 9.4: Star and Snowflake Schemas (Sharda, Delan, and Turban 2014, pg 139) Figure 9.5: Example slices from a OLAP data cube (Sharda, Delan, and Turban 2014, pg 141) Figure 9.6: Star schema reporting (Kimball and Ross 2013, pg 17) 9.4 Kimball Approach Before work begins of the data modelling, it is neccessary to understand the needs of the business and the underlying data (Kimball and Ross 2013, pg 37). The business needs arise out of meetings with manangers, decision makers and other representatives of the business. Kimball also recommends meetings with ‘source system experts and doing high-level data profiling to assess data feasibilities’ (Kimball and Ross 2013, pg 38). Whilst the data modeller is ‘in charge’ the actual model should unfold via a series of interactive workshops with those business representatives. Data governance reps should also be involved to obtain buy-in. In this sense, the Kimball approach covers both the conceptual and physical, it may also include some considerations of physical level at initiation. 9.5 Four-Step Dimensional Design Process Kimball outlines four key decisions that are to be made during the design of a dimensional model include: Select the business process - the operational activities done by the business, these activities create the facts Declare the grain - what a single row represents. The atomic grain is the lowest data captured by the business, which is the ideal and can be aggregared (rolled-up) to other levels. Different grains must not be mixed in the same fact table Identify the dimensions - the descriptive attributes about the facts, to be used for analysis. Provide the “who, what, where, when, why, and how” (6W) context Identify the facts - the measurements (how many) from the business process, it should relate to a physical observable event, rather than reporting needs Typically the output of this process is a star schema, with a fact table at the centre supported by the associated dimension tables, with primary/forenigh key relationships. This is often then structured into a online analytical processing (OLAP) cube, which contains the facts and dimensions appropriate to the analysis, but allows for more detailed analytical capabilities than SQL. Sometimes aggregated fact tables are built to speed up query performance, as are aggregated OLAP cubes which are typically designed for users. A key advantage of the dimensional model approach is that new dimensions can be added to an existing fact table by adding a new foreign key column. Discussion then of * Thomsen diagrams OLAP Solutions (2nd ed) 2002, an abstract, but can be a little simple * ADAPT Diagrams, White Paper Bulos and Foresman - included in some Microsoft products such as Visio and SQL Server, a bit too technical but good for communicating to IT * BEAM/Agile approach 8 mins 9.6 Tips Think about the types of analysis or questions that the user or manager may want to ask. This will help structure the data and help to ensure nothing is missing At the same time, just because something exists in the organisation or in a data source does not mean it has to be included. You need to think about that to include and what to exclude Equally, there may be instances where there is a desire to add something in to the model but it does not currently exist. This should be flagged and discussed with those intending to use the BI tool / output What are the end uses of the system or systems? If there are potentially multiple systems, multiple teams and multiple views on the data, it may make sense to store the data in its original state (3NF) in the EDW or similar store, then do the dimensional mapping in the BI tool, so it can be customised to the audience (Meridith 2017, lecture 4). This can lead to some duplication, however an option might be to share the dimensional models in some central repository, allowing users to customise for their use, whilst still being able to share the same source data and the benefits this brings. Evidently this lends itself to an Inmon or other such approach and less so the Kimball approach References "],
["references-3.html", "References", " References "]
]
