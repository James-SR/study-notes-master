[
["index.html", "Study notes Preface", " Study notes James Solomon-Rounce Last updated:2017-09-24 Preface The following notes were taken by me for educational, non-commercial, purposes. If you find the information useful, buy the material/take the course. Thank you to the original content providers. Additional ramblings are my own. "],
["importing-data-part-1.html", "1 Importing data - Part 1 1.1 Introduction 1.2 Reading CSV files 1.3 Reading tab deliminated files or other table formats 1.4 Readr and data.table 1.5 Reading Excel files 1.6 XLConnect - read and write to excel", " 1 Importing data - Part 1 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 1)’ by Filip Schouwenaars. 1.1 Introduction Data often comes from many different sources and formats, including Flat files - simple tables e.g. csv Excel Databases - MySQL, Postgres Websites - APIs, JSON, scraping Other statistical software - SPSS, STATA, SAS 1.2 Reading CSV files Reading csv files can be achived with simple code like read.csv(&quot;file.csv&quot;, stringsAsFactors = FALSE) We may want to import strings as categorical variables, in which case we would set stringsAsFactors = TRUE which is also the default option, if not stated. When working across different machines or operating systems, problems can arise due to different ways of addressing file locations and differing file locations. Therefore, it can be easier to set a relative path to the users home directory, which would be achieved with the following code. path &lt;- file.path(&quot;~&quot;, &quot;datasets&quot;, &quot;file.csv&quot;) path ## [1] &quot;~/datasets/file.csv&quot; Then use the file path as before, assigning to a dataframe. df &lt;- read.csv(path, stringsAsFactors = FALSE) 1.3 Reading tab deliminated files or other table formats In a similar way to before, we add the path to the file and if we want strings as strings, for instance read.delim(&quot;file.csv&quot;, stringsAsFactors = FALSE) However, if the file comes in another format perhaps due to the system encoding or setup, it is still possible to try and read the file as a tabular formatting converting it to a data frame. To do so, we use the read.table() command which has a lot of arguments that can be customised. You can specify column names and types for instance. If for instance we have a file format where the objects are separated by a / rather than a comma or tab as before, we could use read.table(&quot;file.txt&quot;, header = TRUE, sep = &quot;/&quot;, stringsAsFactors = FALSE) Or, if you have a file which has no column/variable names and tabs as spaces, you would read the file as: # Path to the file.txt file: path path &lt;- file.path(&quot;data&quot;, &quot;file.txt&quot;) # Import the file.txt file: hotdogs file &lt;- read.table(path, sep = &quot;\\t&quot;, # specify seperator - tab in this instance col.names = c(&quot;VarName1&quot;, &quot;VarName2&quot;, &quot;VarName3&quot;), # specifiy variable names colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # specify the column/variable classes Both read.csv and read.delim are wrapper functions of read.table(), both use read.table but have different default options depending on the file type. There are two further wrapper functions - read.csv2 and read.delim2 - which deal with regional differences in formatting, notably that some areas use full stops as decimal places, whereas other areas use commas for decimal places. 1.4 Readr and data.table These two packages are other ways of reading in files. Readr uses the tibble, so will be compatible with other tidyverse packages such as dplyr. It is faster than utils, the r default and also prints out the column classes, depending on what other packages are loaded. It is not necessary to specify stringsAsFactors = FALSE. library(readr) read_csv(&quot;file.csv&quot;) #read comma seperated read_tsv(&quot;file2.txt&quot;) #read tab seperated files #If there are no row heads, you can create a vector then read it in using the col_names argument #specify the vector for column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) #read in the vector df &lt;- read_tsv(&quot;file3.txt&quot;, col_names = properties) Like the utils package, these are wrapper functions, with the base function being read_delim(). Unlike the utils package, read_delim() expects the first row to contain headers, so this doesn’t need to be explicit. As mentioned previously, it is also not necessary to specify the we don’t want strings as factors. You can specify col_names using a vector as before, or we can read them directly at the time. If we also want to explicitly state the column types, perahps because the automatically assigned variable is not correct, we can do so with col_type using abbreviations: c = character d = double i = integer n = number l = logical D = date T = date time t = time ? = guess _ = skip column (underscore) Finally, we can use skip and n_max to specify how many rows to skip at the beginning of a file, perhaps due to a large header, and the maximum now of rows to read, perhaps due to a very large file with many rows. read_delim(&quot;file4.txt&quot;, delim = &quot;/&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;)) read_delim(&quot;file5.txt&quot;, delim = &quot;/&quot;, col_types = &quot;ccid&quot;) read_delim(&quot;file6.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;), skip = 12, n_max = 50000) Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column. Look at the collector documentation for more details. Two examples are shown below, one for columns to be interpreted as integers and one for a column with factors. # The collectors needed for importing fac &lt;- col_factor(levels = c(&quot;Beef&quot;, &quot;Meat&quot;, &quot;Poultry&quot;)) int &lt;- col_integer() # Edit the col_types argument with the specified collectors hotdogs_factor &lt;- read_tsv(&quot;hotdogs.txt&quot;, col_names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), col_types = list(fac, int, int)) 1.4.1 data.table fread data.table is a tool for doing fast data analysis, particularly on large datasets. It also has a function to read data using the fread() command. It can automatically infer column names, types and separators. You can also drop or select columns at read time. df &lt;- fread(&quot;file7.csv&quot;, select = c(&quot;colname1&quot;, &quot;colname2&quot;)) The readr package fill create different dataframe types or object classes - ‘tbl_df’, ‘tbl’ and ‘data.frame’ - which can be useful for different purposes, such as for use in dplyr. Fread creates a data.table object class. 1.5 Reading Excel files There are many packages for reading Excel files, one package is the readxl package by Hadley Wickham. There are to main functions excel_sheets(): lists the sheets within an excel file or workbook read_excel(): import the data, unless specified the first sheet is read, this can either be done with sheet = 7, or sheet = “name”. So to read an urbanpop.xlsx file containing three sheets of urban populations, for different time frames, our code would look similar to that below. library(readxl) #list the sheerts in the file excel_sheets(&quot;urbanpop.xlsx&quot;) # Read the sheets, one by one pop_1 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 1) pop_2 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2) pop_3 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 3) # Put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) # IF we want to read all the files, a more efficient way to read all the files in the file uses lapply pop_list &lt;- lapply(excel_sheets(&quot;urbanpop.xlsx&quot;), read_excel, path = &quot;urbanpop.xlsx&quot;) There are other arguments that can be used with the read_excel() function: col_names: If true, the first row is read, if false R will assign it’s own names or you specify a charecter vector manually col_types: If NULL, R gueses the data types of the columns. Alternatively, they can be specified e.g. text, numeric, date, blank (which ignores the col) skip: Speficies the number of rows to ignore # Some examples # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = FALSE) # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = cols) # Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel urbanpop_sel &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2, col_names = FALSE, skip = 21) # Print out the first observation from urbanpop_sel urbanpop_sel[1,] 1.5.1 Alternatives for importing Excel files One alternative is the gdata package, which is a suite of tools for data. There is a read.xls() function which only, currently, supports XLS files although xlsx could be supported with a driver. The data is interpreted by the read.xls file using perl into a csv file, which is then read using the read.csv function - itself a offshoot of read.table, in to an R data frame. Hadley’s readxl package is faster, but is quite early in it’s development so some of the functions may change. For gdata, as it is an offshoot of read.table(), all of the same arguments can be used by read.xls(). 1.6 XLConnect - read and write to excel Most of the Excel tools can become accessible but inside R, using XLConnect. It is possible to use XLS and XLSX and it will create a ‘workbook’ object in R, but it does require Java to work. library(XLConnect) #create a connect to a file and list the sheets book &lt;- loadWorkbook(&quot;file.xlsx&quot;) getSheets(book) #read in the specific sheet but only the columns we are interested in wardData &lt;- readWorksheet(book, sheet = &quot;sheet_1&quot;, startCol = 3, endCol = 5) # read in the names column, previoulsy excluded wardNames &lt;- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1) #cbind the data and names together selection &lt;- cbind(wardNames, wardData) XLConnect has more features than simply reading sheets. It is possible to write data back to the Excel file also. We can add sheets, write or add data to sheets, rename and remove sheets. # Add a worksheet to my_book, named &quot;summary&quot; createSheet(my_book, &quot;summary&quot;) # Add data in summ to &quot;data_summary&quot; sheet writeWorksheet(my_book, summ, &quot;summary&quot;) # Save workbook as summary.xlsx saveWorkbook(my_book, &quot;summary.xlsx&quot;) # Rename &quot;summary&quot; sheet to &quot;data_summary&quot; renameSheet(my_book, sheet = 4, &quot;data_summary&quot;) # Remove the third sheet removeSheet(my_book, sheet = 3) "],
["importing-data-part-2.html", "2 Importing data - Part 2 2.1 Importing from Databases - 1 2.2 SQL Queries Inside R 2.3 Web Data 2.4 JSON and APIs 2.5 Importing from other statistical software", " 2 Importing data - Part 2 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 2)’ by Filip Schouwenaars. 2.1 Importing from Databases - 1 In a professional or commercial setting, you often deal with more complicated file structures and source systems that simple flat files. Often the data is stored in a DBMS or Database Management System and SQL is the usual way of quering the DBMS. As there can be slight differences, you are likely to need different packages, some include: MySQL: Use the RMySQL package PostgresSQL: Use the RPostgresSQL package Oracle: Use the ROracle (etc…) Conventions are specified in the DBI - another R package, DBI is the interface and the other packages are the implentation. Some of the packages will automaticlaly install the DBI package as well. To connect to a database we would so something like the following. # Load the DBI package library(DBI) ## Loading required package: methods # Edit dbConnect() call - the first part specifies how connections are map to the database con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Build a vector of table names: tables tables &lt;- dbListTables(con) # Display structure of tables str(tables) ## chr [1:3] &quot;comments&quot; &quot;tweats&quot; &quot;users&quot; # Import the users table from tweater: users users &lt;- dbReadTable(con, &quot;users&quot;) # Print users users ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja # Or we can import all tables using lapply tables &lt;- lapply(tables, dbReadTable, conn = con) # Print out tables tables ## [[1]] ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! ## 3 1011 49 5 love it ## 4 1012 87 1 awesome! thanks! ## 5 1010 88 6 yuck! ## 6 1026 77 4 not my thing! ## 7 1004 49 1 this is fabulous! ## 8 1030 75 6 so easy! ## 9 1025 88 2 oh yes ## 10 1007 49 3 serious? ## 11 1020 77 1 couldn&#39;t be better ## 12 1014 77 1 saved my day ## ## [[2]] ## id user_id ## 1 75 3 ## 2 88 4 ## 3 77 6 ## 4 87 5 ## 5 49 1 ## 6 24 7 ## post ## 1 break egg. bake egg. eat egg. ## 2 wash strawberries. add ice. blend. enjoy. ## 3 2 slices of bread. add cheese. grill. heaven. ## 4 open and crush avocado. add shrimps. perfect starter. ## 5 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 6 just eat an apple. simply and healthy. ## date ## 1 2015-09-05 ## 2 2015-09-14 ## 3 2015-09-21 ## 4 2015-09-22 ## 5 2015-09-22 ## 6 2015-09-24 ## ## [[3]] ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja 2.2 SQL Queries Inside R OFten you don’t want an entire tabel from a database, but a selection from the table. You can use SQL queries from inside R to extract only what you are interested in. You can alternatively use subset on the imported table, but often it is easier to extract only what you need first, particularly when working with large databases. The SQL goes inside e.g. dbGetQuery(con, “SQL QUERY”). # Connect to the database library(DBI) con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth &lt;- dbGetQuery(con, &quot;SELECT tweat_id FROM comments WHERE user_id = 1&quot;) # Print elisabeth elisabeth ## tweat_id ## 1 87 ## 2 49 ## 3 77 ## 4 77 # Import post column of tweats where date is higher than &#39;2015-09-21&#39;: latest latest &lt;- dbGetQuery(con, &quot;SELECT post FROM tweats WHERE date &gt; &#39;2015-09-21&#39;&quot;) # Print latest latest ## post ## 1 open and crush avocado. add shrimps. perfect starter. ## 2 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 3 just eat an apple. simply and healthy. # Create data frame specific using boolean specific &lt;- dbGetQuery(con, &quot;SELECT message FROM comments WHERE tweat_id = 77 AND user_id &gt; 4&quot;) # Print specific specific ## message ## 1 great! # Create data frame short selecting two columns short &lt;- dbGetQuery(con, &quot;SELECT id, name FROM users WHERE CHAR_LENGTH(name) &lt; 5&quot;) # Print short short ## id name ## 1 2 mike ## 2 3 thea ## 3 6 kate # We can also join elements from different tables using the same id/key dbGetQuery(con, &quot;SELECT post, message FROM tweats INNER JOIN comments on tweats.id = tweat_id WHERE tweat_id = 77&quot;) ## post message ## 1 2 slices of bread. add cheese. grill. heaven. great! ## 2 2 slices of bread. add cheese. grill. heaven. not my thing! ## 3 2 slices of bread. add cheese. grill. heaven. couldn&#39;t be better ## 4 2 slices of bread. add cheese. grill. heaven. saved my day You’ve used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed: Sending the specified query with dbSendQuery(); Fetching the result of executing the query on the database with dbFetch(); Clearing the result with dbClearResult(). Let’s not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query’s result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch(). It is important to close the connection to the database once complete using the dbDisconnect() function # Send query to the database res &lt;- dbSendQuery(con, &quot;SELECT * FROM comments WHERE user_id &gt; 4&quot;) # Use dbFetch() twice dbFetch(res, n = 2) ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! dbFetch(res) # imports all ## id tweat_id user_id message ## 1 1011 49 5 love it ## 2 1010 88 6 yuck! ## 3 1030 75 6 so easy! # Clear res dbClearResult(res) ## [1] TRUE # Create the data frame long_tweats long_tweats &lt;- dbGetQuery(con, &quot;SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) &gt; 40&quot;) # Print long_tweats print(long_tweats) ## post ## 1 wash strawberries. add ice. blend. enjoy. ## 2 2 slices of bread. add cheese. grill. heaven. ## 3 open and crush avocado. add shrimps. perfect starter. ## 4 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## date ## 1 2015-09-14 ## 2 2015-09-21 ## 3 2015-09-22 ## 4 2015-09-22 # Disconnect from the database dbDisconnect(con) ## [1] TRUE 2.3 Web Data HyperText Transfer Protocol (HTTP) is the ‘language of the web’ and consists of a set of rules about data exchange between computers. If the file is a csv file, we can use functions like read.csv() and add in the url in quotations marks, read.csv will recognise this is a URL and will issue a HTTP GET command to download the file. This will also work on https sites on newer versions of R. We can also use the readr package and other packages. # Load the readr package library(readr) # Import the csv file: pools url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; pools &lt;- read_csv(url_csv) ## Parsed with column specification: ## cols( ## Name = col_character(), ## Address = col_character(), ## Latitude = col_double(), ## Longitude = col_double() ## ) # Import the txt file: potatoes url_delim &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt&quot; potatoes &lt;- read_tsv(url_delim) ## Parsed with column specification: ## cols( ## area = col_integer(), ## temp = col_integer(), ## size = col_integer(), ## storage = col_integer(), ## method = col_integer(), ## texture = col_double(), ## flavor = col_double(), ## moistness = col_double() ## ) # Print pools and potatoes pools ## # A tibble: 20 x 4 ## Name ## &lt;chr&gt; ## 1 Acacia Ridge Leisure Centre ## 2 Bellbowrie Pool ## 3 Carole Park ## 4 Centenary Pool (inner City) ## 5 Chermside Pool ## 6 Colmslie Pool (Morningside) ## 7 Spring Hill Baths (inner City) ## 8 Dunlop Park Pool (Corinda) ## 9 Fortitude Valley Pool ## 10 Hibiscus Sports Complex (upper MtGravatt) ## 11 Ithaca Pool ( Paddington) ## 12 Jindalee Pool ## 13 Manly Pool ## 14 Mt Gravatt East Aquatic Centre ## 15 Musgrave Park Pool (South Brisbane) ## 16 Newmarket Pool ## 17 Runcorn Pool ## 18 Sandgate Pool ## 19 Langlands Parks Pool (Stones Corner) ## 20 Yeronga Park Pool ## # ... with 3 more variables: Address &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt; potatoes ## # A tibble: 160 x 8 ## area temp size storage method texture flavor moistness ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3.0 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3.0 1.7 ## 7 1 1 1 2 2 2.6 3.1 2.4 ## 8 1 1 1 2 3 3.0 3.0 2.9 ## 9 1 1 1 2 4 2.2 3.2 2.5 ## 10 1 1 1 2 5 2.0 2.8 1.9 ## # ... with 150 more rows # https URL to the swimming_pools csv file. url_csv &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; # Import the file using read.csv(): pools1 pools1 &lt;- read.csv(url_csv) str(pools1) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : Factor w/ 20 levels &quot;Acacia Ridge Leisure Centre&quot;,..: 1 2 3 4 5 6 19 7 8 9 ... ## $ Address : Factor w/ 20 levels &quot;1 Fairlead Crescent, Manly&quot;,..: 5 20 18 10 9 11 6 15 12 17 ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... Some packages, like the readxl package, do not currently recognise urls. However, we can use the donwload.file() or other command to download the file and then read it in locally. This process can be much quicker that browsing the internet then downloading the file. library(readxl) # Specification of url: url_xls url_xls &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls&quot; # Download file behind URL, name it local_latitude.xls download.file(url_xls, destfile = &quot;local_latitude.xls&quot;) # Import the local .xls file with readxl: excel_readxl excel_readxl &lt;- read_excel(&quot;local_latitude.xls&quot;) # https URL to the wine RData file. url_rdata &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData&quot; # Download the wine file to your working directory download.file(url_rdata, destfile = &quot;wine_local.RData&quot;) # Load the wine data into your workspace using load() load(&quot;wine_local.RData&quot;) # Print out the summary of the wine data summary(wine) ## Alcohol Malic acid Ash Alcalinity of ash ## Min. :11.03 Min. :0.74 Min. :1.360 Min. :10.60 ## 1st Qu.:12.36 1st Qu.:1.60 1st Qu.:2.210 1st Qu.:17.20 ## Median :13.05 Median :1.87 Median :2.360 Median :19.50 ## Mean :12.99 Mean :2.34 Mean :2.366 Mean :19.52 ## 3rd Qu.:13.67 3rd Qu.:3.10 3rd Qu.:2.560 3rd Qu.:21.50 ## Max. :14.83 Max. :5.80 Max. :3.230 Max. :30.00 ## Magnesium Total phenols Flavanoids Nonflavanoid phenols ## Min. : 70.00 Min. :0.980 Min. :0.340 Min. :0.1300 ## 1st Qu.: 88.00 1st Qu.:1.740 1st Qu.:1.200 1st Qu.:0.2700 ## Median : 98.00 Median :2.350 Median :2.130 Median :0.3400 ## Mean : 99.59 Mean :2.292 Mean :2.023 Mean :0.3623 ## 3rd Qu.:107.00 3rd Qu.:2.800 3rd Qu.:2.860 3rd Qu.:0.4400 ## Max. :162.00 Max. :3.880 Max. :5.080 Max. :0.6600 ## Proanthocyanins Color intensity Hue Proline ## Min. :0.410 Min. : 1.280 Min. :1.270 Min. : 278.0 ## 1st Qu.:1.250 1st Qu.: 3.210 1st Qu.:1.930 1st Qu.: 500.0 ## Median :1.550 Median : 4.680 Median :2.780 Median : 672.0 ## Mean :1.587 Mean : 5.055 Mean :2.604 Mean : 745.1 ## 3rd Qu.:1.950 3rd Qu.: 6.200 3rd Qu.:3.170 3rd Qu.: 985.0 ## Max. :3.580 Max. :13.000 Max. :4.000 Max. :1680.0 We can also read http content using the httr package. This includes JSON formatted text, which httr will convert to a named list. # Load the httr package library(httr) # Get the url, save response to resp url &lt;- &quot;http://www.example.com/&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.example.com/] ## Date: 2017-09-24 03:50 ## Status: 200 ## Content-Type: text/html ## Size: 1.27 kB ## &lt;!doctype html&gt; ## &lt;html&gt; ## &lt;head&gt; ## &lt;title&gt;Example Domain&lt;/title&gt; ## ## &lt;meta charset=&quot;utf-8&quot; /&gt; ## &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; ## &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt; ## &lt;style type=&quot;text/css&quot;&gt; ## body { ## ... # Get the raw content of resp: raw_content raw_content &lt;- content(resp, as = &quot;raw&quot;) # Print the head of raw_content head(raw_content) ## [1] 3c 21 64 6f 63 74 # JSON formatted # Get the url url &lt;- &quot;http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json] ## Date: 2017-09-24 03:50 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 942 B # Print content of resp as text content(resp, as = &quot;text&quot;) ## [1] &quot;{\\&quot;Title\\&quot;:\\&quot;Annie Hall\\&quot;,\\&quot;Year\\&quot;:\\&quot;1977\\&quot;,\\&quot;Rated\\&quot;:\\&quot;PG\\&quot;,\\&quot;Released\\&quot;:\\&quot;20 Apr 1977\\&quot;,\\&quot;Runtime\\&quot;:\\&quot;93 min\\&quot;,\\&quot;Genre\\&quot;:\\&quot;Comedy, Romance\\&quot;,\\&quot;Director\\&quot;:\\&quot;Woody Allen\\&quot;,\\&quot;Writer\\&quot;:\\&quot;Woody Allen, Marshall Brickman\\&quot;,\\&quot;Actors\\&quot;:\\&quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane\\&quot;,\\&quot;Plot\\&quot;:\\&quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.\\&quot;,\\&quot;Language\\&quot;:\\&quot;English, German\\&quot;,\\&quot;Country\\&quot;:\\&quot;USA\\&quot;,\\&quot;Awards\\&quot;:\\&quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.\\&quot;,\\&quot;Poster\\&quot;:\\&quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg\\&quot;,\\&quot;Ratings\\&quot;:[{\\&quot;Source\\&quot;:\\&quot;Internet Movie Database\\&quot;,\\&quot;Value\\&quot;:\\&quot;8.1/10\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Rotten Tomatoes\\&quot;,\\&quot;Value\\&quot;:\\&quot;97%\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Metacritic\\&quot;,\\&quot;Value\\&quot;:\\&quot;92/100\\&quot;}],\\&quot;Metascore\\&quot;:\\&quot;92\\&quot;,\\&quot;imdbRating\\&quot;:\\&quot;8.1\\&quot;,\\&quot;imdbVotes\\&quot;:\\&quot;213,310\\&quot;,\\&quot;imdbID\\&quot;:\\&quot;tt0075686\\&quot;,\\&quot;Type\\&quot;:\\&quot;movie\\&quot;,\\&quot;DVD\\&quot;:\\&quot;28 Apr 1998\\&quot;,\\&quot;BoxOffice\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Production\\&quot;:\\&quot;United Artists\\&quot;,\\&quot;Website\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Response\\&quot;:\\&quot;True\\&quot;}&quot; # Print content of resp content(resp) ## $Title ## [1] &quot;Annie Hall&quot; ## ## $Year ## [1] &quot;1977&quot; ## ## $Rated ## [1] &quot;PG&quot; ## ## $Released ## [1] &quot;20 Apr 1977&quot; ## ## $Runtime ## [1] &quot;93 min&quot; ## ## $Genre ## [1] &quot;Comedy, Romance&quot; ## ## $Director ## [1] &quot;Woody Allen&quot; ## ## $Writer ## [1] &quot;Woody Allen, Marshall Brickman&quot; ## ## $Actors ## [1] &quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane&quot; ## ## $Plot ## [1] &quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.&quot; ## ## $Language ## [1] &quot;English, German&quot; ## ## $Country ## [1] &quot;USA&quot; ## ## $Awards ## [1] &quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.&quot; ## ## $Poster ## [1] &quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg&quot; ## ## $Ratings ## $Ratings[[1]] ## $Ratings[[1]]$Source ## [1] &quot;Internet Movie Database&quot; ## ## $Ratings[[1]]$Value ## [1] &quot;8.1/10&quot; ## ## ## $Ratings[[2]] ## $Ratings[[2]]$Source ## [1] &quot;Rotten Tomatoes&quot; ## ## $Ratings[[2]]$Value ## [1] &quot;97%&quot; ## ## ## $Ratings[[3]] ## $Ratings[[3]]$Source ## [1] &quot;Metacritic&quot; ## ## $Ratings[[3]]$Value ## [1] &quot;92/100&quot; ## ## ## ## $Metascore ## [1] &quot;92&quot; ## ## $imdbRating ## [1] &quot;8.1&quot; ## ## $imdbVotes ## [1] &quot;213,310&quot; ## ## $imdbID ## [1] &quot;tt0075686&quot; ## ## $Type ## [1] &quot;movie&quot; ## ## $DVD ## [1] &quot;28 Apr 1998&quot; ## ## $BoxOffice ## [1] &quot;N/A&quot; ## ## $Production ## [1] &quot;United Artists&quot; ## ## $Website ## [1] &quot;N/A&quot; ## ## $Response ## [1] &quot;True&quot; 2.4 JSON and APIs JSON is both easy for machines to parse and generate and is human readable. APIs are programtical ways of getting data, consisting of a set of protocols to interact with some other system or database. JSON can be useful since it is often well structured and can save time over, say, parsing a html page. So for instance, you can use the OMDb API to return JSON formatted text about a movie, rather than parse an IMDB html page entry. One package for handling JSON in R is jsonlite. library(jsonlite) # wine_json is a JSON wine_json &lt;- &#39;{&quot;name&quot;:&quot;Chateau Migraine&quot;, &quot;year&quot;:1997, &quot;alcohol_pct&quot;:12.4, &quot;color&quot;:&quot;red&quot;, &quot;awarded&quot;:false}&#39; # Convert wine_json into a list: wine wine &lt;- fromJSON(wine_json) # Print structure of wine str(wine) ## List of 5 ## $ name : chr &quot;Chateau Migraine&quot; ## $ year : int 1997 ## $ alcohol_pct: num 12.4 ## $ color : chr &quot;red&quot; ## $ awarded : logi FALSE There are two types of JSON structures JSON objects - has key value pairs e.g. name:James, age:21 etc JSON arrays - a sequence of values, numbers, nulls e.g. 4, “a”, 10, false, null etc You can also nest JSON objects or arrays within each other. Some examples are below. YOu can also use the minify and prettify functions to convert a JSON string to a more compact of easier to read version. Similar functions can also be used inside the toJSON() function e.g. toJSON(x, pretty = TRUE) # Challenge 1 json1 &lt;- &#39;[1, 2, 3, 4, 5, 6]&#39; fromJSON(json1) ## [1] 1 2 3 4 5 6 # Challenge 2 json2 &lt;- &#39;{&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]}&#39; fromJSON(json2) ## $a ## [1] 1 2 3 ## ## $b ## [1] 4 5 6 # You can also convert data to JSON from other formats. Here we take a csv and format it into a JSON array # URL pointing to the .csv file url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv&quot; # Import the .csv file located at url_csv water &lt;- read.csv(url_csv, stringsAsFactors = FALSE) # Convert the data file according to the requirements water_json &lt;- toJSON(water) # Print out water_json water_json ## [{&quot;water&quot;:&quot;Algeria&quot;,&quot;X1992&quot;:0.064,&quot;X2002&quot;:0.017},{&quot;water&quot;:&quot;American Samoa&quot;},{&quot;water&quot;:&quot;Angola&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Antigua and Barbuda&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Argentina&quot;,&quot;X1992&quot;:0.0007,&quot;X1997&quot;:0.0007,&quot;X2002&quot;:0.0007},{&quot;water&quot;:&quot;Australia&quot;,&quot;X1992&quot;:0.0298,&quot;X2002&quot;:0.0298},{&quot;water&quot;:&quot;Austria&quot;,&quot;X1992&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Bahamas&quot;,&quot;X1992&quot;:0.0013,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Bahrain&quot;,&quot;X1992&quot;:0.0441,&quot;X2002&quot;:0.0441,&quot;X2007&quot;:0.1024},{&quot;water&quot;:&quot;Barbados&quot;,&quot;X2007&quot;:0.0146},{&quot;water&quot;:&quot;British Virgin Islands&quot;,&quot;X2007&quot;:0.0042},{&quot;water&quot;:&quot;Canada&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cape Verde&quot;,&quot;X1992&quot;:0.002,&quot;X1997&quot;:0.0017},{&quot;water&quot;:&quot;Cayman Islands&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Central African Rep.&quot;},{&quot;water&quot;:&quot;Chile&quot;,&quot;X1992&quot;:0.0048,&quot;X2002&quot;:0.0048},{&quot;water&quot;:&quot;Colombia&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cuba&quot;,&quot;X1992&quot;:0.0069,&quot;X1997&quot;:0.0069,&quot;X2002&quot;:0.0069},{&quot;water&quot;:&quot;Cyprus&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.0335},{&quot;water&quot;:&quot;Czech Rep.&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Denmark&quot;,&quot;X1992&quot;:0.015,&quot;X2002&quot;:0.015},{&quot;water&quot;:&quot;Djibouti&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Ecuador&quot;,&quot;X1992&quot;:0.0022,&quot;X1997&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Egypt&quot;,&quot;X1992&quot;:0.025,&quot;X1997&quot;:0.025,&quot;X2002&quot;:0.1},{&quot;water&quot;:&quot;El Salvador&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Finland&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;France&quot;,&quot;X1992&quot;:0.0117,&quot;X2002&quot;:0.0117},{&quot;water&quot;:&quot;Gibraltar&quot;,&quot;X1992&quot;:0.0077},{&quot;water&quot;:&quot;Greece&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01},{&quot;water&quot;:&quot;Honduras&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Hungary&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;India&quot;,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Indonesia&quot;,&quot;X1992&quot;:0.0187,&quot;X2002&quot;:0.0187},{&quot;water&quot;:&quot;Iran&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.003,&quot;X2007&quot;:0.2},{&quot;water&quot;:&quot;Iraq&quot;,&quot;X1997&quot;:0.0074,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Ireland&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Israel&quot;,&quot;X1992&quot;:0.0256,&quot;X2002&quot;:0.0256,&quot;X2007&quot;:0.14},{&quot;water&quot;:&quot;Italy&quot;,&quot;X1992&quot;:0.0973,&quot;X2002&quot;:0.0973},{&quot;water&quot;:&quot;Jamaica&quot;,&quot;X1992&quot;:0.0005,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Japan&quot;,&quot;X1997&quot;:0.04,&quot;X2002&quot;:0.04},{&quot;water&quot;:&quot;Jordan&quot;,&quot;X1997&quot;:0.002,&quot;X2007&quot;:0.0098},{&quot;water&quot;:&quot;Kazakhstan&quot;,&quot;X1997&quot;:1.328,&quot;X2002&quot;:1.328},{&quot;water&quot;:&quot;Kuwait&quot;,&quot;X1992&quot;:0.507,&quot;X1997&quot;:0.231,&quot;X2002&quot;:0.4202},{&quot;water&quot;:&quot;Lebanon&quot;,&quot;X2007&quot;:0.0473},{&quot;water&quot;:&quot;Libya&quot;,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Malaysia&quot;,&quot;X1992&quot;:0.0043,&quot;X2002&quot;:0.0043},{&quot;water&quot;:&quot;Maldives&quot;,&quot;X1992&quot;:0.0004},{&quot;water&quot;:&quot;Malta&quot;,&quot;X1992&quot;:0.024,&quot;X1997&quot;:0.031,&quot;X2002&quot;:0.031},{&quot;water&quot;:&quot;Marshall Islands&quot;,&quot;X1992&quot;:0.0007},{&quot;water&quot;:&quot;Mauritania&quot;,&quot;X1992&quot;:0.002,&quot;X2002&quot;:0.002},{&quot;water&quot;:&quot;Mexico&quot;,&quot;X1992&quot;:0.0307,&quot;X2002&quot;:0.0307},{&quot;water&quot;:&quot;Morocco&quot;,&quot;X1992&quot;:0.0034,&quot;X1997&quot;:0.0034,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Namibia&quot;,&quot;X1992&quot;:0.0003,&quot;X2002&quot;:0.0003},{&quot;water&quot;:&quot;Netherlands Antilles&quot;,&quot;X1992&quot;:0.063},{&quot;water&quot;:&quot;Nicaragua&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Nigeria&quot;,&quot;X1992&quot;:0.003,&quot;X2002&quot;:0.003},{&quot;water&quot;:&quot;Norway&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Oman&quot;,&quot;X1997&quot;:0.034,&quot;X2002&quot;:0.034,&quot;X2007&quot;:0.109},{&quot;water&quot;:&quot;Peru&quot;,&quot;X1992&quot;:0.0054,&quot;X2002&quot;:0.0054},{&quot;water&quot;:&quot;Poland&quot;,&quot;X1992&quot;:0.007,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Portugal&quot;,&quot;X1992&quot;:0.0016,&quot;X2002&quot;:0.0016},{&quot;water&quot;:&quot;Qatar&quot;,&quot;X1992&quot;:0.065,&quot;X1997&quot;:0.099,&quot;X2002&quot;:0.099,&quot;X2007&quot;:0.18},{&quot;water&quot;:&quot;Saudi Arabia&quot;,&quot;X1992&quot;:0.683,&quot;X1997&quot;:0.727,&quot;X2002&quot;:0.863,&quot;X2007&quot;:1.033},{&quot;water&quot;:&quot;Senegal&quot;,&quot;X1992&quot;:0,&quot;X2002&quot;:0},{&quot;water&quot;:&quot;Somalia&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;South Africa&quot;,&quot;X1992&quot;:0.018,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Spain&quot;,&quot;X1992&quot;:0.1002,&quot;X2002&quot;:0.1002},{&quot;water&quot;:&quot;Sudan&quot;,&quot;X1992&quot;:0.0004,&quot;X1997&quot;:0.0004,&quot;X2002&quot;:0.0004},{&quot;water&quot;:&quot;Sweden&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Trinidad and Tobago&quot;,&quot;X2007&quot;:0.036},{&quot;water&quot;:&quot;Tunisia&quot;,&quot;X1992&quot;:0.008,&quot;X2002&quot;:0.013},{&quot;water&quot;:&quot;Turkey&quot;,&quot;X1992&quot;:0.0005,&quot;X2002&quot;:0.0005,&quot;X2007&quot;:0.0005},{&quot;water&quot;:&quot;United Arab Emirates&quot;,&quot;X1992&quot;:0.163,&quot;X1997&quot;:0.385,&quot;X2007&quot;:0.95},{&quot;water&quot;:&quot;United Kingdom&quot;,&quot;X1992&quot;:0.0333,&quot;X2002&quot;:0.0333},{&quot;water&quot;:&quot;United States&quot;,&quot;X1992&quot;:0.58,&quot;X2002&quot;:0.58},{&quot;water&quot;:&quot;Venezuela&quot;,&quot;X1992&quot;:0.0052,&quot;X2002&quot;:0.0052},{&quot;water&quot;:&quot;Yemen, Rep.&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01}] # Convert mtcars to a pretty JSON: pretty_json pretty_json &lt;- toJSON(mtcars, pretty = TRUE) # Print pretty_json pretty_json ## [ ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.62, ## &quot;qsec&quot;: 16.46, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4&quot; ## }, ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.875, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4 Wag&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 108, ## &quot;hp&quot;: 93, ## &quot;drat&quot;: 3.85, ## &quot;wt&quot;: 2.32, ## &quot;qsec&quot;: 18.61, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Datsun 710&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 258, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.215, ## &quot;qsec&quot;: 19.44, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Hornet 4 Drive&quot; ## }, ## { ## &quot;mpg&quot;: 18.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Hornet Sportabout&quot; ## }, ## { ## &quot;mpg&quot;: 18.1, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 225, ## &quot;hp&quot;: 105, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.46, ## &quot;qsec&quot;: 20.22, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Valiant&quot; ## }, ## { ## &quot;mpg&quot;: 14.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.21, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 15.84, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Duster 360&quot; ## }, ## { ## &quot;mpg&quot;: 24.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 146.7, ## &quot;hp&quot;: 62, ## &quot;drat&quot;: 3.69, ## &quot;wt&quot;: 3.19, ## &quot;qsec&quot;: 20, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 240D&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 140.8, ## &quot;hp&quot;: 95, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.15, ## &quot;qsec&quot;: 22.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 230&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.3, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280&quot; ## }, ## { ## &quot;mpg&quot;: 17.8, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280C&quot; ## }, ## { ## &quot;mpg&quot;: 16.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 4.07, ## &quot;qsec&quot;: 17.4, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SE&quot; ## }, ## { ## &quot;mpg&quot;: 17.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.73, ## &quot;qsec&quot;: 17.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SL&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.78, ## &quot;qsec&quot;: 18, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SLC&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 472, ## &quot;hp&quot;: 205, ## &quot;drat&quot;: 2.93, ## &quot;wt&quot;: 5.25, ## &quot;qsec&quot;: 17.98, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Cadillac Fleetwood&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 460, ## &quot;hp&quot;: 215, ## &quot;drat&quot;: 3, ## &quot;wt&quot;: 5.424, ## &quot;qsec&quot;: 17.82, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Lincoln Continental&quot; ## }, ## { ## &quot;mpg&quot;: 14.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 440, ## &quot;hp&quot;: 230, ## &quot;drat&quot;: 3.23, ## &quot;wt&quot;: 5.345, ## &quot;qsec&quot;: 17.42, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Chrysler Imperial&quot; ## }, ## { ## &quot;mpg&quot;: 32.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 78.7, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 2.2, ## &quot;qsec&quot;: 19.47, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat 128&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 75.7, ## &quot;hp&quot;: 52, ## &quot;drat&quot;: 4.93, ## &quot;wt&quot;: 1.615, ## &quot;qsec&quot;: 18.52, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Honda Civic&quot; ## }, ## { ## &quot;mpg&quot;: 33.9, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 71.1, ## &quot;hp&quot;: 65, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 1.835, ## &quot;qsec&quot;: 19.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corolla&quot; ## }, ## { ## &quot;mpg&quot;: 21.5, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.1, ## &quot;hp&quot;: 97, ## &quot;drat&quot;: 3.7, ## &quot;wt&quot;: 2.465, ## &quot;qsec&quot;: 20.01, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corona&quot; ## }, ## { ## &quot;mpg&quot;: 15.5, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 318, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.52, ## &quot;qsec&quot;: 16.87, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Dodge Challenger&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 304, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.435, ## &quot;qsec&quot;: 17.3, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;AMC Javelin&quot; ## }, ## { ## &quot;mpg&quot;: 13.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 350, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.73, ## &quot;wt&quot;: 3.84, ## &quot;qsec&quot;: 15.41, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Camaro Z28&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 400, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.845, ## &quot;qsec&quot;: 17.05, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Pontiac Firebird&quot; ## }, ## { ## &quot;mpg&quot;: 27.3, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 79, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 1.935, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat X1-9&quot; ## }, ## { ## &quot;mpg&quot;: 26, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.3, ## &quot;hp&quot;: 91, ## &quot;drat&quot;: 4.43, ## &quot;wt&quot;: 2.14, ## &quot;qsec&quot;: 16.7, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Porsche 914-2&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 95.1, ## &quot;hp&quot;: 113, ## &quot;drat&quot;: 3.77, ## &quot;wt&quot;: 1.513, ## &quot;qsec&quot;: 16.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Lotus Europa&quot; ## }, ## { ## &quot;mpg&quot;: 15.8, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 351, ## &quot;hp&quot;: 264, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 3.17, ## &quot;qsec&quot;: 14.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Ford Pantera L&quot; ## }, ## { ## &quot;mpg&quot;: 19.7, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 145, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.62, ## &quot;wt&quot;: 2.77, ## &quot;qsec&quot;: 15.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 6, ## &quot;_row&quot;: &quot;Ferrari Dino&quot; ## }, ## { ## &quot;mpg&quot;: 15, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 301, ## &quot;hp&quot;: 335, ## &quot;drat&quot;: 3.54, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 14.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 8, ## &quot;_row&quot;: &quot;Maserati Bora&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 121, ## &quot;hp&quot;: 109, ## &quot;drat&quot;: 4.11, ## &quot;wt&quot;: 2.78, ## &quot;qsec&quot;: 18.6, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Volvo 142E&quot; ## } ## ] # Minify pretty_json: mini_json mini_json &lt;- minify(pretty_json) # Print mini_json mini_json ## [{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.62,&quot;qsec&quot;:16.46,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4&quot;},{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.875,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4 Wag&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:108,&quot;hp&quot;:93,&quot;drat&quot;:3.85,&quot;wt&quot;:2.32,&quot;qsec&quot;:18.61,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Datsun 710&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:6,&quot;disp&quot;:258,&quot;hp&quot;:110,&quot;drat&quot;:3.08,&quot;wt&quot;:3.215,&quot;qsec&quot;:19.44,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Hornet 4 Drive&quot;},{&quot;mpg&quot;:18.7,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:175,&quot;drat&quot;:3.15,&quot;wt&quot;:3.44,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Hornet Sportabout&quot;},{&quot;mpg&quot;:18.1,&quot;cyl&quot;:6,&quot;disp&quot;:225,&quot;hp&quot;:105,&quot;drat&quot;:2.76,&quot;wt&quot;:3.46,&quot;qsec&quot;:20.22,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Valiant&quot;},{&quot;mpg&quot;:14.3,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:245,&quot;drat&quot;:3.21,&quot;wt&quot;:3.57,&quot;qsec&quot;:15.84,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Duster 360&quot;},{&quot;mpg&quot;:24.4,&quot;cyl&quot;:4,&quot;disp&quot;:146.7,&quot;hp&quot;:62,&quot;drat&quot;:3.69,&quot;wt&quot;:3.19,&quot;qsec&quot;:20,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 240D&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:140.8,&quot;hp&quot;:95,&quot;drat&quot;:3.92,&quot;wt&quot;:3.15,&quot;qsec&quot;:22.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 230&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.3,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280&quot;},{&quot;mpg&quot;:17.8,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280C&quot;},{&quot;mpg&quot;:16.4,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:4.07,&quot;qsec&quot;:17.4,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SE&quot;},{&quot;mpg&quot;:17.3,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.73,&quot;qsec&quot;:17.6,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SL&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.78,&quot;qsec&quot;:18,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SLC&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:472,&quot;hp&quot;:205,&quot;drat&quot;:2.93,&quot;wt&quot;:5.25,&quot;qsec&quot;:17.98,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Cadillac Fleetwood&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:460,&quot;hp&quot;:215,&quot;drat&quot;:3,&quot;wt&quot;:5.424,&quot;qsec&quot;:17.82,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Lincoln Continental&quot;},{&quot;mpg&quot;:14.7,&quot;cyl&quot;:8,&quot;disp&quot;:440,&quot;hp&quot;:230,&quot;drat&quot;:3.23,&quot;wt&quot;:5.345,&quot;qsec&quot;:17.42,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Chrysler Imperial&quot;},{&quot;mpg&quot;:32.4,&quot;cyl&quot;:4,&quot;disp&quot;:78.7,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:2.2,&quot;qsec&quot;:19.47,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat 128&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:75.7,&quot;hp&quot;:52,&quot;drat&quot;:4.93,&quot;wt&quot;:1.615,&quot;qsec&quot;:18.52,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Honda Civic&quot;},{&quot;mpg&quot;:33.9,&quot;cyl&quot;:4,&quot;disp&quot;:71.1,&quot;hp&quot;:65,&quot;drat&quot;:4.22,&quot;wt&quot;:1.835,&quot;qsec&quot;:19.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corolla&quot;},{&quot;mpg&quot;:21.5,&quot;cyl&quot;:4,&quot;disp&quot;:120.1,&quot;hp&quot;:97,&quot;drat&quot;:3.7,&quot;wt&quot;:2.465,&quot;qsec&quot;:20.01,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corona&quot;},{&quot;mpg&quot;:15.5,&quot;cyl&quot;:8,&quot;disp&quot;:318,&quot;hp&quot;:150,&quot;drat&quot;:2.76,&quot;wt&quot;:3.52,&quot;qsec&quot;:16.87,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Dodge Challenger&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:304,&quot;hp&quot;:150,&quot;drat&quot;:3.15,&quot;wt&quot;:3.435,&quot;qsec&quot;:17.3,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;AMC Javelin&quot;},{&quot;mpg&quot;:13.3,&quot;cyl&quot;:8,&quot;disp&quot;:350,&quot;hp&quot;:245,&quot;drat&quot;:3.73,&quot;wt&quot;:3.84,&quot;qsec&quot;:15.41,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Camaro Z28&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:8,&quot;disp&quot;:400,&quot;hp&quot;:175,&quot;drat&quot;:3.08,&quot;wt&quot;:3.845,&quot;qsec&quot;:17.05,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Pontiac Firebird&quot;},{&quot;mpg&quot;:27.3,&quot;cyl&quot;:4,&quot;disp&quot;:79,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:1.935,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat X1-9&quot;},{&quot;mpg&quot;:26,&quot;cyl&quot;:4,&quot;disp&quot;:120.3,&quot;hp&quot;:91,&quot;drat&quot;:4.43,&quot;wt&quot;:2.14,&quot;qsec&quot;:16.7,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Porsche 914-2&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:95.1,&quot;hp&quot;:113,&quot;drat&quot;:3.77,&quot;wt&quot;:1.513,&quot;qsec&quot;:16.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Lotus Europa&quot;},{&quot;mpg&quot;:15.8,&quot;cyl&quot;:8,&quot;disp&quot;:351,&quot;hp&quot;:264,&quot;drat&quot;:4.22,&quot;wt&quot;:3.17,&quot;qsec&quot;:14.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Ford Pantera L&quot;},{&quot;mpg&quot;:19.7,&quot;cyl&quot;:6,&quot;disp&quot;:145,&quot;hp&quot;:175,&quot;drat&quot;:3.62,&quot;wt&quot;:2.77,&quot;qsec&quot;:15.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:6,&quot;_row&quot;:&quot;Ferrari Dino&quot;},{&quot;mpg&quot;:15,&quot;cyl&quot;:8,&quot;disp&quot;:301,&quot;hp&quot;:335,&quot;drat&quot;:3.54,&quot;wt&quot;:3.57,&quot;qsec&quot;:14.6,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:8,&quot;_row&quot;:&quot;Maserati Bora&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:4,&quot;disp&quot;:121,&quot;hp&quot;:109,&quot;drat&quot;:4.11,&quot;wt&quot;:2.78,&quot;qsec&quot;:18.6,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Volvo 142E&quot;}] 2.5 Importing from other statistical software Common software packages include SAS, STATA and SPSS. Two packages useful for importing data from these packages are: haven: by Hadley Wickham and is under active development. It aims to be more consistent, easier and faster than foreign. It can read SAS, Stata and SPSS and will read in the file as an D dataframe. foreign: is an older package by the R Core Team. Foreign support more data formats than haven including Weka and Systat # Load the haven package library(haven) # Import sales.sas7bdat: sales sales &lt;- read_sas(&quot;sales.sas7bdat&quot;) # Display the structure of sales str(sales) # Import the data from the URL: sugar sugar &lt;- read_dta(&quot;http://assets.datacamp.com/production/course_1478/datasets/trade.dta&quot;) # Structure of sugar str(sugar) # Convert values in Date column to dates sugar$Date &lt;- as.Date(as_factor(sugar$Date)) # Structure of sugar again str(sugar) # Import person.sav: traits traits &lt;- read_sav(&quot;person.sav&quot;) # Summarize traits summary(traits) # Print out a subset subset(traits, Extroversion &gt; 40 &amp; Agreeableness &gt; 40) When using SPSS files, it is often the case that the variable labels are also imported, it is best to change these in to standard R factors. # Import SPSS data from the URL: work work &lt;- read_sav(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav&quot;) # Display summary of work$GENDER summary(work$GENDER) # Convert work$GENDER to a factor work$GENDER &lt;- as_factor(work$GENDER) # Display summary of work$GENDER again summary(work$GENDER) Foreign cannot use single SAS datafiles like haven, it works with SAS library files .xport. Foreign tends to use dots in the function names rather than underscores in haven e.g. read.dta() vs read_dta(). Foreign does not provide consistency with it’s functions i.e. read.dta() has different arguments than read.spss(), however foreign provides more control over the data importing, such as dealing with multiple types of missing data which are often present in survey data, more comprehensively than haven. Although haven is still being developed. # Load the foreign package library(foreign) # Specify the file path using file.path(): path path &lt;- file.path(&quot;worldbank&quot;, &quot;edequality.dta&quot;) # Create and print structure of edu_equal_1 edu_equal_1 &lt;- read.dta(path) str(edu_equal_1) # Create and print structure of edu_equal_2 edu_equal_2 &lt;- read.dta(path, convert.factors = FALSE) str(edu_equal_2) # Create and print structure of edu_equal_3 edu_equal_3 &lt;- read.dta(path, convert.underscore = TRUE) str(edu_equal_3) # Import international.sav as a data frame: demo demo &lt;- read.spss(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/international.sav&quot;, to.data.frame = TRUE) # Create boxplot of gdp variable of demo boxplot(demo$gdp) "],
["references.html", "References", " References "],
["joining-data-in-r-with-dplyr.html", "3 Joining Data in R with dplyr 3.1 Mutating joins 3.2 Filtering joins and set operations 3.3 Set Operations 3.4 Bind in Dplyr 3.5 Advanced Joining 3.6 Joining mutiple tables 3.7 Other implentations 3.8 Case Study - Lahman DB", " 3 Joining Data in R with dplyr Notes taken during/inspired by the Datacamp course ‘Joining Data in R with dplyr’ by Garrett Grolemund. Other useful info: R for Data Science Book Data Wrangling Cheatsheet dplyr two table verbs vignette dbplyr vignette Course Slides: Part1 - Mutating Joins Part2 - Filtering joins and set operations Part3 - Assembling data Part4 - Advanced joining Part5 - Case Study 3.1 Mutating joins Data is best used in R when in a single data table. This course introduces a number of techniques to achieve this. Dplyr also has connectors to a range of different databases, so can be used to both extract and manipulate data in databases. 3.1.1 Keys We often want to join two tables together, adding a set of values or variables from a second table(s). For this to happen, we need a key, whereby we have a key in the initial table (a primary key) that is uniquely identifies rows in that within that table or dataset (we don’t have duplicates) and we then use this key to add in data from a secondary table (the foreign key to that table). The foreign key in the secondary table may be duplicated or not appear at all. Sometimes no single variable acts as a primary key in a dataset. Instead, it takes a combination of variables to uniquely identify each row, for example a table of addresses with different columns representing sections of the address - house number, street name, postcode/zip code. When working with dplyr, it works with the following tables for the purposes of joining data Tables in dplyr are one of the following: data frames tibbles (tbl_df) - similar to data frame but only what fits in to your R console window will be displayed but you can use View() all the table if needed tbl references (#fig:Dplyr Joins)Joins available in Dplyr ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 3.1.2 Left and right joins left_join() is the basic join function in dplyr. You can use it whenever you want to augment a data frame with information from another data frame. For example, left_join(x, y) joins y to x. The second dataset you specify is joined to the first dataset. In right_join() the order of the datasets reversed # Join artists to bands bands2 &lt;- left_join(bands, artists, by = c(&quot;first&quot;, &quot;last&quot;)) # Examine the results bands2 # Recreate bands3 with a right join bands2 &lt;- left_join(bands, artists, by = c(&quot;first&quot;, &quot;last&quot;)) bands3 &lt;- right_join(artists, bands, by = c(&quot;first&quot;, &quot;last&quot;)) # Check that bands3 is equal to bands2 setequal(bands2, bands3) 3.1.3 Inner and full joins Left_join and right_join are half of a class of ‘mutating joins’ with the name coming from dplyrs mutate() function which returns a copy of the dataset with one or more columns of the data added to it. The other two functions are: inner_join: only retains rows from both/all datasets full_join: retains any row from both/any data set %&gt;%: Can be used to string joins or other functions together # Create goal2 using full_join() and inner_join() goal2 &lt;- artists %&gt;% full_join(bands, by = c(&quot;first&quot;,&quot;last&quot;)) %&gt;% inner_join(songs, by = c(&quot;first&quot;,&quot;last&quot;)) # Create one table that combines all information artists %&gt;% full_join(bands, by = c(&quot;first&quot;,&quot;last&quot;)) %&gt;% full_join(songs, by = c(&quot;first&quot;,&quot;last&quot;)) %&gt;% full_join(albums, by = c(&quot;album&quot;, &quot;band&quot;)) 3.2 Filtering joins and set operations Filtering joins returns a copy of the origianl data set rather than an augmented version of the original dataset. (#fig:Filtering Joins)Filtering and Mutating Joins Semi_join() is one of the filtering join functions, it can be used to check which rows in one table match the rows in another table, perhaps before a mutating join. Semi_join is sometimes easier than many seperate functions together, for instance # View the output of semi_join() artists %&gt;% semi_join(songs, by = c(&quot;first&quot;, &quot;last&quot;)) # Create the same result artists %&gt;% right_join(songs, by = c(&quot;first&quot;, &quot;last&quot;)) %&gt;% filter(!is.na(instrument)) %&gt;% select(first, last, instrument) Semi-joins provide a useful way to explore the connections between multiple tables of data. For example, you can use a semi-join to determine the number of albums in the albums dataset that were made by a band in the bands dataset. albums %&gt;% # Collect the albums made by a band semi_join(bands, by = &quot;band&quot;) %&gt;% # Count the albums made by a band nrow() Anti-joins shows records which are in the primary table but do not have matches in the second data table. This can be useful for checking spelling or key value errors. # Return rows of artists that don&#39;t have bands info artists %&gt;% anti_join(bands, by = c(&quot;first&quot;, &quot;last&quot;)) # Check whether album names in labels are mis-entered labels %&gt;% anti_join(albums, by = &quot;album&quot;) Joins can also be used to count the number of records. # Determine which key joins labels and songs labels songs songs %&gt;% # Find the rows of songs that match a row in labels semi_join(labels, by = &quot;album&quot;) %&gt;% # Number of matches between labels and songs nrow() 3.3 Set Operations When two datasets contain the same variables, it can be useful to combine the datasets with set operations (rather than joins). The three set operations can be used to combine observations from two datasets in to a single dataset. (#fig:Set Ops)Set Operations For instance we can count the number of unique songs in two data tables. # Import files aerosmith &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/aerosmith.csv&quot;, stringsAsFactors = FALSE) greatest_hits &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/greatest_hits.csv&quot;, stringsAsFactors = FALSE) aerosmith %&gt;% # Create the new dataset using a set operation union(greatest_hits) %&gt;% # Count the total number of songs nrow() ## [1] 24 Or use it to see which is in both (where the dataset has the exact same variables) - this is similar to the semi_join function. # Create the new dataset using a set operation aerosmith %&gt;% intersect(greatest_hits) ## song length ## 1 Dream On 4:28 Or use it to identify which are in one dataset but not the other. Here, we also match on just the variable we are interested in that matches across both datasets. # Import file live &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/live.csv&quot;, stringsAsFactors = FALSE) # Select the song names from live live_songs &lt;- live %&gt;% select(song) # Select the song names from greatest_hits greatest_songs &lt;- greatest_hits %&gt;% select(song) # Create the new dataset using a set operation - songs in live that are not in greatest_hits live_songs %&gt;% setdiff(greatest_songs) ## song ## 1 Lord of the Thighs ## 2 Toys in the Attic ## 3 Sick as a Dog ## 4 Sight for Sore Eyes ## 5 S.O.S. (Too Bad) ## 6 I Ain&#39;t Got You ## 7 Mother Popcorn/Draw the Line ## 8 Train Kept A-Rollin&#39;/Strangers in the Night There is no set operation to find rows that appear in one data frame or another, but not both. However, you can accomplish this by combining set operators. live_songs &lt;- live %&gt;% select(song) greatest_songs &lt;- greatest_hits %&gt;% select(song) # Return the songs that only exist in one dataset live_songs %&gt;% setdiff(greatest_songs) %&gt;% union(greatest_songs %&gt;% setdiff(live_songs)) ## song ## 1 Sick as a Dog ## 2 Seasons of Winter ## 3 Same Old Song and Dance ## 4 Toys in the Attic ## 5 I Ain&#39;t Got You ## 6 Mother Popcorn/Draw the Line ## 7 Lord of the Thighs ## 8 Lightning Strikes ## 9 One Way Street (live) ## 10 Train Kept A-Rollin&#39;/Strangers in the Night ## 11 Remember (Walking in the Sand) ## 12 Sweet Emotion (remix) ## 13 S.O.S. (Too Bad) ## 14 Kings and Queens ## 15 Sight for Sore Eyes ## 16 Big Ten Inch Record ## 17 Draw the Line Note: The union() function removes duplicate rows, even if a duplicate is desired (perhaps a different record such as someone with the same name). Its common to want to know if one data set is the same as another dataset dplyr’s setequal will do this easily base R’s identical is will only return true if the datasets have the exact same rows in the exact same order Recap: Mutating Joins: left_join right_join inner_join full_join Filtering Joins: semi_join *anti_join Set Operations: union intersect setdiff Comparisions: setequal The definitive and complete contain the songs that appear in competing Led Zeppelin anthologies: The Definitive Collection and The Complete Studio Recordings, respectively. Both anthologies claim to contain the complete studio recordings of Led Zeppelin, but do the anthologies contain the same exact songs? # Import files complete &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/complete.csv&quot;, stringsAsFactors = FALSE) definitive &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/definitive.csv&quot;, stringsAsFactors = FALSE) # Check if same order: definitive and complete identical(definitive, complete) ## [1] FALSE # Check if any order: definitive and complete setequal(definitive, complete) ## FALSE: Different number of rows # Songs in definitive but not complete complete %&gt;% setdiff(definitive) ## [1] song album ## &lt;0 rows&gt; (or 0-length row.names) # Songs in complete but not definitive definitive %&gt;% setdiff(complete) ## song album ## 1 Rock and Roll The Song Remains the Same ## 2 Celebration Day The Song Remains the Same ## 3 Black Dog The Song Remains the Same ## 4 Over the Hills and Far Away The Song Remains the Same ## 5 Misty Mountain Hop The Song Remains the Same ## 6 Since I&#39;ve Been Loving You The Song Remains the Same ## 7 No Quarter The Song Remains the Same ## 8 The Song Remains the Same The Song Remains the Same ## 9 The Rain Song The Song Remains the Same ## 10 The Ocean The Song Remains the Same ## 11 Dazed and Confused The Song Remains the Same ## 12 Stairway to Heaven The Song Remains the Same ## 13 Moby Dick The Song Remains the Same ## 14 Heartbreaker The Song Remains the Same ## 15 Whole Lotta Love The Song Remains the Same # Return songs in definitive that are not in complete definitive %&gt;% anti_join(complete, by = c(&quot;song&quot;, &quot;album&quot;)) ## song album ## 1 Rock and Roll The Song Remains the Same ## 2 Celebration Day The Song Remains the Same ## 3 Black Dog The Song Remains the Same ## 4 Over the Hills and Far Away The Song Remains the Same ## 5 Misty Mountain Hop The Song Remains the Same ## 6 Since I&#39;ve Been Loving You The Song Remains the Same ## 7 No Quarter The Song Remains the Same ## 8 The Song Remains the Same The Song Remains the Same ## 9 The Rain Song The Song Remains the Same ## 10 The Ocean The Song Remains the Same ## 11 Dazed and Confused The Song Remains the Same ## 12 Stairway to Heaven The Song Remains the Same ## 13 Moby Dick The Song Remains the Same ## 14 Heartbreaker The Song Remains the Same ## 15 Whole Lotta Love The Song Remains the Same # Return songs in complete that are not in definitive complete %&gt;% anti_join(definitive, by = c(&quot;song&quot;, &quot;album&quot;)) ## [1] song album ## &lt;0 rows&gt; (or 0-length row.names) It appears that The Definitive Collection contains songs from the soundtrack of The Song Remains the Same, a movie filmed during a live Led Zeppelin concert. Is this the only difference between The Definitive Collection and The Complete Studio Recordings? Remember: base R’s identical is will only return true if the datasets have the exact same rows in the exact same order. # Import file sounddtrack soundtrack &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/soundtrack.csv&quot;, stringsAsFactors = FALSE) # Check if same order: definitive and union of complete and soundtrack complete %&gt;% union(soundtrack) %&gt;% identical(definitive) ## [1] FALSE # Check if any order: definitive and union of complete and soundtrack complete %&gt;% union(soundtrack) %&gt;% setequal(definitive) ## TRUE 3.4 Bind in Dplyr Whilst base R has rbrind and cbind, dplyr has bind_rows and bind_cols as equivalents. Bind_rows adds the second dataset underneath the first, bind_cols assumes the datasets are in the exact same order and can be thought of as a ‘lazy join’. The benefits of dplyr binds are: Faster Return a tibble Can handle lists of data frames .id The last one (.id) will return a name to indicate which source the data in the new data frame (tibble) the data came from, particularly useful for things like ggplot and keeping track of your data overall. # Examine side_one and side_two side_one side_two # Bind side_one and side_two into a single dataset side_one %&gt;% bind_rows(side_two) discography and jimi contain all of the information you need to create an anthology dataset for the band The Jimi Hendrix Experience. discography contains a data frame of each album by The Jimi Hendrix Experience and the year of the album. jimi contains a list of data frames of album tracks, one for each album released by The Jimi Hendrix Experience. You can pass bind_rows() a list of data frames like jimi to bind together into a single data frame. # Examine discography and jimi discography jimi jimi %&gt;% # Bind jimi into a single data frame bind_rows(.id = &quot;album&quot;) %&gt;% # Make a complete data frame left_join(discography) # Import file sounddtrack hank_years &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/hank_years.csv&quot;, stringsAsFactors = FALSE) hank_charts &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/hank_charts.csv&quot;, stringsAsFactors = FALSE) # Examine hank_years and hank_charts hank_years ## year song ## 1 1947 Move It On Over ## 2 1947 My Love for You (Has Turned to Hate) ## 3 1947 Never Again (Will I Knock on Your Door) ## 4 1947 On the Banks of the Old Ponchartrain ## 5 1947 Pan American ## 6 1947 Wealth Won&#39;t Save Your Soul ## 7 1948 A Mansion on the Hill ## 8 1948 Honky Tonkin&#39; ## 9 1948 I Saw the Light ## 10 1948 I&#39;m a Long Gone Daddy ## 11 1948 My Sweet Love Ain&#39;t Around ## 12 1949 I&#39;m So Lonesome I Could Cry ## 13 1949 Lost Highway ## 14 1949 Lovesick Blues ## 15 1949 Mind Your Own Business ## 16 1949 My Bucket&#39;s Got a Hole in It ## 17 1949 Never Again (Will I Knock on Your Door) ## 18 1949 Wedding Bells ## 19 1949 You&#39;re Gonna Change (Or I&#39;m Gonna Leave) ## 20 1950 I Just Don&#39;t Like This Kind of Living ## 21 1950 Long Gone Lonesome Blues ## 22 1950 Moanin&#39; the Blues ## 23 1950 My Son Calls Another Man Daddy ## 24 1950 Nobody&#39;s Lonesome for Me ## 25 1950 They&#39;ll Never Take Her Love from Me ## 26 1950 Why Don&#39;t You Love Me ## 27 1950 Why Should We Try Anymore ## 28 1951 (I Heard That) Lonesome Whistle ## 29 1951 Baby, We&#39;re Really in Love ## 30 1951 Cold, Cold Heart ## 31 1951 Crazy Heart ## 32 1951 Dear John ## 33 1951 Hey Good Lookin&#39; ## 34 1951 Howlin&#39; At the Moon ## 35 1951 I Can&#39;t Help It (If I&#39;m Still in Love With You) ## 36 1952 Half as Much ## 37 1952 Honky Tonk Blues ## 38 1952 I&#39;ll Never Get Out of This World Alive ## 39 1952 Jambalaya (On the Bayou) ## 40 1952 Settin&#39; the Woods on Fire ## 41 1952 You Win Again ## 42 1953 Calling You ## 43 1953 I Won&#39;t Be Home No More ## 44 1953 Kaw-Liga ## 45 1953 Take These Chains from My Heart ## 46 1953 Weary Blues from Waitin&#39; ## 47 1953 Your Cheatin&#39; Heart ## 48 1954 (I&#39;m Gonna) Sing, Sing, Sing ## 49 1954 How Can You Refuse Him Now ## 50 1954 I&#39;m Satisfied with You ## 51 1954 You Better Keep It on Your Mind ## 52 1955 A Teardrop on a Rose ## 53 1955 At the First Fall of Snow ## 54 1955 Mother Is Gone ## 55 1955 Please Don&#39;t Let Me Love You ## 56 1955 Thank God ## 57 1956 A Home in Heaven ## 58 1956 California Zephyr ## 59 1956 Singing Waterfall ## 60 1956 There&#39;s No Room in My Heart for the Blues ## 61 1957 Leave Me Alone with the Blues ## 62 1957 Ready to Go Home ## 63 1957 The Waltz of the Wind ## 64 1958 Just Waitin&#39; ## 65 1965 The Pale Horse and His Rider ## 66 1966 Kaw-Liga ## 67 1989 There&#39;s a Tear in My Beer hank_charts ## song peak ## 1 (I Heard That) Lonesome Whistle 9 ## 2 (I&#39;m Gonna) Sing, Sing, Sing NA ## 3 A Home in Heaven NA ## 4 A Mansion on the Hill 12 ## 5 A Teardrop on a Rose NA ## 6 At the First Fall of Snow NA ## 7 Baby, We&#39;re Really in Love 4 ## 8 California Zephyr NA ## 9 Calling You NA ## 10 Cold, Cold Heart 1 ## 11 Crazy Heart 4 ## 12 Dear John 8 ## 13 Half as Much 2 ## 14 Hey Good Lookin&#39; 1 ## 15 Honky Tonk Blues 2 ## 16 Honky Tonkin&#39; 14 ## 17 How Can You Refuse Him Now NA ## 18 Howlin&#39; At the Moon 3 ## 19 I Can&#39;t Help It (If I&#39;m Still in Love With You) 2 ## 20 I Just Don&#39;t Like This Kind of Living 5 ## 21 I Saw the Light NA ## 22 I Won&#39;t Be Home No More 4 ## 23 I&#39;ll Never Get Out of This World Alive 1 ## 24 I&#39;m a Long Gone Daddy 6 ## 25 I&#39;m Satisfied with You NA ## 26 I&#39;m So Lonesome I Could Cry 2 ## 27 Jambalaya (On the Bayou) 1 ## 28 Just Waitin&#39; NA ## 29 Kaw-Liga 1 ## 30 Kaw-Liga NA ## 31 Leave Me Alone with the Blues NA ## 32 Long Gone Lonesome Blues 1 ## 33 Lost Highway 12 ## 34 Lovesick Blues 1 ## 35 Mind Your Own Business 5 ## 36 Moanin&#39; the Blues 1 ## 37 Mother Is Gone NA ## 38 Move It On Over 4 ## 39 My Bucket&#39;s Got a Hole in It 2 ## 40 My Love for You (Has Turned to Hate) NA ## 41 My Son Calls Another Man Daddy 9 ## 42 My Sweet Love Ain&#39;t Around NA ## 43 Never Again (Will I Knock on Your Door) NA ## 44 Never Again (Will I Knock on Your Door) 6 ## 45 Nobody&#39;s Lonesome for Me 9 ## 46 On the Banks of the Old Ponchartrain NA ## 47 Pan American NA ## 48 Please Don&#39;t Let Me Love You 9 ## 49 Ready to Go Home NA ## 50 Settin&#39; the Woods on Fire 2 ## 51 Singing Waterfall NA ## 52 Take These Chains from My Heart 1 ## 53 Thank God NA ## 54 The Pale Horse and His Rider NA ## 55 The Waltz of the Wind NA ## 56 There&#39;s a Tear in My Beer 7 ## 57 There&#39;s No Room in My Heart for the Blues NA ## 58 They&#39;ll Never Take Her Love from Me 5 ## 59 Wealth Won&#39;t Save Your Soul NA ## 60 Weary Blues from Waitin&#39; 7 ## 61 Wedding Bells 2 ## 62 Why Don&#39;t You Love Me 1 ## 63 Why Should We Try Anymore 9 ## 64 You Better Keep It on Your Mind NA ## 65 You Win Again 10 ## 66 You&#39;re Gonna Change (Or I&#39;m Gonna Leave) 4 ## 67 Your Cheatin&#39; Heart 1 hank_years %&gt;% # Reorder hank_years alphabetically by song title arrange(song) %&gt;% # Select just the year column select(year) %&gt;% # Bind the year column bind_cols(hank_charts) %&gt;% # Arrange the finished dataset arrange(year, song) ## year song peak ## 1 1947 Move It On Over 4 ## 2 1947 My Love for You (Has Turned to Hate) NA ## 3 1947 Never Again (Will I Knock on Your Door) NA ## 4 1947 On the Banks of the Old Ponchartrain NA ## 5 1947 Pan American NA ## 6 1947 Wealth Won&#39;t Save Your Soul NA ## 7 1948 A Mansion on the Hill 12 ## 8 1948 Honky Tonkin&#39; 14 ## 9 1948 I&#39;m Satisfied with You NA ## 10 1948 I Just Don&#39;t Like This Kind of Living 5 ## 11 1948 My Sweet Love Ain&#39;t Around NA ## 12 1949 I Won&#39;t Be Home No More 4 ## 13 1949 Lost Highway 12 ## 14 1949 Lovesick Blues 1 ## 15 1949 Mind Your Own Business 5 ## 16 1949 My Bucket&#39;s Got a Hole in It 2 ## 17 1949 Never Again (Will I Knock on Your Door) 6 ## 18 1949 Wedding Bells 2 ## 19 1949 You Better Keep It on Your Mind NA ## 20 1950 I&#39;m a Long Gone Daddy 6 ## 21 1950 Long Gone Lonesome Blues 1 ## 22 1950 Moanin&#39; the Blues 1 ## 23 1950 My Son Calls Another Man Daddy 9 ## 24 1950 Nobody&#39;s Lonesome for Me 9 ## 25 1950 They&#39;ll Never Take Her Love from Me 5 ## 26 1950 Why Don&#39;t You Love Me 1 ## 27 1950 Why Should We Try Anymore 9 ## 28 1951 (I&#39;m Gonna) Sing, Sing, Sing NA ## 29 1951 Baby, We&#39;re Really in Love 4 ## 30 1951 Cold, Cold Heart 1 ## 31 1951 Crazy Heart 4 ## 32 1951 Dear John 8 ## 33 1951 Hey Good Lookin&#39; 1 ## 34 1951 Howlin&#39; At the Moon 3 ## 35 1951 I&#39;ll Never Get Out of This World Alive 1 ## 36 1952 Half as Much 2 ## 37 1952 Honky Tonk Blues 2 ## 38 1952 I Can&#39;t Help It (If I&#39;m Still in Love With You) 2 ## 39 1952 Jambalaya (On the Bayou) 1 ## 40 1952 Settin&#39; the Woods on Fire 2 ## 41 1952 You&#39;re Gonna Change (Or I&#39;m Gonna Leave) 4 ## 42 1953 Calling You NA ## 43 1953 I&#39;m So Lonesome I Could Cry 2 ## 44 1953 Kaw-Liga 1 ## 45 1953 Take These Chains from My Heart 1 ## 46 1953 Weary Blues from Waitin&#39; 7 ## 47 1953 Your Cheatin&#39; Heart 1 ## 48 1954 (I Heard That) Lonesome Whistle 9 ## 49 1954 How Can You Refuse Him Now NA ## 50 1954 I Saw the Light NA ## 51 1954 You Win Again 10 ## 52 1955 A Teardrop on a Rose NA ## 53 1955 At the First Fall of Snow NA ## 54 1955 Mother Is Gone NA ## 55 1955 Please Don&#39;t Let Me Love You 9 ## 56 1955 Thank God NA ## 57 1956 A Home in Heaven NA ## 58 1956 California Zephyr NA ## 59 1956 Singing Waterfall NA ## 60 1956 There&#39;s No Room in My Heart for the Blues NA ## 61 1957 Leave Me Alone with the Blues NA ## 62 1957 Ready to Go Home NA ## 63 1957 The Waltz of the Wind NA ## 64 1958 Just Waitin&#39; NA ## 65 1965 The Pale Horse and His Rider NA ## 66 1966 Kaw-Liga NA ## 67 1989 There&#39;s a Tear in My Beer 7 Unfortunately, there is usually no clear way to tell whether or not the rows in two datasets align unless the datasets contain a mutual key. In that scenario, you can use a mutating join to bind the datasets in a foolproof way. 3.4.1 Data frames data.frame() defaults Changes strings to factors Adds row names Changes unusual column names However sometimes we do not want this behaviour, so we can use data_frame() from dplyr data_frame() will not… Change the data type of vectors (e.g. strings to factors) Add row names Change column names Recycle vectors greater than length one # Make combined data frame using data_frame() data_frame(year = hank_year, song = hank_song, peak = hank_peak) %&gt;% # Extract songs where peak equals 1 filter(peak == &quot;1&quot;) # Or if the data was a list of vectors # Convert the hank list into a data frame as_data_frame(hank) %&gt;% # Extract songs where peak equals 1 filter(peak == &quot;1&quot;) # Or if we had nested data tables as a list of vectors bind_rows(michael, .id = &quot;album&quot;) %&gt;% group_by(album) %&gt;% mutate(rank = min_rank(peak)) %&gt;% filter(rank == 1) %&gt;% select(-rank, -peak) 3.4.2 Data Types Usually R will do sensible things when working with data, linke 1 + 1 = 2, but “one” + “one” = error. You should be aware of some of the data types when working in R. Every piece of data in R is a vector, even if it only has a single value in it. Unless the data is in a list, all elements in the vector are going to be in one of six data types - known as atomic data types. You can use typeof() to identfy what is in a vector. Table 3.1: Atomic Data Types in R Type Output Atomic data type &gt;typeof(TRUE) [1] “logical” Logical &gt;typeof(“hello”) [1] “character” Character (i.e. string) &gt;typeof(3.14) [1] “double” Double (i.e. numeric w/ decimal) &gt;typeof(1L) [1] “integer” Integer (i.e. numeric w/o decimal) &gt;typeof(1 + 2i) [1] “complex” Complex &gt;typeof(raw(1)) [1] “raw” Raw New classes of data, such as factors which are used for categorical variables, are created from one of the six types above giving it a class attributing and other metadata stored as attributes. A factor is a an integer vector with a factor class atribute, a levels attribute and sometimes a level attribute. Whilst they contain a sequence of integers, they are dispalyed as the asssocicated labels. In R, each column in a data frame must be of a single type or class of data because each column is stored as a single vector. If mutliple tables or columns are being combined, R uses coercion rules to decide what to do. If any variable being combined has a character, it stores it as a character string Doubles being combined with Integers or logicals gets stored as a double (T = 1, F = 0) If integers are being combined with a logical, it gets stored as a integer (T = 1, F = 0) factors with charecters, the factor lables gets converted to strings (A = “A”, B = “B”) factors with doubles or integers, the factor gets converted to their numeric values (A = 1, B = 2) Note pay particular attention to factor data with numeric labels - e.g. if 4, 5 and 6 values have factor levels of 1, 2 and 3, when combining or coercing with or to a numeric, the result will be 4, 5 and 6 rather than the factor lables. To get the lables which are numeric values, convert the factor to a character string then convert those to a double or integer e.g. as.numeric(as.character(x)). Dplyr won’t try and coerce different data but will throw an error and let you manually determine what you want to do with the data. If combining factors, dplyr will convert them to charecters then give a warning message. For example, sixties contains the top selling albums in the US in the 1960s. It stores year as a numeric (double). When you combine it with seventies, which stores year as a factor, bind_rows() returns an error. You can fix this by coercing seventies$year to a numeric. But if you seventies %&gt;% mutate(year = as.numeric(year)) will not return the correct year. We need to convert the factor to a string. seventies %&gt;% # Coerce seventies$year into a useful numeric mutate(year = as.numeric(as.character(year))) %&gt;% # Bind the updated version of seventies to sixties bind_rows(sixties) %&gt;% arrange(year) 3.5 Advanced Joining Sometimes joins can fail for one of two principle reasons - missing keys or duplicate keys. These can be either related to the values or the columns. One common problem can be that column names are missing and that the rows are called by their actual value. R’s data frames can store important information in the row.names attribute. This is not a tidy way to store data, but it does happen quite commonly. If the primary key of your dataset is stored in row.names, you will have trouble joining it to other datasets. For example, stage_songs contains information about songs that appear in musicals. However, it stores the primary key (song name) in the row.names attribute. As a result, you cannot access the key with a join function. One way to remedy this problem is to use the function rownames_to_column() from the tibble package. rownames_to_column() returns a copy of a dataset with the row names added to the data as a column. # Import the data stage_songs &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/stage_songs.csv&quot;, stringsAsFactors = FALSE) stage_writers &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/stage_writers.csv&quot;, stringsAsFactors = FALSE) # And view stage_songs ## X musical year ## 1 Children Will Listen Into the Woods 1986 ## 2 Maria West Side Story 1957 ## 3 Memory Cats 1981 ## 4 The Music of the Night Phantom of the Opera 1986 stage_writers ## song composer ## 1 Children Will Listen Stephen Sondheim ## 2 Maria Leonard Bernstein ## 3 Memory Andrew Lloyd Webber ## 4 The Music of the Night Andrew Lloyd Webber # Load the tibble package library(tibble) stage_songs %&gt;% # Add row names as a column named song rownames_to_column(var = &quot;song&quot;) %&gt;% # Left join stage_writers to stage_songs left_join(stage_writers) ## Joining, by = &quot;song&quot; ## song X musical year composer ## 1 1 Children Will Listen Into the Woods 1986 &lt;NA&gt; ## 2 2 Maria West Side Story 1957 &lt;NA&gt; ## 3 3 Memory Cats 1981 &lt;NA&gt; ## 4 4 The Music of the Night Phantom of the Opera 1986 &lt;NA&gt; If there are duplicate rows in the primary table, there will be duplicate joins to the second table, resulting in a mutiple of new rows. Equally, if there are multiple rows in the secondary table there will be duplicate entries. This can be made worse if there are what appear to be duplicate key values in both sets of tables resulting in n^2 rows. In the case of missing data, it is best to remove the data with a filter first, before joining. # Remove NA&#39;s from key before joining two_songs %&gt;% filter(!is.na(movie)) %&gt;% inner_join(singers, by = &quot;movie&quot;) It is not always needed to add the by = argument when joining, dplyr when compare both tables looking for a common field. However, where the column names differ (but have the same contents) when can explicitly state the columns to join by. Equally, sometimes you may have to specify the by = argument when column names are the same, but contain different data, so you specify which column to actually join on. # Import the data movie_studios &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/movie_studios.csv&quot;, stringsAsFactors = FALSE) movie_years &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/movie_years.csv&quot;, stringsAsFactors = FALSE) # View the data movie_studios ## movie name ## 1 The Road to Morocco Paramount Pictures ## 2 Going My Way Paramount Pictures ## 3 Anchors Aweigh Metro-Goldwyn-Mayer ## 4 Till the Clouds Roll By Metro-Goldwyn-Mayer ## 5 White Christmas Paramount Pictures ## 6 The Tender Trap Metro-Goldwyn-Mayer ## 7 High Society Metro-Goldwyn-Mayer ## 8 The Joker is Wild Paramount Pictures ## 9 Pal Joey Columbia Pictures ## 10 Can-Can Twentieth-Century Fox movie_years ## movie name year ## 1 The Road to Morocco Bing Crosby 1942 ## 2 Going My Way Bing Crosby 1944 ## 3 Anchors Aweigh Frank Sinatra 1945 ## 4 Till the Clouds Roll By Frank Sinatra 1946 ## 5 White Christmas Bing Crosby 1954 ## 6 The Tender Trap Frank Sinatra 1955 ## 7 High Society Bing Crosby 1956 ## 8 The Joker is Wild Frank Sinatra 1957 ## 9 Pal Joey Frank Sinatra 1957 ## 10 Can-Can Frank Sinatra 1960 In this instance, name refers to the studio and the name of the actor in each data table. By default, dplyr will ignore duplicate column names if you set the by argument and do not include the duplicated name in the argument. When you do this, dplyr will treat the columns in the normal fashion, but it will add .x and .y to the duplicated names to help you tell the columns apart. We can use rename(data, new_name = old_name) renames old_name to new_name in data to something else if desired. movie_years %&gt;% # Left join movie_studios to movie_years left_join(movie_studios, by = &quot;movie&quot;) %&gt;% # Rename the columns: artist and studio rename(artist = name.x, studio = name.y) ## movie artist year studio ## 1 The Road to Morocco Bing Crosby 1942 Paramount Pictures ## 2 Going My Way Bing Crosby 1944 Paramount Pictures ## 3 Anchors Aweigh Frank Sinatra 1945 Metro-Goldwyn-Mayer ## 4 Till the Clouds Roll By Frank Sinatra 1946 Metro-Goldwyn-Mayer ## 5 White Christmas Bing Crosby 1954 Paramount Pictures ## 6 The Tender Trap Frank Sinatra 1955 Metro-Goldwyn-Mayer ## 7 High Society Bing Crosby 1956 Metro-Goldwyn-Mayer ## 8 The Joker is Wild Frank Sinatra 1957 Paramount Pictures ## 9 Pal Joey Frank Sinatra 1957 Columbia Pictures ## 10 Can-Can Frank Sinatra 1960 Twentieth-Century Fox In the next example, name refers to different objects. To make the join, set by to a named vector. The names of the vector will refer to column names in the primary dataset (x). The values of the vector will correspond to the column names in the secondary dataset (y), e.g. x %&gt;% left_join(y, by = c(“x.name1” = “y.name2”)) # Import the data elvis_songs &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/elvis_songs.csv&quot;, stringsAsFactors = FALSE) elvis_movies &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/elvis_movies.csv&quot;, stringsAsFactors = FALSE) # And view elvis_songs ## name movie ## 1 (You&#39;re So Square) Baby I Don&#39;t Care Jailhouse Rock ## 2 I Can&#39;t Help Falling in Love Blue Hawaii ## 3 Jailhouse Rock Jailhouse Rock ## 4 Viva Las Vegas Viva Las Vegas ## 5 You Don&#39;t Know Me Clambake elvis_movies ## name year ## 1 Jailhouse Rock 1957 ## 2 Blue Hawaii 1961 ## 3 Viva Las Vegas 1963 ## 4 Clambake 1967 elvis_movies %&gt;% # Left join elvis_songs to elvis_movies by this column left_join(elvis_songs, by = c(&quot;name&quot; = &quot;movie&quot;)) %&gt;% # Rename columns rename(movie = name, song = name.y) ## movie year song ## 1 Jailhouse Rock 1957 (You&#39;re So Square) Baby I Don&#39;t Care ## 2 Jailhouse Rock 1957 Jailhouse Rock ## 3 Blue Hawaii 1961 I Can&#39;t Help Falling in Love ## 4 Viva Las Vegas 1963 Viva Las Vegas ## 5 Clambake 1967 You Don&#39;t Know Me In the following example, the two datasets in question have the same key variable, however the has a different column name that connects the datasets. We also rename one of the columns within the select statement to aid understanding in the dplyr pipeline. # Import the data movie_directors &lt;- read.csv(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/movie_directors.csv&quot;, stringsAsFactors = FALSE) # Identify the key columns movie_directors ## name director studio ## 1 Anchors Aweigh George Sidney Metro-Goldwyn-Mayer ## 2 Can-Can Walter Lang Twentieth-Century Fox ## 3 Going My Way Leo McCarey Paramount Pictures ## 4 High Society Charles Walters Metro-Goldwyn-Mayer ## 5 Pal Joey George Sidney Columbia Pictures ## 6 The Joker is Wild Charles Vidor Paramount Pictures ## 7 The Road to Morocco David Butler Paramount Pictures ## 8 The Tender Trap Charles Walters Metro-Goldwyn-Mayer ## 9 Till the Clouds Roll By Richard Whorf Metro-Goldwyn-Mayer ## 10 White Christmas Michael Curtiz Paramount Pictures movie_years ## movie name year ## 1 The Road to Morocco Bing Crosby 1942 ## 2 Going My Way Bing Crosby 1944 ## 3 Anchors Aweigh Frank Sinatra 1945 ## 4 Till the Clouds Roll By Frank Sinatra 1946 ## 5 White Christmas Bing Crosby 1954 ## 6 The Tender Trap Frank Sinatra 1955 ## 7 High Society Bing Crosby 1956 ## 8 The Joker is Wild Frank Sinatra 1957 ## 9 Pal Joey Frank Sinatra 1957 ## 10 Can-Can Frank Sinatra 1960 movie_years %&gt;% # Left join movie_directors to movie_years left_join(movie_directors, by = c(&quot;movie&quot; = &quot;name&quot;)) %&gt;% # Arrange the columns using select() select(year, movie, artist = name, director, studio) ## year movie artist director ## 1 1942 The Road to Morocco Bing Crosby David Butler ## 2 1944 Going My Way Bing Crosby Leo McCarey ## 3 1945 Anchors Aweigh Frank Sinatra George Sidney ## 4 1946 Till the Clouds Roll By Frank Sinatra Richard Whorf ## 5 1954 White Christmas Bing Crosby Michael Curtiz ## 6 1955 The Tender Trap Frank Sinatra Charles Walters ## 7 1956 High Society Bing Crosby Charles Walters ## 8 1957 The Joker is Wild Frank Sinatra Charles Vidor ## 9 1957 Pal Joey Frank Sinatra George Sidney ## 10 1960 Can-Can Frank Sinatra Walter Lang ## studio ## 1 Paramount Pictures ## 2 Paramount Pictures ## 3 Metro-Goldwyn-Mayer ## 4 Metro-Goldwyn-Mayer ## 5 Paramount Pictures ## 6 Metro-Goldwyn-Mayer ## 7 Metro-Goldwyn-Mayer ## 8 Paramount Pictures ## 9 Columbia Pictures ## 10 Twentieth-Century Fox 3.6 Joining mutiple tables Whilst it would be possible to join multiple tables iteratively, for instance df1 %&gt;% left_join(df2) %&gt;% left_join(df3) %&gt;% left_join(df4) We can use the purrr package instead. We list the dataframes as a vector first, then pass the vector as the first argument to the reduce() function, the second argument should be the dplyr function (e.g. left_join without brackets), the third argument should be the by = var to specify what to join on. # Load the purrr library library(purrr) # Place supergroups, more_bands, and more_artists into a list list(supergroups, more_bands, more_artists) %&gt;% # Use reduce to join together the contents of the list reduce(left_join, by = c(&quot;first&quot;, &quot;last&quot;)) # Or to list just those appear in all three tables list(more_artists, more_bands, supergroups) %&gt;% # Return rows of more_artists in all three datasets reduce(semi_join, by = c(&quot;first&quot;, &quot;last&quot;)) 3.7 Other implentations Dplyr join functions are similar to SQL statements, as shown below. (#fig:SQL functions Dplyr)Joins available in Dplyr With dplyr it is possible to create a connection to a database using the DBI package: src_sqlite(): for SQLite datbases src_mysql: mySQL and MariaDB src_postgres: PostgreSQL 3.8 Case Study - Lahman DB In this seciton we will use the Sean Lahman baseball statistics data. # Load names and the package lahmanNames &lt;- readRDS(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/lahmanNames.rds&quot;) library(purrr) library(Lahman) # Find variables in common reduce(lahmanNames, intersect) ## # A tibble: 0 x 1 ## # ... with 1 variables: var &lt;chr&gt; There are no common (intersecting) variables across the datasets. But perhaps some variables are in more than one table. lahmanNames %&gt;% # Bind the data frames in lahmanNames bind_rows() %&gt;% # Group the result by var group_by(var) %&gt;% # Tally the number of appearances tally() %&gt;% # Filter the data filter(n &gt; 1) %&gt;% # Arrange the results arrange(desc(n)) ## # A tibble: 59 x 2 ## var n ## &lt;chr&gt; &lt;int&gt; ## 1 yearID 21 ## 2 playerID 19 ## 3 lgID 17 ## 4 teamID 13 ## 5 G 10 ## 6 L 6 ## 7 W 6 ## 8 BB 5 ## 9 CS 5 ## 10 GS 5 ## # ... with 49 more rows So PlayerID appears regularly, but in which tables? lahmanNames %&gt;% # Bind the data frames bind_rows(.id = &#39;dataframe&#39;) %&gt;% # Filter the results filter(var == &quot;playerID&quot;) %&gt;% # Extract the dataframe variable `$`(dataframe) ## [1] &quot;AllstarFull&quot; &quot;Appearances&quot; &quot;AwardsManagers&quot; ## [4] &quot;AwardsPlayers&quot; &quot;AwardsShareManagers&quot; &quot;AwardsSharePlayers&quot; ## [7] &quot;Batting&quot; &quot;BattingPost&quot; &quot;CollegePlaying&quot; ## [10] &quot;Fielding&quot; &quot;FieldingOF&quot; &quot;FieldingPost&quot; ## [13] &quot;HallOfFame&quot; &quot;Managers&quot; &quot;ManagersHalf&quot; ## [16] &quot;Master&quot; &quot;Pitching&quot; &quot;PitchingPost&quot; ## [19] &quot;Salaries&quot; Next we can begin to look at the salaries data. First, let’s begin by ensuring that we have salary information for each player in the database, or at least no systematic holes in our coverage. Our new table will be concise and players contains only one row for each distinct player. players &lt;- Master %&gt;% # Return one row for each distinct player distinct(playerID, nameFirst, nameLast) Next, how many missing values do we have? players %&gt;% # Find all players who do not appear in Salaries anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # Count them count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 13958 The answer - a lot! Is it possible that these players somehow did not play (and hence did not earn a salary)? We can check with the Appearances data frame. Appearances contains information about every game played in major league baseball. That is, if a player played a game, it would show up as a row in Appearances. players %&gt;% anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # How many unsalaried players appear in Appearances? semi_join(Appearances, by = &quot;playerID&quot;) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 13765 So a large number of players played a game but are missing salary information. Interestingly, 193 players neither played a game nor have a recorded salary. Perhaps the unsalaried players only played one or two games, and hence did not earn a full salary. players %&gt;% # Find all players who do not appear in Salaries anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # Join them to Appearances left_join(Appearances, by = &quot;playerID&quot;) %&gt;% # Calculate total_games for each player group_by(playerID) %&gt;% summarise(total_games = sum(G_all, na.rm = T)) %&gt;% # Arrange in descending order by total_games arrange(desc(total_games)) ## # A tibble: 13,958 x 2 ## playerID total_games ## &lt;chr&gt; &lt;int&gt; ## 1 yastrca01 3308 ## 2 aaronha01 3298 ## 3 cobbty01 3034 ## 4 musiast01 3026 ## 5 mayswi01 2992 ## 6 robinbr01 2896 ## 7 kalinal01 2834 ## 8 collied01 2824 ## 9 robinfr02 2808 ## 10 wagneho01 2795 ## # ... with 13,948 more rows Here we some some players played thousands of games, so the idea that some didn’t play enough games doesn’t seem to hold. Is it possible that the unsalaried players did not actually play in the games that they appeared in? One way to check would be to determine if the players had an at-bat (i.e. batted) in the games that they appeared in. players %&gt;% # Find unsalaried players anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # Join Batting to the unsalaried players left_join(Batting, by = &quot;playerID&quot;) %&gt;% # Group by player group_by(playerID) %&gt;% # Sum at-bats for each player summarise(total_at_bat = sum(AB, na.rm = T)) %&gt;% # Arrange in descending order arrange(desc(total_at_bat)) ## # A tibble: 13,958 x 2 ## playerID total_at_bat ## &lt;chr&gt; &lt;int&gt; ## 1 aaronha01 12364 ## 2 yastrca01 11988 ## 3 cobbty01 11434 ## 4 musiast01 10972 ## 5 mayswi01 10881 ## 6 robinbr01 10654 ## 7 wagneho01 10430 ## 8 brocklo01 10332 ## 9 ansonca01 10277 ## 10 aparilu01 10230 ## # ... with 13,948 more rows The unpaid players definitely participated in the games. The highest number of at bats is Hank Aaron so it looks like we are dealing with missing data here and not unsalaried players. Next, lets look at the Hall of Fame players # Find the distinct players that appear in HallOfFame nominated &lt;- HallOfFame %&gt;% distinct(playerID) nominated %&gt;% # Count the number of players in nominated count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1260 nominated_full &lt;- nominated %&gt;% # Join to Master left_join(Master, by = &quot;playerID&quot;) %&gt;% # Return playerID, nameFirst, nameLast select(playerID, nameFirst, nameLast) Next, let’s find out how many of those nominated are now inducted in to the HoF # Find distinct players in HallOfFame with inducted == &quot;Y&quot; inducted &lt;- HallOfFame %&gt;% filter(inducted == &quot;Y&quot;) %&gt;% distinct(playerID) inducted %&gt;% # Count the number of players in inducted count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 317 inducted_full &lt;- inducted %&gt;% # Join to Master left_join(Master, by = &quot;playerID&quot;) %&gt;% # Return playerID, nameFirst, nameLast select(playerID, nameFirst, nameLast) Now that we know who was inducted and who was nominated, let’s examine what separates the nominees who were inducted from the nominees who were not. Let’s start with a simple question: Did nominees who were inducted get more awards than nominees who were not inducted? # Tally the number of awards in AwardsPlayers by playerID nAwards &lt;- AwardsPlayers %&gt;% group_by(playerID) %&gt;% tally() nAwards %&gt;% # Filter to just the players in inducted semi_join(inducted, by = &quot;playerID&quot;) %&gt;% # Calculate the mean number of awards per player summarize(avg_n = mean(n, na.rm = T)) ## # A tibble: 1 x 1 ## avg_n ## &lt;dbl&gt; ## 1 12.14583 nAwards %&gt;% # Filter to just the players in nominated semi_join(nominated, by = &quot;playerID&quot;) %&gt;% # Filter to players NOT in inducted anti_join(inducted, by = &quot;playerID&quot;) %&gt;% # Calculate the mean number of awards per player summarize(avg_n = mean(n, na.rm = T)) ## # A tibble: 1 x 1 ## avg_n ## &lt;dbl&gt; ## 1 4.231054 The answer is yes - it looks like about 3 times the number. Was the salary much higher for those who were inducted? # Find the players who are in nominated, but not inducted notInducted &lt;- nominated %&gt;% setdiff(inducted) Salaries %&gt;% # Find the players who are in notInducted semi_join(notInducted, by = &quot;playerID&quot;) %&gt;% # Calculate the max salary by player group_by(playerID) %&gt;% summarize(max_salary = max(salary, na.rm = T)) %&gt;% # Calculate the average of the max salaries summarize(avg_salary = mean(max_salary, na.rm = T)) ## # A tibble: 1 x 1 ## avg_salary ## &lt;dbl&gt; ## 1 5124653 # Repeat for players who were inducted Salaries %&gt;% semi_join(inducted, by = &quot;playerID&quot;) %&gt;% # Calculate the max salary by player group_by(playerID) %&gt;% summarize(max_salary = max(salary, na.rm = T)) %&gt;% # Calculate the average of the max salaries summarize(avg_salary = mean(max_salary, na.rm = T)) ## # A tibble: 1 x 1 ## avg_salary ## &lt;dbl&gt; ## 1 6092038 So the salaries of the players who were inducted was higher. Were any players nominated 5 years before they retired? players &lt;- Appearances %&gt;% # Filter Appearances against nominated semi_join(nominated, by = &quot;playerID&quot;) %&gt;% # Find last year played by player group_by(playerID) %&gt;% summarize(last_year = max(yearID, na.rm = T)) %&gt;% # Join to full HallOfFame left_join(HallOfFame, by = &quot;playerID&quot;) %&gt;% # Filter for unusual observations filter((yearID - last_year) &lt; 5 ) players ## # A tibble: 194 x 10 ## playerID last_year yearID votedBy ballots needed votes inducted ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fctr&gt; ## 1 altroni01 1933 1937 BBWAA 201 151 3 N ## 2 applilu01 1950 1953 BBWAA 264 198 2 N ## 3 bartedi01 1946 1948 BBWAA 121 91 1 N ## 4 beckro01 2004 2008 BBWAA 543 408 2 N ## 5 boudrlo01 1952 1956 BBWAA 193 145 2 N ## 6 camildo01 1945 1948 BBWAA 121 91 1 N ## 7 chandsp01 1947 1950 BBWAA 168 126 2 N ## 8 chandsp01 1947 1951 BBWAA 226 170 1 N ## 9 chapmbe01 1946 1949 BBWAA 153 115 1 N ## 10 cissebi01 1938 1937 BBWAA 201 151 1 N ## # ... with 184 more rows, and 2 more variables: category &lt;fctr&gt;, ## # needed_note &lt;chr&gt; players %&gt;% group_by(playerID) %&gt;% tally() %&gt;% # Arrange the results arrange(desc(n)) ## # A tibble: 92 x 2 ## playerID n ## &lt;chr&gt; &lt;int&gt; ## 1 deandi01 9 ## 2 dickebi01 7 ## 3 foxxji01 6 ## 4 lyonste01 6 ## 5 greenha01 5 ## 6 ruffire01 5 ## 7 cronijo01 4 ## 8 dimagjo01 4 ## 9 gehrich01 4 ## 10 hackst01 4 ## # ... with 82 more rows So we get a list of 194 players who were nominated within 5 years of having last played, with some players being nominated many times. And of those, how many were nominated whilst still playing? Appearances %&gt;% # Filter Appearances against nominated semi_join(nominated, by = &quot;playerID&quot;) %&gt;% # Find last year played by player group_by(playerID) %&gt;% summarize(last_year = max(yearID, na.rm = T)) %&gt;% # Join to full HallOfFame left_join(HallOfFame, by = &quot;playerID&quot;) %&gt;% # Filter for unusual observations filter(yearID &lt;= last_year) %&gt;% # look for the most recent incident of nomination before reirement arrange(desc(yearID)) ## # A tibble: 39 x 10 ## playerID last_year yearID votedBy ballots needed votes inducted ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fctr&gt; ## 1 francju02 2014 2013 BBWAA 569 427 6 N ## 2 rijojo01 2002 2001 BBWAA 515 387 1 N ## 3 stephjo03 2002 1979 BBWAA 432 324 0 N ## 4 minosmi01 1980 1969 BBWAA 340 255 6 N ## 5 spahnwa01 1965 1958 BBWAA 266 200 1 N ## 6 rizzuph01 1956 1956 BBWAA 193 145 1 N ## 7 paigesa01 1965 1951 BBWAA 226 170 1 N ## 8 waltebu01 1950 1950 BBWAA 168 126 4 N ## 9 medwijo01 1948 1948 BBWAA 121 91 1 N ## 10 deandi01 1947 1947 BBWAA 161 121 88 N ## # ... with 29 more rows, and 2 more variables: category &lt;fctr&gt;, ## # needed_note &lt;chr&gt; So around 40 players were nominated whilst still plaing "],
["cleaning-data.html", "4 Cleaning Data 4.1 Tidying data 4.2 Preparing data for analysis 4.3 String manipulation 4.4 Missing, Specials and Outliers 4.5 Examples", " 4 Cleaning Data Notes taken during/inspired by the Datacamp course ‘Cleaning Data in R’ by Nick Carchedi. 4.1 Tidying data In Hadley’s paper on tidy data, he talked about how columns in a data frame should be variables or attributes and rows should be observations - this somestimes does not happen if there are things like dummy variables as columns, that could be collpased in to a single column. The entire data table (data frame) should be about one particular set of data i.e. we have countries without an embedded table about cats. Hadley introduced the tidyr package to try and help clean some data. There are two fundamental verbs of data tidying: gather() takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer spread() takes two columns (key &amp; value) and spreads in to multiple columns, it makes “long” data wider gather(data, key, value …) data: is a data frame key: the name of the new key column value: the name of the new value column …: names of columns to gather or not (if not, state -col e.g. -time to not include the time column in the gathered table) spread(data, key, value) data: is a data frame key: the name containing the key column value: the name containing the value column # Apply gather() to bmi and save the result as bmi_long bmi_long &lt;- gather(bmi, year, bmi_value, -Country) # Apply spread() to bmi_long bmi_wide &lt;- spread(bmi_long, year, bmi_val) Another useful feature is separate(). This takes a single variable and separates it into two separate columns or variable, for instance converting a year-month (2015-10) into a separate column for year and month. separate(data, col, into, sep = “”) data: a data frame col: bare name of column to separate into: charecter vector of new column names Optional sep = “”: in the separate command you can designate on what item (/, @ etc) to break the data by. This is optional and can depend on the column type (numeric vs char) # separate year-mo into two columns separate(treatments, year_mo, c(&quot;year&quot;, &quot;month&quot;)) We can also use the unite function to combine two columns together unite(data, col, …) data: a data frame col: name of the new column …: columns to unite The default seperator within the new column is an underscore, however we can specify something different Optional sep = “-”: would add the seperator as a hyphen head(bmi_cc) Country_ISO year bmi_val 1 Afghanistan/AF Y1980 21.48678 2 Albania/AL Y1980 25.22533 3 Algeria/DZ Y1980 22.25703 4 Andorra/AD Y1980 25.66652 5 Angola/AO Y1980 20.94876 6 Antigua and Barbuda/AG Y1980 23.31424 So to separate Country_ISO into two columns # Apply separate() to bmi_cc bmi_cc_clean &lt;- separate(bmi_cc, col = Country_ISO, into = c(&quot;Country&quot;, &quot;ISO&quot;), sep = &quot;/&quot;) # Apply unite() to bmi_cc_clean aand reverse bmi_cc &lt;- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = &quot;-&quot;) 4.2 Preparing data for analysis Often we need to convert, or in the case of raw data, create the appropriate data type for each variable prior to analysis. Some common data types include character: “treatment”, “123”, “A” numeric: 23.44, 120, NaN, Inf integer: 4L, 1123L factor: factor(“Hello”), factor(8) logical: TRUE, FALSE, NA We can use the class() function to detmine the variable type, or we can also include a value to determine the appropriate type e.g. class(77L) will return [1] “integer”. We can also use the coercion functions to change the types, such as as.numeric, as.factor() and as.character(). For dates and times, we can use the lubridate package. # Load the lubridate package library(lubridate) # Parse as date dmy(&quot;17 Sep 2015&quot;) # Parse as date and time (with no seconds!) mdy_hm(&quot;July 15, 2012 12:56&quot;) # Coerce dob to a date (with no time) students2$dob &lt;- ymd(students2$dob) # Coerce nurse_visit to a date and time students2$nurse_visit &lt;- ymd_hms(students2$nurse_visit) 4.3 String manipulation Another useful package is stringr, which like lubridate and other Hadley packages has a consistent interface, providing a range of functions for dealing with strings. Some functions include str_trim() - Trim leading and trailing white space str_pad() - Pad with additional characters str_detect() - Detect a pattern str_replace() - Find and replace a pattern # Load the stringr package library(stringr) # Trim all leading and trailing whitespace str_trim(c(&quot; Filip &quot;, &quot;Nick &quot;, &quot; Jonathan&quot;)) # Pad these strings with leading zeros str_pad(c(&quot;23485W&quot;, &quot;8823453Q&quot;, &quot;994Z&quot;), width = 9, side = &quot;left&quot;, pad = 0) # Detect all dates of birth (dob) in 1997 str_detect(students2$dob, &quot;1997&quot;) # In the sex column, replace &quot;F&quot; with &quot;Female&quot;... students2$sex &lt;- str_replace(students2$sex, &quot;F&quot;, &quot;Female&quot;) # ...And &quot;M&quot; with &quot;Male&quot; students2$sex &lt;- str_replace(students2$sex, &quot;M&quot;, &quot;Male&quot;) R {base} also has some handy features for strings, including toupper() and tolower(). 4.4 Missing, Specials and Outliers Generally missing values in R are represented by NA. However, if the data has been imported from other systems, the values can be different, such as a . (dot) if imported from SPSS. We can use the is.na(df) to return a TRUE/FALSE array of where there are NA values in a data frame. Or, for large datasets, we can use the any(is.na(df)) to return a true or false if there is an NA anywhere in the data frame. Alternatively we can use the sum(is.na(df)) to count how many NAs are in the dataframe. Use complete.cases() to see which rows have no missing values. Special values include inf for infinite value, NaN for Not a number. Outliers are best detected by measures such as the IQR or other nuemrical measures (see the EDA section), by using a boxplot or a histogram/density plot. There are a number of likely reasons for an outlier: Valid measurements Variability in measurement Experimental error Data entry error May be discarded or retained depending on cause. In some instances we may want to cap, or put a limit on, the maximum number the outlier can. Looking at the actual values and considering possible values can help, for instance negative age values or a perons age above 200 are not plausible values. However, they may be data entry errors or in the case of negative numbers, represent a deliberately coded missing value. 4.5 Examples The weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. head(weather) X year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 1 2014 12 Max.TemperatureF 64 42 51 43 42 45 38 29 49 48 39 39 42 45 2 2014 12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39 43 36 35 37 39 3 2014 12 Min.TemperatureF 39 33 37 30 26 38 21 18 29 38 32 31 32 33 4 2014 12 Max.Dew.PointF 46 40 49 24 37 45 36 28 49 45 37 28 28 29 # Load the tidyr package library(tidyr) # Gather the columns weather2 &lt;- gather(weather, day, value, X1:X31, na.rm = TRUE) becomes X year month measure day value 1 2014 12 Max.TemperatureF X1 64 2 2014 12 Mean.TemperatureF X1 52 3 2014 12 Min.TemperatureF X1 39 4 2014 12 Max.Dew.PointF X1 46 5 2014 12 MeanDew.PointF X1 40 6 2014 12 Min.DewpointF X1 26 Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. WE also have an additional column (X) which is not needed as it is just the row number. # First remove column of row names weather2 &lt;- weather2[, -1] # Spread the data weather3 &lt;- spread(weather2, measure, value) Table 3.1: year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH 2014 12 X1 6 Rain 46 29 2014 12 X2 7 Rain-Snow 40 29 2014 12 X3 8 Rain 49 38 2014 12 X4 3 24 33 2014 12 X5 5 Rain 37 26 2014 12 X6 8 Rain 45 25 … Now that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We’ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. # Remove X&#39;s from day column weather3$day &lt;- str_replace(weather3$day, &quot;X&quot;, &quot;&quot;) # Unite the year, month, and day columns weather4 &lt;- unite(weather3, date, year, month, day, sep = &quot;-&quot;) # Convert date column to proper date format using lubridates&#39;s ymd() weather4$date &lt;- ymd(weather4$date) # Rearrange columns using dplyr&#39;s select() weather5 &lt;- select(weather4, date, Events, CloudCover:WindDirDegrees) It’s important for analysis that variables are coded appropriately. This is not yet the case with our weather data. Recall that functions such as as.numeric() and as.character() can be used to coerce variables into different types. It’s important to keep in mind that coercions are not always successful, particularly if there’s some data in a column that you don’t expect. For example, the following will cause problems: as.numeric(c(4, 6.44, “some string”, 222)) So you can use the str_replace function to change character values to something else. If we have missing data, we can use indices and is.na function to identify then only see those rows with NA values on a variable of interest. # Count missing values sum(is.na(weather6)) # Find missing values summary(weather6) # Find indices of NAs in Max.Gust.SpeedMPH ind &lt;- which(is.na(weather6$Max.Gust.SpeedMPH)) # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind, ] Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary(). Once implausible values are identified, they must be dealt with in an intelligent and informed way. Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data. # Find row with Max.Humidity of 1000 ind &lt;- which(weather6$Max.Humidity == 1000) # Look at the data for that day weather6[ind, ] # Change 1000 to 100 weather6$Max.Humidity[ind] &lt;- 100 Before officially calling our weather data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we generally recommend using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. "],
["importing-cleaning-data-in-r-case-studies.html", "5 Importing &amp; Cleaning Data in R: Case Studies 5.1 Ticket Sales Data 5.2 Working with dates 5.3 MBTA Ridership Data 5.4 World Food Facts 5.5 School Attendance Data", " 5 Importing &amp; Cleaning Data in R: Case Studies Notes taken during/inspired by the Datacamp course ‘Importing &amp; Cleaning Data in R: Case Studies’ by Nick Carchedi. 5.1 Ticket Sales Data # Import sales.csv: sales sales &lt;- read.csv(&quot;https://assets.datacamp.com/production/course_1294/datasets/sales.csv&quot;, stringsAsFactors = FALSE) # View dimensions of sales dim(sales) ## [1] 5000 46 # Inspect first 6 rows of sales head(sales, n = 6) ## X event_id primary_act_id secondary_act_id ## 1 1 abcaf1adb99a935fc661 43f0436b905bfa7c2eec b85143bf51323b72e53c ## 2 2 6c56d7f08c95f2aa453c 1a3e9aecd0617706a794 f53529c5679ea6ca5a48 ## 3 3 c7ab4524a121f9d687d2 4b677c3f5bec71eec8d1 b85143bf51323b72e53c ## 4 4 394cb493f893be9b9ed1 b1ccea01ad6ef8522796 b85143bf51323b72e53c ## 5 5 55b5f67e618557929f48 91c03a34b562436efa3c b85143bf51323b72e53c ## 6 6 4f10fd8b9f550352bd56 ac4b847b3fde66f2117e 63814f3d63317f1b56c4 ## purch_party_lkup_id ## 1 7dfa56dd7d5956b17587 ## 2 4f9e6fc637eaf7b736c2 ## 3 6c2545703bd527a7144d ## 4 527d6b1eaffc69ddd882 ## 5 8bd62c394a35213bdf52 ## 6 3b3a628f83135acd0676 ## event_name ## 1 Xfinity Center Mansfield Premier Parking: Florida Georgia Line ## 2 Gorge Camping - dave matthews band - sept 3-7 ## 3 Dodge Theatre Adams Street Parking - benise ## 4 Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow ## 5 Premier Parking - motley crue ## 6 Fast Lane Access: Journey ## primary_act_name secondary_act_name ## 1 XFINITY Center Mansfield Premier Parking NULL ## 2 Gorge Camping Dave Matthews Band ## 3 Parking Event NULL ## 4 Gexa Energy Pavilion VIP Parking NULL ## 5 White River Amphitheatre Premier Parking NULL ## 6 Fast Lane Access Journey ## major_cat_name minor_cat_name la_event_type_cat ## 1 MISC PARKING PARKING ## 2 MISC CAMPING INVALID ## 3 MISC PARKING PARKING ## 4 MISC PARKING PARKING ## 5 MISC PARKING PARKING ## 6 MISC SPECIAL ENTRY (UPSELL) UPSELL ## event_disp_name ## 1 Xfinity Center Mansfield Premier Parking: Florida Georgia Line ## 2 Gorge Camping - dave matthews band - sept 3-7 ## 3 Dodge Theatre Adams Street Parking - benise ## 4 Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow ## 5 Premier Parking - motley crue ## 6 Fast Lane Access: Journey ## ticket_text ## 1 THIS TICKET IS VALID FOR PARKING ONLY GOOD THIS DAY ONLY PREMIER PARKING PASS XFINITY CENTER,LOTS 4 PM SAT SEP 12 2015 7:30 PM ## 2 %OVERNIGHT C A M P I N G%* * * * * *%GORGE CAMPGROUND%* GOOD THIS DATE ONLY *%SEP 3 - 6, 2009 ## 3 ADAMS STREET GARAGE%PARKING FOR 4/21/06 ONLY%DODGE THEATRE PARKING PASS%ENTRANCE ON ADAMS STREET%BENISE%GARAGE OPENS AT 6:00PM ## 4 THIS TICKET IS VALID FOR PARKING ONLY GOOD FOR THIS DATE ONLY VIP PARKING PASS GEXA ENERGY PAVILION FRI SEP 02 2011 7:00 PM ## 5 THIS TICKET IS VALID%FOR PARKING ONLY%GOOD THIS DATE ONLY%PREMIER PARKING PASS%WHITE RIVER AMPHITHEATRE%SAT JUL 30, 2005 6:00PM ## 6 FAST LANE JOURNEY FAST LANE EVENT THIS IS NOT A TICKET SAN MANUEL AMPHITHEATER SAT JUL 21 2012 7:00 PM ## tickets_purchased_qty trans_face_val_amt delivery_type_cd ## 1 1 45 eTicket ## 2 1 75 TicketFast ## 3 1 5 TicketFast ## 4 1 20 Mail ## 5 1 20 Mail ## 6 2 10 TicketFast ## event_date_time event_dt presale_dt onsale_dt ## 1 2015-09-12 23:30:00 2015-09-12 NULL 2015-05-15 ## 2 2009-09-05 01:00:00 2009-09-04 NULL 2009-03-13 ## 3 2006-04-22 01:30:00 2006-04-21 NULL 2006-02-25 ## 4 2011-09-03 00:00:00 2011-09-02 NULL 2011-04-22 ## 5 2005-07-31 01:00:00 2005-07-30 2005-03-02 2005-03-04 ## 6 2012-07-22 02:00:00 2012-07-21 NULL 2012-04-11 ## sales_ord_create_dttm sales_ord_tran_dt print_dt timezn_nm ## 1 2015-09-11 18:17:45 2015-09-11 2015-09-12 EST ## 2 2009-07-06 00:00:00 2009-07-05 2009-09-01 PST ## 3 2006-04-05 00:00:00 2006-04-05 2006-04-05 MST ## 4 2011-07-01 17:38:50 2011-07-01 2011-07-06 CST ## 5 2005-06-18 00:00:00 2005-06-18 2005-06-28 PST ## 6 2012-07-21 17:20:18 2012-07-21 2012-07-21 PST ## venue_city venue_state venue_postal_cd_sgmt_1 ## 1 MANSFIELD MASSACHUSETTS 02048 ## 2 QUINCY WASHINGTON 98848 ## 3 PHOENIX ARIZONA 85003 ## 4 DALLAS TEXAS 75210 ## 5 AUBURN WASHINGTON 98092 ## 6 SAN BERNARDINO CALIFORNIA 92407 ## sales_platform_cd print_flg la_valid_tkt_event_flg fin_mkt_nm ## 1 www.concerts.livenation.com T N Boston ## 2 NULL T N Seattle ## 3 NULL T N Arizona ## 4 NULL T N Dallas ## 5 NULL T N Seattle ## 6 www.livenation.com T N Los Angeles ## web_session_cookie_val gndr_cd age_yr income_amt edu_val ## 1 7dfa56dd7d5956b17587 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 4f9e6fc637eaf7b736c2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 6c2545703bd527a7144d &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 527d6b1eaffc69ddd882 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 8bd62c394a35213bdf52 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 3b3a628f83135acd0676 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## edu_1st_indv_val edu_2nd_indv_val adults_in_hh_num married_ind ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## child_present_ind home_owner_ind occpn_val occpn_1st_val occpn_2nd_val ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## dist_to_ven ## 1 NA ## 2 59 ## 3 NA ## 4 NA ## 5 NA ## 6 NA # View column names of sales names(sales) ## [1] &quot;X&quot; &quot;event_id&quot; ## [3] &quot;primary_act_id&quot; &quot;secondary_act_id&quot; ## [5] &quot;purch_party_lkup_id&quot; &quot;event_name&quot; ## [7] &quot;primary_act_name&quot; &quot;secondary_act_name&quot; ## [9] &quot;major_cat_name&quot; &quot;minor_cat_name&quot; ## [11] &quot;la_event_type_cat&quot; &quot;event_disp_name&quot; ## [13] &quot;ticket_text&quot; &quot;tickets_purchased_qty&quot; ## [15] &quot;trans_face_val_amt&quot; &quot;delivery_type_cd&quot; ## [17] &quot;event_date_time&quot; &quot;event_dt&quot; ## [19] &quot;presale_dt&quot; &quot;onsale_dt&quot; ## [21] &quot;sales_ord_create_dttm&quot; &quot;sales_ord_tran_dt&quot; ## [23] &quot;print_dt&quot; &quot;timezn_nm&quot; ## [25] &quot;venue_city&quot; &quot;venue_state&quot; ## [27] &quot;venue_postal_cd_sgmt_1&quot; &quot;sales_platform_cd&quot; ## [29] &quot;print_flg&quot; &quot;la_valid_tkt_event_flg&quot; ## [31] &quot;fin_mkt_nm&quot; &quot;web_session_cookie_val&quot; ## [33] &quot;gndr_cd&quot; &quot;age_yr&quot; ## [35] &quot;income_amt&quot; &quot;edu_val&quot; ## [37] &quot;edu_1st_indv_val&quot; &quot;edu_2nd_indv_val&quot; ## [39] &quot;adults_in_hh_num&quot; &quot;married_ind&quot; ## [41] &quot;child_present_ind&quot; &quot;home_owner_ind&quot; ## [43] &quot;occpn_val&quot; &quot;occpn_1st_val&quot; ## [45] &quot;occpn_2nd_val&quot; &quot;dist_to_ven&quot; Luckily, the rows and columns appear to be arranged in a meaningful way: each row represents an observation and each column a variable, or piece of information about that observation. In R, there are a great many tools at your disposal to help get a feel for your data. Besides the three you used in the previous exercise, the functions str() and summary() can be very helpful. The dplyr package, introduced in Cleaning Data in R, offers the glimpse() function, which can also be used for this purpose. The package is already installed on DataCamp; you just need to load it. # Look at structure of sales str(sales) ## &#39;data.frame&#39;: 5000 obs. of 46 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ event_id : chr &quot;abcaf1adb99a935fc661&quot; &quot;6c56d7f08c95f2aa453c&quot; &quot;c7ab4524a121f9d687d2&quot; &quot;394cb493f893be9b9ed1&quot; ... ## $ primary_act_id : chr &quot;43f0436b905bfa7c2eec&quot; &quot;1a3e9aecd0617706a794&quot; &quot;4b677c3f5bec71eec8d1&quot; &quot;b1ccea01ad6ef8522796&quot; ... ## $ secondary_act_id : chr &quot;b85143bf51323b72e53c&quot; &quot;f53529c5679ea6ca5a48&quot; &quot;b85143bf51323b72e53c&quot; &quot;b85143bf51323b72e53c&quot; ... ## $ purch_party_lkup_id : chr &quot;7dfa56dd7d5956b17587&quot; &quot;4f9e6fc637eaf7b736c2&quot; &quot;6c2545703bd527a7144d&quot; &quot;527d6b1eaffc69ddd882&quot; ... ## $ event_name : chr &quot;Xfinity Center Mansfield Premier Parking: Florida Georgia Line&quot; &quot;Gorge Camping - dave matthews band - sept 3-7&quot; &quot;Dodge Theatre Adams Street Parking - benise&quot; &quot;Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow&quot; ... ## $ primary_act_name : chr &quot;XFINITY Center Mansfield Premier Parking&quot; &quot;Gorge Camping&quot; &quot;Parking Event&quot; &quot;Gexa Energy Pavilion VIP Parking&quot; ... ## $ secondary_act_name : chr &quot;NULL&quot; &quot;Dave Matthews Band&quot; &quot;NULL&quot; &quot;NULL&quot; ... ## $ major_cat_name : chr &quot;MISC&quot; &quot;MISC&quot; &quot;MISC&quot; &quot;MISC&quot; ... ## $ minor_cat_name : chr &quot;PARKING&quot; &quot;CAMPING&quot; &quot;PARKING&quot; &quot;PARKING&quot; ... ## $ la_event_type_cat : chr &quot;PARKING&quot; &quot;INVALID&quot; &quot;PARKING&quot; &quot;PARKING&quot; ... ## $ event_disp_name : chr &quot;Xfinity Center Mansfield Premier Parking: Florida Georgia Line&quot; &quot;Gorge Camping - dave matthews band - sept 3-7&quot; &quot;Dodge Theatre Adams Street Parking - benise&quot; &quot;Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow&quot; ... ## $ ticket_text : chr &quot; THIS TICKET IS VALID FOR PARKING ONLY GOOD THIS DAY ONLY PREMIER PARKING PASS XFINIT&quot;| __truncated__ &quot;%OVERNIGHT C A M P I N G%* * * * * *%GORGE CAMPGROUND%* GOOD THIS DATE ONLY *%SEP 3 - 6, 2009&quot; &quot;ADAMS STREET GARAGE%PARKING FOR 4/21/06 ONLY%DODGE THEATRE PARKING PASS%ENTRANCE ON ADAMS STREET%BENISE%GARAGE OPENS AT 6:00PM&quot; &quot; THIS TICKET IS VALID FOR PARKING ONLY GOOD FOR THIS DATE ONLY VIP PARKING PASS GEXA&quot;| __truncated__ ... ## $ tickets_purchased_qty : int 1 1 1 1 1 2 1 1 1 1 ... ## $ trans_face_val_amt : num 45 75 5 20 20 10 30 28 20 25 ... ## $ delivery_type_cd : chr &quot;eTicket&quot; &quot;TicketFast&quot; &quot;TicketFast&quot; &quot;Mail&quot; ... ## $ event_date_time : chr &quot;2015-09-12 23:30:00&quot; &quot;2009-09-05 01:00:00&quot; &quot;2006-04-22 01:30:00&quot; &quot;2011-09-03 00:00:00&quot; ... ## $ event_dt : chr &quot;2015-09-12&quot; &quot;2009-09-04&quot; &quot;2006-04-21&quot; &quot;2011-09-02&quot; ... ## $ presale_dt : chr &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; ... ## $ onsale_dt : chr &quot;2015-05-15&quot; &quot;2009-03-13&quot; &quot;2006-02-25&quot; &quot;2011-04-22&quot; ... ## $ sales_ord_create_dttm : chr &quot;2015-09-11 18:17:45&quot; &quot;2009-07-06 00:00:00&quot; &quot;2006-04-05 00:00:00&quot; &quot;2011-07-01 17:38:50&quot; ... ## $ sales_ord_tran_dt : chr &quot;2015-09-11&quot; &quot;2009-07-05&quot; &quot;2006-04-05&quot; &quot;2011-07-01&quot; ... ## $ print_dt : chr &quot;2015-09-12&quot; &quot;2009-09-01&quot; &quot;2006-04-05&quot; &quot;2011-07-06&quot; ... ## $ timezn_nm : chr &quot;EST&quot; &quot;PST&quot; &quot;MST&quot; &quot;CST&quot; ... ## $ venue_city : chr &quot;MANSFIELD&quot; &quot;QUINCY&quot; &quot;PHOENIX&quot; &quot;DALLAS&quot; ... ## $ venue_state : chr &quot;MASSACHUSETTS&quot; &quot;WASHINGTON&quot; &quot;ARIZONA&quot; &quot;TEXAS&quot; ... ## $ venue_postal_cd_sgmt_1: chr &quot;02048&quot; &quot;98848&quot; &quot;85003&quot; &quot;75210&quot; ... ## $ sales_platform_cd : chr &quot;www.concerts.livenation.com&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; ... ## $ print_flg : chr &quot;T &quot; &quot;T &quot; &quot;T &quot; &quot;T &quot; ... ## $ la_valid_tkt_event_flg: chr &quot;N &quot; &quot;N &quot; &quot;N &quot; &quot;N &quot; ... ## $ fin_mkt_nm : chr &quot;Boston&quot; &quot;Seattle&quot; &quot;Arizona&quot; &quot;Dallas&quot; ... ## $ web_session_cookie_val: chr &quot;7dfa56dd7d5956b17587&quot; &quot;4f9e6fc637eaf7b736c2&quot; &quot;6c2545703bd527a7144d&quot; &quot;527d6b1eaffc69ddd882&quot; ... ## $ gndr_cd : chr NA NA NA NA ... ## $ age_yr : chr NA NA NA NA ... ## $ income_amt : chr NA NA NA NA ... ## $ edu_val : chr NA NA NA NA ... ## $ edu_1st_indv_val : chr NA NA NA NA ... ## $ edu_2nd_indv_val : chr NA NA NA NA ... ## $ adults_in_hh_num : chr NA NA NA NA ... ## $ married_ind : chr NA NA NA NA ... ## $ child_present_ind : chr NA NA NA NA ... ## $ home_owner_ind : chr NA NA NA NA ... ## $ occpn_val : chr NA NA NA NA ... ## $ occpn_1st_val : chr NA NA NA NA ... ## $ occpn_2nd_val : chr NA NA NA NA ... ## $ dist_to_ven : int NA 59 NA NA NA NA NA NA NA NA ... # View a summary of sales summary(sales) ## X event_id primary_act_id secondary_act_id ## Min. : 1 Length:5000 Length:5000 Length:5000 ## 1st Qu.:1251 Class :character Class :character Class :character ## Median :2500 Mode :character Mode :character Mode :character ## Mean :2500 ## 3rd Qu.:3750 ## Max. :5000 ## ## purch_party_lkup_id event_name primary_act_name ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## secondary_act_name major_cat_name minor_cat_name ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## la_event_type_cat event_disp_name ticket_text ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## tickets_purchased_qty trans_face_val_amt delivery_type_cd ## Min. :1.000 Min. : 1.00 Length:5000 ## 1st Qu.:1.000 1st Qu.: 20.00 Class :character ## Median :1.000 Median : 30.00 Mode :character ## Mean :1.639 Mean : 77.08 ## 3rd Qu.:2.000 3rd Qu.: 85.00 ## Max. :8.000 Max. :1520.88 ## ## event_date_time event_dt presale_dt ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## onsale_dt sales_ord_create_dttm sales_ord_tran_dt ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## print_dt timezn_nm venue_city ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## venue_state venue_postal_cd_sgmt_1 sales_platform_cd ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## print_flg la_valid_tkt_event_flg fin_mkt_nm ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## web_session_cookie_val gndr_cd age_yr ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## income_amt edu_val edu_1st_indv_val ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## edu_2nd_indv_val adults_in_hh_num married_ind ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## child_present_ind home_owner_ind occpn_val ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## occpn_1st_val occpn_2nd_val dist_to_ven ## Length:5000 Length:5000 Min. : 0.0 ## Class :character Class :character 1st Qu.: 12.0 ## Mode :character Mode :character Median : 26.0 ## Mean : 158.2 ## 3rd Qu.: 77.5 ## Max. :2548.0 ## NA&#39;s :4677 # Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Get a glimpse of sales glimpse(sales) ## Observations: 5,000 ## Variables: 46 ## $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ... ## $ event_id &lt;chr&gt; &quot;abcaf1adb99a935fc661&quot;, &quot;6c56d7f08c95f2... ## $ primary_act_id &lt;chr&gt; &quot;43f0436b905bfa7c2eec&quot;, &quot;1a3e9aecd06177... ## $ secondary_act_id &lt;chr&gt; &quot;b85143bf51323b72e53c&quot;, &quot;f53529c5679ea6... ## $ purch_party_lkup_id &lt;chr&gt; &quot;7dfa56dd7d5956b17587&quot;, &quot;4f9e6fc637eaf7... ## $ event_name &lt;chr&gt; &quot;Xfinity Center Mansfield Premier Parki... ## $ primary_act_name &lt;chr&gt; &quot;XFINITY Center Mansfield Premier Parki... ## $ secondary_act_name &lt;chr&gt; &quot;NULL&quot;, &quot;Dave Matthews Band&quot;, &quot;NULL&quot;, &quot;... ## $ major_cat_name &lt;chr&gt; &quot;MISC&quot;, &quot;MISC&quot;, &quot;MISC&quot;, &quot;MISC&quot;, &quot;MISC&quot;,... ## $ minor_cat_name &lt;chr&gt; &quot;PARKING&quot;, &quot;CAMPING&quot;, &quot;PARKING&quot;, &quot;PARKI... ## $ la_event_type_cat &lt;chr&gt; &quot;PARKING&quot;, &quot;INVALID&quot;, &quot;PARKING&quot;, &quot;PARKI... ## $ event_disp_name &lt;chr&gt; &quot;Xfinity Center Mansfield Premier Parki... ## $ ticket_text &lt;chr&gt; &quot; THIS TICKET IS VALID FOR PAR... ## $ tickets_purchased_qty &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 4, ... ## $ trans_face_val_amt &lt;dbl&gt; 45, 75, 5, 20, 20, 10, 30, 28, 20, 25, ... ## $ delivery_type_cd &lt;chr&gt; &quot;eTicket&quot;, &quot;TicketFast&quot;, &quot;TicketFast&quot;, ... ## $ event_date_time &lt;chr&gt; &quot;2015-09-12 23:30:00&quot;, &quot;2009-09-05 01:0... ## $ event_dt &lt;chr&gt; &quot;2015-09-12&quot;, &quot;2009-09-04&quot;, &quot;2006-04-21... ## $ presale_dt &lt;chr&gt; &quot;NULL&quot;, &quot;NULL&quot;, &quot;NULL&quot;, &quot;NULL&quot;, &quot;2005-0... ## $ onsale_dt &lt;chr&gt; &quot;2015-05-15&quot;, &quot;2009-03-13&quot;, &quot;2006-02-25... ## $ sales_ord_create_dttm &lt;chr&gt; &quot;2015-09-11 18:17:45&quot;, &quot;2009-07-06 00:0... ## $ sales_ord_tran_dt &lt;chr&gt; &quot;2015-09-11&quot;, &quot;2009-07-05&quot;, &quot;2006-04-05... ## $ print_dt &lt;chr&gt; &quot;2015-09-12&quot;, &quot;2009-09-01&quot;, &quot;2006-04-05... ## $ timezn_nm &lt;chr&gt; &quot;EST&quot;, &quot;PST&quot;, &quot;MST&quot;, &quot;CST&quot;, &quot;PST&quot;, &quot;PST... ## $ venue_city &lt;chr&gt; &quot;MANSFIELD&quot;, &quot;QUINCY&quot;, &quot;PHOENIX&quot;, &quot;DALL... ## $ venue_state &lt;chr&gt; &quot;MASSACHUSETTS&quot;, &quot;WASHINGTON&quot;, &quot;ARIZONA... ## $ venue_postal_cd_sgmt_1 &lt;chr&gt; &quot;02048&quot;, &quot;98848&quot;, &quot;85003&quot;, &quot;75210&quot;, &quot;98... ## $ sales_platform_cd &lt;chr&gt; &quot;www.concerts.livenation.com&quot;, &quot;NULL&quot;, ... ## $ print_flg &lt;chr&gt; &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T ... ## $ la_valid_tkt_event_flg &lt;chr&gt; &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N ... ## $ fin_mkt_nm &lt;chr&gt; &quot;Boston&quot;, &quot;Seattle&quot;, &quot;Arizona&quot;, &quot;Dallas... ## $ web_session_cookie_val &lt;chr&gt; &quot;7dfa56dd7d5956b17587&quot;, &quot;4f9e6fc637eaf7... ## $ gndr_cd &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;M&quot;, NA, NA, NA... ## $ age_yr &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;28&quot;, NA, NA, N... ## $ income_amt &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;112500&quot;, NA, N... ## $ edu_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;High School&quot;, ... ## $ edu_1st_indv_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;High School&quot;, ... ## $ edu_2nd_indv_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;NULL&quot;, NA, NA,... ## $ adults_in_hh_num &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;4&quot;, NA, NA, NA... ## $ married_ind &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;0&quot;, NA, NA, NA... ## $ child_present_ind &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;1&quot;, NA, NA, NA... ## $ home_owner_ind &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;0&quot;, NA, NA, NA... ## $ occpn_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;NULL&quot;, NA, NA,... ## $ occpn_1st_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;Craftsman Blue... ## $ occpn_2nd_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;NULL&quot;, NA, NA,... ## $ dist_to_ven &lt;int&gt; NA, 59, NA, NA, NA, NA, NA, NA, NA, NA,... 5.1.1 Removing redundant info The first column of data is just a duplication of the row numbers. Not very useful. Go ahead and delete that column. Remember that nrow() and ncol() return the number of rows and columns in a data frame, respectively. Also, recall that you can use square brackets to subset a data frame as follows: my_df[1:5, ] # First 5 rows of my_df my_df[, 4] # Fourth column of my_df Alternatively, you can remove rows and columns using negative indices. For example: my_df[-(1:5), ] # Omit first 5 rows of my_df my_df[, -4] # Omit fourth column of my_df # Remove the first column of sales: sales2 sales2 &lt;- sales[, -1] Many of the columns have information that’s of no use to us. For example, the first four columns contain internal codes representing particular events. The last fifteen columns also aren’t worth keeping; there are too many missing values to make them worthwhile. An easy way to get rid of unnecessary columns is to create a vector containing the column indices you want to keep, then subset the data based on that vector using single bracket subsetting. # Define a vector of column indices: keep keep &lt;- c(5:30) # Subset sales2 using keep: sales3 sales3 &lt;- sales2[keep] Some of the columns in your data frame include multiple pieces of information that should be in separate columns. In this exercise, you will separate such a column into two: one for date and one for time. You will use the separate() function from the tidyr package (already installed for you). For isntance the event_date_time column has a date and time separated by a space. Therefore, you’ll use sep = &quot; &quot; as an argument to separate(). head(sales3$event_date_time) ## [1] &quot;2015-09-12 23:30:00&quot; &quot;2009-09-05 01:00:00&quot; &quot;2006-04-22 01:30:00&quot; ## [4] &quot;2011-09-03 00:00:00&quot; &quot;2005-07-31 01:00:00&quot; &quot;2012-07-22 02:00:00&quot; head(sales3$sales_ord_create_dttm) ## [1] &quot;2015-09-11 18:17:45&quot; &quot;2009-07-06 00:00:00&quot; &quot;2006-04-05 00:00:00&quot; ## [4] &quot;2011-07-01 17:38:50&quot; &quot;2005-06-18 00:00:00&quot; &quot;2012-07-21 17:20:18&quot; # Load tidyr library(tidyr) # Split event_date_time: sales4 sales4 &lt;- separate(sales3, event_date_time, into = c(&quot;event_dt&quot;, &quot;event_time&quot;), sep = &quot; &quot;) Looks like that second call to separate() threw a warning. Not to worry; warnings aren’t as bad as error messages. It’s not saying that the command didn’t execute; it’s just a heads-up that something unusual happened. The warning says Too few values at 4 locations. You may be able to guess already what the issue is, but it’s still good to take a look. sales3$sales_ord_create_dttm[c(2516, 3863, 4082, 4183)] ## [1] &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; # Define an issues vector issues &lt;- c(2516, 3863, 4082, 4183) # Print values of sales_ord_create_dttm at these indices sales3$sales_ord_create_dttm[issues] ## [1] &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; # Print a well-behaved value of sales_ord_create_dttm sales3$sales_ord_create_dttm[2517] ## [1] &quot;2013-08-04 23:07:19&quot; 5.2 Working with dates Some of the columns in your dataset contain dates of different events. Right now, they are stored as character strings. That’s fine if all you want to do is look up the date associated with an event, but if you want to do any comparisons or math with the dates, it’s MUCH easier to store them as Date objects. Luckily, all of the date columns in this dataset have the substring “dt” in their name, so you can use the str_detect() function of the stringr package to find the date columns. Then you can coerce them to Date objects using a function from the lubridate package. You’ll use lapply() to apply the appropriate lubridate function to all of the columns that contain dates. Recall the following syntax for lapply() applied to some data frame columns of interest: lapply(my_data_frame[, cols], function_name) Also recall that function names in lubridate combine the letters y, m, d, h, m, and s depending on the format of the date/time string being read in. # Load stringr library(stringr) # Find columns of sales5 containing &quot;dt&quot;: date_cols date_cols &lt;- str_detect(names(sales4),&quot;dt&quot;) # Load lubridate library(lubridate) ## Loading required package: methods ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date # Coerce date columns into Date objects sales4[, date_cols] &lt;- lapply(sales4[, date_cols], ymd) ## Warning: 2892 failed to parse. ## Warning: 101 failed to parse. ## Warning: All formats failed to parse. No formats found. ## Warning: 424 failed to parse. Some of the calls to ymd() caused a failure to parse warning. That’s probably because of more missing data, but again, it’s good to check to be sure. ## stringr is loaded # Find date columns (don&#39;t change) date_cols &lt;- str_detect(names(sales4), &quot;dt&quot;) # Create logical vectors indicating missing values (don&#39;t change) missing &lt;- lapply(sales4[, date_cols], is.na) # Create a numerical vector that counts missing values: num_missing num_missing &lt;- sapply(missing, sum) # Print num_missing num_missing ## event_dt event_dt.1 presale_dt ## 0 0 2892 ## onsale_dt sales_ord_create_dttm sales_ord_tran_dt ## 101 5000 0 ## print_dt ## 424 The number of NAs in each column match the numbers from the warning messages, so missing data is the culprit. How to proceed depends on your desired analysis. If you really need complete sets of date/time information, you might delete the rows or columns containing NAs. 5.3 MBTA Ridership Data The Massachusetts Bay Transportation Authority (“MBTA” or just “the T” for short) manages America’s oldest subway, as well as Greater Boston’s commuter rail, ferry, and bus systems. It’s your first day on the job as the T’s data analyst and you’ve been tasked with analyzing average ridership through time. You’re in luck, because this chapter of the course will guide you through cleaning a set of MBTA ridership data! The dataset is stored as an Excel spreadsheet called mbta.xlsx. You’ll use the read_excel() function from Hadley Wickham’s readxl package to import it. The first time you import a dataset, you might not know how many rows need to be skipped. In this case, the first row is a title (see this Excel screenshot), so you’ll need to skip the first row. # Load readxl library(readxl) # Import mbta.xlsx and skip first row: mbta mbta &lt;- read_excel(&quot;C:/Users/DEsktop/Nextcloud/Documents/2017/RData/mbta.xlsx&quot;, skip = 1) # View the structure of mbta str(mbta) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 11 obs. of 60 variables: ## $ X__1 : num 1 2 3 4 5 6 7 8 9 10 ... ## $ mode : chr &quot;All Modes by Qtr&quot; &quot;Boat&quot; &quot;Bus&quot; &quot;Commuter Rail&quot; ... ## $ 2007-01: chr &quot;NA&quot; &quot;4&quot; &quot;335.819&quot; &quot;142.2&quot; ... ## $ 2007-02: chr &quot;NA&quot; &quot;3.6&quot; &quot;338.675&quot; &quot;138.5&quot; ... ## $ 2007-03: num 1188 40 340 138 459 ... ## $ 2007-04: chr &quot;NA&quot; &quot;4.3&quot; &quot;352.162&quot; &quot;139.5&quot; ... ## $ 2007-05: chr &quot;NA&quot; &quot;4.9&quot; &quot;354.367&quot; &quot;139&quot; ... ## $ 2007-06: num 1246 5.8 350.5 143 477 ... ## $ 2007-07: chr &quot;NA&quot; &quot;6.521&quot; &quot;357.519&quot; &quot;142.391&quot; ... ## $ 2007-08: chr &quot;NA&quot; &quot;6.572&quot; &quot;355.479&quot; &quot;142.364&quot; ... ## $ 2007-09: num 1256.57 5.47 372.6 143.05 499.57 ... ## $ 2007-10: chr &quot;NA&quot; &quot;5.145&quot; &quot;368.847&quot; &quot;146.542&quot; ... ## $ 2007-11: chr &quot;NA&quot; &quot;3.763&quot; &quot;330.826&quot; &quot;145.089&quot; ... ## $ 2007-12: num 1216.89 2.98 312.92 141.59 448.27 ... ## $ 2008-01: chr &quot;NA&quot; &quot;3.175&quot; &quot;340.324&quot; &quot;142.145&quot; ... ## $ 2008-02: chr &quot;NA&quot; &quot;3.111&quot; &quot;352.905&quot; &quot;142.607&quot; ... ## $ 2008-03: num 1253.52 3.51 361.15 137.45 494.05 ... ## $ 2008-04: chr &quot;NA&quot; &quot;4.164&quot; &quot;368.189&quot; &quot;140.389&quot; ... ## $ 2008-05: chr &quot;NA&quot; &quot;4.015&quot; &quot;363.903&quot; &quot;142.585&quot; ... ## $ 2008-06: num 1314.82 5.19 362.96 142.06 518.35 ... ## $ 2008-07: chr &quot;NA&quot; &quot;6.016&quot; &quot;370.921&quot; &quot;145.731&quot; ... ## $ 2008-08: chr &quot;NA&quot; &quot;5.8&quot; &quot;361.057&quot; &quot;144.565&quot; ... ## $ 2008-09: num 1307.04 4.59 389.54 141.91 517.32 ... ## $ 2008-10: chr &quot;NA&quot; &quot;4.285&quot; &quot;357.974&quot; &quot;151.957&quot; ... ## $ 2008-11: chr &quot;NA&quot; &quot;3.488&quot; &quot;345.423&quot; &quot;152.952&quot; ... ## $ 2008-12: num 1232.65 3.01 325.77 140.81 446.74 ... ## $ 2009-01: chr &quot;NA&quot; &quot;3.014&quot; &quot;338.532&quot; &quot;141.448&quot; ... ## $ 2009-02: chr &quot;NA&quot; &quot;3.196&quot; &quot;360.412&quot; &quot;143.529&quot; ... ## $ 2009-03: num 1209.79 3.33 353.69 142.89 467.22 ... ## $ 2009-04: chr &quot;NA&quot; &quot;4.049&quot; &quot;359.38&quot; &quot;142.34&quot; ... ## $ 2009-05: chr &quot;NA&quot; &quot;4.119&quot; &quot;354.75&quot; &quot;144.225&quot; ... ## $ 2009-06: num 1233.1 4.9 347.9 142 473.1 ... ## $ 2009-07: chr &quot;NA&quot; &quot;6.444&quot; &quot;339.477&quot; &quot;137.691&quot; ... ## $ 2009-08: chr &quot;NA&quot; &quot;5.903&quot; &quot;332.661&quot; &quot;139.158&quot; ... ## $ 2009-09: num 1230.5 4.7 374.3 139.1 500.4 ... ## $ 2009-10: chr &quot;NA&quot; &quot;4.212&quot; &quot;385.868&quot; &quot;137.104&quot; ... ## $ 2009-11: chr &quot;NA&quot; &quot;3.576&quot; &quot;366.98&quot; &quot;129.343&quot; ... ## $ 2009-12: num 1207.85 3.11 332.39 126.07 440.93 ... ## $ 2010-01: chr &quot;NA&quot; &quot;3.207&quot; &quot;362.226&quot; &quot;130.91&quot; ... ## $ 2010-02: chr &quot;NA&quot; &quot;3.195&quot; &quot;361.138&quot; &quot;131.918&quot; ... ## $ 2010-03: num 1208.86 3.48 373.44 131.25 483.4 ... ## $ 2010-04: chr &quot;NA&quot; &quot;4.452&quot; &quot;378.611&quot; &quot;131.722&quot; ... ## $ 2010-05: chr &quot;NA&quot; &quot;4.415&quot; &quot;380.171&quot; &quot;128.8&quot; ... ## $ 2010-06: num 1244.41 5.41 363.27 129.14 490.26 ... ## $ 2010-07: chr &quot;NA&quot; &quot;6.513&quot; &quot;353.04&quot; &quot;122.935&quot; ... ## $ 2010-08: chr &quot;NA&quot; &quot;6.269&quot; &quot;343.688&quot; &quot;129.732&quot; ... ## $ 2010-09: num 1225.5 4.7 381.6 132.9 521.1 ... ## $ 2010-10: chr &quot;NA&quot; &quot;4.402&quot; &quot;384.987&quot; &quot;131.033&quot; ... ## $ 2010-11: chr &quot;NA&quot; &quot;3.731&quot; &quot;367.955&quot; &quot;130.889&quot; ... ## $ 2010-12: num 1216.26 3.16 326.34 121.42 450.43 ... ## $ 2011-01: chr &quot;NA&quot; &quot;3.14&quot; &quot;334.958&quot; &quot;128.396&quot; ... ## $ 2011-02: chr &quot;NA&quot; &quot;3.284&quot; &quot;346.234&quot; &quot;125.463&quot; ... ## $ 2011-03: num 1223.45 3.67 380.4 134.37 516.73 ... ## $ 2011-04: chr &quot;NA&quot; &quot;4.251&quot; &quot;380.446&quot; &quot;134.169&quot; ... ## $ 2011-05: chr &quot;NA&quot; &quot;4.431&quot; &quot;385.289&quot; &quot;136.14&quot; ... ## $ 2011-06: num 1302.41 5.47 376.32 135.58 529.53 ... ## $ 2011-07: chr &quot;NA&quot; &quot;6.581&quot; &quot;361.585&quot; &quot;132.41&quot; ... ## $ 2011-08: chr &quot;NA&quot; &quot;6.733&quot; &quot;353.793&quot; &quot;130.616&quot; ... ## $ 2011-09: num 1291 5 388 137 550 ... ## $ 2011-10: chr &quot;NA&quot; &quot;4.484&quot; &quot;398.456&quot; &quot;128.72&quot; ... # View the first 6 rows of mbta head(mbta, n = 6) ## # A tibble: 6 x 60 ## X__1 mode `2007-01` `2007-02` `2007-03` `2007-04` `2007-05` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 All Modes by Qtr NA NA 1187.653 NA NA ## 2 2 Boat 4 3.6 40.000 4.3 4.9 ## 3 3 Bus 335.819 338.675 339.867 352.162 354.367 ## 4 4 Commuter Rail 142.2 138.5 137.700 139.5 139 ## 5 5 Heavy Rail 435.294 448.271 458.583 472.201 474.579 ## 6 6 Light Rail 227.231 240.262 241.444 255.557 248.262 ## # ... with 53 more variables: `2007-06` &lt;dbl&gt;, `2007-07` &lt;chr&gt;, ## # `2007-08` &lt;chr&gt;, `2007-09` &lt;dbl&gt;, `2007-10` &lt;chr&gt;, `2007-11` &lt;chr&gt;, ## # `2007-12` &lt;dbl&gt;, `2008-01` &lt;chr&gt;, `2008-02` &lt;chr&gt;, `2008-03` &lt;dbl&gt;, ## # `2008-04` &lt;chr&gt;, `2008-05` &lt;chr&gt;, `2008-06` &lt;dbl&gt;, `2008-07` &lt;chr&gt;, ## # `2008-08` &lt;chr&gt;, `2008-09` &lt;dbl&gt;, `2008-10` &lt;chr&gt;, `2008-11` &lt;chr&gt;, ## # `2008-12` &lt;dbl&gt;, `2009-01` &lt;chr&gt;, `2009-02` &lt;chr&gt;, `2009-03` &lt;dbl&gt;, ## # `2009-04` &lt;chr&gt;, `2009-05` &lt;chr&gt;, `2009-06` &lt;dbl&gt;, `2009-07` &lt;chr&gt;, ## # `2009-08` &lt;chr&gt;, `2009-09` &lt;dbl&gt;, `2009-10` &lt;chr&gt;, `2009-11` &lt;chr&gt;, ## # `2009-12` &lt;dbl&gt;, `2010-01` &lt;chr&gt;, `2010-02` &lt;chr&gt;, `2010-03` &lt;dbl&gt;, ## # `2010-04` &lt;chr&gt;, `2010-05` &lt;chr&gt;, `2010-06` &lt;dbl&gt;, `2010-07` &lt;chr&gt;, ## # `2010-08` &lt;chr&gt;, `2010-09` &lt;dbl&gt;, `2010-10` &lt;chr&gt;, `2010-11` &lt;chr&gt;, ## # `2010-12` &lt;dbl&gt;, `2011-01` &lt;chr&gt;, `2011-02` &lt;chr&gt;, `2011-03` &lt;dbl&gt;, ## # `2011-04` &lt;chr&gt;, `2011-05` &lt;chr&gt;, `2011-06` &lt;dbl&gt;, `2011-07` &lt;chr&gt;, ## # `2011-08` &lt;chr&gt;, `2011-09` &lt;dbl&gt;, `2011-10` &lt;chr&gt; # View a summary of mbta summary(mbta) ## X__1 mode 2007-01 2007-02 ## Min. : 1.0 Length:11 Length:11 Length:11 ## 1st Qu.: 3.5 Class :character Class :character Class :character ## Median : 6.0 Mode :character Mode :character Mode :character ## Mean : 6.0 ## 3rd Qu.: 8.5 ## Max. :11.0 ## 2007-03 2007-04 2007-05 ## Min. : 0.114 Length:11 Length:11 ## 1st Qu.: 9.278 Class :character Class :character ## Median : 137.700 Mode :character Mode :character ## Mean : 330.293 ## 3rd Qu.: 399.225 ## Max. :1204.725 ## 2007-06 2007-07 2007-08 ## Min. : 0.096 Length:11 Length:11 ## 1st Qu.: 5.700 Class :character Class :character ## Median : 143.000 Mode :character Mode :character ## Mean : 339.846 ## 3rd Qu.: 413.788 ## Max. :1246.129 ## 2007-09 2007-10 2007-11 ## Min. : -0.007 Length:11 Length:11 ## 1st Qu.: 5.539 Class :character Class :character ## Median : 143.051 Mode :character Mode :character ## Mean : 352.554 ## 3rd Qu.: 436.082 ## Max. :1310.764 ## 2007-12 2008-01 2008-02 ## Min. : -0.060 Length:11 Length:11 ## 1st Qu.: 4.385 Class :character Class :character ## Median : 141.585 Mode :character Mode :character ## Mean : 321.588 ## 3rd Qu.: 380.594 ## Max. :1216.890 ## 2008-03 2008-04 2008-05 ## Min. : 0.058 Length:11 Length:11 ## 1st Qu.: 5.170 Class :character Class :character ## Median : 137.453 Mode :character Mode :character ## Mean : 345.604 ## 3rd Qu.: 427.601 ## Max. :1274.031 ## 2008-06 2008-07 2008-08 ## Min. : 0.060 Length:11 Length:11 ## 1st Qu.: 5.742 Class :character Class :character ## Median : 142.057 Mode :character Mode :character ## Mean : 359.667 ## 3rd Qu.: 440.656 ## Max. :1320.728 ## 2008-09 2008-10 2008-11 ## Min. : 0.021 Length:11 Length:11 ## 1st Qu.: 5.691 Class :character Class :character ## Median : 141.907 Mode :character Mode :character ## Mean : 362.099 ## 3rd Qu.: 453.430 ## Max. :1338.015 ## 2008-12 2009-01 2009-02 ## Min. : -0.015 Length:11 Length:11 ## 1st Qu.: 4.689 Class :character Class :character ## Median : 140.810 Mode :character Mode :character ## Mean : 319.882 ## 3rd Qu.: 386.255 ## Max. :1232.655 ## 2009-03 2009-04 2009-05 ## Min. : -0.050 Length:11 Length:11 ## 1st Qu.: 5.003 Class :character Class :character ## Median : 142.893 Mode :character Mode :character ## Mean : 330.142 ## 3rd Qu.: 410.455 ## Max. :1210.912 ## 2009-06 2009-07 2009-08 ## Min. : -0.079 Length:11 Length:11 ## 1st Qu.: 5.845 Class :character Class :character ## Median : 142.006 Mode :character Mode :character ## Mean : 333.194 ## 3rd Qu.: 410.482 ## Max. :1233.085 ## 2009-09 2009-10 2009-11 ## Min. : -0.035 Length:11 Length:11 ## 1st Qu.: 5.693 Class :character Class :character ## Median : 139.087 Mode :character Mode :character ## Mean : 346.687 ## 3rd Qu.: 437.332 ## Max. :1291.564 ## 2009-12 2010-01 2010-02 ## Min. : -0.022 Length:11 Length:11 ## 1st Qu.: 4.784 Class :character Class :character ## Median : 126.066 Mode :character Mode :character ## Mean : 312.962 ## 3rd Qu.: 386.659 ## Max. :1207.845 ## 2010-03 2010-04 2010-05 ## Min. : 0.012 Length:11 Length:11 ## 1st Qu.: 5.274 Class :character Class :character ## Median : 131.252 Mode :character Mode :character ## Mean : 332.726 ## 3rd Qu.: 428.420 ## Max. :1225.556 ## 2010-06 2010-07 2010-08 ## Min. : 0.008 Length:11 Length:11 ## 1st Qu.: 6.436 Class :character Class :character ## Median : 129.144 Mode :character Mode :character ## Mean : 335.964 ## 3rd Qu.: 426.769 ## Max. :1244.409 ## 2010-09 2010-10 2010-11 ## Min. : 0.001 Length:11 Length:11 ## 1st Qu.: 5.567 Class :character Class :character ## Median : 132.892 Mode :character Mode :character ## Mean : 346.524 ## 3rd Qu.: 451.361 ## Max. :1293.117 ## 2010-12 2011-01 2011-02 ## Min. : -0.004 Length:11 Length:11 ## 1st Qu.: 4.466 Class :character Class :character ## Median : 121.422 Mode :character Mode :character ## Mean : 312.917 ## 3rd Qu.: 388.385 ## Max. :1216.262 ## 2011-03 2011-04 2011-05 ## Min. : 0.05 Length:11 Length:11 ## 1st Qu.: 6.03 Class :character Class :character ## Median : 134.37 Mode :character Mode :character ## Mean : 345.17 ## 3rd Qu.: 448.56 ## Max. :1286.66 ## 2011-06 2011-07 2011-08 ## Min. : 0.054 Length:11 Length:11 ## 1st Qu.: 6.926 Class :character Class :character ## Median : 135.581 Mode :character Mode :character ## Mean : 353.331 ## 3rd Qu.: 452.923 ## Max. :1302.414 ## 2011-09 2011-10 ## Min. : 0.043 Length:11 ## 1st Qu.: 6.660 Class :character ## Median : 136.901 Mode :character ## Mean : 362.555 ## 3rd Qu.: 469.204 ## Max. :1348.754 The data are organized with observations stored as columns rather than as rows. First, though, you can address the missing data. All of the NA values are stored in the All Modes by Qtr row. This row really belongs in a different data frame; it is a quarterly average of weekday MBTA ridership. Since this dataset tracks monthly average ridership, you’ll remove that row. Similarly, the 7th row (Pct Chg / Yr) and the 11th row (TOTAL) are not really observations as much as they are analysis. Go ahead and remove the 7th and 11th rows as well. The first column also needs to be removed because it’s just listing the row numbers. # Remove rows 1, 7, and 11 of mbta: mbta2 keep &lt;- !(mbta$mode %in% c(&#39;All Modes by Qtr&#39;, &#39;Pct Chg / Yr&#39;, &#39;TOTAL&#39;)) mbta2 &lt;- mbta[keep,] # Remove the first column of mbta2: mbta3 mbta3 &lt;- mbta2[,-1] Our next problem is variables are stored in rows instead of columns. The different modes of transportation (commuter rail, bus, subway, ferry, …) are variables, providing information about each month’s average ridership. The months themselves are observations. You can tell which is which because as you go through time, the month changes, but the modes of transport offered by the T do not. As is customary, you want to represent variables in columns rather than rows. The first step is to use the gather() function from the tidyr package, which will gather columns into key-value pairs. # Load tidyr library(tidyr) # Gather columns of mbta3: mbta4 mbta4 &lt;- gather(mbta3, month, thou_riders, -mode) # View the head of mbta4 head(mbta4) ## # A tibble: 6 x 3 ## mode month thou_riders ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Boat 2007-01 4 ## 2 Bus 2007-01 335.819 ## 3 Commuter Rail 2007-01 142.2 ## 4 Heavy Rail 2007-01 435.294 ## 5 Light Rail 2007-01 227.231 ## 6 Private Bus 2007-01 4.772 The thousand riders coloumn is still charecter data, so lets change that. # Coerce thou_riders to numeric mbta4$thou_riders &lt;- as.numeric(mbta4$thou_riders) Now, you can finish the job you started earlier: getting variables into columns. Right now, variables are stored as “keys” in the mode column. You’ll use the tidyr function spread() to make them into columns containing average weekday ridership for the given month and mode of transport. # Spread the contents of mbta4: mbta5 mbta5 &lt;- spread(mbta4, mode, thou_riders) # View the head of mbta5 head(mbta5) ## # A tibble: 6 x 9 ## month Boat Bus `Commuter Rail` `Heavy Rail` `Light Rail` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007-01 4.0 335.819 142.2 435.294 227.231 ## 2 2007-02 3.6 338.675 138.5 448.271 240.262 ## 3 2007-03 40.0 339.867 137.7 458.583 241.444 ## 4 2007-04 4.3 352.162 139.5 472.201 255.557 ## 5 2007-05 4.9 354.367 139.0 474.579 248.262 ## 6 2007-06 5.8 350.543 143.0 477.032 246.108 ## # ... with 3 more variables: `Private Bus` &lt;dbl&gt;, RIDE &lt;dbl&gt;, `Trackless ## # Trolley` &lt;dbl&gt; If we want to look at the data by year, we can seperate the month field out in to month and year. # View the head of mbta5 head(mbta5) ## # A tibble: 6 x 9 ## month Boat Bus `Commuter Rail` `Heavy Rail` `Light Rail` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007-01 4.0 335.819 142.2 435.294 227.231 ## 2 2007-02 3.6 338.675 138.5 448.271 240.262 ## 3 2007-03 40.0 339.867 137.7 458.583 241.444 ## 4 2007-04 4.3 352.162 139.5 472.201 255.557 ## 5 2007-05 4.9 354.367 139.0 474.579 248.262 ## 6 2007-06 5.8 350.543 143.0 477.032 246.108 ## # ... with 3 more variables: `Private Bus` &lt;dbl&gt;, RIDE &lt;dbl&gt;, `Trackless ## # Trolley` &lt;dbl&gt; # Split month column into month and year: mbta6 mbta6 &lt;- separate(mbta5, month, into = c(&quot;month&quot;, &quot;year&quot;), sep =&quot;-&quot;) # View the head of mbta6 head(mbta6) ## # A tibble: 6 x 10 ## month year Boat Bus `Commuter Rail` `Heavy Rail` `Light Rail` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007 01 4.0 335.819 142.2 435.294 227.231 ## 2 2007 02 3.6 338.675 138.5 448.271 240.262 ## 3 2007 03 40.0 339.867 137.7 458.583 241.444 ## 4 2007 04 4.3 352.162 139.5 472.201 255.557 ## 5 2007 05 4.9 354.367 139.0 474.579 248.262 ## 6 2007 06 5.8 350.543 143.0 477.032 246.108 ## # ... with 3 more variables: `Private Bus` &lt;dbl&gt;, RIDE &lt;dbl&gt;, `Trackless ## # Trolley` &lt;dbl&gt; Looks like some of the data might be a bit out, which you can check using different functions, histogram being one such function. # View a summary of mbta6 summary(mbta6) ## month year Boat Bus ## Length:58 Length:58 Min. : 2.985 Min. :312.9 ## Class :character Class :character 1st Qu.: 3.494 1st Qu.:345.6 ## Mode :character Mode :character Median : 4.293 Median :359.9 ## Mean : 5.068 Mean :358.6 ## 3rd Qu.: 5.356 3rd Qu.:372.2 ## Max. :40.000 Max. :398.5 ## Commuter Rail Heavy Rail Light Rail Private Bus ## Min. :121.4 Min. :435.3 Min. :194.4 Min. :2.213 ## 1st Qu.:131.4 1st Qu.:471.1 1st Qu.:220.6 1st Qu.:2.641 ## Median :138.8 Median :487.3 Median :231.9 Median :2.820 ## Mean :137.4 Mean :489.3 Mean :233.0 Mean :3.352 ## 3rd Qu.:142.4 3rd Qu.:511.3 3rd Qu.:244.5 3rd Qu.:4.167 ## Max. :153.0 Max. :554.9 Max. :271.1 Max. :4.878 ## RIDE Trackless Trolley ## Min. :4.900 Min. : 5.777 ## 1st Qu.:5.965 1st Qu.:11.679 ## Median :6.615 Median :12.598 ## Mean :6.604 Mean :12.125 ## 3rd Qu.:7.149 3rd Qu.:13.320 ## Max. :8.598 Max. :15.109 # Generate a histogram of Boat ridership hist(mbta6$Boat) Looks like we may have an input or typo on the value close to 40 - perhaps should have been a 4.0 or just a 4. Because it’s an error, you don’t want this value influencing your analysis. In this exercise, you’ll locate the incorrect value and change it to 4. # Find the row number of the incorrect value: i i &lt;- which(mbta6$Boat == 40) # Replace the incorrect value with 4 mbta6$Boat[i] &lt;- 4 # Generate a histogram of Boat column hist(mbta6$Boat) library(ggplot2) # Look at all T ridership over time (example plot) ggplot(mbta4, aes(x = month, y = thou_riders, col = mode)) + geom_point() + scale_x_discrete(name = &quot;Month&quot;, breaks = c(200701, 200801, 200901, 201001, 201101)) + scale_y_continuous(name = &quot;Avg Weekday Ridership (thousands)&quot;) 5.4 World Food Facts library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, ## week, yday, year ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last # Import sales.csv: food food &lt;- fread(&quot;https://assets.datacamp.com/production/course_1294/datasets/food.csv&quot;, stringsAsFactors = FALSE) # Convert food to a data frame food &lt;- data.frame(food) # View summary of food summary(food) ## V1 code url creator ## Min. : 1.0 Min. :100030 Length:1500 Length:1500 ## 1st Qu.: 375.8 1st Qu.:124975 Class :character Class :character ## Median : 750.5 Median :149514 Mode :character Mode :character ## Mean : 750.5 Mean :149613 ## 3rd Qu.:1125.2 3rd Qu.:174506 ## Max. :1500.0 Max. :199880 ## ## created_t created_datetime last_modified_t ## Min. :1.332e+09 Length:1500 Min. :1.340e+09 ## 1st Qu.:1.394e+09 Class :character 1st Qu.:1.424e+09 ## Median :1.425e+09 Mode :character Median :1.437e+09 ## Mean :1.414e+09 Mean :1.430e+09 ## 3rd Qu.:1.436e+09 3rd Qu.:1.446e+09 ## Max. :1.453e+09 Max. :1.453e+09 ## ## last_modified_datetime product_name generic_name ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## quantity packaging packaging_tags ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## brands brands_tags categories ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## categories_tags categories_en origins ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## origins_tags manufacturing_places manufacturing_places_tags ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## labels labels_tags labels_en ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## emb_codes emb_codes_tags first_packaging_code_geo ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## cities cities_tags purchase_places stores ## Mode:logical Length:1500 Length:1500 Length:1500 ## NA&#39;s:1500 Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## countries countries_tags countries_en ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## ingredients_text allergens allergens_en traces ## Length:1500 Length:1500 Mode:logical Length:1500 ## Class :character Class :character NA&#39;s:1500 Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## traces_tags traces_en serving_size no_nutriments ## Length:1500 Length:1500 Length:1500 Mode:logical ## Class :character Class :character Class :character NA&#39;s:1500 ## Mode :character Mode :character Mode :character ## ## ## ## ## additives_n additives additives_tags additives_en ## Min. : 0.000 Length:1500 Length:1500 Length:1500 ## 1st Qu.: 0.000 Class :character Class :character Class :character ## Median : 1.000 Mode :character Mode :character Mode :character ## Mean : 1.846 ## 3rd Qu.: 3.000 ## Max. :17.000 ## NA&#39;s :514 ## ingredients_from_palm_oil_n ingredients_from_palm_oil ## Min. :0.0000 Mode:logical ## 1st Qu.:0.0000 NA&#39;s:1500 ## Median :0.0000 ## Mean :0.0487 ## 3rd Qu.:0.0000 ## Max. :1.0000 ## NA&#39;s :514 ## ingredients_from_palm_oil_tags ingredients_that_may_be_from_palm_oil_n ## Length:1500 Min. :0.0000 ## Class :character 1st Qu.:0.0000 ## Mode :character Median :0.0000 ## Mean :0.1379 ## 3rd Qu.:0.0000 ## Max. :4.0000 ## NA&#39;s :514 ## ingredients_that_may_be_from_palm_oil ## Mode:logical ## NA&#39;s:1500 ## ## ## ## ## ## ingredients_that_may_be_from_palm_oil_tags nutrition_grade_uk ## Length:1500 Mode:logical ## Class :character NA&#39;s:1500 ## Mode :character ## ## ## ## ## nutrition_grade_fr pnns_groups_1 pnns_groups_2 ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## states states_tags states_en ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## main_category main_category_en image_url ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## image_small_url energy_100g energy_from_fat_100g fat_100g ## Length:1500 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## Class :character 1st Qu.: 369.8 1st Qu.: 35.98 1st Qu.: 0.90 ## Mode :character Median : 966.5 Median : 237.00 Median : 6.00 ## Mean :1083.2 Mean : 668.41 Mean : 13.39 ## 3rd Qu.:1641.5 3rd Qu.: 974.00 3rd Qu.: 20.00 ## Max. :3700.0 Max. :2900.00 Max. :100.00 ## NA&#39;s :700 NA&#39;s :1486 NA&#39;s :708 ## saturated_fat_100g butyric_acid_100g caproic_acid_100g caprylic_acid_100g ## Min. : 0.000 Mode:logical Mode:logical Mode:logical ## 1st Qu.: 0.200 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## Median : 1.700 ## Mean : 4.874 ## 3rd Qu.: 6.500 ## Max. :57.000 ## NA&#39;s :797 ## capric_acid_100g lauric_acid_100g myristic_acid_100g palmitic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## stearic_acid_100g arachidic_acid_100g behenic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## lignoceric_acid_100g cerotic_acid_100g montanic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## melissic_acid_100g monounsaturated_fat_100g polyunsaturated_fat_100g ## Mode:logical Min. : 0.00 Min. : 0.400 ## NA&#39;s:1500 1st Qu.: 3.87 1st Qu.: 1.653 ## Median : 9.50 Median : 3.900 ## Mean :19.77 Mean : 9.986 ## 3rd Qu.:29.00 3rd Qu.:12.700 ## Max. :75.00 Max. :46.200 ## NA&#39;s :1465 NA&#39;s :1464 ## omega_3_fat_100g alpha_linolenic_acid_100g eicosapentaenoic_acid_100g ## Min. : 0.033 Min. :0.0800 Min. :0.721 ## 1st Qu.: 1.300 1st Qu.:0.0905 1st Qu.:0.721 ## Median : 3.000 Median :0.1010 Median :0.721 ## Mean : 3.726 Mean :0.1737 Mean :0.721 ## 3rd Qu.: 3.200 3rd Qu.:0.2205 3rd Qu.:0.721 ## Max. :12.400 Max. :0.3400 Max. :0.721 ## NA&#39;s :1491 NA&#39;s :1497 NA&#39;s :1499 ## docosahexaenoic_acid_100g omega_6_fat_100g linoleic_acid_100g ## Min. :1.09 Min. :0.25 Min. :0.5000 ## 1st Qu.:1.09 1st Qu.:0.25 1st Qu.:0.5165 ## Median :1.09 Median :0.25 Median :0.5330 ## Mean :1.09 Mean :0.25 Mean :0.5330 ## 3rd Qu.:1.09 3rd Qu.:0.25 3rd Qu.:0.5495 ## Max. :1.09 Max. :0.25 Max. :0.5660 ## NA&#39;s :1499 NA&#39;s :1499 NA&#39;s :1498 ## arachidonic_acid_100g gamma_linolenic_acid_100g ## Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## dihomo_gamma_linolenic_acid_100g omega_9_fat_100g oleic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## elaidic_acid_100g gondoic_acid_100g mead_acid_100g erucic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## nervonic_acid_100g trans_fat_100g cholesterol_100g carbohydrates_100g ## Mode:logical Min. :0.0000 Min. :0.0000 Min. : 0.000 ## NA&#39;s:1500 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.: 3.792 ## Median :0.0000 Median :0.0000 Median : 13.500 ## Mean :0.0105 Mean :0.0265 Mean : 27.958 ## 3rd Qu.:0.0000 3rd Qu.:0.0026 3rd Qu.: 55.000 ## Max. :0.1000 Max. :0.4300 Max. :100.000 ## NA&#39;s :1481 NA&#39;s :1477 NA&#39;s :708 ## sugars_100g sucrose_100g glucose_100g fructose_100g ## Min. : 0.00 Mode:logical Mode:logical Min. :100 ## 1st Qu.: 1.00 NA&#39;s:1500 NA&#39;s:1500 1st Qu.:100 ## Median : 4.05 Median :100 ## Mean : 12.66 Mean :100 ## 3rd Qu.: 14.70 3rd Qu.:100 ## Max. :100.00 Max. :100 ## NA&#39;s :788 NA&#39;s :1499 ## lactose_100g maltose_100g maltodextrins_100g starch_100g ## Min. :0.000 Mode:logical Mode:logical Min. : 0.00 ## 1st Qu.:0.250 NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 9.45 ## Median :0.500 Median :39.50 ## Mean :2.933 Mean :30.73 ## 3rd Qu.:4.400 3rd Qu.:42.85 ## Max. :8.300 Max. :71.00 ## NA&#39;s :1497 NA&#39;s :1493 ## polyols_100g fiber_100g proteins_100g casein_100g ## Min. : 8.60 Min. : 0.000 Min. : 0.000 Min. :1.1 ## 1st Qu.:59.10 1st Qu.: 0.500 1st Qu.: 1.500 1st Qu.:1.1 ## Median :67.00 Median : 1.750 Median : 6.000 Median :1.1 ## Mean :56.06 Mean : 2.823 Mean : 7.563 Mean :1.1 ## 3rd Qu.:69.80 3rd Qu.: 3.500 3rd Qu.:10.675 3rd Qu.:1.1 ## Max. :70.00 Max. :46.700 Max. :61.000 Max. :1.1 ## NA&#39;s :1491 NA&#39;s :994 NA&#39;s :710 NA&#39;s :1499 ## serum_proteins_100g nucleotides_100g salt_100g sodium_100g ## Mode:logical Mode:logical Min. : 0.0000 Min. : 0.0000 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 0.0438 1st Qu.: 0.0172 ## Median : 0.4498 Median : 0.1771 ## Mean : 1.1205 Mean : 0.4409 ## 3rd Qu.: 1.1938 3rd Qu.: 0.4700 ## Max. :102.0000 Max. :40.0000 ## NA&#39;s :780 NA&#39;s :780 ## alcohol_100g vitamin_a_100g beta_carotene_100g vitamin_d_100g ## Min. : 0.00 Min. :0.0000 Mode:logical Min. :0e+00 ## 1st Qu.: 0.00 1st Qu.:0.0000 NA&#39;s:1500 1st Qu.:0e+00 ## Median : 5.50 Median :0.0001 Median :0e+00 ## Mean :10.07 Mean :0.0003 Mean :0e+00 ## 3rd Qu.:13.00 3rd Qu.:0.0006 3rd Qu.:0e+00 ## Max. :50.00 Max. :0.0013 Max. :1e-04 ## NA&#39;s :1433 NA&#39;s :1477 NA&#39;s :1485 ## vitamin_e_100g vitamin_k_100g vitamin_c_100g vitamin_b1_100g ## Min. :0.0005 Min. :0 Min. :0.000 Min. :0.0001 ## 1st Qu.:0.0021 1st Qu.:0 1st Qu.:0.002 1st Qu.:0.0003 ## Median :0.0044 Median :0 Median :0.019 Median :0.0004 ## Mean :0.0069 Mean :0 Mean :0.025 Mean :0.0006 ## 3rd Qu.:0.0097 3rd Qu.:0 3rd Qu.:0.030 3rd Qu.:0.0010 ## Max. :0.0320 Max. :0 Max. :0.217 Max. :0.0013 ## NA&#39;s :1478 NA&#39;s :1498 NA&#39;s :1459 NA&#39;s :1478 ## vitamin_b2_100g vitamin_pp_100g vitamin_b6_100g vitamin_b9_100g ## Min. :0.0002 Min. :0.0006 Min. :0.0001 Min. :0e+00 ## 1st Qu.:0.0003 1st Qu.:0.0033 1st Qu.:0.0002 1st Qu.:0e+00 ## Median :0.0009 Median :0.0069 Median :0.0008 Median :1e-04 ## Mean :0.0011 Mean :0.0086 Mean :0.0112 Mean :1e-04 ## 3rd Qu.:0.0013 3rd Qu.:0.0140 3rd Qu.:0.0012 3rd Qu.:2e-04 ## Max. :0.0066 Max. :0.0160 Max. :0.2000 Max. :2e-04 ## NA&#39;s :1483 NA&#39;s :1484 NA&#39;s :1481 NA&#39;s :1483 ## vitamin_b12_100g biotin_100g pantothenic_acid_100g silica_100g ## Min. :0 Min. :0 Min. :0.0000 Min. :8e-04 ## 1st Qu.:0 1st Qu.:0 1st Qu.:0.0007 1st Qu.:8e-04 ## Median :0 Median :0 Median :0.0020 Median :8e-04 ## Mean :0 Mean :0 Mean :0.0027 Mean :8e-04 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0.0051 3rd Qu.:8e-04 ## Max. :0 Max. :0 Max. :0.0060 Max. :8e-04 ## NA&#39;s :1489 NA&#39;s :1498 NA&#39;s :1486 NA&#39;s :1499 ## bicarbonate_100g potassium_100g chloride_100g calcium_100g ## Min. :0.0006 Min. :0.0000 Min. :0.0003 Min. :0.0000 ## 1st Qu.:0.0678 1st Qu.:0.0650 1st Qu.:0.0006 1st Qu.:0.0450 ## Median :0.1350 Median :0.1940 Median :0.0009 Median :0.1200 ## Mean :0.1692 Mean :0.3288 Mean :0.0144 Mean :0.2040 ## 3rd Qu.:0.2535 3rd Qu.:0.3670 3rd Qu.:0.0214 3rd Qu.:0.1985 ## Max. :0.3720 Max. :1.4300 Max. :0.0420 Max. :1.0000 ## NA&#39;s :1497 NA&#39;s :1487 NA&#39;s :1497 NA&#39;s :1449 ## phosphorus_100g iron_100g magnesium_100g zinc_100g ## Min. :0.0430 Min. :0.0000 Min. :0.0000 Min. :0.0005 ## 1st Qu.:0.1938 1st Qu.:0.0012 1st Qu.:0.0670 1st Qu.:0.0009 ## Median :0.3185 Median :0.0042 Median :0.1040 Median :0.0017 ## Mean :0.3777 Mean :0.0045 Mean :0.1066 Mean :0.0016 ## 3rd Qu.:0.4340 3rd Qu.:0.0077 3rd Qu.:0.1300 3rd Qu.:0.0022 ## Max. :1.1550 Max. :0.0137 Max. :0.3330 Max. :0.0026 ## NA&#39;s :1488 NA&#39;s :1463 NA&#39;s :1479 NA&#39;s :1493 ## copper_100g manganese_100g fluoride_100g selenium_100g ## Min. :0e+00 Min. :0 Min. :0 Min. :0 ## 1st Qu.:1e-04 1st Qu.:0 1st Qu.:0 1st Qu.:0 ## Median :1e-04 Median :0 Median :0 Median :0 ## Mean :1e-04 Mean :0 Mean :0 Mean :0 ## 3rd Qu.:1e-04 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 ## Max. :1e-04 Max. :0 Max. :0 Max. :0 ## NA&#39;s :1498 NA&#39;s :1499 NA&#39;s :1498 NA&#39;s :1499 ## chromium_100g molybdenum_100g iodine_100g caffeine_100g ## Mode:logical Mode:logical Min. :0 Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:0 NA&#39;s:1500 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## NA&#39;s :1499 ## taurine_100g ph_100g fruits_vegetables_nuts_100g ## Mode:logical Mode:logical Min. : 2.00 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:11.25 ## Median :42.00 ## Mean :36.88 ## 3rd Qu.:52.25 ## Max. :80.00 ## NA&#39;s :1470 ## collagen_meat_protein_ratio_100g cocoa_100g chlorophyl_100g ## Min. :12.00 Min. :30 Mode:logical ## 1st Qu.:13.50 1st Qu.:47 NA&#39;s:1500 ## Median :15.00 Median :60 ## Mean :15.67 Mean :57 ## 3rd Qu.:17.50 3rd Qu.:70 ## Max. :20.00 Max. :81 ## NA&#39;s :1497 NA&#39;s :1491 ## carbon_footprint_100g nutrition_score_fr_100g nutrition_score_uk_100g ## Min. : 12.00 Min. :-12.000 Min. :-12.000 ## 1st Qu.: 97.42 1st Qu.: 1.000 1st Qu.: 0.000 ## Median :182.85 Median : 7.000 Median : 6.000 ## Mean :131.18 Mean : 7.941 Mean : 7.631 ## 3rd Qu.:190.78 3rd Qu.: 15.000 3rd Qu.: 16.000 ## Max. :198.70 Max. : 28.000 Max. : 28.000 ## NA&#39;s :1497 NA&#39;s :825 NA&#39;s :825 # View head of food head(food) ## V1 code ## 1 1 100030 ## 2 2 100050 ## 3 3 100079 ## 4 4 100094 ## 5 5 100124 ## 6 6 100136 ## url ## 1 http://world-en.openfoodfacts.org/product/3222475745867/confiture-de-fraise-fraise-des-bois-au-sucre-de-canne-casino-delices ## 2 http://world-en.openfoodfacts.org/product/5410976880110/guylian-sea-shells-selection ## 3 http://world-en.openfoodfacts.org/product/3264750423503/pates-de-fruits-aromatisees-jacquot ## 4 http://world-en.openfoodfacts.org/product/8006040247001/nata-vegetal-a-base-de-soja-valsoia ## 5 http://world-en.openfoodfacts.org/product/8480000340764/semillas-de-girasol-con-cascara-tostadas-aguasal-hacendado ## 6 http://world-en.openfoodfacts.org/product/0087703177727/soft-drink ## creator created_t created_datetime last_modified_t ## 1 sebleouf 1424747544 2015-02-24T03:12:24Z 1438445887 ## 2 foodorigins 1450316429 2015-12-17T01:40:29Z 1450817956 ## 3 domdom26 1428674916 2015-04-10T14:08:36Z 1428739289 ## 4 javichu 1420416591 2015-01-05T00:09:51Z 1420417876 ## 5 javichu 1420501121 2015-01-05T23:38:41Z 1445700917 ## 6 foodorigins 1437983923 2015-07-27T07:58:43Z 1445577476 ## last_modified_datetime ## 1 2015-08-01T16:18:07Z ## 2 2015-12-22T20:59:16Z ## 3 2015-04-11T08:01:29Z ## 4 2015-01-05T00:31:16Z ## 5 2015-10-24T15:35:17Z ## 6 2015-10-23T05:17:56Z ## product_name ## 1 Confiture de fraise fraise des bois au sucre de canne ## 2 Guylian Sea Shells Selection ## 3 PÃ¢tes de fruits aromatisÃ©es ## 4 Nata vegetal a base de soja &amp;quot;Valsoia&amp;quot; ## 5 Semillas de girasol con cÃ¡scara tostadas aguasal ## 6 Soft Drink ## generic_name quantity ## 1 265 g ## 2 375g ## 3 PÃ¢tes de fruits 1 kg ## 4 Nata vegetal a base de soja 200 ml ## 5 Semillas de girasol con cÃ¡scara tostadas aguasal 200 g ## 6 ## packaging ## 1 Bocal,Verre ## 2 Plastic,Box ## 3 Carton,plastique ## 4 Tetra Brik ## 5 Bolsa de plÃ¡stico,Envasado en atmÃ³sfera protectora ## 6 ## packaging_tags ## 1 bocal,verre ## 2 plastic,box ## 3 carton,plastique ## 4 tetra-brik ## 5 bolsa-de-plastico,envasado-en-atmosfera-protectora ## 6 ## brands ## 1 Casino DÃ©lices ## 2 Guylian ## 3 Jacquot ## 4 Valsoia,//Propiedad de://,Valsoia S.p.A. ## 5 Hacendado,//Propiedad de://,Mercadona S.A. ## 6 ## brands_tags ## 1 casino-delices ## 2 guylian ## 3 jacquot ## 4 valsoia,propiedad-de,valsoia-s-p-a ## 5 hacendado,propiedad-de,mercadona-s-a ## 6 ## categories ## 1 Aliments et boissons Ã base de vÃ©gÃ©taux,Aliments d&#39;origine vÃ©gÃ©tale,Aliments Ã base de fruits et de lÃ©gumes,Petit-dÃ©jeuners,Produits Ã tartiner,Fruits et produits dÃ©rivÃ©s,PÃ¢tes Ã tartiner vÃ©gÃ©taux,Produits Ã tartiner sucrÃ©s,Confitures et marmelades,Confitures,Confitures de fruits,Confitures de fruits rouges,Confitures de fraises ## 2 Chocolate ## 3 pÃ¢tes de fruits ## 4 Alimentos y bebidas de origen vegetal,Alimentos de origen vegetal,Natas vegetales,Natas vegetales a base de soja para cocinar,Natas vegetales para cocinar ## 5 Semillas de girasol y derivados, Semillas, Semillas de girasol, Semillas de girasol con cÃ¡scara, Semillas de girasol tostadas, Semillas de girasol con cÃ¡scara tostadas, Semillas de girasol con cÃ¡scara tostadas aguasal ## 6 ## categories_tags ## 1 en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:breakfasts,en:spreads,en:fruits-based-foods,en:plant-based-spreads,en:sweet-spreads,en:fruit-preserves,en:jams,en:fruit-jams,en:berry-jams,en:strawberry-jams ## 2 en:sugary-snacks,en:chocolates ## 3 en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:sugary-snacks,en:confectioneries,en:fruits-based-foods,en:fruit-pastes ## 4 en:plant-based-foods-and-beverages,en:plant-based-foods,en:plant-based-creams,en:plant-based-creams-for-cooking,en:soy-based-creams-for-cooking ## 5 en:plant-based-foods-and-beverages,en:plant-based-foods,en:seeds,en:sunflower-seeds-and-their-products,en:sunflower-seeds,en:roasted-sunflower-seeds,en:unshelled-sunflower-seeds,en:roasted-unshelled-sunflower-seeds,es:semillas-de-girasol-con-cascara-tostadas-aguasal ## 6 ## categories_en ## 1 Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Breakfasts,Spreads,Fruits based foods,Plant-based spreads,Sweet spreads,Fruit preserves,Jams,Fruit jams,Berry jams,Strawberry jams ## 2 Sugary snacks,Chocolates ## 3 Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Sugary snacks,Confectioneries,Fruits based foods,Fruit pastes ## 4 Plant-based foods and beverages,Plant-based foods,Plant-based creams,Plant-based creams for cooking,Soy-based creams for cooking ## 5 Plant-based foods and beverages,Plant-based foods,Seeds,Sunflower seeds and their products,Sunflower seeds,Roasted sunflower seeds,Unshelled sunflower seeds,Roasted unshelled sunflower seeds,es:Semillas-de-girasol-con-cascara-tostadas-aguasal ## 6 ## origins origins_tags ## 1 ## 2 ## 3 ## 4 ## 5 Argentina argentina ## 6 South Korea south-korea ## manufacturing_places ## 1 France ## 2 Belgium ## 3 ## 4 Italia ## 5 Beniparrell,Valencia (provincia),Comunidad Valenciana,EspaÃ±a ## 6 South Korea ## manufacturing_places_tags ## 1 france ## 2 belgium ## 3 ## 4 italia ## 5 beniparrell,valencia-provincia,comunidad-valenciana,espana ## 6 south-korea ## labels ## 1 ## 2 ## 3 ## 4 Vegetariano,Vegano,Sin gluten,Sin OMG,Sin lactosa ## 5 Vegetariano,Vegano,Sin gluten ## 6 ## labels_tags ## 1 ## 2 ## 3 ## 4 en:vegetarian,en:vegan,en:gluten-free,en:no-gmos,en:no-lactose ## 5 en:vegetarian,en:vegan,en:gluten-free ## 6 ## labels_en ## 1 ## 2 ## 3 ## 4 Vegetarian,Vegan,Gluten-free,No GMOs,No lactose ## 5 Vegetarian,Vegan,Gluten-free ## 6 ## emb_codes ## 1 EMB 78015 ## 2 ## 3 ## 4 ## 5 ES 21.016540/V EC,ENVASADOR:,IMPORTACO S.A. ## 6 ## emb_codes_tags first_packaging_code_geo ## 1 emb-78015 48.983333,2.066667 ## 2 ## 3 ## 4 ## 5 es-21-016540-v-ec,envasador,importaco-s-a ## 6 ## cities cities_tags purchase_places stores ## 1 NA andresy-yvelines-france Lyon,France Casino ## 2 NA NSW,Australia ## 3 NA France ## 4 NA Madrid,EspaÃ±a El Corte InglÃ©s ## 5 NA Madrid,EspaÃ±a Mercadona ## 6 NA ## countries countries_tags countries_en ## 1 France en:france France ## 2 Australia en:australia Australia ## 3 France en:france France ## 4 EspaÃ±a en:spain Spain ## 5 EspaÃ±a en:spain Spain ## 6 Australia en:australia Australia ## ingredients_text ## 1 Sucre de canne, fraises 40 g, fraises des bois 14 g, gÃ©lifiant : pectines de fruits, jus de citron concentrÃ©. PrÃ©parÃ©e avec 54 g de fruits pour 100 g de produit fini. ## 2 ## 3 Pulpe de pommes 50% , sucre, sirop de glucose, gÃ©lifiant : pectine, acidifiant : acide citrique, arÃ´mes, colorants naturels : extrait de paprika â complexes cuivreâchlorophyllines â curcumine â antnocyanes ## 4 Extracto de soja (78%) (agua, semillas de soja 8,3%), grasas vegetales, jarabe de glucosa, dextrosa, emulsionante: mono- y diglicÃ©ridos de Ã¡cidos grasos (E-471), sal marina, estabilizantes: goma xantana (E-415), carragenatos (E-407), goma guar (E-412); aromas, antioxidante: extractos de tocoferoles (de soja) (E-306). (Nota: el envase en italiano del paquete -que puede verse en el enlace-, especifica que el producto es 100% vegetal. Por tanto los mono- y diglicÃ©ridos de Ã¡cidos grasos (E-471) son de origen no animal). ## 5 Pipas de girasol y sal. ## 6 ## allergens allergens_en traces traces_tags ## 1 NA Lait,Fruits Ã coque en:milk,en:nuts ## 2 NA ## 3 NA ## 4 NA ## 5 NA Frutos de cÃ¡scara,Cacahuetes en:nuts,en:peanuts ## 6 NA ## traces_en serving_size no_nutriments additives_n ## 1 Milk,Nuts 15 g NA 1 ## 2 NA NA ## 3 NA 2 ## 4 NA 5 ## 5 Nuts,Peanuts NA 0 ## 6 NA NA ## additives ## 1 [ sucre-de-canne -&gt; fr:sucre-de-canne ] [ sucre-de -&gt; fr:sucre-de ] [ sucre -&gt; fr:sucre ] [ fraises-40-g -&gt; fr:fraises-40-g ] [ fraises-40 -&gt; fr:fraises-40 ] [ fraises -&gt; fr:fraises ] [ fraises-des-bois-14-g -&gt; fr:fraises-des-bois-14-g ] [ fraises-des-bois-14 -&gt; fr:fraises-des-bois-14 ] [ fraises-des-bois -&gt; fr:fraises-des-bois ] [ fraises-des -&gt; fr:fraises-des ] [ fraises -&gt; fr:fraises ] [ pectines-de-fruits -&gt; fr:pectines-de-fruits ] [ pectines-de -&gt; fr:pectines-de ] [ pectines -&gt; en:e440 -&gt; exists ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit-fini -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit-fini ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100 -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100 ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits ] [ jus-de-citron-concentre-preparee-avec-54-g-de -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de ] [ jus-de-citron-concentre-preparee-avec-54-g -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g ] [ jus-de-citron-concentre-preparee-avec-54 -&gt; fr:jus-de-citron-concentre-preparee-avec-54 ] [ jus-de-citron-concentre-preparee-avec -&gt; fr:jus-de-citron-concentre-preparee-avec ] [ jus-de-citron-concentre-preparee -&gt; fr:jus-de-citron-concentre-preparee ] [ jus-de-citron-concentre -&gt; fr:jus-de-citron-concentre ] [ jus-de-citron -&gt; fr:jus-de-citron ] [ jus-de -&gt; fr:jus-de ] [ jus -&gt; fr:jus ] ## 2 ## 3 [ pulpe-de-pommes-50 -&gt; fr:pulpe-de-pommes-50 ] [ pulpe-de-pommes -&gt; fr:pulpe-de-pommes ] [ pulpe-de -&gt; fr:pulpe-de ] [ pulpe -&gt; fr:pulpe ] [ sucre -&gt; fr:sucre ] [ sirop-de-glucose -&gt; fr:sirop-de-glucose ] [ sirop-de -&gt; fr:sirop-de ] [ sirop -&gt; fr:sirop ] [ pectine -&gt; en:e440 -&gt; exists ] [ acide-citrique -&gt; en:e330 -&gt; exists ] [ aromes -&gt; fr:aromes ] [ naturels -&gt; fr:naturels ] [ extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine-antnocyanes -&gt; fr:extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine-antnocyanes ] [ extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine -&gt; fr:extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine ] [ extrait-de-paprika-complexes-cuivre-chlorophyllines -&gt; fr:extrait-de-paprika-complexes-cuivre-chlorophyllines ] [ extrait-de-paprika-complexes-cuivre -&gt; fr:extrait-de-paprika-complexes-cuivre ] [ extrait-de-paprika-complexes -&gt; fr:extrait-de-paprika-complexes ] [ extrait-de-paprika -&gt; fr:extrait-de-paprika ] [ extrait-de -&gt; fr:extrait-de ] [ extrait -&gt; fr:extrait ] ## 4 [ extracto-de-soja -&gt; es:extracto-de-soja ] [ 78 -&gt; es:78 ] [ agua -&gt; es:agua ] [ semillas-de-soja-8 -&gt; es:semillas-de-soja-8 ] [ 3 -&gt; en:fd-c ] [ grasas-vegetales -&gt; es:grasas-vegetales ] [ jarabe-de-glucosa -&gt; es:jarabe-de-glucosa ] [ dextrosa -&gt; es:dextrosa ] [ emulsionante -&gt; es:emulsionante ] [ mono-y-digliceridos-de-acidos-grasos -&gt; en:e471 -&gt; exists ] [ e471 -&gt; en:e471 ] [ sal-marina -&gt; es:sal-marina ] [ estabilizantes -&gt; es:estabilizantes ] [ goma-xantana -&gt; en:e415 -&gt; exists ] [ e415 -&gt; en:e415 ] [ carragenatos -&gt; en:e407 -&gt; exists ] [ e407 -&gt; en:e407 ] [ goma-guar -&gt; en:e412 -&gt; exists ] [ e412 -&gt; en:e412 ] [ aromas -&gt; es:aromas ] [ antioxidante -&gt; es:antioxidante ] [ extractos-de-tocoferoles -&gt; es:extractos-de-tocoferoles ] [ de-soja -&gt; es:de-soja ] [ e306 -&gt; en:e306 -&gt; exists ] [ nota -&gt; es:nota ] [ el-envase-en-italiano-del-paquete-que-puede-verse-en-el-enlace -&gt; es:el-envase-en-italiano-del-paquete-que-puede-verse-en-el-enlace ] [ especifica-que-el-producto-es-100-vegetal-por-tanto-los-mono-y-digliceridos-de-acidos-grasos -&gt; es:especifica-que-el-producto-es-100-vegetal-por-tanto-los-mono-y-digliceridos-de-acidos-grasos ] [ e471 -&gt; en:e471 ] [ son-de-origen-no-animal -&gt; es:son-de-origen-no-animal ] [ -&gt; es: ] ## 5 [ pipas-de-girasol-y-sal -&gt; es:pipas-de-girasol-y-sal ] ## 6 ## additives_tags ## 1 en:e440 ## 2 ## 3 en:e440,en:e330 ## 4 en:e471,en:e415,en:e407,en:e412,en:e306 ## 5 ## 6 ## additives_en ## 1 E440 - Pectins ## 2 ## 3 E440 - Pectins,E330 - Citric acid ## 4 E471 - Mono- and diglycerides of fatty acids,E415 - Xanthan gum,E407 - Carrageenan,E412 - Guar gum,E306 - Tocopherol-rich extract ## 5 ## 6 ## ingredients_from_palm_oil_n ingredients_from_palm_oil ## 1 0 NA ## 2 NA NA ## 3 0 NA ## 4 0 NA ## 5 0 NA ## 6 NA NA ## ingredients_from_palm_oil_tags ingredients_that_may_be_from_palm_oil_n ## 1 0 ## 2 NA ## 3 0 ## 4 1 ## 5 0 ## 6 NA ## ingredients_that_may_be_from_palm_oil ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA ## ingredients_that_may_be_from_palm_oil_tags nutrition_grade_uk ## 1 NA ## 2 NA ## 3 NA ## 4 e471-mono-et-diglycerides-d-acides-gras-alimentaires NA ## 5 NA ## 6 NA ## nutrition_grade_fr pnns_groups_1 pnns_groups_2 ## 1 d Sugary snacks Sweets ## 2 Sugary snacks Chocolate products ## 3 Fruits and vegetables Fruits ## 4 d unknown unknown ## 5 d unknown unknown ## 6 unknown unknown ## states ## 1 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 2 en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-be-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 3 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 4 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 5 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 6 en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-be-completed, en:characteristics-to-be-completed, en:categories-to-be-completed, en:brands-to-be-completed, en:packaging-to-be-completed, en:quantity-to-be-completed, en:photos-to-be-validated, en:photos-uploaded ## states_tags ## 1 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 2 en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 3 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 4 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 5 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 6 en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-completed,en:characteristics-to-be-completed,en:categories-to-be-completed,en:brands-to-be-completed,en:packaging-to-be-completed,en:quantity-to-be-completed,en:photos-to-be-validated,en:photos-uploaded ## states_en ## 1 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characteristics completed,Photos validated,Photos uploaded ## 2 To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Characteristics completed,Photos validated,Photos uploaded ## 3 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characteristics completed,Photos validated,Photos uploaded ## 4 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date completed,Characteristics completed,Photos validated,Photos uploaded ## 5 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date completed,Characteristics completed,Photos validated,Photos uploaded ## 6 To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Characteristics to be completed,Categories to be completed,Brands to be completed,Packaging to be completed,Quantity to be completed,Photos to be validated,Photos uploaded ## main_category main_category_en ## 1 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 2 en:sugary-snacks Sugary snacks ## 3 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 4 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 5 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 6 ## image_url ## 1 http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.400.jpg ## 2 http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.400.jpg ## 3 http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.400.jpg ## 4 http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.400.jpg ## 5 http://en.openfoodfacts.org/images/products/848/000/034/0764/front.6.400.jpg ## 6 http://en.openfoodfacts.org/images/products/008/770/317/7727/front.8.400.jpg ## image_small_url ## 1 http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.200.jpg ## 2 http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.200.jpg ## 3 http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.200.jpg ## 4 http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.200.jpg ## 5 http://en.openfoodfacts.org/images/products/848/000/034/0764/front.6.200.jpg ## 6 http://en.openfoodfacts.org/images/products/008/770/317/7727/front.8.200.jpg ## energy_100g energy_from_fat_100g fat_100g saturated_fat_100g ## 1 918 NA 0.0 0.0 ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 766 NA 16.7 9.9 ## 5 2359 NA 45.5 5.2 ## 6 NA NA NA NA ## butyric_acid_100g caproic_acid_100g caprylic_acid_100g capric_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## lauric_acid_100g myristic_acid_100g palmitic_acid_100g stearic_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## arachidic_acid_100g behenic_acid_100g lignoceric_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## cerotic_acid_100g montanic_acid_100g melissic_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## monounsaturated_fat_100g polyunsaturated_fat_100g omega_3_fat_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 2.9 3.9 NA ## 5 9.5 32.8 NA ## 6 NA NA NA ## alpha_linolenic_acid_100g eicosapentaenoic_acid_100g ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## docosahexaenoic_acid_100g omega_6_fat_100g linoleic_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## arachidonic_acid_100g gamma_linolenic_acid_100g ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## dihomo_gamma_linolenic_acid_100g omega_9_fat_100g oleic_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## elaidic_acid_100g gondoic_acid_100g mead_acid_100g erucic_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## nervonic_acid_100g trans_fat_100g cholesterol_100g carbohydrates_100g ## 1 NA NA NA 54.0 ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA 2e-04 5.7 ## 5 NA NA NA 17.3 ## 6 NA NA NA NA ## sugars_100g sucrose_100g glucose_100g fructose_100g lactose_100g ## 1 54.0 NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 4.2 NA NA NA NA ## 5 2.7 NA NA NA NA ## 6 NA NA NA NA NA ## maltose_100g maltodextrins_100g starch_100g polyols_100g fiber_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA 0.2 ## 5 NA NA NA NA 9.0 ## 6 NA NA NA NA NA ## proteins_100g casein_100g serum_proteins_100g nucleotides_100g salt_100g ## 1 0.0 NA NA NA 0.0000 ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 2.9 NA NA NA 0.0508 ## 5 18.2 NA NA NA 3.9878 ## 6 NA NA NA NA NA ## sodium_100g alcohol_100g vitamin_a_100g beta_carotene_100g ## 1 0.00 NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 0.02 NA NA NA ## 5 1.57 NA NA NA ## 6 NA NA NA NA ## vitamin_d_100g vitamin_e_100g vitamin_k_100g vitamin_c_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## vitamin_b1_100g vitamin_b2_100g vitamin_pp_100g vitamin_b6_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## vitamin_b9_100g vitamin_b12_100g biotin_100g pantothenic_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## silica_100g bicarbonate_100g potassium_100g chloride_100g calcium_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## phosphorus_100g iron_100g magnesium_100g zinc_100g copper_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 1.155 0.0038 0.129 NA NA ## 6 NA NA NA NA NA ## manganese_100g fluoride_100g selenium_100g chromium_100g molybdenum_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## iodine_100g caffeine_100g taurine_100g ph_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## fruits_vegetables_nuts_100g collagen_meat_protein_ratio_100g cocoa_100g ## 1 54 NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## chlorophyl_100g carbon_footprint_100g nutrition_score_fr_100g ## 1 NA NA 11 ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA 11 ## 5 NA NA 17 ## 6 NA NA NA ## nutrition_score_uk_100g ## 1 11 ## 2 NA ## 3 NA ## 4 11 ## 5 17 ## 6 NA # View structure of food str(food) ## &#39;data.frame&#39;: 1500 obs. of 160 variables: ## $ V1 : int 1 2 3 4 5 6 7 8 9 10 ... ## $ code : int 100030 100050 100079 100094 100124 100136 100194 100221 100257 100258 ... ## $ url : chr &quot;http://world-en.openfoodfacts.org/product/3222475745867/confiture-de-fraise-fraise-des-bois-au-sucre-de-canne-casino-delices&quot; &quot;http://world-en.openfoodfacts.org/product/5410976880110/guylian-sea-shells-selection&quot; &quot;http://world-en.openfoodfacts.org/product/3264750423503/pates-de-fruits-aromatisees-jacquot&quot; &quot;http://world-en.openfoodfacts.org/product/8006040247001/nata-vegetal-a-base-de-soja-valsoia&quot; ... ## $ creator : chr &quot;sebleouf&quot; &quot;foodorigins&quot; &quot;domdom26&quot; &quot;javichu&quot; ... ## $ created_t : int 1424747544 1450316429 1428674916 1420416591 1420501121 1437983923 1442420988 1435686217 1436991777 1400516512 ... ## $ created_datetime : chr &quot;2015-02-24T03:12:24Z&quot; &quot;2015-12-17T01:40:29Z&quot; &quot;2015-04-10T14:08:36Z&quot; &quot;2015-01-05T00:09:51Z&quot; ... ## $ last_modified_t : int 1438445887 1450817956 1428739289 1420417876 1445700917 1445577476 1442420988 1451405288 1436991779 1437236856 ... ## $ last_modified_datetime : chr &quot;2015-08-01T16:18:07Z&quot; &quot;2015-12-22T20:59:16Z&quot; &quot;2015-04-11T08:01:29Z&quot; &quot;2015-01-05T00:31:16Z&quot; ... ## $ product_name : chr &quot;Confiture de fraise fraise des bois au sucre de canne&quot; &quot;Guylian Sea Shells Selection&quot; &quot;PÃ¢tes de fruits aromatisÃ©es&quot; &quot;Nata vegetal a base de soja &amp;quot;Valsoia&amp;quot;&quot; ... ## $ generic_name : chr &quot;&quot; &quot;&quot; &quot;PÃ¢tes de fruits&quot; &quot;Nata vegetal a base de soja&quot; ... ## $ quantity : chr &quot;265 g&quot; &quot;375g&quot; &quot;1 kg&quot; &quot;200 ml&quot; ... ## $ packaging : chr &quot;Bocal,Verre&quot; &quot;Plastic,Box&quot; &quot;Carton,plastique&quot; &quot;Tetra Brik&quot; ... ## $ packaging_tags : chr &quot;bocal,verre&quot; &quot;plastic,box&quot; &quot;carton,plastique&quot; &quot;tetra-brik&quot; ... ## $ brands : chr &quot;Casino DÃ©lices&quot; &quot;Guylian&quot; &quot;Jacquot&quot; &quot;Valsoia,//Propiedad de://,Valsoia S.p.A.&quot; ... ## $ brands_tags : chr &quot;casino-delices&quot; &quot;guylian&quot; &quot;jacquot&quot; &quot;valsoia,propiedad-de,valsoia-s-p-a&quot; ... ## $ categories : chr &quot;Aliments et boissons Ã base de vÃ©gÃ©taux,Aliments d&#39;origine vÃ©gÃ©tale,Aliments Ã base de fruits et de lÃ©gu&quot;| __truncated__ &quot;Chocolate&quot; &quot;pÃ¢tes de fruits&quot; &quot;Alimentos y bebidas de origen vegetal,Alimentos de origen vegetal,Natas vegetales,Natas vegetales a base de soj&quot;| __truncated__ ... ## $ categories_tags : chr &quot;en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:breakfasts,en:s&quot;| __truncated__ &quot;en:sugary-snacks,en:chocolates&quot; &quot;en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:sugary-snacks,e&quot;| __truncated__ &quot;en:plant-based-foods-and-beverages,en:plant-based-foods,en:plant-based-creams,en:plant-based-creams-for-cooking&quot;| __truncated__ ... ## $ categories_en : chr &quot;Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Breakfasts,Spreads,Fruits b&quot;| __truncated__ &quot;Sugary snacks,Chocolates&quot; &quot;Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Sugary snacks,Confectioneri&quot;| __truncated__ &quot;Plant-based foods and beverages,Plant-based foods,Plant-based creams,Plant-based creams for cooking,Soy-based c&quot;| __truncated__ ... ## $ origins : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ origins_tags : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ manufacturing_places : chr &quot;France&quot; &quot;Belgium&quot; &quot;&quot; &quot;Italia&quot; ... ## $ manufacturing_places_tags : chr &quot;france&quot; &quot;belgium&quot; &quot;&quot; &quot;italia&quot; ... ## $ labels : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;Vegetariano,Vegano,Sin gluten,Sin OMG,Sin lactosa&quot; ... ## $ labels_tags : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;en:vegetarian,en:vegan,en:gluten-free,en:no-gmos,en:no-lactose&quot; ... ## $ labels_en : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;Vegetarian,Vegan,Gluten-free,No GMOs,No lactose&quot; ... ## $ emb_codes : chr &quot;EMB 78015&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ emb_codes_tags : chr &quot;emb-78015&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ first_packaging_code_geo : chr &quot;48.983333,2.066667&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ cities : logi NA NA NA NA NA NA ... ## $ cities_tags : chr &quot;andresy-yvelines-france&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ purchase_places : chr &quot;Lyon,France&quot; &quot;NSW,Australia&quot; &quot;France&quot; &quot;Madrid,EspaÃ±a&quot; ... ## $ stores : chr &quot;Casino&quot; &quot;&quot; &quot;&quot; &quot;El Corte InglÃ©s&quot; ... ## $ countries : chr &quot;France&quot; &quot;Australia&quot; &quot;France&quot; &quot;EspaÃ±a&quot; ... ## $ countries_tags : chr &quot;en:france&quot; &quot;en:australia&quot; &quot;en:france&quot; &quot;en:spain&quot; ... ## $ countries_en : chr &quot;France&quot; &quot;Australia&quot; &quot;France&quot; &quot;Spain&quot; ... ## $ ingredients_text : chr &quot;Sucre de canne, fraises 40 g, fraises des bois 14 g, gÃ©lifiant : pectines de fruits, jus de citron concentrÃ©.&quot;| __truncated__ &quot;&quot; &quot;Pulpe de pommes 50% , sucre, sirop de glucose, gÃ©lifiant : pectine, acidifiant : acide citrique, arÃ´mes, colo&quot;| __truncated__ &quot;Extracto de soja (78%) (agua, semillas de soja 8,3%), grasas vegetales, jarabe de glucosa, dextrosa, emulsionan&quot;| __truncated__ ... ## $ allergens : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ allergens_en : logi NA NA NA NA NA NA ... ## $ traces : chr &quot;Lait,Fruits Ã coque&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ traces_tags : chr &quot;en:milk,en:nuts&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ traces_en : chr &quot;Milk,Nuts&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ serving_size : chr &quot;15 g&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ no_nutriments : logi NA NA NA NA NA NA ... ## $ additives_n : int 1 NA 2 5 0 NA NA 0 NA 1 ... ## $ additives : chr &quot;[ sucre-de-canne -&gt; fr:sucre-de-canne ] [ sucre-de -&gt; fr:sucre-de ] [ sucre -&gt; fr:sucre ] [ fraises-40-g &quot;| __truncated__ &quot;&quot; &quot;[ pulpe-de-pommes-50 -&gt; fr:pulpe-de-pommes-50 ] [ pulpe-de-pommes -&gt; fr:pulpe-de-pommes ] [ pulpe-de -&gt; fr:&quot;| __truncated__ &quot;[ extracto-de-soja -&gt; es:extracto-de-soja ] [ 78 -&gt; es:78 ] [ agua -&gt; es:agua ] [ semillas-de-soja-8 -&gt; e&quot;| __truncated__ ... ## $ additives_tags : chr &quot;en:e440&quot; &quot;&quot; &quot;en:e440,en:e330&quot; &quot;en:e471,en:e415,en:e407,en:e412,en:e306&quot; ... ## $ additives_en : chr &quot;E440 - Pectins&quot; &quot;&quot; &quot;E440 - Pectins,E330 - Citric acid&quot; &quot;E471 - Mono- and diglycerides of fatty acids,E415 - Xanthan gum,E407 - Carrageenan,E412 - Guar gum,E306 - Tocop&quot;| __truncated__ ... ## $ ingredients_from_palm_oil_n : int 0 NA 0 0 0 NA NA 0 NA 0 ... ## $ ingredients_from_palm_oil : logi NA NA NA NA NA NA ... ## $ ingredients_from_palm_oil_tags : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ ingredients_that_may_be_from_palm_oil_n : int 0 NA 0 1 0 NA NA 0 NA 0 ... ## $ ingredients_that_may_be_from_palm_oil : logi NA NA NA NA NA NA ... ## $ ingredients_that_may_be_from_palm_oil_tags: chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;e471-mono-et-diglycerides-d-acides-gras-alimentaires&quot; ... ## $ nutrition_grade_uk : logi NA NA NA NA NA NA ... ## $ nutrition_grade_fr : chr &quot;d&quot; &quot;&quot; &quot;&quot; &quot;d&quot; ... ## $ pnns_groups_1 : chr &quot;Sugary snacks&quot; &quot;Sugary snacks&quot; &quot;Fruits and vegetables&quot; &quot;unknown&quot; ... ## $ pnns_groups_2 : chr &quot;Sweets&quot; &quot;Chocolate products&quot; &quot;Fruits&quot; &quot;unknown&quot; ... ## $ states : chr &quot;en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be&quot;| __truncated__ &quot;en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-b&quot;| __truncated__ &quot;en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be&quot;| __truncated__ &quot;en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-compl&quot;| __truncated__ ... ## $ states_tags : chr &quot;en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-com&quot;| __truncated__ &quot;en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-c&quot;| __truncated__ &quot;en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-com&quot;| __truncated__ &quot;en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-completed&quot;| __truncated__ ... ## $ states_en : chr &quot;To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characte&quot;| __truncated__ &quot;To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Cha&quot;| __truncated__ &quot;To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characte&quot;| __truncated__ &quot;To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date completed,Characteristic&quot;| __truncated__ ... ## $ main_category : chr &quot;en:plant-based-foods-and-beverages&quot; &quot;en:sugary-snacks&quot; &quot;en:plant-based-foods-and-beverages&quot; &quot;en:plant-based-foods-and-beverages&quot; ... ## $ main_category_en : chr &quot;Plant-based foods and beverages&quot; &quot;Sugary snacks&quot; &quot;Plant-based foods and beverages&quot; &quot;Plant-based foods and beverages&quot; ... ## $ image_url : chr &quot;http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.400.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.400.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.400.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.400.jpg&quot; ... ## $ image_small_url : chr &quot;http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.200.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.200.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.200.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.200.jpg&quot; ... ## $ energy_100g : num 918 NA NA 766 2359 ... ## $ energy_from_fat_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ fat_100g : num 0 NA NA 16.7 45.5 NA NA 25 NA 4 ... ## $ saturated_fat_100g : num 0 NA NA 9.9 5.2 NA NA 17 NA 0.54 ... ## $ butyric_acid_100g : logi NA NA NA NA NA NA ... ## $ caproic_acid_100g : logi NA NA NA NA NA NA ... ## $ caprylic_acid_100g : logi NA NA NA NA NA NA ... ## $ capric_acid_100g : logi NA NA NA NA NA NA ... ## $ lauric_acid_100g : logi NA NA NA NA NA NA ... ## $ myristic_acid_100g : logi NA NA NA NA NA NA ... ## $ palmitic_acid_100g : logi NA NA NA NA NA NA ... ## $ stearic_acid_100g : logi NA NA NA NA NA NA ... ## $ arachidic_acid_100g : logi NA NA NA NA NA NA ... ## $ behenic_acid_100g : logi NA NA NA NA NA NA ... ## $ lignoceric_acid_100g : logi NA NA NA NA NA NA ... ## $ cerotic_acid_100g : logi NA NA NA NA NA NA ... ## $ montanic_acid_100g : logi NA NA NA NA NA NA ... ## $ melissic_acid_100g : logi NA NA NA NA NA NA ... ## $ monounsaturated_fat_100g : num NA NA NA 2.9 9.5 NA NA NA NA NA ... ## $ polyunsaturated_fat_100g : num NA NA NA 3.9 32.8 NA NA NA NA NA ... ## $ omega_3_fat_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ alpha_linolenic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ eicosapentaenoic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ docosahexaenoic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ omega_6_fat_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ linoleic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ arachidonic_acid_100g : logi NA NA NA NA NA NA ... ## $ gamma_linolenic_acid_100g : logi NA NA NA NA NA NA ... ## $ dihomo_gamma_linolenic_acid_100g : logi NA NA NA NA NA NA ... ## $ omega_9_fat_100g : logi NA NA NA NA NA NA ... ## $ oleic_acid_100g : logi NA NA NA NA NA NA ... ## $ elaidic_acid_100g : logi NA NA NA NA NA NA ... ## $ gondoic_acid_100g : logi NA NA NA NA NA NA ... ## $ mead_acid_100g : logi NA NA NA NA NA NA ... ## $ erucic_acid_100g : logi NA NA NA NA NA NA ... ## [list output truncated] This is a large dataset and it is difficult to see what is going on. So let’s try dplyr. # Load dplyr library(dplyr) # View a glimpse of food glimpse(food) ## Observations: 1,500 ## Variables: 160 ## $ V1 &lt;int&gt; 1, 2, 3, 4, 5, 6, 7... ## $ code &lt;int&gt; 100030, 100050, 100... ## $ url &lt;chr&gt; &quot;http://world-en.op... ## $ creator &lt;chr&gt; &quot;sebleouf&quot;, &quot;foodor... ## $ created_t &lt;int&gt; 1424747544, 1450316... ## $ created_datetime &lt;chr&gt; &quot;2015-02-24T03:12:2... ## $ last_modified_t &lt;int&gt; 1438445887, 1450817... ## $ last_modified_datetime &lt;chr&gt; &quot;2015-08-01T16:18:0... ## $ product_name &lt;chr&gt; &quot;Confiture de frais... ## $ generic_name &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;PÃ¢tes de ... ## $ quantity &lt;chr&gt; &quot;265 g&quot;, &quot;375g&quot;, &quot;1... ## $ packaging &lt;chr&gt; &quot;Bocal,Verre&quot;, &quot;Pla... ## $ packaging_tags &lt;chr&gt; &quot;bocal,verre&quot;, &quot;pla... ## $ brands &lt;chr&gt; &quot;Casino DÃ©lices&quot;, ... ## $ brands_tags &lt;chr&gt; &quot;casino-delices&quot;, &quot;... ## $ categories &lt;chr&gt; &quot;Aliments et boisso... ## $ categories_tags &lt;chr&gt; &quot;en:plant-based-foo... ## $ categories_en &lt;chr&gt; &quot;Plant-based foods ... ## $ origins &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Ar... ## $ origins_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;ar... ## $ manufacturing_places &lt;chr&gt; &quot;France&quot;, &quot;Belgium&quot;... ## $ manufacturing_places_tags &lt;chr&gt; &quot;france&quot;, &quot;belgium&quot;... ## $ labels &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Vegeta... ## $ labels_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;en:veg... ## $ labels_en &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Vegeta... ## $ emb_codes &lt;chr&gt; &quot;EMB 78015&quot;, &quot;&quot;, &quot;&quot;... ## $ emb_codes_tags &lt;chr&gt; &quot;emb-78015&quot;, &quot;&quot;, &quot;&quot;... ## $ first_packaging_code_geo &lt;chr&gt; &quot;48.983333,2.066667... ## $ cities &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ cities_tags &lt;chr&gt; &quot;andresy-yvelines-f... ## $ purchase_places &lt;chr&gt; &quot;Lyon,France&quot;, &quot;NSW... ## $ stores &lt;chr&gt; &quot;Casino&quot;, &quot;&quot;, &quot;&quot;, &quot;... ## $ countries &lt;chr&gt; &quot;France&quot;, &quot;Australi... ## $ countries_tags &lt;chr&gt; &quot;en:france&quot;, &quot;en:au... ## $ countries_en &lt;chr&gt; &quot;France&quot;, &quot;Australi... ## $ ingredients_text &lt;chr&gt; &quot;Sucre de canne, fr... ## $ allergens &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,... ## $ allergens_en &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ traces &lt;chr&gt; &quot;Lait,Fruits Ã coq... ## $ traces_tags &lt;chr&gt; &quot;en:milk,en:nuts&quot;, ... ## $ traces_en &lt;chr&gt; &quot;Milk,Nuts&quot;, &quot;&quot;, &quot;&quot;... ## $ serving_size &lt;chr&gt; &quot;15 g&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,... ## $ no_nutriments &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ additives_n &lt;int&gt; 1, NA, 2, 5, 0, NA,... ## $ additives &lt;chr&gt; &quot;[ sucre-de-canne -... ## $ additives_tags &lt;chr&gt; &quot;en:e440&quot;, &quot;&quot;, &quot;en:... ## $ additives_en &lt;chr&gt; &quot;E440 - Pectins&quot;, &quot;... ## $ ingredients_from_palm_oil_n &lt;int&gt; 0, NA, 0, 0, 0, NA,... ## $ ingredients_from_palm_oil &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ ingredients_from_palm_oil_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,... ## $ ingredients_that_may_be_from_palm_oil_n &lt;int&gt; 0, NA, 0, 1, 0, NA,... ## $ ingredients_that_may_be_from_palm_oil &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ ingredients_that_may_be_from_palm_oil_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;e471-m... ## $ nutrition_grade_uk &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ nutrition_grade_fr &lt;chr&gt; &quot;d&quot;, &quot;&quot;, &quot;&quot;, &quot;d&quot;, &quot;... ## $ pnns_groups_1 &lt;chr&gt; &quot;Sugary snacks&quot;, &quot;S... ## $ pnns_groups_2 &lt;chr&gt; &quot;Sweets&quot;, &quot;Chocolat... ## $ states &lt;chr&gt; &quot;en:to-be-checked, ... ## $ states_tags &lt;chr&gt; &quot;en:to-be-checked,e... ## $ states_en &lt;chr&gt; &quot;To be checked,Comp... ## $ main_category &lt;chr&gt; &quot;en:plant-based-foo... ## $ main_category_en &lt;chr&gt; &quot;Plant-based foods ... ## $ image_url &lt;chr&gt; &quot;http://en.openfood... ## $ image_small_url &lt;chr&gt; &quot;http://en.openfood... ## $ energy_100g &lt;dbl&gt; 918, NA, NA, 766, 2... ## $ energy_from_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ fat_100g &lt;dbl&gt; 0.00, NA, NA, 16.70... ## $ saturated_fat_100g &lt;dbl&gt; 0.000, NA, NA, 9.90... ## $ butyric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ caproic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ caprylic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ capric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ lauric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ myristic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ palmitic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ stearic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ arachidic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ behenic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ lignoceric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ cerotic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ montanic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ melissic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ monounsaturated_fat_100g &lt;dbl&gt; NA, NA, NA, 2.9, 9.... ## $ polyunsaturated_fat_100g &lt;dbl&gt; NA, NA, NA, 3.9, 32... ## $ omega_3_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ alpha_linolenic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ eicosapentaenoic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ docosahexaenoic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ omega_6_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ linoleic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ arachidonic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ gamma_linolenic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ dihomo_gamma_linolenic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ omega_9_fat_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ oleic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ elaidic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ gondoic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ mead_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ erucic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ nervonic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ trans_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ cholesterol_100g &lt;dbl&gt; NA, NA, NA, 0.00020... ## $ carbohydrates_100g &lt;dbl&gt; 54.00, NA, NA, 5.70... ## $ sugars_100g &lt;dbl&gt; 54.00, NA, NA, 4.20... ## $ sucrose_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ glucose_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ fructose_100g &lt;int&gt; NA, NA, NA, NA, NA,... ## $ lactose_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ maltose_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ maltodextrins_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ starch_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ polyols_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ fiber_100g &lt;dbl&gt; NA, NA, NA, 0.2, 9.... ## $ proteins_100g &lt;dbl&gt; 0.00, NA, NA, 2.90,... ## $ casein_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ serum_proteins_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ nucleotides_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ salt_100g &lt;dbl&gt; 0.0000000, NA, NA, ... ## $ sodium_100g &lt;dbl&gt; 0.0000000, NA, NA, ... ## $ alcohol_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_a_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ beta_carotene_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_d_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_e_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_k_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_c_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b1_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b2_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_pp_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b6_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b9_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b12_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ biotin_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ pantothenic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ silica_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ bicarbonate_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ potassium_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ chloride_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ calcium_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ phosphorus_100g &lt;dbl&gt; NA, NA, NA, NA, 1.1... ## $ iron_100g &lt;dbl&gt; NA, NA, NA, NA, 0.0... ## $ magnesium_100g &lt;dbl&gt; NA, NA, NA, NA, 0.1... ## $ zinc_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ copper_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ manganese_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ fluoride_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ selenium_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ chromium_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ molybdenum_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ iodine_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ caffeine_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ taurine_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ ph_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ fruits_vegetables_nuts_100g &lt;dbl&gt; 54, NA, NA, NA, NA,... ## $ collagen_meat_protein_ratio_100g &lt;int&gt; NA, NA, NA, NA, NA,... ## $ cocoa_100g &lt;int&gt; NA, NA, NA, NA, NA,... ## $ chlorophyl_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ carbon_footprint_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ nutrition_score_fr_100g &lt;int&gt; 11, NA, NA, 11, 17,... ## $ nutrition_score_uk_100g &lt;int&gt; 11, NA, NA, 11, 17,... # View column names of food names(food) ## [1] &quot;V1&quot; ## [2] &quot;code&quot; ## [3] &quot;url&quot; ## [4] &quot;creator&quot; ## [5] &quot;created_t&quot; ## [6] &quot;created_datetime&quot; ## [7] &quot;last_modified_t&quot; ## [8] &quot;last_modified_datetime&quot; ## [9] &quot;product_name&quot; ## [10] &quot;generic_name&quot; ## [11] &quot;quantity&quot; ## [12] &quot;packaging&quot; ## [13] &quot;packaging_tags&quot; ## [14] &quot;brands&quot; ## [15] &quot;brands_tags&quot; ## [16] &quot;categories&quot; ## [17] &quot;categories_tags&quot; ## [18] &quot;categories_en&quot; ## [19] &quot;origins&quot; ## [20] &quot;origins_tags&quot; ## [21] &quot;manufacturing_places&quot; ## [22] &quot;manufacturing_places_tags&quot; ## [23] &quot;labels&quot; ## [24] &quot;labels_tags&quot; ## [25] &quot;labels_en&quot; ## [26] &quot;emb_codes&quot; ## [27] &quot;emb_codes_tags&quot; ## [28] &quot;first_packaging_code_geo&quot; ## [29] &quot;cities&quot; ## [30] &quot;cities_tags&quot; ## [31] &quot;purchase_places&quot; ## [32] &quot;stores&quot; ## [33] &quot;countries&quot; ## [34] &quot;countries_tags&quot; ## [35] &quot;countries_en&quot; ## [36] &quot;ingredients_text&quot; ## [37] &quot;allergens&quot; ## [38] &quot;allergens_en&quot; ## [39] &quot;traces&quot; ## [40] &quot;traces_tags&quot; ## [41] &quot;traces_en&quot; ## [42] &quot;serving_size&quot; ## [43] &quot;no_nutriments&quot; ## [44] &quot;additives_n&quot; ## [45] &quot;additives&quot; ## [46] &quot;additives_tags&quot; ## [47] &quot;additives_en&quot; ## [48] &quot;ingredients_from_palm_oil_n&quot; ## [49] &quot;ingredients_from_palm_oil&quot; ## [50] &quot;ingredients_from_palm_oil_tags&quot; ## [51] &quot;ingredients_that_may_be_from_palm_oil_n&quot; ## [52] &quot;ingredients_that_may_be_from_palm_oil&quot; ## [53] &quot;ingredients_that_may_be_from_palm_oil_tags&quot; ## [54] &quot;nutrition_grade_uk&quot; ## [55] &quot;nutrition_grade_fr&quot; ## [56] &quot;pnns_groups_1&quot; ## [57] &quot;pnns_groups_2&quot; ## [58] &quot;states&quot; ## [59] &quot;states_tags&quot; ## [60] &quot;states_en&quot; ## [61] &quot;main_category&quot; ## [62] &quot;main_category_en&quot; ## [63] &quot;image_url&quot; ## [64] &quot;image_small_url&quot; ## [65] &quot;energy_100g&quot; ## [66] &quot;energy_from_fat_100g&quot; ## [67] &quot;fat_100g&quot; ## [68] &quot;saturated_fat_100g&quot; ## [69] &quot;butyric_acid_100g&quot; ## [70] &quot;caproic_acid_100g&quot; ## [71] &quot;caprylic_acid_100g&quot; ## [72] &quot;capric_acid_100g&quot; ## [73] &quot;lauric_acid_100g&quot; ## [74] &quot;myristic_acid_100g&quot; ## [75] &quot;palmitic_acid_100g&quot; ## [76] &quot;stearic_acid_100g&quot; ## [77] &quot;arachidic_acid_100g&quot; ## [78] &quot;behenic_acid_100g&quot; ## [79] &quot;lignoceric_acid_100g&quot; ## [80] &quot;cerotic_acid_100g&quot; ## [81] &quot;montanic_acid_100g&quot; ## [82] &quot;melissic_acid_100g&quot; ## [83] &quot;monounsaturated_fat_100g&quot; ## [84] &quot;polyunsaturated_fat_100g&quot; ## [85] &quot;omega_3_fat_100g&quot; ## [86] &quot;alpha_linolenic_acid_100g&quot; ## [87] &quot;eicosapentaenoic_acid_100g&quot; ## [88] &quot;docosahexaenoic_acid_100g&quot; ## [89] &quot;omega_6_fat_100g&quot; ## [90] &quot;linoleic_acid_100g&quot; ## [91] &quot;arachidonic_acid_100g&quot; ## [92] &quot;gamma_linolenic_acid_100g&quot; ## [93] &quot;dihomo_gamma_linolenic_acid_100g&quot; ## [94] &quot;omega_9_fat_100g&quot; ## [95] &quot;oleic_acid_100g&quot; ## [96] &quot;elaidic_acid_100g&quot; ## [97] &quot;gondoic_acid_100g&quot; ## [98] &quot;mead_acid_100g&quot; ## [99] &quot;erucic_acid_100g&quot; ## [100] &quot;nervonic_acid_100g&quot; ## [101] &quot;trans_fat_100g&quot; ## [102] &quot;cholesterol_100g&quot; ## [103] &quot;carbohydrates_100g&quot; ## [104] &quot;sugars_100g&quot; ## [105] &quot;sucrose_100g&quot; ## [106] &quot;glucose_100g&quot; ## [107] &quot;fructose_100g&quot; ## [108] &quot;lactose_100g&quot; ## [109] &quot;maltose_100g&quot; ## [110] &quot;maltodextrins_100g&quot; ## [111] &quot;starch_100g&quot; ## [112] &quot;polyols_100g&quot; ## [113] &quot;fiber_100g&quot; ## [114] &quot;proteins_100g&quot; ## [115] &quot;casein_100g&quot; ## [116] &quot;serum_proteins_100g&quot; ## [117] &quot;nucleotides_100g&quot; ## [118] &quot;salt_100g&quot; ## [119] &quot;sodium_100g&quot; ## [120] &quot;alcohol_100g&quot; ## [121] &quot;vitamin_a_100g&quot; ## [122] &quot;beta_carotene_100g&quot; ## [123] &quot;vitamin_d_100g&quot; ## [124] &quot;vitamin_e_100g&quot; ## [125] &quot;vitamin_k_100g&quot; ## [126] &quot;vitamin_c_100g&quot; ## [127] &quot;vitamin_b1_100g&quot; ## [128] &quot;vitamin_b2_100g&quot; ## [129] &quot;vitamin_pp_100g&quot; ## [130] &quot;vitamin_b6_100g&quot; ## [131] &quot;vitamin_b9_100g&quot; ## [132] &quot;vitamin_b12_100g&quot; ## [133] &quot;biotin_100g&quot; ## [134] &quot;pantothenic_acid_100g&quot; ## [135] &quot;silica_100g&quot; ## [136] &quot;bicarbonate_100g&quot; ## [137] &quot;potassium_100g&quot; ## [138] &quot;chloride_100g&quot; ## [139] &quot;calcium_100g&quot; ## [140] &quot;phosphorus_100g&quot; ## [141] &quot;iron_100g&quot; ## [142] &quot;magnesium_100g&quot; ## [143] &quot;zinc_100g&quot; ## [144] &quot;copper_100g&quot; ## [145] &quot;manganese_100g&quot; ## [146] &quot;fluoride_100g&quot; ## [147] &quot;selenium_100g&quot; ## [148] &quot;chromium_100g&quot; ## [149] &quot;molybdenum_100g&quot; ## [150] &quot;iodine_100g&quot; ## [151] &quot;caffeine_100g&quot; ## [152] &quot;taurine_100g&quot; ## [153] &quot;ph_100g&quot; ## [154] &quot;fruits_vegetables_nuts_100g&quot; ## [155] &quot;collagen_meat_protein_ratio_100g&quot; ## [156] &quot;cocoa_100g&quot; ## [157] &quot;chlorophyl_100g&quot; ## [158] &quot;carbon_footprint_100g&quot; ## [159] &quot;nutrition_score_fr_100g&quot; ## [160] &quot;nutrition_score_uk_100g&quot; There is a lot of information there, there’s some information on what and when information was added (1:9), meta information about food (10:17, 22:27), where it came from (18:21, 28:34), what it’s made of (35:52), nutrition grades (53:54), some unclear (55:63), and some nutritional information (64:159). There are also some duplicates, different pairs of columns that contain duplicate information. There are many columns containing information that you just can’t use. # Define vector of duplicate cols duplicates &lt;- c(4, 6, 11, 13, 15, 17, 18, 20, 22, 24, 25, 28, 32, 34, 36, 38, 40, 44, 46, 48, 51, 54, 65, 158) # Remove duplicates from food: food2 food2 &lt;- food[, -duplicates] # Define useless vector useless &lt;- c(1, 2, 3, 32:41) # Remove useless columns from food2: food3 food3 &lt;- food2[, -useless] Earlier on we saw that there are many columns containing nutritional information in them, identified with a ‘100g’ label in the column name. If we want to use the nutritional information, we can therefore use this to identify those columns. library(stringr) # Create vector of column indices: nutrition nutrition &lt;- str_detect(names(food3), &quot;100g&quot;) # View the number of columns it applies to summary(nutrition) ## Mode FALSE TRUE ## logical 29 94 # View a summary of nutrition columns summary(food3[, nutrition]) ## energy_from_fat_100g fat_100g saturated_fat_100g ## Min. : 0.00 Min. : 0.00 Min. : 0.000 ## 1st Qu.: 35.98 1st Qu.: 0.90 1st Qu.: 0.200 ## Median : 237.00 Median : 6.00 Median : 1.700 ## Mean : 668.41 Mean : 13.39 Mean : 4.874 ## 3rd Qu.: 974.00 3rd Qu.: 20.00 3rd Qu.: 6.500 ## Max. :2900.00 Max. :100.00 Max. :57.000 ## NA&#39;s :1486 NA&#39;s :708 NA&#39;s :797 ## butyric_acid_100g caproic_acid_100g caprylic_acid_100g capric_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## lauric_acid_100g myristic_acid_100g palmitic_acid_100g stearic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## arachidic_acid_100g behenic_acid_100g lignoceric_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## cerotic_acid_100g montanic_acid_100g melissic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## monounsaturated_fat_100g polyunsaturated_fat_100g omega_3_fat_100g ## Min. : 0.00 Min. : 0.400 Min. : 0.033 ## 1st Qu.: 3.87 1st Qu.: 1.653 1st Qu.: 1.300 ## Median : 9.50 Median : 3.900 Median : 3.000 ## Mean :19.77 Mean : 9.986 Mean : 3.726 ## 3rd Qu.:29.00 3rd Qu.:12.700 3rd Qu.: 3.200 ## Max. :75.00 Max. :46.200 Max. :12.400 ## NA&#39;s :1465 NA&#39;s :1464 NA&#39;s :1491 ## alpha_linolenic_acid_100g eicosapentaenoic_acid_100g ## Min. :0.0800 Min. :0.721 ## 1st Qu.:0.0905 1st Qu.:0.721 ## Median :0.1010 Median :0.721 ## Mean :0.1737 Mean :0.721 ## 3rd Qu.:0.2205 3rd Qu.:0.721 ## Max. :0.3400 Max. :0.721 ## NA&#39;s :1497 NA&#39;s :1499 ## docosahexaenoic_acid_100g omega_6_fat_100g linoleic_acid_100g ## Min. :1.09 Min. :0.25 Min. :0.5000 ## 1st Qu.:1.09 1st Qu.:0.25 1st Qu.:0.5165 ## Median :1.09 Median :0.25 Median :0.5330 ## Mean :1.09 Mean :0.25 Mean :0.5330 ## 3rd Qu.:1.09 3rd Qu.:0.25 3rd Qu.:0.5495 ## Max. :1.09 Max. :0.25 Max. :0.5660 ## NA&#39;s :1499 NA&#39;s :1499 NA&#39;s :1498 ## arachidonic_acid_100g gamma_linolenic_acid_100g ## Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## dihomo_gamma_linolenic_acid_100g omega_9_fat_100g oleic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## elaidic_acid_100g gondoic_acid_100g mead_acid_100g erucic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## nervonic_acid_100g trans_fat_100g cholesterol_100g carbohydrates_100g ## Mode:logical Min. :0.0000 Min. :0.0000 Min. : 0.000 ## NA&#39;s:1500 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.: 3.792 ## Median :0.0000 Median :0.0000 Median : 13.500 ## Mean :0.0105 Mean :0.0265 Mean : 27.958 ## 3rd Qu.:0.0000 3rd Qu.:0.0026 3rd Qu.: 55.000 ## Max. :0.1000 Max. :0.4300 Max. :100.000 ## NA&#39;s :1481 NA&#39;s :1477 NA&#39;s :708 ## sugars_100g sucrose_100g glucose_100g fructose_100g ## Min. : 0.00 Mode:logical Mode:logical Min. :100 ## 1st Qu.: 1.00 NA&#39;s:1500 NA&#39;s:1500 1st Qu.:100 ## Median : 4.05 Median :100 ## Mean : 12.66 Mean :100 ## 3rd Qu.: 14.70 3rd Qu.:100 ## Max. :100.00 Max. :100 ## NA&#39;s :788 NA&#39;s :1499 ## lactose_100g maltose_100g maltodextrins_100g starch_100g ## Min. :0.000 Mode:logical Mode:logical Min. : 0.00 ## 1st Qu.:0.250 NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 9.45 ## Median :0.500 Median :39.50 ## Mean :2.933 Mean :30.73 ## 3rd Qu.:4.400 3rd Qu.:42.85 ## Max. :8.300 Max. :71.00 ## NA&#39;s :1497 NA&#39;s :1493 ## polyols_100g fiber_100g proteins_100g casein_100g ## Min. : 8.60 Min. : 0.000 Min. : 0.000 Min. :1.1 ## 1st Qu.:59.10 1st Qu.: 0.500 1st Qu.: 1.500 1st Qu.:1.1 ## Median :67.00 Median : 1.750 Median : 6.000 Median :1.1 ## Mean :56.06 Mean : 2.823 Mean : 7.563 Mean :1.1 ## 3rd Qu.:69.80 3rd Qu.: 3.500 3rd Qu.:10.675 3rd Qu.:1.1 ## Max. :70.00 Max. :46.700 Max. :61.000 Max. :1.1 ## NA&#39;s :1491 NA&#39;s :994 NA&#39;s :710 NA&#39;s :1499 ## serum_proteins_100g nucleotides_100g salt_100g sodium_100g ## Mode:logical Mode:logical Min. : 0.0000 Min. : 0.0000 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 0.0438 1st Qu.: 0.0172 ## Median : 0.4498 Median : 0.1771 ## Mean : 1.1205 Mean : 0.4409 ## 3rd Qu.: 1.1938 3rd Qu.: 0.4700 ## Max. :102.0000 Max. :40.0000 ## NA&#39;s :780 NA&#39;s :780 ## alcohol_100g vitamin_a_100g beta_carotene_100g vitamin_d_100g ## Min. : 0.00 Min. :0.0000 Mode:logical Min. :0e+00 ## 1st Qu.: 0.00 1st Qu.:0.0000 NA&#39;s:1500 1st Qu.:0e+00 ## Median : 5.50 Median :0.0001 Median :0e+00 ## Mean :10.07 Mean :0.0003 Mean :0e+00 ## 3rd Qu.:13.00 3rd Qu.:0.0006 3rd Qu.:0e+00 ## Max. :50.00 Max. :0.0013 Max. :1e-04 ## NA&#39;s :1433 NA&#39;s :1477 NA&#39;s :1485 ## vitamin_e_100g vitamin_k_100g vitamin_c_100g vitamin_b1_100g ## Min. :0.0005 Min. :0 Min. :0.000 Min. :0.0001 ## 1st Qu.:0.0021 1st Qu.:0 1st Qu.:0.002 1st Qu.:0.0003 ## Median :0.0044 Median :0 Median :0.019 Median :0.0004 ## Mean :0.0069 Mean :0 Mean :0.025 Mean :0.0006 ## 3rd Qu.:0.0097 3rd Qu.:0 3rd Qu.:0.030 3rd Qu.:0.0010 ## Max. :0.0320 Max. :0 Max. :0.217 Max. :0.0013 ## NA&#39;s :1478 NA&#39;s :1498 NA&#39;s :1459 NA&#39;s :1478 ## vitamin_b2_100g vitamin_pp_100g vitamin_b6_100g vitamin_b9_100g ## Min. :0.0002 Min. :0.0006 Min. :0.0001 Min. :0e+00 ## 1st Qu.:0.0003 1st Qu.:0.0033 1st Qu.:0.0002 1st Qu.:0e+00 ## Median :0.0009 Median :0.0069 Median :0.0008 Median :1e-04 ## Mean :0.0011 Mean :0.0086 Mean :0.0112 Mean :1e-04 ## 3rd Qu.:0.0013 3rd Qu.:0.0140 3rd Qu.:0.0012 3rd Qu.:2e-04 ## Max. :0.0066 Max. :0.0160 Max. :0.2000 Max. :2e-04 ## NA&#39;s :1483 NA&#39;s :1484 NA&#39;s :1481 NA&#39;s :1483 ## vitamin_b12_100g biotin_100g pantothenic_acid_100g silica_100g ## Min. :0 Min. :0 Min. :0.0000 Min. :8e-04 ## 1st Qu.:0 1st Qu.:0 1st Qu.:0.0007 1st Qu.:8e-04 ## Median :0 Median :0 Median :0.0020 Median :8e-04 ## Mean :0 Mean :0 Mean :0.0027 Mean :8e-04 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0.0051 3rd Qu.:8e-04 ## Max. :0 Max. :0 Max. :0.0060 Max. :8e-04 ## NA&#39;s :1489 NA&#39;s :1498 NA&#39;s :1486 NA&#39;s :1499 ## bicarbonate_100g potassium_100g chloride_100g calcium_100g ## Min. :0.0006 Min. :0.0000 Min. :0.0003 Min. :0.0000 ## 1st Qu.:0.0678 1st Qu.:0.0650 1st Qu.:0.0006 1st Qu.:0.0450 ## Median :0.1350 Median :0.1940 Median :0.0009 Median :0.1200 ## Mean :0.1692 Mean :0.3288 Mean :0.0144 Mean :0.2040 ## 3rd Qu.:0.2535 3rd Qu.:0.3670 3rd Qu.:0.0214 3rd Qu.:0.1985 ## Max. :0.3720 Max. :1.4300 Max. :0.0420 Max. :1.0000 ## NA&#39;s :1497 NA&#39;s :1487 NA&#39;s :1497 NA&#39;s :1449 ## phosphorus_100g iron_100g magnesium_100g zinc_100g ## Min. :0.0430 Min. :0.0000 Min. :0.0000 Min. :0.0005 ## 1st Qu.:0.1938 1st Qu.:0.0012 1st Qu.:0.0670 1st Qu.:0.0009 ## Median :0.3185 Median :0.0042 Median :0.1040 Median :0.0017 ## Mean :0.3777 Mean :0.0045 Mean :0.1066 Mean :0.0016 ## 3rd Qu.:0.4340 3rd Qu.:0.0077 3rd Qu.:0.1300 3rd Qu.:0.0022 ## Max. :1.1550 Max. :0.0137 Max. :0.3330 Max. :0.0026 ## NA&#39;s :1488 NA&#39;s :1463 NA&#39;s :1479 NA&#39;s :1493 ## copper_100g manganese_100g fluoride_100g selenium_100g ## Min. :0e+00 Min. :0 Min. :0 Min. :0 ## 1st Qu.:1e-04 1st Qu.:0 1st Qu.:0 1st Qu.:0 ## Median :1e-04 Median :0 Median :0 Median :0 ## Mean :1e-04 Mean :0 Mean :0 Mean :0 ## 3rd Qu.:1e-04 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 ## Max. :1e-04 Max. :0 Max. :0 Max. :0 ## NA&#39;s :1498 NA&#39;s :1499 NA&#39;s :1498 NA&#39;s :1499 ## chromium_100g molybdenum_100g iodine_100g caffeine_100g ## Mode:logical Mode:logical Min. :0 Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:0 NA&#39;s:1500 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## NA&#39;s :1499 ## taurine_100g ph_100g fruits_vegetables_nuts_100g ## Mode:logical Mode:logical Min. : 2.00 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:11.25 ## Median :42.00 ## Mean :36.88 ## 3rd Qu.:52.25 ## Max. :80.00 ## NA&#39;s :1470 ## collagen_meat_protein_ratio_100g cocoa_100g chlorophyl_100g ## Min. :12.00 Min. :30 Mode:logical ## 1st Qu.:13.50 1st Qu.:47 NA&#39;s:1500 ## Median :15.00 Median :60 ## Mean :15.67 Mean :57 ## 3rd Qu.:17.50 3rd Qu.:70 ## Max. :20.00 Max. :81 ## NA&#39;s :1497 NA&#39;s :1491 ## nutrition_score_fr_100g nutrition_score_uk_100g ## Min. :-12.000 Min. :-12.000 ## 1st Qu.: 1.000 1st Qu.: 0.000 ## Median : 7.000 Median : 6.000 ## Mean : 7.941 Mean : 7.631 ## 3rd Qu.: 15.000 3rd Qu.: 16.000 ## Max. : 28.000 Max. : 28.000 ## NA&#39;s :825 NA&#39;s :825 We can see there are a large number of missing (NA) values. For some variables however, NA is sometimes left as the default where the actual number is zero. This is the case with the sugars_100g column. # Find indices of sugar NA values: missing missing &lt;- is.na(food3$sugars_100g) # Replace NA values with 0 food3$sugars_100g[missing] &lt;- 0 # Create first histogram hist(food3$sugars_100g, breaks = 100) # Create food4 food4 &lt;- food3[food3$sugars_100g != 0, ] # Create second histogram hist(food4$sugars_100g, breaks = 100) Your dataset has information about packaging, but there’s a bit of a problem: it’s stored in several different languages (Spanish, French, and English). The root word for plastic is same in English (plastic), French (plastique), and Spanish (plastico). To get a general idea of how many of these foods are packaged in plastic, you can look through the packaging_tags column for the string “plasti”. # Find entries containing &quot;plasti&quot;: plastic plastic &lt;- str_detect(food3$packaging_tags, &quot;plasti&quot;) # Print the sum of plastic sum(plastic) ## [1] 0 5.5 School Attendance Data In this section we will work with attendance data from public schools in the US, organized by school level and state, during the 2007-2008 academic year. The data contain information on average daily attendance (ADA) as a percentage of total enrollment, school day length, and school year length. # Load the gdata package library(gdata) # Import the spreadsheet: att. NOTE: Requires perl to be installed url &lt;- &#39;http://s3.amazonaws.com/assets.datacamp.com/production/course_1294/datasets/attendance.xls&#39; att &lt;- read.xls(url) # Print the column names names(att) ## [1] &quot;Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08&quot; ## [2] &quot;X&quot; ## [3] &quot;X.1&quot; ## [4] &quot;X.2&quot; ## [5] &quot;X.3&quot; ## [6] &quot;X.4&quot; ## [7] &quot;X.5&quot; ## [8] &quot;X.6&quot; ## [9] &quot;X.7&quot; ## [10] &quot;X.8&quot; ## [11] &quot;X.9&quot; ## [12] &quot;X.10&quot; ## [13] &quot;X.11&quot; ## [14] &quot;X.12&quot; ## [15] &quot;X.13&quot; ## [16] &quot;X.14&quot; ## [17] &quot;X.15&quot; # Print the first 6 rows head(att) ## Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08 ## 1 ## 2 ## 3 1 ## 4 United States ........ ## 5 Alabama ................. ## 6 Alaska .................. ## X ## 1 Total elementary, secondary, and combined elementary/secondary schools ## 2 ADA as percent of enrollment ## 3 2 ## 4 93.1 ## 5 93.8 ## 6 89.9 ## X.1 X.2 X.3 X.4 ## 1 ## 2 Average hours in school day Average days in school year ## 3 3 4 ## 4 (0.22) 6.6 (0.02) 180 ## 5 (1.24) 7.0 (0.07) 180 ## 6 (1.22) 6.5 (0.05) 180 ## X.5 X.6 X.7 X.8 ## 1 Elementary schools ## 2 Average hours in school year ADA as percent of enrollment ## 3 5 6 ## 4 (0.1) 1,193 (3.1) 94.0 ## 5 (0.8) 1,267 (12.3) 93.8 ## 6 (3.4) 1,163 (22.9) 91.3 ## X.9 X.10 X.11 X.12 ## 1 Secondary schools ## 2 Average hours in school day ADA as percent of enrollment ## 3 7 8 ## 4 (0.27) 6.7 (0.02) 91.1 ## 5 (1.84) 7.0 (0.08) 94.6 ## 6 (1.56) 6.5 (0.05) 93.2 ## X.13 X.14 X.15 ## 1 ## 2 Average hours in school day ## 3 9 ## 4 (0.43) 6.6 (0.04) ## 5 (0.38) 7.1 (0.17) ## 6 (1.57) 6.2 (0.15) # Print the last 6 rows tail(att) ## Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08 ## 54 Wisconsin ............... ## 55 Wyoming ................. ## 56 â Not applicable. ## 57 â¡Reporting standards not met (too few cases). ## 58 NOTE: Averages reflect data reported by schools rather than state requirements. School-reported length of day may exceed state requirements, and there is a range of statistical error in reported estimates. Standard errors appear in parentheses. ## 59 SOURCE: U.S. Department of Education, National Center for Education Statistics, Schools and Staffing Survey (SASS), \\\\Public School Questionnaire ## X X.1 X.2 ## 54 95.0 (0.57) 6.9 ## 55 92.4 (1.15) 6.9 ## 56 ## 57 ## 58 ## 59 \\\\ 2003-04 and 2007-08. (This table was prepared June 2011.) ## X.3 X.4 X.5 X.6 X.7 X.8 X.9 X.10 X.11 X.12 X.13 X.14 ## 54 (0.04) 180 (0.7) 1,246 (8.6) 95.4 (0.41) 6.9 (0.05) 93.0 (1.91) 7.0 ## 55 (0.05) 175 (1.3) 1,201 (8.3) 92.2 (1.65) 6.9 (0.05) 92.4 (0.75) 7.0 ## 56 ## 57 ## 58 ## 59 ## X.15 ## 54 (0.14) ## 55 (0.07) ## 56 ## 57 ## 58 ## 59 # Print the structure str(att) ## &#39;data.frame&#39;: 59 obs. of 17 variables: ## $ Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08: Factor w/ 58 levels &quot;&quot;,&quot; United States ........&quot;,..: 1 1 3 2 6 7 8 9 10 11 ... ## $ X : Factor w/ 42 levels &quot;&quot;,&quot;\\\\ 2003-04 and 2007-08. (This table was prepared June 2011.)&quot;,..: 42 41 3 22 28 8 6 14 23 29 ... ## $ X.1 : Factor w/ 45 levels &quot;&quot;,&quot;(0.22)&quot;,&quot;(0.23)&quot;,..: 1 1 1 2 22 21 41 27 14 6 ... ## $ X.2 : Factor w/ 14 levels &quot;&quot;,&quot;3&quot;,&quot;6.2&quot;,&quot;6.3&quot;,..: 1 14 2 7 11 6 5 10 3 11 ... ## $ X.3 : Factor w/ 14 levels &quot;&quot;,&quot;(0.02)&quot;,&quot;(0.03)&quot;,..: 1 1 1 2 7 5 9 6 7 5 ... ## $ X.4 : Factor w/ 15 levels &quot;&quot;,&quot;171&quot;,&quot;172&quot;,..: 1 15 14 10 10 10 11 9 11 2 ... ## $ X.5 : Factor w/ 22 levels &quot;&quot;,&quot;(0.0)&quot;,&quot;(0.1)&quot;,..: 1 1 1 3 10 21 19 4 6 12 ... ## $ X.6 : Factor w/ 48 levels &quot;&quot;,&quot;1,102&quot;,&quot;1,117&quot;,..: 1 48 47 26 45 15 13 36 5 28 ... ## $ X.7 : Factor w/ 48 levels &quot;&quot;,&quot;(10.1)&quot;,&quot;(10.3)&quot;,..: 1 1 1 36 11 35 22 6 13 48 ... ## $ X.8 : Factor w/ 40 levels &quot;&quot;,&quot;6&quot;,&quot;81.0&quot;,..: 40 39 2 22 20 9 5 13 27 25 ... ## $ X.9 : Factor w/ 51 levels &quot;&quot;,&quot;(0.24)&quot;,&quot;(0.25)&quot;,..: 1 1 1 4 32 25 48 35 20 13 ... ## $ X.10 : Factor w/ 14 levels &quot;&quot;,&quot;6.2&quot;,&quot;6.3&quot;,..: 1 14 10 7 11 5 4 9 3 11 ... ## $ X.11 : Factor w/ 13 levels &quot;&quot;,&quot;(0.02)&quot;,&quot;(0.03)&quot;,..: 1 1 1 2 8 5 10 8 5 7 ... ## $ X.12 : Factor w/ 41 levels &quot;&quot;,&quot;8&quot;,&quot;85.8&quot;,..: 41 40 2 18 34 29 7 17 8 19 ... ## $ X.13 : Factor w/ 47 levels &quot;&quot;,&quot;(0.35)&quot;,&quot;(0.37)&quot;,..: 1 1 1 6 4 24 42 34 22 19 ... ## $ X.14 : Factor w/ 17 levels &quot;&quot;,&quot;5.9&quot;,&quot;6.1&quot;,..: 1 17 15 8 13 4 6 10 3 12 ... ## $ X.15 : Factor w/ 29 levels &quot;&quot;,&quot;(0.03)&quot;,&quot;(0.04)&quot;,..: 1 1 1 3 14 12 20 7 17 8 ... In the table there is some metadata placed at the bottom of the table, we can remove this. Also some of the columns don’t contain attendance figures, they contain daily hours in the even odd number columns 3-17, so we can remove these too. # Create remove for the rows remove &lt;- c(3,56:59) # Create att2 att2 &lt;- att[-remove, ] # Create remove for the odd columns remove &lt;- seq(3,17,2) # Create att3 att3 &lt;- att2[, -remove] In this data frame, columns 1, 6, and 7 represent attendance data for US elementary schools, columns 1, 8, and 9 represent data for secondary schools, and columns 1 through 5 represent data for all schools in the US. Each of these should be stored as its own separate data frame and split accordingly. # Subset just elementary schools: att_elem att_elem &lt;- att3[,c(1,6,7)] # Subset just secondary schools: att_sec att_sec &lt;- att3[,c(1,8,9)] # Subset all schools: att4 att4 &lt;- att3[,1:5] Next we can assign column names to the variables, then remove the now un-neccessary first two columns # Define cnames vector cnames &lt;- c(&quot;state&quot;, &quot;avg_attend_pct&quot;, &quot;avg_hr_per_day&quot;, &quot;avg_day_per_yr&quot;, &quot;avg_hr_per_yr&quot;) # Assign column names of att4 colnames(att4) &lt;- cnames # Remove first two rows of att4: att5 att5 &lt;- att4[-c(1,2), ] # View the names of att5 names(att5) ## [1] &quot;state&quot; &quot;avg_attend_pct&quot; &quot;avg_hr_per_day&quot; &quot;avg_day_per_yr&quot; ## [5] &quot;avg_hr_per_yr&quot; Next the state variable has periods (.) for spaces and trailing charecters to pad the length for the field. We can tidy this up. # View the head of att5 head(att5) ## state avg_attend_pct avg_hr_per_day avg_day_per_yr ## 4 United States ........ 93.1 6.6 180 ## 5 Alabama ................. 93.8 7.0 180 ## 6 Alaska .................. 89.9 6.5 180 ## 7 Arizona ................. 89.0 6.4 181 ## 8 Arkansas ................ 91.8 6.9 179 ## 9 California .............. 93.2 6.2 181 ## avg_hr_per_yr ## 4 1,193 ## 5 1,267 ## 6 1,163 ## 7 1,159 ## 8 1,229 ## 9 1,129 # Remove all periods in state column att5$state &lt;- str_replace_all(att5$state, &quot;\\\\.&quot;, &quot;&quot;) # Remove white space around state names att5$state &lt;- str_trim(att5$state) # View the head of att5 head(att5) ## state avg_attend_pct avg_hr_per_day avg_day_per_yr avg_hr_per_yr ## 4 United States 93.1 6.6 180 1,193 ## 5 Alabama 93.8 7.0 180 1,267 ## 6 Alaska 89.9 6.5 180 1,163 ## 7 Arizona 89.0 6.4 181 1,159 ## 8 Arkansas 91.8 6.9 179 1,229 ## 9 California 93.2 6.2 181 1,129 Looking at the first few lines we can see that some of the data types are incorrect - upon import, numerical data has come in as character strings and is currently as factor variables. # View the structure str(att5) ## &#39;data.frame&#39;: 52 obs. of 5 variables: ## $ state : chr &quot;United States&quot; &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; ... ## $ avg_attend_pct: Factor w/ 42 levels &quot;&quot;,&quot;\\\\ 2003-04 and 2007-08. (This table was prepared June 2011.)&quot;,..: 22 28 8 6 14 23 29 5 7 11 ... ## $ avg_hr_per_day: Factor w/ 14 levels &quot;&quot;,&quot;3&quot;,&quot;6.2&quot;,&quot;6.3&quot;,..: 7 11 6 5 10 3 11 6 8 10 ... ## $ avg_day_per_yr: Factor w/ 15 levels &quot;&quot;,&quot;171&quot;,&quot;172&quot;,..: 10 10 10 11 9 11 2 11 11 11 ... ## $ avg_hr_per_yr : Factor w/ 48 levels &quot;&quot;,&quot;1,102&quot;,&quot;1,117&quot;,..: 26 45 15 13 36 5 28 19 31 42 ... # Change columns to numeric using dplyr (one way to acheive numeric conversion) library(dplyr) example &lt;- mutate_at(att5, c(2:5), funs(as.numeric)) # Define vector containing numerical columns: cols cols &lt;- c(2:5) # Use sapply to coerce cols to numeric (another way to convert) att5[, cols] &lt;- sapply(att5[,cols], as.numeric) # View the structure str(att5) ## &#39;data.frame&#39;: 52 obs. of 5 variables: ## $ state : chr &quot;United States&quot; &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; ... ## $ avg_attend_pct: num 22 28 8 6 14 23 29 5 7 11 ... ## $ avg_hr_per_day: num 7 11 6 5 10 3 11 6 8 10 ... ## $ avg_day_per_yr: num 10 10 10 11 9 11 2 11 11 11 ... ## $ avg_hr_per_yr : num 26 45 15 13 36 5 28 19 31 42 ... "],
["introduction-to-data.html", "6 Introduction to Data 6.1 Language of Data 6.2 Observational Studies and Experiments 6.3 Sampling strategies and experimental design", " 6 Introduction to Data Notes taken during/inspired by the Datacamp course ‘Introduction to Data’ by Mine Cetinkaya-Rundel. The supporting textbook is Diez, Barr, and Cetinkaya-Rundel (2015). 6.1 Language of Data The course makes use of the openintro package, accompanying the textbook. Let’s load the package and our first dataset, email50. # Load packages library(&quot;openintro&quot;) library(&quot;dplyr&quot;) # Load data data(email50) # View its structure str(email50) ## &#39;data.frame&#39;: 50 obs. of 21 variables: ## $ spam : num 0 0 1 0 0 0 0 0 0 0 ... ## $ to_multiple : num 0 0 0 0 0 0 0 0 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 4 0 0 0 0 0 1 0 ... ## $ sent_email : num 1 0 0 0 0 0 0 1 1 0 ... ## $ time : POSIXct, format: &quot;2012-01-04 13:19:16&quot; &quot;2012-02-16 20:10:06&quot; ... ## $ image : num 0 0 0 0 0 0 0 0 0 0 ... ## $ attach : num 0 0 2 0 0 0 0 0 0 0 ... ## $ dollar : num 0 0 0 0 9 0 0 0 0 23 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 0 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 1 0 0 0 0 0 ... ## $ num_char : num 21.705 7.011 0.631 2.454 41.623 ... ## $ line_breaks : int 551 183 28 61 1088 5 17 88 242 578 ... ## $ format : num 1 1 0 0 1 0 0 1 1 1 ... ## $ re_subj : num 1 0 0 0 0 0 0 1 1 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 1 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 8 1 2 1 43 0 0 2 22 3 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 2 3 1 2 2 2 2 2 2 2 ... #glimpse the first few items using dplyr glimpse(email50) ## Observations: 50 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0... ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0... ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ cc &lt;int&gt; 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ sent_email &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ time &lt;dttm&gt; 2012-01-04 13:19:16, 2012-02-16 20:10:06, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ attach &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0... ## $ dollar &lt;dbl&gt; 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, ... ## $ winner &lt;fctr&gt; no, no, no, no, no, no, no, no, no, no, no, no, ... ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ password &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0... ## $ num_char &lt;dbl&gt; 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809... ## $ line_breaks &lt;int&gt; 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167... ## $ format &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1... ## $ re_subj &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ exclaim_mess &lt;dbl&gt; 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, ... ## $ number &lt;fctr&gt; small, big, none, small, small, small, small, sm... When using certain functions, such as filters on categorical variables, the way R handles the filtered out variables is to leave the items in as place holders (empty containers), even though the place holder is empty. This can have undesirable effects, particularly if using the filtered object for modelling. We then end up with zero values which are actually filtered out factors. # Subset of emails with big numbers: email50_big email50_big &lt;- email50 %&gt;% filter(number == &quot;big&quot;) # Glimpse the subset glimpse(email50_big) ## Observations: 7 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0 ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ cc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0 ## $ sent_email &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0 ## $ time &lt;dttm&gt; 2012-02-16 20:10:06, 2012-02-04 23:26:09, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ attach &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ dollar &lt;dbl&gt; 0, 0, 3, 2, 0, 0, 0 ## $ winner &lt;fctr&gt; no, no, yes, no, no, no, no ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ password &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 8 ## $ num_char &lt;dbl&gt; 7.011, 10.368, 42.793, 26.520, 6.563, 11.223, 10.613 ## $ line_breaks &lt;int&gt; 183, 198, 712, 692, 140, 512, 225 ## $ format &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ re_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0 ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_mess &lt;dbl&gt; 1, 1, 2, 7, 2, 9, 9 ## $ number &lt;fctr&gt; big, big, big, big, big, big, big # Table of number variable - now we have just 7 values table(email50_big$number) ## ## none small big ## 0 0 7 # Drop levels email50_big$number &lt;- droplevels(email50_big$number) # Another table of number variable table(email50_big$number) ## ## big ## 7 In some instance you want to create a discreet function from a numeric value. That is to say we want to create a categorical value based on some groups of numbers. This can be achived as shown below. Note that when calculating a function R will typically either: Assign a value e.g. med_num_char &lt;- median(email50$num_char) Print a result e.g. median(email50$num_char) But we can do both by adding brackets (med_num_char &lt;- median(email50$num_char)) # Calculate median number of characters: med_num_char (med_num_char &lt;- median(email50$num_char)) ## [1] 6.8895 # Create num_char_cat variable in email50 email50 &lt;- email50 %&gt;% mutate(num_char_cat = ifelse(num_char &lt; med_num_char, &quot;below median&quot;, &quot;at or above median&quot;)) # Count emails in each category table(email50$num_char_cat) ## ## at or above median below median ## 25 25 We can also use the mutate function from dplyr to create a new variable from categorical variables # Create number_yn column in email50 email50 &lt;- email50 %&gt;% mutate(number_yn, ifelse(number == &quot;none&quot;, &quot;no&quot;, &quot;yes&quot;)) # Visualize number_yn ggplot(email50, aes(x = number_yn)) + geom_bar() We often want to compare two or three variables, which is most easily done using the ggplot package # Load ggplot2 library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:openintro&#39;: ## ## diamonds # Scatterplot of exclaim_mess vs. num_char ggplot(email50, aes(x = num_char, y = exclaim_mess, color = factor(spam))) + geom_point() 6.2 Observational Studies and Experiments Typically there are two types of study, if we are interested in whether variable Y is caused by some factors (X) we could have two types of studies. Observational Study: We are observing, rather than specifically interfere or direct how the data is collected - only correlation can be inferred. In this case, we might survey people and look for patterns in their characteristics (X) and the outcome variable (Y) Experimental Study: We randomly assign subjects to various treatments - causation can be inferred. In this case, we would get a group of individuals together then randomly assign them to a group of interest (X), removing the decision from the subjects of the study, we often have a control group also. Another differentiation to be aware of is between Random sampling: We select our subjects at random in order that we can make inferences from our sample, to the wider population Random assignment: Subjects are randomly assigned to various treatments and helps us to make causal conclusions We can therefore combine random sampling with random assignment, to allow causal and generalisable conclusions, however in practice we typically have one or the other - random sampling only (not causal but generalisable), or random assignment (causal but not generalisable) - the negation of both leads to results that are neither causal nor generalisable, but may highlight a need for further research. Sometimes when there are looking for associations between variables, it is possible to omit variables of interest, which may be confounding variables. For instance, we may have two variables (x) that appear to show a relationship with another (y) but the inclusion of a third variable (x’) causes the apparent relationship to breakdown. If we fail to consider other associated variables, we may fall in to a Simpsons Paradox in which a trend appears in different groups, but disappears when the groups are combined together. Simpsons paradox is a form of Ecological Fallacy. One of the best known examples of Simpsons Paradox comes from admissions data for University of California, Berkeley. library(tidyr) data(&quot;UCBAdmissions&quot;) ucb_admit &lt;- as.data.frame(UCBAdmissions) # Restrucutre data - this is to follow the example provided, it takes the aggregated data from the original data frame and disaggregates # it using indexing by repeating the row indices Freq times for each row - see https://stackoverflow.com/questions/45445919/convert-wide-to-long-with-frequency-column ucb_admit_disagg = ucb_admit[rep(1:nrow(ucb_admit), ucb_admit$Freq), -grep(&quot;Freq&quot;, names(ucb_admit))] # Count number of male and female applicants admitted ucb_counts &lt;- ucb_admit_disagg %&gt;% count(Gender, Admit) # View result ucb_counts ## # A tibble: 4 x 3 ## Gender Admit n ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 Male Admitted 1198 ## 2 Male Rejected 1493 ## 3 Female Admitted 557 ## 4 Female Rejected 1278 # Spread the output across columns and calculate percentages ucb_counts %&gt;% spread(Admit, n) %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 2 x 4 ## Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Male 1198 1493 0.4451877 ## 2 Female 557 1278 0.3035422 So far, it seems that the results suggest females are less likely to be admitted, but what if we look at the results by department? # Table of counts of admission status and gender for each department admit_by_dept &lt;- ucb_admit_disagg %&gt;% count(Dept, Gender, Admit) %&gt;% spread(Admit, n) # View result admit_by_dept ## # A tibble: 12 x 4 ## Dept Gender Admitted Rejected ## * &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 A Male 512 313 ## 2 A Female 89 19 ## 3 B Male 353 207 ## 4 B Female 17 8 ## 5 C Male 120 205 ## 6 C Female 202 391 ## 7 D Male 138 279 ## 8 D Female 131 244 ## 9 E Male 53 138 ## 10 E Female 94 299 ## 11 F Male 22 351 ## 12 F Female 24 317 # Percentage of those admitted to each department admit_by_dept %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 12 x 5 ## Dept Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A Male 512 313 0.62060606 ## 2 A Female 89 19 0.82407407 ## 3 B Male 353 207 0.63035714 ## 4 B Female 17 8 0.68000000 ## 5 C Male 120 205 0.36923077 ## 6 C Female 202 391 0.34064081 ## 7 D Male 138 279 0.33093525 ## 8 D Female 131 244 0.34933333 ## 9 E Male 53 138 0.27748691 ## 10 E Female 94 299 0.23918575 ## 11 F Male 22 351 0.05898123 ## 12 F Female 24 317 0.07038123 Now we begin to see that for some departments, there is a higher proportion of females being accpeted than males. Equally for some departments, the rejection rate is very high for both males and females e.g. Dept F. In 4 of the 6 departments, females have a higher proportion of applications being admitted than males. Males tended to apply to less competitive departments than females, the less competitive departments had higher admission rates. 6.3 Sampling strategies and experimental design We use sampling when we do not want, for whatever reason, to conduct a full Census. A simple random sample is usually the most basic method. We can also use stratified sampling to ensure representation from certain groups. Or we use cluster sampling usually for economic reasons. Or some combination in multistage sampling. data(county) # Simple random sample: county_srs county_srs &lt;- county %&gt;% sample_n(size = 20) # Count counties by state county_srs %&gt;% group_by(state) %&gt;% count() ## # A tibble: 15 x 2 ## # Groups: state [15] ## state n ## &lt;fctr&gt; &lt;int&gt; ## 1 Florida 1 ## 2 Georgia 3 ## 3 Illinois 1 ## 4 Massachusetts 1 ## 5 Michigan 1 ## 6 Minnesota 1 ## 7 Mississippi 1 ## 8 Missouri 3 ## 9 Nebraska 1 ## 10 New Jersey 1 ## 11 Ohio 1 ## 12 Oklahoma 1 ## 13 Texas 2 ## 14 West Virginia 1 ## 15 Wisconsin 1 For a stratified sample we would do something similar. # Stratified sample states_str &lt;- us_regions %&gt;% group_by(region) %&gt;% sample_n(size = 2) # Count states by region states_str %&gt;% group_by(region) %&gt;% count() The principles of experimental design include 4 key components: Control: compare treatment of interest to a control group Randomise: randomly assign subjects to treatments Replicate: collect a sufficiently large sample within a study, or replicate the entire study Block: account for the potential effect of confounding variables We group subjects into blocks based on these confounding variables, then randomise within each block to treatment groups. So for instance, if we were testing whether an online or classroom R course was more effective using an experiment, one possible confounding variable would be previous programming experience. Therefore we would seperate out - block - those with and those without previous programming experience, ensuring we have an equal number in each treatment group (online vs classroom) of those with and without previous experience. In random sampling, you use stratifying to control for a variable. In random assignment, you use blocking to achieve the same goal. References "],
["references-1.html", "References", " References "],
["foundations-of-inference.html", "7 Foundations of Inference 7.1 Introduction to Inference 7.2 Home Ownership by Gender 7.3 Density Plots 7.4 Gender Discrimination (p-values) 7.5 Opportunity Cost 7.6 Type I and Type II errors 7.7 Bootstrapping", " 7 Foundations of Inference Notes taken during/inspired by the Datacamp course ‘Foundations of Inference’ by Jo Hardin, collaborators; Nick Carchedi and Tom Jeon. 7.1 Introduction to Inference Classical statistical inference is the process of making claims about a population based on a sample of information. We are making an inference from a small group (sample) to a much larger one (population). We typically have: Null Hypothesis \\(H_{0}\\): What we are researching has no effect Alternate Hypothesis \\(H_{A}\\): What we are researching does have an effect Under the null hypothesis, chance alone is responsible for the results. Under the alternate hypothesis, we reject the null hypothesis, by using statistical techniques that indicate that chance is not responsible for our findings. Hypothesis or statistical testing goes back over 300 years, with the first recorded use by John Arbuthnot: Table 3.1: Statistical Testing Applications Year Person Context 1710 Arbuthnot Sex ratio at birth 1767 Michelle Distribution of stars 1823 Laplace Moon phase and barometric changes 1900 K. Pearson Goodness of fit 1908 Gosset A single mean Source: (Huberty 1993, pg 318) Contemporary statistical testing is a usually that of either Fisher or Neyman-Pearson approaches. Fisher tends to use a single hypothesis test and a p-value strength of evidence test, where as the Neyman-Pearson test will set a critical alpha value and compare the null hypothesis against an alternative hypothesis, rejecting the null if the test statistic is high enough (Huberty 1993, pg 318). The course goes on to say that idea behind statistical inference is to understand samples from a hypothetical population, where the null hypothesis is true - there is no difference between two groups. We can do this by calculating one statistic - for instance the proportion (mean) of a test group who show a positive response when testing a new drug, compared to a placebo control group - for each repeated sample from a population, then work out the difference between these two groups means. With each sample, the mean will change, resulting in a changing difference for each sample. We can then generate a distribution (histogram) of differences, assuming the null hypothesis - that there is no link between drug effectiveness between a test group and a control group - is true. “Generating a distribution of the statistic from the null population gives information about whether the observed data are inconsistent with the null hypothesis”. That is to say, by taking repeated samples and creating a distribution, we can then say whether our observed difference is consistent (within an acceptable value range due to chance) to the null hypothesis. The null samples consist of randomly shuffled drug effectiveness variables (permuted samples from the population), so that the samples don’t have any dependency between the two groups and effectiveness. 7.2 Home Ownership by Gender Data used in the exercises are from NHANES 2009-2012 With Adjusted Weighting. This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. # Load packages library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) library(&quot;NHANES&quot;) library(&quot;oilabs&quot;) # Create bar plot for Home Ownership by Gender ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Relative frequencies&quot;) # Density for SleepHrsNight coloured by SleepTrouble, faceted by HealthGen ggplot(NHANES, aes(x = SleepHrsNight, col = SleepTrouble)) + geom_density(adjust = 2) + facet_wrap(~ HealthGen) Next we want to create a selection for just our variables of interest - rent and owner occupation. # Subset the data: homes homes &lt;- NHANES %&gt;% select(Gender, HomeOwn) %&gt;% filter(HomeOwn %in% c(&quot;Own&quot;, &quot;Rent&quot;)) We build a distribution of differences assuming the null hypothesis - that there is no link between gender and home ownership - is true. In this first step, we just do a single iteration, or permutation from the true values. The null (permuted) version here will create a randomly shuffled home ownership variable, so that the permuted version does not have any dependency between gender and homeownership. We effectively have the same gender split variables as per the original, with the same owned and rented proportions, but disassociated from the gender variable - just randomly shuffled. # Perform one permutation homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 0.003292086 It is easier to see what is going on by breaking the results down iteratively. Our selected and filtered homes dataset looks like. head(homes) ## # A tibble: 6 x 2 ## Gender HomeOwn ## &lt;fctr&gt; &lt;fctr&gt; ## 1 male Own ## 2 male Own ## 3 male Own ## 4 male Own ## 5 female Rent ## 6 male Rent Next we shuffle this data, let’s call it homes 2. we can then check the total number of owns and rents are the same using the summary function, which confirms the data is just randomly shuffled. homes2 &lt;- homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) tail(homes2) ## # A tibble: 6 x 3 ## # Groups: Gender [2] ## Gender HomeOwn HomeOwn_perm ## &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; ## 1 male Rent Own ## 2 male Rent Own ## 3 female Own Own ## 4 male Own Own ## 5 male Own Own ## 6 male Own Own summary(homes2) ## Gender HomeOwn HomeOwn_perm ## female:4890 Own :6425 Own :6425 ## male :4822 Rent :3287 Rent :3287 ## Other: 0 Other: 0 Then we calculate the mean value of home ownership (Own) across our original and shuffled (permutated) data homes3 &lt;- homes2 %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) homes3 ## # A tibble: 2 x 3 ## Gender prop_own_perm prop_own ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 0.6629857 0.6654397 ## 2 male 0.6600995 0.6576109 FFinally we calculate the differences in ownership - note that the difference for the permuted value here may be different from the full code above, as it a new random permutation and we have used the set.seed() function which would create an identical permutation. homes4 &lt;- homes3 %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) homes4 ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 -0.002886141 7.3 Density Plots Next we can make multiple permutations using the rep_sample_n from the oilabs package. We specify the data (tbl), the sample size, the number of samples to take (reps), and whether sampling should be done with or without replacement (replace). The output includes a new column, replicate, which indicates the sample number. We can create 100 permutations and create a dot plot of the results. # Perform 100 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 100) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Dotplot of 100 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_dotplot(binwidth = .001) We can go further and run 1000 permutations and create a density chart. set.seed(666) # Perform 1000 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 1000) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Density plot of 1000 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() Now we have our density plot of the null hypothesis - randomly permuted samples - we can see where our actual observed difference lies, plus how many randomly permuted differences were less than the observed difference. # Plot permuted differences ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences homeown_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 21.5 So in this instance, when we set the seed of 666 we end up with 20.5% of randomly shuffled (permuted) differences being greater than the observed difference, so the observed difference is consistent with the null hypothesis. That it to say it is within the range we may expect by chance alone, were we to repeat the exercise, although we should specify a distribtion we are comparing against, in this which is inferred as being the normal distribution in this instance. We can therefore say that there is no statistically significant difference between gender and home ownership. Or put more formally We fail to reject the null hypothesis: There is no evidence that our data are inconsistent with the null hypothesis 7.4 Gender Discrimination (p-values) In this section we use data from Rosen and Jerdee (1974), where 48 male bank supervisors were given personnel files and asked if they should be promoted to Branch Manager. All files were identical, but half (24) were named as female, and the other half (24) were named male. The results showed 21 males were promoted and 14 females, meaning 35 of the total 48 were promoted. In Rosen and Jerdee (1974) sex was given along with an indication of the difficulty - routine or complex - here we only look at the routine promotion candidates. Do we know if gender is a statistically significant factor? Null Hypothesis \\(H_{0}\\): Gender and promotion are unrelated variables Alternate Hypothesis \\(H_{A}\\): Men are more likely to be promoted First, we create the data frame disc disc &lt;- data.frame( promote = c(rep(&quot;promoted&quot;, 35), rep(&quot;not_promoted&quot;, 13)), sex = c(rep(&quot;male&quot;, 21), rep(&quot;female&quot;, 14), rep(&quot;male&quot;, 3), rep(&quot;female&quot;, 10)) ) Then let’s see the resulting table and proportion who were promoted table(disc) ## sex ## promote female male ## not_promoted 10 3 ## promoted 14 21 disc %&gt;% group_by(sex) %&gt;% summarise(promoted_prop = mean(promote == &quot;promoted&quot;)) ## # A tibble: 2 x 2 ## sex promoted_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 female 0.5833333 ## 2 male 0.8750000 So there difference in promotions by gender is around 0.3 or around 30%, but could this be due to chance? We can create 1000 permutations and compare our observed diffrence to the distribution, plus how many randomly permuted differences were less than the observed difference. # Create a data frame of differences in promotion rates set.seed(42) disc_perm &lt;- disc %&gt;% rep_sample_n(size = nrow(disc), reps = 1000) %&gt;% mutate(prom_perm = sample(promote)) %&gt;% group_by(replicate, sex) %&gt;% summarize(prop_prom_perm = mean(prom_perm == &quot;promoted&quot;), prop_prom = mean(promote == &quot;promoted&quot;)) %&gt;% summarize(diff_perm = diff(prop_prom_perm), diff_orig = diff(prop_prom)) # male - female # Histogram of permuted differences ggplot(disc_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences disc_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 99.3 So here, just 0.5% of the randomly permuted/shuffled results are greater than our observed promotion differences, or 99.5% are lower, so our results are definitely quite extreme. We typically use a 5% cut off, which the course mentions is arbitrary and historic, being attributed to Fisher. So we can say at 0.5% our value is within this critical region, meaning the results are statistically significant - we should not ignore them. We can calculate quantiles of the null statistic using our randomly generated shuffles. disc_perm %&gt;% summarize(q.90 = quantile(diff_perm, p = 0.90), q.95 = quantile(diff_perm, p = 0.95), q.99 = quantile(diff_perm, p = 0.99)) ## # A tibble: 1 x 3 ## q.90 q.95 q.99 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2083333 0.2083333 0.2916667 So here, 95% of our null differences are 0.208 or lower, indeed 99% are 0.292 or lower, so our observed difference of 0.3 is quite extreme - it is in the critical region of the distribution. We can go one step further by calculating the p-value. The p-value is: the probability of observing data as or more extreme than what we actually got given that the null hypothesis is true. disc_perm %&gt;% summarize(mean(diff_orig &lt;= diff_perm)) ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.025 So the p-value here is 0.028 (less than 3 %). If repeat the exercise with smaller and larger number of shuffles we would get different p-values. ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.01 ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.0227 With 100 shuffles our p-value is 0.03, and with 10,000 shuffles our p-value is 0.0235. If we had a two-tailed test - for instance if we said the original research hypothesis had focused on any difference in promotion rates between men and women instead of focusing on whether men are more likely to be promoted than women - we could simple double the p-value. In both cases, the p-value is below or close to the 0.05 (5%) critical value, meaning we can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. However, as both values are close to the critical value, we should indicate that more work should be done. Indeed since the Rosen and Jerdee (1974) study, many further studies have been undertaken and found a similar pattern of discrimination. 7.5 Opportunity Cost In Frederick et al. (2009) their study showed that when potential purchasers were reminded that if they did not buy a particular DVD they could instead save the money, when compared to a control group who were just told they could not buy the DVD, those being reminded of the saving appeared to be more inclined not to make the purchase - 34 in the treatment group did not buy compared to 19 in the control. So our test is setup as: Null Hypothesis \\(H_{0}\\): Reminding students will have no impact on their spending decisions Alternate Hypothesis \\(H_{A}\\): Reminding students will reduce the chance they continue with a purchase We can create a data frame containing the results and find the initial proportions. #create the data frame opportunity &lt;- data.frame( decision = c(rep(&quot;buyDVD&quot;, 97), rep(&quot;nobuyDVD&quot;, 53)), group = c(rep(&quot;control&quot;, 56), rep(&quot;treatment&quot;, 41), rep(&quot;control&quot;, 19), rep(&quot;treatment&quot;, 34)) ) # Tabulate the data opportunity %&gt;% select(decision, group) %&gt;% table() ## group ## decision control treatment ## buyDVD 56 41 ## nobuyDVD 19 34 # Find the proportion who bought the DVD in each group opportunity %&gt;% group_by(group) %&gt;% summarize(buy_prop = mean(decision == &quot;buyDVD&quot;)) ## # A tibble: 2 x 2 ## group buy_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 control 0.7466667 ## 2 treatment 0.5466667 So around 55% of the treatment group - those who were reminded they could save the money - bought the DVD, comapred to 75% of the control group. We can represent this with a bar plot. As before, we can calculate 1000 random shuffles and then compare our difference in proportions, to the distribution of those 1000 samples. And finally, we can calculate the p-value ## # A tibble: 1 x 1 ## `mean(diff_perm &lt;= diff_orig)` ## &lt;dbl&gt; ## 1 0.012 In this instance, of p-value is substantially less than the usual critical value - 0.8% versus the usual value of 5% - so we can can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. Our results would only occur 8 times in 1000 by chance. We can therefore accept the alternative hypothesis (\\(H_{A}\\)) that reminding students does cause them to be less likely to buy a DVD, as they were randomly assigned to the treatment and control groups, therefore any difference is due to the reminder to save. Who can we therefore make the inference to? Our sample was drawn from the student population for the Frederick et al. (2009) study, so we would be able to generalise to that student population however defined, but not to another wider population. 7.6 Type I and Type II errors In our research and conslusions there is a risk that we will be incorrect, we will make an error. The two errors are: Type I error : The null hypothesis (\\(H_{0}\\)) is true, but is rejected. On the basis of the evidence, we have decided to erroneously accept the alternative hypothesis (\\(H_{A}\\)) when in fact the null hypothesis is correct. It is sometimes called a false positive. Type II error : the null hypothesis is false, but erroneously fails to be rejected. On the basis of the evidence, we have failed to accept the alternative hypothesis despite it being correct - an effect that exists in the population. It is sometimes called a false negative. If we return to our previous example, our associated errors would be Type I: There is not a difference in proportions, but the observed difference is big enough to indicate that the proportions are different. Type II: There is a difference in proportions, but the observed difference is not large enough to indicate that the proportions are different. 7.7 Bootstrapping Sometimes we are not neccessarily interested in testing a hypothesis, we are instead interested in making a claim about how our sample can be inferred to a large population. To do so we use confidece intervals. When calculating confidence intervals there is no null hypothesis like in hypothesis testing. We need to understand how samples from our population vary around the parameter of interest. In an ideal world we would take many samples from the population or know what the true value is in the population, but realistically this is not possible, so we use booststrapping. Bootstrapping is the process of taking repeated samples from the same sample, to estimate the variability. As our population parameters are not known, we can use our sample to estimate a simulated population parameter (\\(\\hat{p}*\\)) by repeated sampling. We can then estimate other parameters such as the standard deviation, s.e. and the confidence interval. Instead of taking repeated samples from our population, we take repeated samples from our data, with replacement, each bootstrap sample is the same size as the original sample. Figure 7.1: Illustration of the bootstrap approach on a small sample containing n = 3 observations (James et al. 2013, pg 190) Firstly we setup our single poll, where 70% (21/30) are intended to vote for a particular candidate # Setup our single poll example one_poll &lt;- sample(rep(c(0, 1), times = c(9,21))) one_poll &lt;- tbl_df(one_poll) colnames(one_poll) &lt;- &quot;vote&quot; Next we can create 1000 bootstrap samples from this original poll, then calculate the variability set.seed(42) # Generate 1000 resamples of one_poll: one_poll_boot_30 one_poll_boot_30 &lt;- one_poll %&gt;% rep_sample_n(size = 30, replace = TRUE, reps = 1000) # Compute p-hat* for each resampled poll ex1_props &lt;- one_poll_boot_30 %&gt;% summarize(prop_yes = mean(vote)) %&gt;% summarize(sd(prop_yes)) #compute variability p-hat* ex1_props ## # A tibble: 1 x 1 ## `sd(prop_yes)` ## &lt;dbl&gt; ## 1 0.08624387 So the variability - the standard error or SE - of \\(\\hat{p}*\\) is 0.0841. We can now use this SE to calculate a confidence interval, since 95% of samples will be within +/- 1.96 standard errors of the centre of the distribution assuming a normal distribution \\(N(\\mu, \\sigma ^2)\\). We also use the bootstrap to calculate our bootstrap confidence interval, to give a range of possible values. # Compute p-hat for one poll p_hat &lt;- mean(one_poll$vote) set.seed(42) # Bootstrap to find the SE of p-hat: one_poll_boot one_poll_boot &lt;- one_poll %&gt;% rep_sample_n(30, replace = TRUE, reps = 1000) %&gt;% summarize(prop_yes_boot = mean(vote)) # Create an interval of possible values one_poll_boot %&gt;% summarize(lower = p_hat - 1.96 * sd(prop_yes_boot), upper = p_hat + 1.96 * sd(prop_yes_boot)) ## # A tibble: 1 x 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.530962 0.869038 So our possible range of values, using the bootstrap at 95%, is between 53.2% and 86.8%. Going back to our original statement, we had a single poll where 70% of those polled intended to vote for a particular candidate. We can now say, using the bootstrap t-confidence interval, we are 95% confident that the true proportion planning to vote for the candidate is between 53% and 87%. We are assuming that the distribution is normally distributed \\(N(\\mu, \\sigma ^2)\\). References "],
["references-2.html", "References", " References "],
["exploratory-data-analysis.html", "8 Exploratory Data Analysis 8.1 Categorical Data 8.2 Numerical Data 8.3 Numerical Summaries 8.4 Email Case Study", " 8 Exploratory Data Analysis Notes taken during/inspired by the Datacamp course “Exploratory Data Analysis” by Andrew Bray. 8.1 Categorical Data Common functions when looking at categorical, aka factors variables, are levels(df\\(var) and to get a contigency or xtab table the table(df\\)var1, df$var2). We can also create bar charts to visually represent the data using ggplot. # Read in our dataset thanks to fivethirtyeight https://github.com/fivethirtyeight/data/tree/master/comic-characters comics &lt;- read.csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/comic-characters/dc-wikia-data.csv&quot;, stringsAsFactors = TRUE) comics$name &lt;- as.character(comics$name) # Check levels of align levels(comics$ALIGN) ## [1] &quot;&quot; &quot;Bad Characters&quot; &quot;Good Characters&quot; ## [4] &quot;Neutral Characters&quot; &quot;Reformed Criminals&quot; # Check the levels of gender levels(comics$SEX) ## [1] &quot;&quot; &quot;Female Characters&quot; ## [3] &quot;Genderless Characters&quot; &quot;Male Characters&quot; ## [5] &quot;Transgender Characters&quot; # Create a 2-way contingency table table(comics$ALIGN, comics$SEX) ## ## Female Characters Genderless Characters ## 25 220 0 ## Bad Characters 63 597 11 ## Good Characters 30 953 6 ## Neutral Characters 7 196 3 ## Reformed Criminals 0 1 0 ## ## Male Characters Transgender Characters ## 356 0 ## Bad Characters 2223 1 ## Good Characters 1843 0 ## Neutral Characters 359 0 ## Reformed Criminals 2 0 To simplify an analysis, it often helps to drop levels with small amounts of data. In R, this requires two steps: first filtering out any rows with the levels that have very low counts, then removing these levels from the factor variable with droplevels(). This is because the droplevels() function would keep levels that have just 1 or 2 counts; it only drops levels that don“t exist in a dataset. # Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Remove align level comics &lt;- comics %&gt;% filter(ALIGN != &quot;Reformed Criminals&quot;) %&gt;% droplevels() While a contingency table represents the counts numerically, it“s often more useful to represent them graphically. Here you“ll construct two side-by-side barcharts of the comics data. This shows that there can often be two or more options for presenting the same data. Passing the argument position =”dodge&quot; to geom_bar() says that you want a side-by-side (i.e. not stacked) barchart. # Load ggplot2 library(ggplot2) # Create side-by-side barchart of gender by alignment ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;dodge&quot;) # Create side-by-side barchart of alignment by gender ggplot(comics, aes(x = SEX, fill = ALIGN)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 90)) When creatign tables, it is often easier to look at proportions for patterns rather than counts. We can do this using conditional proportions, by using the prop.table(df_counts, n) where n is the number we want to condition our frequency/count table by, 1 = rows and 2 = columns. tab &lt;- table(comics$ALIGN, comics$SEX) options(scipen = 999, digits = 2) # Print fewer digits prop.table(tab) # Joint proportions (totals in the entire table) ## ## Female Characters Genderless Characters ## 0.00363 0.03192 0.00000 ## Bad Characters 0.00914 0.08661 0.00160 ## Good Characters 0.00435 0.13826 0.00087 ## Neutral Characters 0.00102 0.02843 0.00044 ## ## Male Characters Transgender Characters ## 0.05165 0.00000 ## Bad Characters 0.32250 0.00015 ## Good Characters 0.26737 0.00000 ## Neutral Characters 0.05208 0.00000 prop.table(tab, 2) # Conditional on columns (column totals) ## ## Female Characters Genderless Characters ## 0.200 0.112 0.000 ## Bad Characters 0.504 0.304 0.550 ## Good Characters 0.240 0.485 0.300 ## Neutral Characters 0.056 0.100 0.150 ## ## Male Characters Transgender Characters ## 0.074 0.000 ## Bad Characters 0.465 1.000 ## Good Characters 0.385 0.000 ## Neutral Characters 0.075 0.000 Here we see that approx. 49% of female characters are good, compared to 39% for males. Bar charts can tell dramatically different stories depending on whether they represent counts or proportions and, if proportions, what the proportions are conditioned on. To demonstrate this difference, you“ll construct two barcharts in this exercise: one of counts and one of proportions. # Plot of gender by align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar() # Plot proportion of gender, conditional on align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;fill&quot;) Conditional barchart Now, if you want to break down the distribution of alignment based on gender, you“re looking for conditional distributions. You could make these by creating multiple filtered datasets (one for each gender) or by faceting the plot of alignment based on gender. As a point of comparison, we“ve provided your plot of the marginal distribution of alignment from the last exercise. # Plot of alignment broken down by gender ggplot(comics, aes(x = ALIGN)) + geom_bar() + facet_wrap(~ SEX) 8.2 Numerical Data # Data courtesy of http://www.idvbook.com/teaching-aid/data-sets/ with some variable name modifictions to match those in the exercise library(readxl) cars &lt;- read_excel(&quot;04cars data.xls&quot;, sheet = 1) cars &lt;- cars[-2] # remove variable 2 # Rename vars names(cars) &lt;- c(&quot;name&quot;, &quot;sports_car&quot;, &quot;suv&quot;, &quot;wagon&quot;, &quot;minivan&quot;, &quot;pickup&quot;, &quot;all_wheel&quot;, &quot;rear_wheel&quot;, &quot;msrp&quot;, &quot;dealer_cost&quot;, &quot;eng_size&quot;, &quot;ncyl&quot;, &quot;horsepwr&quot;, &quot;city_mpg&quot;,&quot;hwy_mpg&quot;, &quot;weight&quot;, &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;) # Change data tpyes as needed cars[2:7] &lt;- sapply(cars[2:7],as.logical) cars[c(8:10,12:19)] &lt;- sapply(cars[c(8:10,12:19)],as.integer) ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion # Learn data structure str(cars) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 428 obs. of 19 variables: ## $ name : chr &quot;Acura 3.5 RL 4dr&quot; &quot;Acura 3.5 RL w/Navigation 4dr&quot; &quot;Acura MDX&quot; &quot;Acura NSX coupe 2dr manual S&quot; ... ## $ sports_car : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ suv : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ wagon : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ rear_wheel : int 0 0 0 1 0 0 0 0 0 0 ... ## $ msrp : int 43755 46100 36945 89765 23820 33195 26990 25940 31840 42490 ... ## $ dealer_cost: int 39014 41100 33337 79978 21761 30299 24647 23508 28846 38325 ... ## $ eng_size : num 3.5 3.5 3.5 3.2 2 3.2 2.4 1.8 3 3 ... ## $ ncyl : int 6 6 6 6 4 6 4 4 6 6 ... ## $ horsepwr : int 225 225 265 290 200 270 200 170 220 220 ... ## $ city_mpg : int 18 18 17 17 24 20 22 22 20 20 ... ## $ hwy_mpg : int 24 24 23 24 31 28 29 31 28 27 ... ## $ weight : int 3880 3893 4451 3153 2778 3575 3230 3252 3462 3814 ... ## $ wheel_base : int 115 115 106 100 101 108 105 104 104 105 ... ## $ length : int 197 197 189 174 172 186 183 179 179 180 ... ## $ width : int 72 72 77 71 68 72 69 70 70 70 ... # Create faceted histogram ggplot(cars, aes(x = city_mpg)) + geom_histogram() + facet_wrap(~ suv) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). The mileage of a car tends to be associated with the size of its engine (as measured by the number of cylinders). To explore the relationship between these two variables, you could stick to using histograms, but in this exercise you“ll try your hand at two alternatives: the box plot and the density plot. # Filter cars with 4, 6, 8 cylinders common_cyl &lt;- filter(cars, ncyl %in% c(4,6,8)) # Create box plots of city mpg by ncyl ggplot(common_cyl, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 11 rows containing non-finite values (stat_boxplot). # Create overlaid density plots for same data ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) + geom_density(alpha = .3) ## Warning: Removed 11 rows containing non-finite values (stat_density). Now, turn your attention to a new variable: horsepwr. The goal is to get a sense of the marginal distribution of this variable and then compare it to the distribution of horsepower conditional on the price of the car being less than $25,000. You“ll be making two plots using the”data pipeline&quot; paradigm, where you start with the raw data and end with the plot. In addition to indicating the center and spread of a distribution, a box plot provides a graphical means to detect outliers. You can apply this method to the msrp column (manufacturer“s suggested retail price) to detect if there are unusually expensive or cheap cars. # Create hist cars %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + ggtitle(&quot;ALL Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Create hist of horsepwr for affordable cars cars %&gt;% filter(msrp &lt; 25000) %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ggtitle(&quot;Affordable Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 1 rows containing missing values (geom_bar). # Construct box plot of msrp cars %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() # Exclude outliers from data cars_no_out &lt;- cars %&gt;% filter(msrp &lt; 100000) # Construct box plot of msrp using the reduced dataset cars_no_out %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() Consider two other columns in the cars dataset: city_mpg and width. Which is the most appropriate plot for displaying the important features of their distributions? Remember, both density plots and box plots display the central tendency and spread of the data, but the box plot is more robust to outliers. # Create plot of city_mpg cars %&gt;% ggplot(aes(x = width)) + geom_density() ## Warning: Removed 28 rows containing non-finite values (stat_density). # Create plot of width cars %&gt;% ggplot(aes(x = 1, y = city_mpg)) + geom_boxplot() ## Warning: Removed 14 rows containing non-finite values (stat_boxplot). Faceting is a valuable technique for looking at several conditional distributions at the same time. If the faceted distributions are laid out in a grid, you can consider the association between a variable and two others, one on the rows of the grid and the other on the columns. # Facet hists using hwy mileage and ncyl common_cyl %&gt;% ggplot(aes(x = hwy_mpg)) + geom_histogram() + facet_grid(ncyl ~ suv) + ggtitle(&quot;Faceted heavy mpg histograms by No. of Cyl and Suv&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 11 rows containing non-finite values (stat_bin). 8.3 Numerical Summaries Throughout this chapter, you will use data from gapminder, which tracks demographic data in countries of the world over time. To learn more about it, you can bring up the help file with ?gapminder. For this exercise, focus on how the life expectancy differs from continent to continent. This requires that you conduct your analysis not at the country level, but aggregated up to the continent level. This is made possible by the one-two punch of group_by() and summarize(), a very powerful syntax for carrying out the same analysis on different subsets of the full dataset. library(gapminder) # Create dataset of 2007 data gap2007 &lt;- filter(gapminder, year == 2007) # Compute groupwise mean and median lifeExp gap2007 %&gt;% group_by(continent) %&gt;% summarize(mean(lifeExp), median(lifeExp)) ## # A tibble: 5 x 3 ## continent `mean(lifeExp)` `median(lifeExp)` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 55 53 ## 2 Americas 74 73 ## 3 Asia 71 72 ## 4 Europe 78 79 ## 5 Oceania 81 81 # Generate box plots of lifeExp for each continent gap2007 %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() Let“s extend the powerful group_by() and summarize() syntax to measures of spread. If you”re unsure whether you“re working with symmetric or skewed distributions, it”s a good idea to consider a robust measure like IQR in addition to the usual measures of variance or standard deviation. # Compute groupwise measures of spread gap2007 %&gt;% group_by(continent) %&gt;% summarize(sd(lifeExp), IQR(lifeExp), n()) ## # A tibble: 5 x 4 ## continent `sd(lifeExp)` `IQR(lifeExp)` `n()` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 9.63 11.61 52 ## 2 Americas 4.44 4.63 25 ## 3 Asia 7.96 10.15 33 ## 4 Europe 2.98 4.78 30 ## 5 Oceania 0.73 0.52 2 # Generate overlaid density plots gap2007 %&gt;% ggplot(aes(x = lifeExp, fill = continent)) + geom_density(alpha = 0.3) # Compute stats for lifeExp in Americas gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% summarize(mean(lifeExp), sd(lifeExp)) ## # A tibble: 1 x 2 ## `mean(lifeExp)` `sd(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 74 4.4 # Compute stats for population gap2007 %&gt;% summarize(median(pop), IQR(pop)) ## # A tibble: 1 x 2 ## `median(pop)` `IQR(pop)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10517531 26702008 8.3.1 Transformations In some data, there are different ‘humps’ in the distribution, which are calls the modes or the modality of the dataset Unimode - single mean, this is a normal distribtion Bimodal - two common distributions Multimodal - three modes or more We also should consider whether the distribution is skewed. Right skewed data has a long tail to the right, with the majority of the distribution to the left - we often see this with income distributions. Left skewed has a small number of observations to the left and the majoirty of the distribution to the right. A normal distribution is typically smyterical. Highly skewed distributions can make it very difficult to learn anything from a visualization. Transformations can be helpful in revealing the more subtle structure. Here you’ll focus on the population variable, which exhibits strong right skew, and transform it with the natural logarithm function (log() in R). # Create density plot of old variable gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() # Transform the skewed pop variable gap2007 &lt;- gap2007 %&gt;% mutate(log_pop = log(pop)) # Create density plot of new variable gap2007 %&gt;% ggplot(aes(x = log_pop)) + geom_density() 8.3.2 Outliers It is often useful within a dataset to identify, using a column, whether the data is an outlier, this can be done by using the mutate function e.g. df &lt;- df %&gt;% mutate(is_outlier &gt; value), then filtering and arranging the resulting table e.g. df %&gt;% filter(is_outlier) %&gt;% arrange(desc(value)). We can also use this outlier column to remove the values from a plot e.g. df %&gt;% filter(!is_outlier) %&gt;% ggplot … The determination of the outlier value might be arbitary, or you could use a percentile value (say top or bottom 2%). # Filter for Asia, add column indicating outliers gap_asia &lt;- gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% mutate(is_outlier = lifeExp &lt;50) # Remove outliers, create box plot of lifeExp gap_asia %&gt;% filter(!is_outlier) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() 8.4 Email Case Study The example EDA comes from manually classified 3,900+ emails from the openintro package. Is there an association between spam and the length of an email? You could imagine a story either way: Spam is more likely to be a short message tempting me to click on a link, or *My normal email is likely shorter since I exchange brief emails with my friends all the time. Here, you’ll use the email dataset to settle that question. Begin by bringing up the help file and learning about all the variables with ?email. As you explore the association between spam and the length of an email, use this opportunity to try out linking a dplyr chain with the layers in a ggplot2 object. library(openintro) ## Please visit openintro.org for free statistics materials ## ## Attaching package: &#39;openintro&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## cars ## The following object is masked from &#39;package:ggplot2&#39;: ## ## diamonds ## The following objects are masked from &#39;package:datasets&#39;: ## ## cars, trees # Compute summary statistics email %&gt;% group_by(spam) %&gt;% summarise(median(num_char), IQR(num_char)) ## # A tibble: 2 x 3 ## spam `median(num_char)` `IQR(num_char)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.8 13.6 ## 2 1 1.0 2.8 # Create plot email %&gt;% mutate(log_num_char = log(num_char)) %&gt;% ggplot(aes(x = factor(spam), y = log_num_char)) + geom_boxplot() Let’s look at a more obvious indicator of spam: exclamation marks. exclaim_mess contains the number of exclamation marks in each message. Using summary statistics and visualization, see if there is a relationship between this variable and whether or not a message is spam. Note: when computing the log(0) is -Inf in R, which isn’t a very useful value! You can get around this by adding a small number (like .01) to the quantity inside the log() function. This way, your value is never zero. This small shift to the right won’t affect your results. # Compute center and spread for exclaim_mess by spam email %&gt;% group_by(spam) %&gt;% summarise(mean(exclaim_mess), sd(exclaim_mess)) ## # A tibble: 2 x 3 ## spam `mean(exclaim_mess)` `sd(exclaim_mess)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.5 48 ## 2 1 7.3 80 # Create plot for spam and exclaim_mess email %&gt;% ggplot(aes(log(exclaim_mess)+0.1)) + geom_histogram() + facet_wrap( ~ spam) + ggtitle(&quot;Number of exclamation marks by not-spam vs spam&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1435 rows containing non-finite values (stat_bin). If it was difficult to work with the heavy skew of exclaim_mess, the number of images attached to each email (image) poses even more of a challenge. table(email$image) ## ## 0 1 2 3 4 5 9 20 ## 3811 76 17 11 2 2 1 1 Recall that this tabulates the number of cases in each category (so there were 3811 emails with 0 images, for example). Given the very low counts at the higher number of images, let’s collapse image into a categorical variable that indicates whether or not the email had at least one image. In this exercise, you’ll create this new variable and explore its association with spam. ** Here we deal with zero inflation** by converting the many zero values and the non zeros in to a categorical variable. There are other strategies, such as doing analysis on these two groups seperatley. # Create plot of proportion of spam by image email %&gt;% mutate(has_image = image &gt; 0) %&gt;% ggplot(aes(x = has_image, fill = factor(spam))) + geom_bar(position = &quot;fill&quot;) Sometimes it is neccessary to check if our understanding of the data and how it has been created is correct and if the values we expect are in fact true. In this instance, we check first if the number of charecters in the email is greater than zero (which it should be), then secondly whether images count as attachments using a boolean operator. If image is never greater than attach, we can infer that images are counted as attachments. # Verify that all emails have non-negative values for num_char sum(email$num_char &lt; 0) ## [1] 0 # Test if images count as attachments sum(email$images &gt;= email$attach) ## [1] 0 When you have a specific question about a dataset, you can find your way to an answer by carefully constructing the appropriate chain of R code. For example, consider the following question: “Within non-spam emails, is the typical length of emails shorter for those that were sent to multiple people?” This can be answered with the following chain: email %&gt;% filter(spam == &quot;not-spam&quot;) %&gt;% group_by(to_multiple) %&gt;% summarize(median(num_char)) ## # A tibble: 0 x 2 ## # ... with 2 variables: to_multiple &lt;dbl&gt;, median(num_char) &lt;lgl&gt; The code makes it clear that you are using num_char to measure the length of an email and median() as the measure of what is typical. If you run this code, you’ll learn that the answer to the question is “yes”: the typical length of non-spam sent to multiple people is a bit lower than those sent to only one person. This chain concluded with summary statistics, but others might end in a plot; it all depends on the question that you’re trying to answer. For emails containing the word “dollar”, does the typical spam email contain a greater number of occurrences of the word than the typical non-spam email? Create a summary statistic that answers this question. If you encounter an email with greater than 10 occurrences of the word “dollar”, is it more likely to be spam or not-spam? Create a barchart that answers this question. # Question 1 email %&gt;% filter(dollar &gt; 0) %&gt;% group_by(spam) %&gt;% summarize(median(dollar)) ## # A tibble: 2 x 2 ## spam `median(dollar)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 4 ## 2 1 2 # Question 2 email %&gt;% filter(dollar &gt; 10) %&gt;% ggplot(aes(x = spam)) + geom_bar() Turn your attention to the variable called number. To explore the association between this variable and spam, select and construct an informative plot. For illustrating relationships between categorical variables, you’ve seen # Reorder levels email$number &lt;- factor(email$number, levels = c(&quot;none&quot;, &quot;small&quot;, &quot;big&quot;)) # Construct plot of number ggplot(email, aes(x = number)) + geom_bar() + facet_wrap(~ spam) "],
["correlation-and-regression.html", "9 Correlation and Regression 9.1 Visualizing two variables 9.2 Correlation 9.3 Linear Regression 9.4 Model fit", " 9 Correlation and Regression Notes taken during/inspired by the Datacamp course ‘Correlation and Regression’ by Ben Baumer. 9.1 Visualizing two variables Some common terminology of data includes Response variable a.k.a. y, dependent (usually on the vertical axis if using a scatter plot) Explanatory variable, something you think might be related to the response a.k.a. x, independent, predictor (usually on the horizontal axis) library(openintro) library(ggplot2) library(dplyr) library(tidyr) # load the data data(ncbirths) # Scatterplot of weight vs. weeks ggplot(ncbirths, aes(weeks, weight)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). If it is helpful, you can think of boxplots as scatterplots for which the variable on the x-axis has been discretized. The cut() function takes two arguments: the continuous variable you want to discretize and the number of breaks that you want to make in that continuous variable in order to discretize it. # Boxplot of weight vs. weeks ggplot(data = ncbirths, aes(x = cut(weeks, breaks = 5), y = weight)) + geom_boxplot() 9.1.1 Transformations Here the relationship is hard to see. data(mammals) # Mammals scatterplot ggplot(mammals, aes(BodyWt, BrainWt)) + geom_point() The relationship between two variables may not be linear. In these cases we can sometimes see strange and even inscrutable patterns in a scatterplot of the data. Sometimes there really is no meaningful relationship between the two variables. Other times, a careful transformation of one or both of the variables can reveal a clear relationship. ggplot2 provides several different mechanisms for viewing transformed relationships. The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes. # Scatterplot with coord_trans() ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + geom_point() + coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) # Scatterplot with scale_x_log10() and scale_y_log10() ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + geom_point() + scale_x_log10() + scale_y_log10() 9.1.2 Identifying Outliers It is clear here, using the Using the mlbBat10 dataset, a scatterplot illustrates how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP). data(&quot;mlbBat10&quot;) # Baseball player scatterplot ggplot(mlbBat10, aes(OBP, SLG)) + geom_point() Most of the points are clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty is caused by a few outlying players whose on-base percentages (OBPs) were exceptionally high. These values are present in our dataset only because these players had very few batting opportunities. Both OBP and SLG are known as rate statistics, since they measure the frequency of certain events (as opposed to their count). In order to compare these rates sensibly, it makes sense to include only players with a reasonable number of opportunities, so that these observed rates have the chance to approach their long-run frequencies. In Major League Baseball, batters qualify for the batting title only if they have 3.1 plate appearances per game. This translates into roughly 502 plate appearances in a 162-game season. The mlbBat10 dataset does not include plate appearances as a variable, but we can use at-bats (AB) – which constitute a subset of plate appearances – as a proxy. # Scatterplot of SLG vs. OBP mlbBat10 %&gt;% filter(AB &gt;= 200) %&gt;% ggplot(aes(x = OBP, y = SLG)) + geom_point() # Identify the outlying player mlbBat10 %&gt;% filter(AB &gt;= 200, OBP &lt; 0.2) ## name team position G AB R H 2B 3B HR RBI TB BB SO SB CS OBP ## 1 B Wood LAA 3B 81 226 20 33 2 0 4 14 47 6 71 1 0 0.174 ## SLG AVG ## 1 0.208 0.146 9.2 Correlation We typically calculate the Pearsons aka Pearson product-moment correlation. The cor(x, y) function will compute the Pearson product-moment correlation between variables, x and y. Since this quantity is symmetric with respect to x and y, it doesn’t matter in which order you put the variables. At the same time, the cor() function is very conservative when it encounters missing data (e.g. NAs). The use argument allows you to override the default behavior of returning NA whenever any of the values encountered is NA. Setting the use argument to “pairwise.complete.obs” allows cor() to compute the correlation coefficient for those observations where the values of x and y are both not missing. data(ncbirths) # Compute correlation between the birthweight and mother&#39;s age ncbirths %&gt;% summarize(N = n(), r = cor(mage, weight)) ## N r ## 1 1000 0.05506589 # Compute correlation for all non-missing pairs ncbirths %&gt;% summarize(N = n(), r = cor(weight, weeks, use = &quot;pairwise.complete.obs&quot;)) ## N r ## 1 1000 0.6701013 9.2.1 Anscombe Dataset In 1973, Francis Anscombe famously created four synthetic datasets with remarkably similar numerical properties, but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable, set, that distinguishes the quartet. data(&quot;anscombe&quot;) # Tidy the data for plotting Anscombe &lt;- anscombe %&gt;% mutate(id = seq_len(n())) %&gt;% gather(key, value, -id) %&gt;% separate(key, c(&quot;variable&quot;, &quot;set&quot;), 1, convert = TRUE) %&gt;% mutate(set = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)[set]) %&gt;% spread(variable, value) # Plot the four variants ggplot(data = Anscombe, aes(x = x, y = y)) + geom_point() + facet_wrap(~ set) # Compute statistics for the sets Anscombe %&gt;% group_by(set) %&gt;% summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x,y)) ## # A tibble: 4 x 7 ## set N `mean(x)` `sd(x)` `mean(y)` `sd(y)` `cor(x, y)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11 9 3.316625 7.500909 2.031568 0.8164205 ## 2 2 11 9 3.316625 7.500909 2.031657 0.8162365 ## 3 3 11 9 3.316625 7.500000 2.030424 0.8162867 ## 4 4 11 9 3.316625 7.500909 2.030579 0.8165214 9.3 Linear Regression The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a “best fit” line that cuts through the data in a way that minimizes the distance between the line and the data points. We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we’ll set this argument to the value “lm”. data(bdims) # Scatterplot with regression line ggplot(data = bdims, aes(x = hgt, y = wgt)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) Sometimes it is better to think of the model error as ‘noise’ which we might try to better incorporate by creating a better model. Two facts enable you to compute the slope b1 and intercept b0 of a simple linear regression model from some basic summary statistics. First, the slope can be defined as: b1=rX,Y⋅sYsX where rX,Y represents the correlation (cor()) of X and Y and sX and sY represent the standard deviation (sd()) of X and Y, respectively. Second, the point (x¯,y¯)is always on the least squares regression line, where x¯and y¯denote the average of x and y, respectively. The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (Y) as a function of height (X). N &lt;- c(1507) r &lt;- c(0.7173011) mean_hgt &lt;- c(171.1438) sd_hgt &lt;- c(9.407205) mean_wgt &lt;- c(69.14753) sd_wgt &lt;- c(13.34576) bdims_summary &lt;- data.frame(N, r, mean_hgt, sd_hgt, mean_wgt, sd_wgt) # Print bdims_summary bdims_summary ## N r mean_hgt sd_hgt mean_wgt sd_wgt ## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 # Add slope and intercept bdims_summary %&gt;% mutate(slope = r * sd_wgt / sd_hgt, intercept = mean_wgt - slope * mean_hgt) ## N r mean_hgt sd_hgt mean_wgt sd_wgt slope intercept ## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 1.017617 -105.0112 9.3.1 Regression to the Mean Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While “regression to the mean” and “linear regression” are not the same thing, we will examine them together in this exercise. One way to see the effects of regression to the mean is to compare the heights of parents to their children’s heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches. # Galton data from http://www.math.uah.edu/stat/data/Galton.html Galton &lt;- read.csv(&quot;Galton.csv&quot;) # Height of children vs. height of father Galton %&gt;% filter(Gender == &quot;M&quot;) %&gt;% ggplot(aes(x = Father, y = Height)) + geom_point() + geom_abline(slope = 1, intercept = 0) + geom_smooth(method = &quot;lm&quot;, se = FALSE) # Height of children vs. height of mother Galton %&gt;% filter(Gender == &quot;F&quot;) %&gt;% ggplot(aes(x = Mother, y = Height)) + geom_point() + geom_abline(slope = 1, intercept = 0) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 9.3.2 Fitting linear models While the geom_smooth(method = “lm”) function is useful for drawing linear models on a scatterplot, it doesn’t actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is lm(). This function generally takes two arguments: A formula that specifies the model A data argument for the data frame that contains the data you want to use to fit the model The lm() function return a model object having class “lm”. This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc. # Linear model for weight as a function of height lm(wgt ~ hgt, data = bdims) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Coefficients: ## (Intercept) hgt ## -105.011 1.018 # Linear model for SLG as a function of OBP lm(SLG ~ OBP, data = mlbBat10) ## ## Call: ## lm(formula = SLG ~ OBP, data = mlbBat10) ## ## Coefficients: ## (Intercept) OBP ## 0.009407 1.110323 # Log-linear model for body weight as a function of brain weight lm(log(BodyWt) ~ log(BrainWt), data = mammals) ## ## Call: ## lm(formula = log(BodyWt) ~ log(BrainWt), data = mammals) ## ## Coefficients: ## (Intercept) log(BrainWt) ## -2.509 1.225 An “lm” object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information. The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2R2, adjusted R2R2, and the residual standard error. The summary of an “lm” object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.). Once you have fit a regression model, you are often interested in the fitted values (y^i) and the residuals (ei), where i indexes the observations. The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable. In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the fitted.values() and residuals() functions mod &lt;- lm(wgt ~ hgt, data = bdims) # Show the coefficients coef(mod) ## (Intercept) hgt ## -105.011254 1.017617 # Show the full output summary(mod) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## hgt 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 # Mean of weights equal to mean of fitted values? mean(bdims$wgt) == mean(fitted.values(mod)) ## [1] TRUE # Mean of the residuals mean(residuals(mod)) ## [1] -1.266971e-15 As you fit a regression model, there are some quantities (e.g. R2) that apply to the model as a whole, while others apply to each observation (e.g. y^i). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables. The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals. # Load broom library(broom) # Create bdims_tidy bdims_tidy &lt;- augment(mod) ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead # Glimpse the resulting data frame glimpse(bdims_tidy) ## Observations: 507 ## Variables: 9 ## $ wgt &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.... ## $ hgt &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 18... ## $ .fitted &lt;dbl&gt; 72.05406, 73.37697, 91.89759, 84.77427, 85.48661, 7... ## $ .se.fit &lt;dbl&gt; 0.4320546, 0.4520060, 1.0667332, 0.7919264, 0.81834... ## $ .resid &lt;dbl&gt; -6.4540648, -1.5769666, -11.1975919, -12.1742745, -... ## $ .hat &lt;dbl&gt; 0.002154570, 0.002358152, 0.013133942, 0.007238576,... ## $ .sigma &lt;dbl&gt; 9.312824, 9.317005, 9.303732, 9.301360, 9.312471, 9... ## $ .cooksd &lt;dbl&gt; 5.201807e-04, 3.400330e-05, 9.758463e-03, 6.282074e... ## $ .std.resid &lt;dbl&gt; -0.69413418, -0.16961994, -1.21098084, -1.31269063,... 9.4 Model fit One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are. However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or root mean squared error (RMSERMSE). R calls this quantity the residual standard error. To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. # View summary of model summary(mod) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## hgt 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 # Compute the mean of the residuals mean(residuals(mod)) ## [1] -1.266971e-15 # Compute RMSE sqrt(sum(residuals(mod)^2) / df.residual(mod)) ## [1] 9.30804 Another measure we can use is R squared, whihc is the he coefficient of determination. This gives us the interpretation of R2 as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model. In the example above, our model has an r-squared value of 51.5%. We can also calculate the R-squared value manually if desired. # Compute R-squared bdims_tidy %&gt;% summarize(var_y = var(wgt), var_e = var(.resid)) %&gt;% mutate(R_squared = 1 - var_e / var_y) ## var_y var_e R_squared ## 1 178.1094 86.46839 0.5145208 9.4.1 Unusual points As the model tries to fit the data on average, some extreme values can overly influence the model. We can quantify how much influence a particular point has by using the leverage, which is a measure for each observation as a function of the value of the explanatory variable and the mean of the explanatory variable. Therefore points to the centre line have a low leverage score, whilst points far from the line have a higher leverage. The explanatory variable y does not come in to effect. This can be calculated as the .hat value using augment() from broom. It is possible to have a value with a high leverage but a low overall impact on the model, if the point lies close to the line of the model. In this case, the residual is small for the point. Conversely, a point with a high leverage score and a high residual - a point laying a distance a way from other meaures and not predicted well by the model - does have an impact.We say such a point is influential. Numerically we can use cooks distance (.cooksd)to quantify this influence, which can also be calculated using the augment() function from broom. # Rank points of high leverage mod %&gt;% augment() %&gt;% arrange(desc(.hat)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## wgt hgt .fitted .se.fit .resid .hat .sigma .cooksd ## 1 85.5 198.1 96.57863 1.255712 -11.078629 0.01819968 9.303950 0.0133734319 ## 2 90.9 197.1 95.56101 1.214264 -4.661012 0.01701803 9.314916 0.0022081690 ## 3 49.8 147.2 44.78194 1.131432 5.018065 0.01477545 9.314548 0.0022120570 ## 4 80.7 193.5 91.89759 1.066733 -11.197592 0.01313394 9.303732 0.0097584634 ## 5 95.9 193.0 91.38878 1.046493 4.511216 0.01264027 9.315075 0.0015228117 ## 6 44.8 149.5 47.12245 1.037916 -2.322454 0.01243391 9.316688 0.0003968468 ## .std.resid ## 1 -1.2012024 ## 2 -0.5050673 ## 3 0.5431383 ## 4 -1.2109808 ## 5 0.4877505 ## 6 -0.2510763 # Rank influential points mod %&gt;% augment() %&gt;% arrange(desc(.cooksd)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## wgt hgt .fitted .se.fit .resid .hat .sigma .cooksd ## 1 73.2 151.1 48.75064 0.9737632 24.44936 0.010944356 9.252694 0.03859555 ## 2 116.4 177.8 75.92101 0.5065670 40.47899 0.002961811 9.140611 0.02817388 ## 3 104.1 165.1 62.99728 0.4914889 41.10272 0.002788117 9.135102 0.02733574 ## 4 108.6 190.5 88.84474 0.9464667 19.75526 0.010339372 9.275186 0.02377609 ## 5 67.3 152.4 50.07354 0.9223084 17.22646 0.009818289 9.285305 0.01714950 ## 6 76.8 157.5 55.26339 0.7287405 21.53661 0.006129560 9.267446 0.01661032 ## .std.resid ## 1 2.641185 ## 2 4.355274 ## 3 4.421999 ## 4 2.133444 ## 5 1.859860 ## 6 2.320888 When you have such outlying variables, you need to decide what to do. The main thing is to remove the variables from the model, but you need to consider the implications. There are other statistical techniques (see the EDA Chapter) for removing outliers. Think about whether the scope of the inference changes if you remove those values. Observations can be outliers for a number of different reasons. Statisticians must always be careful—and more importantly, transparent—when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher R2R2 is not a good enough reason! In the mlbBat10 data, the outlier with an OBP of 0.550 is Bobby Scales, an infielder who had four hits in 13 at-bats for the Chicago Cubs. Scales also walked seven times, resulting in his unusually high OBP. The justification for removing Scales here is weak. While his performance was unusual, there is nothing to suggest that it is not a valid data point, nor is there a good reason to think that somehow we will learn more about Major League Baseball players by excluding him. Nevertheless, we can demonstrate how removing him will affect our model. # Create nontrivial_players nontrivial_players &lt;- mlbBat10 %&gt;% filter(AB &gt;= 10 &amp; OBP &lt; 0.5) # Fit model to new data mod_cleaner &lt;- lm(SLG ~ OBP, data = nontrivial_players) # View model summary summary(mod_cleaner) ## ## Call: ## lm(formula = SLG ~ OBP, data = nontrivial_players) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31383 -0.04165 -0.00261 0.03992 0.35819 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.043326 0.009823 -4.411 1.18e-05 *** ## OBP 1.345816 0.033012 40.768 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07011 on 734 degrees of freedom ## Multiple R-squared: 0.6937, Adjusted R-squared: 0.6932 ## F-statistic: 1662 on 1 and 734 DF, p-value: &lt; 2.2e-16 # Visualize new model ggplot(data = nontrivial_players, aes(x = OBP, y = SLG)) + geom_point() + geom_smooth(method = &quot;lm&quot;) 9.4.2 High leverage Points Not all points of high leverage are influential. While the high leverage observation corresponding to Bobby Scales in the previous exercise is influential, the three observations for players with OBP and SLG values of 0 are not influential. This is because they happen to lie right near the regression anyway. Thus, while their extremely low OBP gives them the power to exert influence over the slope of the regression line, their low SLG prevents them from using it. mod &lt;- lm(formula = SLG ~ OBP, data = filter(mlbBat10, AB &gt;= 10)) # Rank high leverage points mod %&gt;% augment() %&gt;% arrange(desc(.hat),desc(.cooksd)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## SLG OBP .fitted .se.fit .resid .hat .sigma ## 1 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 2 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 3 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 4 0.308 0.550 0.69049108 0.009158810 -0.38249108 0.01641049 0.07011360 ## 5 0.000 0.037 0.01152451 0.008770891 -0.01152451 0.01504981 0.07154283 ## 6 0.038 0.038 0.01284803 0.008739031 0.02515197 0.01494067 0.07153800 ## .cooksd .std.resid ## 1 0.0027664282 0.5289049 ## 2 0.0027664282 0.5289049 ## 3 0.0027664282 0.5289049 ## 4 0.2427446800 -5.3943121 ## 5 0.0002015398 -0.1624191 ## 6 0.0009528017 0.3544561 "],
["supervised-learning.html", "10 Supervised Learning 10.1 Tree Based Models 10.2 Gradient Boosting Machines", " 10 Supervised Learning Notes taken during/inspired by the Datacamp course ‘Supervised Learning in R: Regression’ by Nina Zumel and John Mount. 10.1 Tree Based Models Tree based models can be used for both regression and classification models. Decision Trees say ‘if a AND b AND c THEN y’. We can therefore model non-linear models and multiplicative relationships - what is the affect of this AND that when combined together. We can use RMSE as a measure of accuracy of the model. The challenge with tree models is that they are interested in the model space as a whole, splitting this in to regions. Linear models can be better for linear relationships. We can adjust the tree depth, but there is a risk of overfitting (too deep/complex) or underfitting (to shallow/coarse). An ensemble model can be built combining different trees or indeed different models together, which will usually have the outcome of being better than a sinlge tree and less prone to overfitting, but at the loss of interpretability. 10.1.1 Random Forests One example of an ensemble approach is a random forest, building multiple trees from the training data. We can average the results of multiple models together to reduce the degree of overfitting. To build a random forest we perform the following Draw bootstrapped sample from training data For each sample grow a tree At each node, pick best variable to split on (from a random subset of all variables) Continue until tree is grown To score a datum, evaluate it with all the trees and average the results. We can use the ranger package to fit random forests. If the outcome is numeric, ranger will automatically do regression rather than classification. The default is for 500 trees, a minimum approach is 200. The value respect.unordered.factors will handle categorical values, set it to “order” if using cateogrical values, which will convert the values to numeric values. The measures of accuracy are R squared and OOB (Out of Bag or out of sample performance). You should still evaluate the model further using test data. In this exercise you will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July. You will use the ranger package to fit the random forest model. For this exercise, the key arguments to the ranger() call are: formula data num.trees: the number of trees in the forest. respect.unordered.factors : Specifies how to treat unordered factor variables. We recommend setting this to “order” for regression. seed: because this is a random algorithm, you will set the seed to get reproducible results Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in the variables outcome and vars, and use paste() to assemble a string representing the model formula. bikes &lt;- load(url(&quot;https://assets.datacamp.com/production/course_3851/datasets/Bikes.RData&quot;)) # bikesJuly is in the workspace str(bikesJuly) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 ... ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 ... ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ... ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... # Random seed to reproduce results seed &lt;- 423563 # The outcome column (outcome &lt;- &quot;cnt&quot;) ## [1] &quot;cnt&quot; # The input variables (vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;)) ## [1] &quot;hr&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot; ## [6] &quot;atemp&quot; &quot;hum&quot; &quot;windspeed&quot; # Create the formula string for bikes rented as a function of the inputs (fmla &lt;- paste(&quot;cnt&quot;, &quot;~&quot;, paste(vars, collapse = &quot; + &quot;))) ## [1] &quot;cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed&quot; # Load the package ranger library(ranger) # Fit and print the random forest model (bike_model_rf &lt;- ranger(fmla, # formula bikesJuly, # data num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed)) ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 In this exercise you will use the model that you fit in the previous exercise to predict bike rentals for the month of August. The predict() function for a ranger model produces a list. One of the elements of this list is predictions, a vector of predicted values. You can access predictions with the $ notation for accessing named elements of a list: predict(model, data)$predictions library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) # bikesAugust is in the workspace str(bikesAugust) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... ## $ cnt : int 47 33 13 7 4 49 185 487 681 350 ... ## $ instant : int 13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ... ## $ mnth : int 8 8 8 8 8 8 8 8 8 8 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... # bike_model_rf is in the workspace bike_model_rf ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 # Make predictions on the August data bikesAugust$pred &lt;- predict(bike_model_rf, bikesAugust)$predictions # Calculate the RMSE of the predictions bikesAugust %&gt;% mutate(residual = cnt - pred) %&gt;% # calculate the residual summarize(rmse = sqrt(mean(residual^2))) # calculate rmse ## rmse ## 1 97.18347 # Plot actual outcome vs predictions (predictions on x-axis) ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() In the previous exercise, you saw that the random forest bike model did better on the August data than the quasiposson model, in terms of RMSE. In this exercise you will visualize the random forest model’s August predictions as a function of time. The corresponding plot from the quasipoisson model that you built in a previous exercise is in the workspace for you to compare. Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares. library(tidyr) # Plot predictions and cnt by date/time randomforest_plot &lt;- bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% # set start to 0, convert unit to days gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Random Forest plot&quot;) randomforest_plot The random forest model captured the day-to-day variations in peak demand better than the quasipoisson model, but it still underestmates peak demand, and also overestimates minimum demand. So there is still room for improvement. 10.1.2 One-Hot-Encoding Categorical Variables For modelling purposes, we need to convert categorical variables to indicator variables. Some R packages do this automatically, but some non-native R packages, such as the xgboost package does not. So, these categorical variables need to be converted to numeric ones. We can use the vtreat package. DesignTreatmentsZ() to design a treatment plan from the training data, then prepare() to created “clean” data all numerical no missing values use prepare() with treatment plan for all future data In this exercise you will use vtreat to one-hot-encode a categorical variable on a small example. vtreat creates a treatment plan to transform categorical variables into indicator variables (coded “lev”), and to clean bad values out of numerical variables (coded “clean”). To design a treatment plan use the function designTreatmentsZ() treatplan &lt;- designTreatmentsZ(data, varlist) data: the original training data frame varlist: a vector of input variables to be treated (as strings). designTreatmentsZ() returns a list with an element scoreFrame: a data frame that includes the names and types of the new variables: scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code) varName: the name of the new treated variable origName: the name of the original variable that the treated variable comes from code: the type of the new variable. “clean”: a numerical variable with no NAs or NaNs “lev”: an indicator variable for a specific level of the original categorical variable. (magrittr::use_series() is an alias for $ that you can use in pipes.) For these exercises, we want varName where code is either “clean” or “lev”: (newvarlist &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;) %&gt;% magrittr::use_series(varName)) To transform the data set into all numerical and one-hot-encoded variables, use prepare(): data.treat &lt;- prepare(treatplan, data, varRestrictions = newvarlist) treatplan: the treatment plan data: the data frame to be treated varRestrictions: the variables desired in the treated data # Create the dataframe for cleaning color &lt;- c(&quot;b&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;b&quot;, &quot;r&quot;, &quot;g&quot;, &quot;b&quot;, &quot;b&quot;) size &lt;- c(13, 11, 15, 14, 13, 11, 9, 12, 7, 12) popularity &lt;- c(1.0785088, 1.3956245, 0.9217988, 1.2025453, 1.0838662, 0.8043527, 1.1035440, 0.8746332, 0.6947058, 0.8832502) dframe &lt;- cbind(color, size, popularity) dframe &lt;- as.data.frame((dframe)) # dframe is in the workspace dframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 # Create and print a vector of variable names (vars &lt;- c(&quot;color&quot;, &quot;size&quot;)) ## [1] &quot;color&quot; &quot;size&quot; # Load the package vtreat library(vtreat) # Create the treatment plan treatplan &lt;- designTreatmentsZ(dframe, vars) ## [1] &quot;desigining treatments Sun Sep 24 04:51:31 2017&quot; ## [1] &quot;designing treatments Sun Sep 24 04:51:31 2017&quot; ## [1] &quot; have level statistics Sun Sep 24 04:51:31 2017&quot; ## [1] &quot;design var color Sun Sep 24 04:51:31 2017&quot; ## [1] &quot;design var size Sun Sep 24 04:51:31 2017&quot; ## [1] &quot; scoring treatments Sun Sep 24 04:51:31 2017&quot; ## [1] &quot;have treatment plan Sun Sep 24 04:51:31 2017&quot; # Examine the scoreFrame (scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code)) ## varName origName code ## 1 color_lev_x.b color lev ## 2 color_lev_x.g color lev ## 3 color_lev_x.r color lev ## 4 color_catP color catP ## 5 size_lev_x.11 size lev ## 6 size_lev_x.12 size lev ## 7 size_lev_x.13 size lev ## 8 size_lev_x.14 size lev ## 9 size_lev_x.15 size lev ## 10 size_lev_x.7 size lev ## 11 size_lev_x.9 size lev ## 12 size_catP size catP # We only want the rows with codes &quot;clean&quot; or &quot;lev&quot; (newvars &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% magrittr::use_series(varName)) ## [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot; ## [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot; ## [9] &quot;size_lev_x.7&quot; &quot;size_lev_x.9&quot; # Create the treated training data (dframe.treat &lt;- prepare(treatplan, dframe, varRestriction = newvars)) ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 The new indicator variables have ‘lev’ in their names, and the new cleaned continuous variables have ’_clean’ in their names. The treated data is all numerical, with no missing values, and is suitable for use with xgboost and other R modeling functions. When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such novel levels appear, using model.matrix or caret::dummyVars to one-hot-encode will not work correctly. vtreat is a “safer” alternative to model.matrix for one-hot-encoding, because it can manage novel levels safely. vtreat also manages missing values in the data (both categorical and continuous). # Create the testframe for testing new vars color &lt;- c(&quot;g&quot;, &quot;g&quot;, &quot;y&quot;, &quot;g&quot;, &quot;g&quot;, &quot;y&quot;, &quot;b&quot;, &quot;g&quot;, &quot;g&quot;, &quot;r&quot;) size &lt;- c(7, 8, 10, 12, 6, 8, 12, 12, 12, 8) popularity &lt;- c(0.9733920, 0.9122529, 1.4217153, 1.1905828, 0.9866464, 1.3697515, 1.0959387, 0.9161547, 1.0000460, 1.3137360) testframe &lt;- cbind(color, size, popularity) testframe &lt;- as.data.frame((dframe)) # treatplan is in the workspace summary(treatplan) ## Length Class Mode ## treatments 4 -none- list ## scoreFrame 8 data.frame list ## outcomename 1 -none- character ## vtreatVersion 1 package_version list ## outcomeType 1 -none- character ## outcomeTarget 1 -none- character ## meanY 1 -none- logical ## splitmethod 1 -none- character # newvars is in the workspace newvars ## [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot; ## [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot; ## [9] &quot;size_lev_x.7&quot; &quot;size_lev_x.9&quot; # Print dframe and testframe dframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 testframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 # Use prepare() to one-hot-encode testframe (testframe.treat &lt;- prepare(treatplan, testframe, varRestriction = newvars)) ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 vtreat encodes novel colors like yellow that were not present in the data as all zeros: ‘none of the known colors’. This allows downstream models to accept these novel values without crashing. In this exercise you will create one-hot-encoded data frames of the July/August bike data, for use with xgboost later on. vars defines the variable vars with the list of variable columns for the model. # The outcome column (outcome &lt;- &quot;cnt&quot;) ## [1] &quot;cnt&quot; # The input columns (vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;)) ## [1] &quot;hr&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot; ## [6] &quot;atemp&quot; &quot;hum&quot; &quot;windspeed&quot; # Load the package vtreat library(vtreat) # Create the treatment plan from bikesJuly (the training data) treatplan &lt;- designTreatmentsZ(bikesJuly, vars, verbose = FALSE) # Get the &quot;clean&quot; and &quot;lev&quot; variables from the scoreFrame (newvars &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% # get the rows you care about magrittr::use_series(varName)) # get the varName column ## [1] &quot;hr_lev_x.0&quot; ## [2] &quot;hr_lev_x.1&quot; ## [3] &quot;hr_lev_x.10&quot; ## [4] &quot;hr_lev_x.11&quot; ## [5] &quot;hr_lev_x.12&quot; ## [6] &quot;hr_lev_x.13&quot; ## [7] &quot;hr_lev_x.14&quot; ## [8] &quot;hr_lev_x.15&quot; ## [9] &quot;hr_lev_x.16&quot; ## [10] &quot;hr_lev_x.17&quot; ## [11] &quot;hr_lev_x.18&quot; ## [12] &quot;hr_lev_x.19&quot; ## [13] &quot;hr_lev_x.2&quot; ## [14] &quot;hr_lev_x.20&quot; ## [15] &quot;hr_lev_x.21&quot; ## [16] &quot;hr_lev_x.22&quot; ## [17] &quot;hr_lev_x.23&quot; ## [18] &quot;hr_lev_x.3&quot; ## [19] &quot;hr_lev_x.4&quot; ## [20] &quot;hr_lev_x.5&quot; ## [21] &quot;hr_lev_x.6&quot; ## [22] &quot;hr_lev_x.7&quot; ## [23] &quot;hr_lev_x.8&quot; ## [24] &quot;hr_lev_x.9&quot; ## [25] &quot;holiday_clean&quot; ## [26] &quot;workingday_clean&quot; ## [27] &quot;weathersit_lev_x.Clear.to.partly.cloudy&quot; ## [28] &quot;weathersit_lev_x.Light.Precipitation&quot; ## [29] &quot;weathersit_lev_x.Misty&quot; ## [30] &quot;temp_clean&quot; ## [31] &quot;atemp_clean&quot; ## [32] &quot;hum_clean&quot; ## [33] &quot;windspeed_clean&quot; # Prepare the training data bikesJuly.treat &lt;- prepare(treatplan, bikesJuly, varRestriction = newvars) # Prepare the test data bikesAugust.treat &lt;- prepare(treatplan, bikesAugust, varRestriction = newvars) # Call str() on the treated data str(bikesAugust.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday_clean : num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 0 0 1 0 0 0 ... ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Misty : num 0 0 0 0 1 1 0 1 1 1 ... ## $ temp_clean : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp_clean : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum_clean : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed_clean : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... str(bikesJuly.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Misty : num 0 0 0 0 0 0 0 0 0 0 ... ## $ temp_clean : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp_clean : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum_clean : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed_clean : num 0 0.1343 0.0896 0.1343 0.194 ... 10.2 Gradient Boosting Machines Gradient boosting is an interative ensemble method, by improving the model each time. We start the model with a usually shallow tree. Next, we fit another model to the residuals ofd the model, then find the weighted sum of the second and first models that give the best fit. We can regualrise the learning by the factor eta, eta = 1 gives fast learning but with overfitting risk, smaller eta reduces speed of learning but reduces the risk of overfitting. We then repeat this process until the stopping condition is met. Gradient boosting works on the training data, so it can be easy to overfit. The best approach then is to use OOB and cross validation (CV) for each model, then determine how many trees to use. xgb.cv() is the function we use and has a number of diagnostic measures. One such measure is the xgb.cv()$evaluation_log: records estimated RMSE for each round - find the number that minimises the RMSE Inputs to xgb.cv() and xgboost() are: data: input data as matrix ; label: outcome label: vector of outcomes (also numeric) objective: for regression - “reg:linear” nrounds: maximum number of trees to fit eta: learning rate max_depth: depth of trees early_stopping_rounds: after this many rounds without improvement, stop nfold (xgb.cv() only): number of folds for cross validation. 5 is a good number verbose: 0 to stay silent. Then we use elog &lt;- as.data.frame(cv\\(evaluation_log) nrounds &lt;- which.min(elog\\)test_rmse_mean) With the resulting number being the best number of trees. We then use xbgoost with this number (nrounds &lt;- n) to get the final model. In this exercise you will get ready to build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July. The July data is loaded into your workspace. Remember that bikesJuly.treat no longer has the outcome column, so you must get it from the untreated data: bikesJuly$cnt. You will use the xgboost package to fit the random forest model. The function xgb.cv() uses cross-validation to estimate the out-of-sample learning error as each new tree is added to the model. The appropriate number of trees to use in the final model is the number that minimizes the holdout RMSE. # The July data is in the workspace ls() ## [1] &quot;bike_model_rf&quot; &quot;bikes&quot; &quot;bikesAugust&quot; ## [4] &quot;bikesAugust.treat&quot; &quot;bikesJuly&quot; &quot;bikesJuly.treat&quot; ## [7] &quot;color&quot; &quot;dframe&quot; &quot;dframe.treat&quot; ## [10] &quot;fmla&quot; &quot;newvars&quot; &quot;outcome&quot; ## [13] &quot;popularity&quot; &quot;randomforest_plot&quot; &quot;scoreFrame&quot; ## [16] &quot;seed&quot; &quot;size&quot; &quot;testframe&quot; ## [19] &quot;testframe.treat&quot; &quot;treatplan&quot; &quot;vars&quot; # Load the package xgboost library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice # Run xgb.cv cv &lt;- xgb.cv(data = as.matrix(bikesJuly.treat), label = bikesJuly$cnt, nrounds = 100, nfold = 5, objective = &quot;reg:linear&quot;, eta = 0.3, max_depth = 6, early_stopping_rounds = 10, verbose = 0 # silent ) # Get the evaluation log elog &lt;- as.data.frame(cv$evaluation_log) # Determine and print how many trees minimize training and test error elog %&gt;% summarize(ntrees.train = which.min(elog$train_rmse_mean), # find the index of min(train_rmse_mean) ntrees.test = which.min(elog$test_rmse_mean)) # find the index of min(test_rmse_mean) ## ntrees.train ntrees.test ## 1 86 76 In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test error starts to increase. It’s important to use cross-validation to find the right number of trees (as determined by ntrees.test) and avoid an overfit model. # The number of trees to use, as determined by xgb.cv ntrees &lt;- 84 # Run xgboost bike_model_xgb &lt;- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix label = bikesJuly$cnt, # column of outcomes nrounds = ntrees, # number of trees to build objective = &quot;reg:linear&quot;, # objective eta = 0.3, depth = 6, verbose = 0 # silent ) # Make predictions bikesAugust$pred &lt;- predict(bike_model_xgb, as.matrix(bikesAugust.treat)) # Plot predictions (on x axis) vs actual bike rental count ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() Overall, the scatterplot looked pretty good, but did you notice that the model made some negative predictions? In the next exercise, you’ll compare this model’s RMSE to the previous bike models that you’ve built. Finally we can calculate the RMSE # Calculate RMSE bikesAugust %&gt;% mutate(residuals = cnt - pred) %&gt;% summarize(rmse = sqrt(mean(residuals ^ 2))) ## rmse ## 1 76.36407 Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous model. Perhaps rounding negative predictions up to zero is a reasonable tradeoff. Finally we can compare the results graphically. randomforest_plot # Plot predictions and actual bike rentals as a function of time (days) bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% # set start to 0, convert unit to days gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Gradient Boosting model&quot;) We can also plot the importance of the top factors names &lt;- dimnames(data.matrix(bikesJuly.treat[,-1]))[[2]] importance_matrix &lt;- xgb.importance(names, model = bike_model_xgb) xgb.plot.importance(importance_matrix[1:10,]) Looking at the results indicates that the temperature and clear/partly cloudy and the two most important factors, followed by the windspeed. The other factors relate to the time of day - higher at commuting times (9-10 am and 6-7 pm) and lower at night (2 and 4 am). "],
["dimensional-modelling.html", "11 Dimensional Modelling 11.1 Introduction to Dimensional Data 11.2 Architecture considerations 11.3 Graphical Representations 11.4 Kimball Approach 11.5 Four-Step Dimensional Design Process 11.6 Tips", " 11 Dimensional Modelling 11.1 Introduction to Dimensional Data Dimensional modelling helps to build the ability for users to query the information, for instance analysing results by a geographic region. Multi-dimensional modelling is an extension to allowing multiple ways to analsye the information, by geographic region but also over time, by product or service, by store or office and so on. It provides a way for a system user, manager or analyst to navigate what information - the ‘information space’ - is available in a database or data warehouse, but at a more intuitive level (see Meridith 2017, lecture 4). The goal is to help understanding, exploration and to make better decisions. A dimension is simply a direction, usually query or analytically based, in which you can move. Dimensional modelling is different from Entity Relationship diagrams which are more typically used for database design, however they do share some similarities and are sometimes used for dimensional modelling particuarly by those from a database or IT background. The dimensions used therefore become the ways in which the end user wants to query the information. Typical terms used in the BI arena for helping to navigate this ‘information space’ include; ‘slice and dice’ meaning to make a large data space in to a smaller one (you are making a selection or subset of all the available data), ‘drill down’ meaning to go in to a lower level of a hierachy (moving from a geographic region to a particular store), ‘drill up’ meaning to go in to a higher level (sometimes called rolling-up) and ‘drill across’ meaning adding more data (or facts) about something, typically from another source (a different fact table). There are two slightly different interpretations of a dimensional model (Meridith 2017, lecture 4): OLAP: A dimension is a structural attribute of a data cube. A dimension acts as an index for identifying values in a multi-dimensional array Kimball: A dimension table are where the textual descriptions which relate to aspects of the business are stored In both instances however, they provide ways to interact and understand our information. There are two things we are typically trying to map: Facts: Data itself, values, sales and so on e.g. a sales transaction number and the products sold Dimensions: Different ways of presenting or quering the information, this is often in the form of attributes about the fact e.g. product specific and store details 11.1.1 Data Modelling levels There are three aspects of information with a Business Intelligence system - conceptual, logical and physical - which exist on a spectrum. Conceptual: The business needs are usually the high level conceptual solution, what things we want to include at a more abstract level Logical: We start thinking about what data to include in the model and what data is available, it starts giving something which can be implemented in to a warehouse Physical: The final solution which is usually then what is implemented in the data warehouse. It is the more technical/IT solution and may include normalisation (3NF or higher) and perhaps other database optimisations to improve performance of the system. In some instances, the conceptual and logical can become one and the same thing. Table 3.1: The three levels of data modelling Feature Conceptual Logical Logical Entity Names Y Y Entity Relationships Y Y Attributes Y Primary Keys Y Y Foreign Keys Y Y Table Names Y Column Names Y Column Data Types 11.2 Architecture considerations There are a number of different approaches to implementing a data warehouse, or Enterprise Data Warehouse (EDW) from the IT or technical perspective. However, all approaches use the dimensional data modelling technique. A full detailed explanation of all possible architecture approaches, including hybrid approaches, is not included here. Instead we discuss at a high level the three main approaches - Kimball Inmon and Data Valut - and touch on a couple of others. Where they differ in terms of data modelling in part depends on the location of the dimensional model. Kimball - as the last part of the Extract Transform and Load (ETL) process the data is structured and loaded in to the desired dimensional model(s). There is no EDW in the Kimball approach, instead the presentation area is where data is organized, stored, and made available for direct querying by users, report writers, and other analytical BI applications. Data is stored in the multi-dimensional views as different data marts, which are typically subsets of all the data originally extracted, perhaps for different business users or services Inmon - suggests that the data should be relationally designed. The data is stored in an EDW in third normal form (3NF). The dimensional model then transates the data from the EDW in to something for an end user, visualisation tool or other such BI tool, potentially including data marts. A Hub and Spoke system is often used to describe the approach, with the EDW being the hub and the spokes being the depdendent data marts. This helps to ensure a ‘single verison of the truth’ Data Vault - Centralised approach - similar to Inmon but without the dependent data marts (spokes). Users directly target the EDW and there may be many different dimensional data models Hybrid - there are various different ways this could be setup, however one way would be that data is still stored in the EDW, but the dimensional model is used to help structure the data in the EDW. Therefore the extra translation required from the EDW to a BI tool is reduced. In the Kimball approach when attributes in separate dimension tables have the same column names and domain contents. After validating the data for conformance with the defined one-to-one and many-to-one business rules [as part of the ETL processs], it may be pointless to take the final step of building a 3NF physical database, just before transforming the data once again into denormalized structures for the BI presentation area. (Kimball and Ross 2013, pg 20) For the kimball approach to work, so called ‘conformed dimensions’ must be developed which are said to conform when attributes in separate dimension tables have the same column names and domain contents (Kimball and Ross 2013, pg 51). Inmon sees that the dimensional modelling technique can cause problems when teams need different star schemas - dimensional models - which then lead to a need to combine the different joins together, or lead to issues of duplication and inconsistencies. simply doing dimensional modeling as a basis for data warehouse design leads down a dark path when multiple star joins are considered. It is never apparent that there is a problem with star joins when you are looking at just one star join. But when you look at multiple star joins, the limitations of dimensional modeling become apparent. (Inmon 2000) Inmon concludes that dimensional modelling is only really suitable for data marts (ibid). 11.3 Graphical Representations Figure 11.1: High Level Overview of a Data Warehouse (Schnider, Martino, and Eschermann 2014, pg 3) Figure 11.2: Star schema versus OLAP cube (Kimball and Ross 2013, pg 9) Figure 11.3: Star schema example (Kimball and Ross 2013, pg 16) Figure 11.4: Star and Snowflake Schemas (Sharda, Delan, and Turban 2014, pg 139) Figure 11.5: Example slices from a OLAP data cube (Sharda, Delan, and Turban 2014, pg 141) Figure 11.6: Star schema reporting (Kimball and Ross 2013, pg 17) 11.4 Kimball Approach Before work begins of the data modelling, it is neccessary to understand the needs of the business and the underlying data (Kimball and Ross 2013, pg 37). The business needs arise out of meetings with manangers, decision makers and other representatives of the business. Kimball also recommends meetings with ‘source system experts and doing high-level data profiling to assess data feasibilities’ (Kimball and Ross 2013, pg 38). Whilst the data modeller is ‘in charge’ the actual model should unfold via a series of interactive workshops with those business representatives. Data governance reps should also be involved to obtain buy-in. In this sense, the Kimball approach covers both the conceptual and physical, it may also include some considerations of physical level at initiation. 11.5 Four-Step Dimensional Design Process Kimball outlines four key decisions that are to be made during the design of a dimensional model include: Select the business process - the operational activities done by the business, these activities create the facts Declare the grain - what a single row represents. The atomic grain is the lowest data captured by the business, which is the ideal and can be aggregared (rolled-up) to other levels. Different grains must not be mixed in the same fact table Identify the dimensions - the descriptive attributes about the facts, to be used for analysis. Provide the “who, what, where, when, why, and how” (6W) context Identify the facts - the measurements (how many) from the business process, it should relate to a physical observable event, rather than reporting needs Typically the output of this process is a star schema, with a fact table at the centre supported by the associated dimension tables, with primary/forenigh key relationships. This is often then structured into a online analytical processing (OLAP) cube, which contains the facts and dimensions appropriate to the analysis, but allows for more detailed analytical capabilities than SQL. Sometimes aggregated fact tables are built to speed up query performance, as are aggregated OLAP cubes which are typically designed for users. A key advantage of the dimensional model approach is that new dimensions can be added to an existing fact table by adding a new foreign key column. Discussion then of * Thomsen diagrams OLAP Solutions (2nd ed) 2002, an abstract, but can be a little simple * ADAPT Diagrams, White Paper Bulos and Foresman - included in some Microsoft products such as Visio and SQL Server, a bit too technical but good for communicating to IT * BEAM/Agile approach 8 mins 11.6 Tips Think about the types of analysis or questions that the user or manager may want to ask. This will help structure the data and help to ensure nothing is missing At the same time, just because something exists in the organisation or in a data source does not mean it has to be included. You need to think about that to include and what to exclude Equally, there may be instances where there is a desire to add something in to the model but it does not currently exist. This should be flagged and discussed with those intending to use the BI tool / output What are the end uses of the system or systems? If there are potentially multiple systems, multiple teams and multiple views on the data, it may make sense to store the data in its original state (3NF) in the EDW or similar store, then do the dimensional mapping in the BI tool, so it can be customised to the audience (Meridith 2017, lecture 4). This can lead to some duplication, however an option might be to share the dimensional models in some central repository, allowing users to customise for their use, whilst still being able to share the same source data and the benefits this brings. Evidently this lends itself to an Inmon or other such approach and less so the Kimball approach References "],
["references-3.html", "References", " References "]
]
