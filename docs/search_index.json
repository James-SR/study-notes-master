[
["index.html", "Study notes Preface", " Study notes James Solomon-Rounce Last updated:2017-11-10 Preface The following notes were taken by me for educational, non-commercial, purposes. If you find the information useful, buy the material/take the course. Thank you to the original content providers. Additional ramblings are my own. "],
["importing-data-part-1.html", "1 Importing data - Part 1 1.1 Introduction 1.2 Reading CSV files 1.3 Reading tab deliminated files or other table formats 1.4 Readr and data.table 1.5 Reading Excel files 1.6 XLConnect - read and write to excel", " 1 Importing data - Part 1 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 1)’ by Filip Schouwenaars. 1.1 Introduction Data often comes from many different sources and formats, including Flat files - simple tables e.g. csv Excel Databases - MySQL, Postgres Websites - APIs, JSON, scraping Other statistical software - SPSS, STATA, SAS 1.2 Reading CSV files Reading csv files can be achived with simple code like read.csv(&quot;file.csv&quot;, stringsAsFactors = FALSE) We may want to import strings as categorical variables, in which case we would set stringsAsFactors = TRUE which is also the default option, if not stated. When working across different machines or operating systems, problems can arise due to different ways of addressing file locations and differing file locations. Therefore, it can be easier to set a relative path to the users home directory, which would be achieved with the following code. path &lt;- file.path(&quot;~&quot;, &quot;datasets&quot;, &quot;file.csv&quot;) path ## [1] &quot;~/datasets/file.csv&quot; Then use the file path as before, assigning to a dataframe. df &lt;- read.csv(path, stringsAsFactors = FALSE) 1.3 Reading tab deliminated files or other table formats In a similar way to before, we add the path to the file and if we want strings as strings, for instance read.delim(&quot;file.csv&quot;, stringsAsFactors = FALSE) However, if the file comes in another format perhaps due to the system encoding or setup, it is still possible to try and read the file as a tabular formatting converting it to a data frame. To do so, we use the read.table() command which has a lot of arguments that can be customised. You can specify column names and types for instance. If for instance we have a file format where the objects are separated by a / rather than a comma or tab as before, we could use read.table(&quot;file.txt&quot;, header = TRUE, sep = &quot;/&quot;, stringsAsFactors = FALSE) Or, if you have a file which has no column/variable names and tabs as spaces, you would read the file as: # Path to the file.txt file: path path &lt;- file.path(&quot;data&quot;, &quot;file.txt&quot;) # Import the file.txt file: hotdogs file &lt;- read.table(path, sep = &quot;\\t&quot;, # specify seperator - tab in this instance col.names = c(&quot;VarName1&quot;, &quot;VarName2&quot;, &quot;VarName3&quot;), # specifiy variable names colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # specify the column/variable classes Both read.csv and read.delim are wrapper functions of read.table(), both use read.table but have different default options depending on the file type. There are two further wrapper functions - read.csv2 and read.delim2 - which deal with regional differences in formatting, notably that some areas use full stops as decimal places, whereas other areas use commas for decimal places. 1.4 Readr and data.table These two packages are other ways of reading in files. Readr uses the tibble, so will be compatible with other tidyverse packages such as dplyr. It is faster than utils, the r default and also prints out the column classes, depending on what other packages are loaded. It is not necessary to specify stringsAsFactors = FALSE. library(readr) read_csv(&quot;file.csv&quot;) #read comma seperated read_tsv(&quot;file2.txt&quot;) #read tab seperated files #If there are no row heads, you can create a vector then read it in using the col_names argument #specify the vector for column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) #read in the vector df &lt;- read_tsv(&quot;file3.txt&quot;, col_names = properties) Like the utils package, these are wrapper functions, with the base function being read_delim(). Unlike the utils package, read_delim() expects the first row to contain headers, so this doesn’t need to be explicit. As mentioned previously, it is also not necessary to specify the we don’t want strings as factors. You can specify col_names using a vector as before, or we can read them directly at the time. If we also want to explicitly state the column types, perahps because the automatically assigned variable is not correct, we can do so with col_type using abbreviations: c = character d = double i = integer n = number l = logical D = date T = date time t = time ? = guess _ = skip column (underscore) Finally, we can use skip and n_max to specify how many rows to skip at the beginning of a file, perhaps due to a large header, and the maximum now of rows to read, perhaps due to a very large file with many rows. read_delim(&quot;file4.txt&quot;, delim = &quot;/&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;)) read_delim(&quot;file5.txt&quot;, delim = &quot;/&quot;, col_types = &quot;ccid&quot;) read_delim(&quot;file6.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;), skip = 12, n_max = 50000) Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column. Look at the collector documentation for more details. Two examples are shown below, one for columns to be interpreted as integers and one for a column with factors. # The collectors needed for importing fac &lt;- col_factor(levels = c(&quot;Beef&quot;, &quot;Meat&quot;, &quot;Poultry&quot;)) int &lt;- col_integer() # Edit the col_types argument with the specified collectors hotdogs_factor &lt;- read_tsv(&quot;hotdogs.txt&quot;, col_names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), col_types = list(fac, int, int)) 1.4.1 data.table fread data.table is a tool for doing fast data analysis, particularly on large datasets. It also has a function to read data using the fread() command. It can automatically infer column names, types and separators. You can also drop or select columns at read time. df &lt;- fread(&quot;file7.csv&quot;, select = c(&quot;colname1&quot;, &quot;colname2&quot;)) The readr package fill create different dataframe types or object classes - ‘tbl_df’, ‘tbl’ and ‘data.frame’ - which can be useful for different purposes, such as for use in dplyr. Fread creates a data.table object class. 1.5 Reading Excel files There are many packages for reading Excel files, one package is the readxl package by Hadley Wickham. There are to main functions excel_sheets(): lists the sheets within an excel file or workbook read_excel(): import the data, unless specified the first sheet is read, this can either be done with sheet = 7, or sheet = “name”. So to read an urbanpop.xlsx file containing three sheets of urban populations, for different time frames, our code would look similar to that below. library(readxl) #list the sheerts in the file excel_sheets(&quot;urbanpop.xlsx&quot;) # Read the sheets, one by one pop_1 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 1) pop_2 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2) pop_3 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 3) # Put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) # IF we want to read all the files, a more efficient way to read all the files in the file uses lapply pop_list &lt;- lapply(excel_sheets(&quot;urbanpop.xlsx&quot;), read_excel, path = &quot;urbanpop.xlsx&quot;) There are other arguments that can be used with the read_excel() function: col_names: If true, the first row is read, if false R will assign it’s own names or you specify a charecter vector manually col_types: If NULL, R gueses the data types of the columns. Alternatively, they can be specified e.g. text, numeric, date, blank (which ignores the col) skip: Speficies the number of rows to ignore # Some examples # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = FALSE) # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = cols) # Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel urbanpop_sel &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2, col_names = FALSE, skip = 21) # Print out the first observation from urbanpop_sel urbanpop_sel[1,] 1.5.1 Alternatives for importing Excel files One alternative is the gdata package, which is a suite of tools for data. There is a read.xls() function which only, currently, supports XLS files although xlsx could be supported with a driver. The data is interpreted by the read.xls file using perl into a csv file, which is then read using the read.csv function - itself a offshoot of read.table, in to an R data frame. Hadley’s readxl package is faster, but is quite early in it’s development so some of the functions may change. For gdata, as it is an offshoot of read.table(), all of the same arguments can be used by read.xls(). 1.6 XLConnect - read and write to excel Most of the Excel tools can become accessible but inside R, using XLConnect. It is possible to use XLS and XLSX and it will create a ‘workbook’ object in R, but it does require Java to work. library(XLConnect) #create a connect to a file and list the sheets book &lt;- loadWorkbook(&quot;file.xlsx&quot;) getSheets(book) #read in the specific sheet but only the columns we are interested in wardData &lt;- readWorksheet(book, sheet = &quot;sheet_1&quot;, startCol = 3, endCol = 5) # read in the names column, previoulsy excluded wardNames &lt;- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1) #cbind the data and names together selection &lt;- cbind(wardNames, wardData) XLConnect has more features than simply reading sheets. It is possible to write data back to the Excel file also. We can add sheets, write or add data to sheets, rename and remove sheets. # Add a worksheet to my_book, named &quot;summary&quot; createSheet(my_book, &quot;summary&quot;) # Add data in summ to &quot;data_summary&quot; sheet writeWorksheet(my_book, summ, &quot;summary&quot;) # Save workbook as summary.xlsx saveWorkbook(my_book, &quot;summary.xlsx&quot;) # Rename &quot;summary&quot; sheet to &quot;data_summary&quot; renameSheet(my_book, sheet = 4, &quot;data_summary&quot;) # Remove the third sheet removeSheet(my_book, sheet = 3) "],
["importing-data-part-2.html", "2 Importing data - Part 2 2.1 Importing from Databases - 1 2.2 SQL Queries Inside R 2.3 Web Data 2.4 JSON and APIs 2.5 Importing from other statistical software", " 2 Importing data - Part 2 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 2)’ by Filip Schouwenaars. 2.1 Importing from Databases - 1 In a professional or commercial setting, you often deal with more complicated file structures and source systems that simple flat files. Often the data is stored in a DBMS or Database Management System and SQL is the usual way of quering the DBMS. As there can be slight differences, you are likely to need different packages, some include: MySQL: Use the RMySQL package PostgresSQL: Use the RPostgresSQL package Oracle: Use the ROracle (etc…) Conventions are specified in the DBI - another R package, DBI is the interface and the other packages are the implentation. Some of the packages will automaticlaly install the DBI package as well. To connect to a database we would so something like the following. # Load the DBI package library(DBI) ## Loading required package: methods # Edit dbConnect() call - the first part specifies how connections are map to the database con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Build a vector of table names: tables tables &lt;- dbListTables(con) # Display structure of tables str(tables) ## chr [1:3] &quot;comments&quot; &quot;tweats&quot; &quot;users&quot; # Import the users table from tweater: users users &lt;- dbReadTable(con, &quot;users&quot;) # Print users users ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja # Or we can import all tables using lapply tables &lt;- lapply(tables, dbReadTable, conn = con) # Print out tables tables ## [[1]] ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! ## 3 1011 49 5 love it ## 4 1012 87 1 awesome! thanks! ## 5 1010 88 6 yuck! ## 6 1026 77 4 not my thing! ## 7 1004 49 1 this is fabulous! ## 8 1030 75 6 so easy! ## 9 1025 88 2 oh yes ## 10 1007 49 3 serious? ## 11 1020 77 1 couldn&#39;t be better ## 12 1014 77 1 saved my day ## ## [[2]] ## id user_id ## 1 75 3 ## 2 88 4 ## 3 77 6 ## 4 87 5 ## 5 49 1 ## 6 24 7 ## post ## 1 break egg. bake egg. eat egg. ## 2 wash strawberries. add ice. blend. enjoy. ## 3 2 slices of bread. add cheese. grill. heaven. ## 4 open and crush avocado. add shrimps. perfect starter. ## 5 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 6 just eat an apple. simply and healthy. ## date ## 1 2015-09-05 ## 2 2015-09-14 ## 3 2015-09-21 ## 4 2015-09-22 ## 5 2015-09-22 ## 6 2015-09-24 ## ## [[3]] ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja 2.2 SQL Queries Inside R OFten you don’t want an entire tabel from a database, but a selection from the table. You can use SQL queries from inside R to extract only what you are interested in. You can alternatively use subset on the imported table, but often it is easier to extract only what you need first, particularly when working with large databases. The SQL goes inside e.g. dbGetQuery(con, “SQL QUERY”). # Connect to the database library(DBI) con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth &lt;- dbGetQuery(con, &quot;SELECT tweat_id FROM comments WHERE user_id = 1&quot;) # Print elisabeth elisabeth ## tweat_id ## 1 87 ## 2 49 ## 3 77 ## 4 77 # Import post column of tweats where date is higher than &#39;2015-09-21&#39;: latest latest &lt;- dbGetQuery(con, &quot;SELECT post FROM tweats WHERE date &gt; &#39;2015-09-21&#39;&quot;) # Print latest latest ## post ## 1 open and crush avocado. add shrimps. perfect starter. ## 2 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 3 just eat an apple. simply and healthy. # Create data frame specific using boolean specific &lt;- dbGetQuery(con, &quot;SELECT message FROM comments WHERE tweat_id = 77 AND user_id &gt; 4&quot;) # Print specific specific ## message ## 1 great! # Create data frame short selecting two columns short &lt;- dbGetQuery(con, &quot;SELECT id, name FROM users WHERE CHAR_LENGTH(name) &lt; 5&quot;) # Print short short ## id name ## 1 2 mike ## 2 3 thea ## 3 6 kate # We can also join elements from different tables using the same id/key dbGetQuery(con, &quot;SELECT post, message FROM tweats INNER JOIN comments on tweats.id = tweat_id WHERE tweat_id = 77&quot;) ## post message ## 1 2 slices of bread. add cheese. grill. heaven. great! ## 2 2 slices of bread. add cheese. grill. heaven. not my thing! ## 3 2 slices of bread. add cheese. grill. heaven. couldn&#39;t be better ## 4 2 slices of bread. add cheese. grill. heaven. saved my day You’ve used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed: Sending the specified query with dbSendQuery(); Fetching the result of executing the query on the database with dbFetch(); Clearing the result with dbClearResult(). Let’s not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query’s result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch(). It is important to close the connection to the database once complete using the dbDisconnect() function # Send query to the database res &lt;- dbSendQuery(con, &quot;SELECT * FROM comments WHERE user_id &gt; 4&quot;) # Use dbFetch() twice dbFetch(res, n = 2) ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! dbFetch(res) # imports all ## id tweat_id user_id message ## 1 1011 49 5 love it ## 2 1010 88 6 yuck! ## 3 1030 75 6 so easy! # Clear res dbClearResult(res) ## [1] TRUE # Create the data frame long_tweats long_tweats &lt;- dbGetQuery(con, &quot;SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) &gt; 40&quot;) # Print long_tweats print(long_tweats) ## post ## 1 wash strawberries. add ice. blend. enjoy. ## 2 2 slices of bread. add cheese. grill. heaven. ## 3 open and crush avocado. add shrimps. perfect starter. ## 4 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## date ## 1 2015-09-14 ## 2 2015-09-21 ## 3 2015-09-22 ## 4 2015-09-22 # Disconnect from the database dbDisconnect(con) ## [1] TRUE 2.3 Web Data HyperText Transfer Protocol (HTTP) is the ‘language of the web’ and consists of a set of rules about data exchange between computers. If the file is a csv file, we can use functions like read.csv() and add in the url in quotations marks, read.csv will recognise this is a URL and will issue a HTTP GET command to download the file. This will also work on https sites on newer versions of R. We can also use the readr package and other packages. # Load the readr package library(readr) # Import the csv file: pools url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; pools &lt;- read_csv(url_csv) ## Parsed with column specification: ## cols( ## Name = col_character(), ## Address = col_character(), ## Latitude = col_double(), ## Longitude = col_double() ## ) # Import the txt file: potatoes url_delim &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt&quot; potatoes &lt;- read_tsv(url_delim) ## Parsed with column specification: ## cols( ## area = col_integer(), ## temp = col_integer(), ## size = col_integer(), ## storage = col_integer(), ## method = col_integer(), ## texture = col_double(), ## flavor = col_double(), ## moistness = col_double() ## ) # Print pools and potatoes pools ## # A tibble: 20 x 4 ## Name ## &lt;chr&gt; ## 1 Acacia Ridge Leisure Centre ## 2 Bellbowrie Pool ## 3 Carole Park ## 4 Centenary Pool (inner City) ## 5 Chermside Pool ## 6 Colmslie Pool (Morningside) ## 7 Spring Hill Baths (inner City) ## 8 Dunlop Park Pool (Corinda) ## 9 Fortitude Valley Pool ## 10 Hibiscus Sports Complex (upper MtGravatt) ## 11 Ithaca Pool ( Paddington) ## 12 Jindalee Pool ## 13 Manly Pool ## 14 Mt Gravatt East Aquatic Centre ## 15 Musgrave Park Pool (South Brisbane) ## 16 Newmarket Pool ## 17 Runcorn Pool ## 18 Sandgate Pool ## 19 Langlands Parks Pool (Stones Corner) ## 20 Yeronga Park Pool ## # ... with 3 more variables: Address &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt; potatoes ## # A tibble: 160 x 8 ## area temp size storage method texture flavor moistness ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3.0 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3.0 1.7 ## 7 1 1 1 2 2 2.6 3.1 2.4 ## 8 1 1 1 2 3 3.0 3.0 2.9 ## 9 1 1 1 2 4 2.2 3.2 2.5 ## 10 1 1 1 2 5 2.0 2.8 1.9 ## # ... with 150 more rows # https URL to the swimming_pools csv file. url_csv &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; # Import the file using read.csv(): pools1 pools1 &lt;- read.csv(url_csv) str(pools1) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : Factor w/ 20 levels &quot;Acacia Ridge Leisure Centre&quot;,..: 1 2 3 4 5 6 19 7 8 9 ... ## $ Address : Factor w/ 20 levels &quot;1 Fairlead Crescent, Manly&quot;,..: 5 20 18 10 9 11 6 15 12 17 ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... Some packages, like the readxl package, do not currently recognise urls. However, we can use the donwload.file() or other command to download the file and then read it in locally. This process can be much quicker that browsing the internet then downloading the file. library(readxl) # Specification of url: url_xls url_xls &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls&quot; # Download file behind URL, name it local_latitude.xls download.file(url_xls, destfile = &quot;local_latitude.xls&quot;) # Import the local .xls file with readxl: excel_readxl excel_readxl &lt;- read_excel(&quot;local_latitude.xls&quot;) # https URL to the wine RData file. url_rdata &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData&quot; # Download the wine file to your working directory download.file(url_rdata, destfile = &quot;wine_local.RData&quot;) # Load the wine data into your workspace using load() load(&quot;wine_local.RData&quot;) # Print out the summary of the wine data summary(wine) ## Alcohol Malic acid Ash Alcalinity of ash ## Min. :11.03 Min. :0.74 Min. :1.360 Min. :10.60 ## 1st Qu.:12.36 1st Qu.:1.60 1st Qu.:2.210 1st Qu.:17.20 ## Median :13.05 Median :1.87 Median :2.360 Median :19.50 ## Mean :12.99 Mean :2.34 Mean :2.366 Mean :19.52 ## 3rd Qu.:13.67 3rd Qu.:3.10 3rd Qu.:2.560 3rd Qu.:21.50 ## Max. :14.83 Max. :5.80 Max. :3.230 Max. :30.00 ## Magnesium Total phenols Flavanoids Nonflavanoid phenols ## Min. : 70.00 Min. :0.980 Min. :0.340 Min. :0.1300 ## 1st Qu.: 88.00 1st Qu.:1.740 1st Qu.:1.200 1st Qu.:0.2700 ## Median : 98.00 Median :2.350 Median :2.130 Median :0.3400 ## Mean : 99.59 Mean :2.292 Mean :2.023 Mean :0.3623 ## 3rd Qu.:107.00 3rd Qu.:2.800 3rd Qu.:2.860 3rd Qu.:0.4400 ## Max. :162.00 Max. :3.880 Max. :5.080 Max. :0.6600 ## Proanthocyanins Color intensity Hue Proline ## Min. :0.410 Min. : 1.280 Min. :1.270 Min. : 278.0 ## 1st Qu.:1.250 1st Qu.: 3.210 1st Qu.:1.930 1st Qu.: 500.0 ## Median :1.550 Median : 4.680 Median :2.780 Median : 672.0 ## Mean :1.587 Mean : 5.055 Mean :2.604 Mean : 745.1 ## 3rd Qu.:1.950 3rd Qu.: 6.200 3rd Qu.:3.170 3rd Qu.: 985.0 ## Max. :3.580 Max. :13.000 Max. :4.000 Max. :1680.0 We can also read http content using the httr package. This includes JSON formatted text, which httr will convert to a named list. # Load the httr package library(httr) # Get the url, save response to resp url &lt;- &quot;http://www.example.com/&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.example.com/] ## Date: 2017-11-10 05:02 ## Status: 200 ## Content-Type: text/html ## Size: 1.27 kB ## &lt;!doctype html&gt; ## &lt;html&gt; ## &lt;head&gt; ## &lt;title&gt;Example Domain&lt;/title&gt; ## ## &lt;meta charset=&quot;utf-8&quot; /&gt; ## &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; ## &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt; ## &lt;style type=&quot;text/css&quot;&gt; ## body { ## ... # Get the raw content of resp: raw_content raw_content &lt;- content(resp, as = &quot;raw&quot;) # Print the head of raw_content head(raw_content) ## [1] 3c 21 64 6f 63 74 # JSON formatted # Get the url url &lt;- &quot;http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json] ## Date: 2017-11-10 05:02 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 942 B # Print content of resp as text content(resp, as = &quot;text&quot;) ## [1] &quot;{\\&quot;Title\\&quot;:\\&quot;Annie Hall\\&quot;,\\&quot;Year\\&quot;:\\&quot;1977\\&quot;,\\&quot;Rated\\&quot;:\\&quot;PG\\&quot;,\\&quot;Released\\&quot;:\\&quot;20 Apr 1977\\&quot;,\\&quot;Runtime\\&quot;:\\&quot;93 min\\&quot;,\\&quot;Genre\\&quot;:\\&quot;Comedy, Romance\\&quot;,\\&quot;Director\\&quot;:\\&quot;Woody Allen\\&quot;,\\&quot;Writer\\&quot;:\\&quot;Woody Allen, Marshall Brickman\\&quot;,\\&quot;Actors\\&quot;:\\&quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane\\&quot;,\\&quot;Plot\\&quot;:\\&quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.\\&quot;,\\&quot;Language\\&quot;:\\&quot;English, German\\&quot;,\\&quot;Country\\&quot;:\\&quot;USA\\&quot;,\\&quot;Awards\\&quot;:\\&quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.\\&quot;,\\&quot;Poster\\&quot;:\\&quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg\\&quot;,\\&quot;Ratings\\&quot;:[{\\&quot;Source\\&quot;:\\&quot;Internet Movie Database\\&quot;,\\&quot;Value\\&quot;:\\&quot;8.1/10\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Rotten Tomatoes\\&quot;,\\&quot;Value\\&quot;:\\&quot;97%\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Metacritic\\&quot;,\\&quot;Value\\&quot;:\\&quot;92/100\\&quot;}],\\&quot;Metascore\\&quot;:\\&quot;92\\&quot;,\\&quot;imdbRating\\&quot;:\\&quot;8.1\\&quot;,\\&quot;imdbVotes\\&quot;:\\&quot;214,867\\&quot;,\\&quot;imdbID\\&quot;:\\&quot;tt0075686\\&quot;,\\&quot;Type\\&quot;:\\&quot;movie\\&quot;,\\&quot;DVD\\&quot;:\\&quot;28 Apr 1998\\&quot;,\\&quot;BoxOffice\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Production\\&quot;:\\&quot;United Artists\\&quot;,\\&quot;Website\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Response\\&quot;:\\&quot;True\\&quot;}&quot; # Print content of resp content(resp) ## $Title ## [1] &quot;Annie Hall&quot; ## ## $Year ## [1] &quot;1977&quot; ## ## $Rated ## [1] &quot;PG&quot; ## ## $Released ## [1] &quot;20 Apr 1977&quot; ## ## $Runtime ## [1] &quot;93 min&quot; ## ## $Genre ## [1] &quot;Comedy, Romance&quot; ## ## $Director ## [1] &quot;Woody Allen&quot; ## ## $Writer ## [1] &quot;Woody Allen, Marshall Brickman&quot; ## ## $Actors ## [1] &quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane&quot; ## ## $Plot ## [1] &quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.&quot; ## ## $Language ## [1] &quot;English, German&quot; ## ## $Country ## [1] &quot;USA&quot; ## ## $Awards ## [1] &quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.&quot; ## ## $Poster ## [1] &quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg&quot; ## ## $Ratings ## $Ratings[[1]] ## $Ratings[[1]]$Source ## [1] &quot;Internet Movie Database&quot; ## ## $Ratings[[1]]$Value ## [1] &quot;8.1/10&quot; ## ## ## $Ratings[[2]] ## $Ratings[[2]]$Source ## [1] &quot;Rotten Tomatoes&quot; ## ## $Ratings[[2]]$Value ## [1] &quot;97%&quot; ## ## ## $Ratings[[3]] ## $Ratings[[3]]$Source ## [1] &quot;Metacritic&quot; ## ## $Ratings[[3]]$Value ## [1] &quot;92/100&quot; ## ## ## ## $Metascore ## [1] &quot;92&quot; ## ## $imdbRating ## [1] &quot;8.1&quot; ## ## $imdbVotes ## [1] &quot;214,867&quot; ## ## $imdbID ## [1] &quot;tt0075686&quot; ## ## $Type ## [1] &quot;movie&quot; ## ## $DVD ## [1] &quot;28 Apr 1998&quot; ## ## $BoxOffice ## [1] &quot;N/A&quot; ## ## $Production ## [1] &quot;United Artists&quot; ## ## $Website ## [1] &quot;N/A&quot; ## ## $Response ## [1] &quot;True&quot; 2.4 JSON and APIs JSON is both easy for machines to parse and generate and is human readable. APIs are programtical ways of getting data, consisting of a set of protocols to interact with some other system or database. JSON can be useful since it is often well structured and can save time over, say, parsing a html page. So for instance, you can use the OMDb API to return JSON formatted text about a movie, rather than parse an IMDB html page entry. One package for handling JSON in R is jsonlite. library(jsonlite) # wine_json is a JSON wine_json &lt;- &#39;{&quot;name&quot;:&quot;Chateau Migraine&quot;, &quot;year&quot;:1997, &quot;alcohol_pct&quot;:12.4, &quot;color&quot;:&quot;red&quot;, &quot;awarded&quot;:false}&#39; # Convert wine_json into a list: wine wine &lt;- fromJSON(wine_json) # Print structure of wine str(wine) ## List of 5 ## $ name : chr &quot;Chateau Migraine&quot; ## $ year : int 1997 ## $ alcohol_pct: num 12.4 ## $ color : chr &quot;red&quot; ## $ awarded : logi FALSE There are two types of JSON structures JSON objects - has key value pairs e.g. name:James, age:21 etc JSON arrays - a sequence of values, numbers, nulls e.g. 4, “a”, 10, false, null etc You can also nest JSON objects or arrays within each other. Some examples are below. YOu can also use the minify and prettify functions to convert a JSON string to a more compact of easier to read version. Similar functions can also be used inside the toJSON() function e.g. toJSON(x, pretty = TRUE) # Challenge 1 json1 &lt;- &#39;[1, 2, 3, 4, 5, 6]&#39; fromJSON(json1) ## [1] 1 2 3 4 5 6 # Challenge 2 json2 &lt;- &#39;{&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]}&#39; fromJSON(json2) ## $a ## [1] 1 2 3 ## ## $b ## [1] 4 5 6 # You can also convert data to JSON from other formats. Here we take a csv and format it into a JSON array # URL pointing to the .csv file url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv&quot; # Import the .csv file located at url_csv water &lt;- read.csv(url_csv, stringsAsFactors = FALSE) # Convert the data file according to the requirements water_json &lt;- toJSON(water) # Print out water_json water_json ## [{&quot;water&quot;:&quot;Algeria&quot;,&quot;X1992&quot;:0.064,&quot;X2002&quot;:0.017},{&quot;water&quot;:&quot;American Samoa&quot;},{&quot;water&quot;:&quot;Angola&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Antigua and Barbuda&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Argentina&quot;,&quot;X1992&quot;:0.0007,&quot;X1997&quot;:0.0007,&quot;X2002&quot;:0.0007},{&quot;water&quot;:&quot;Australia&quot;,&quot;X1992&quot;:0.0298,&quot;X2002&quot;:0.0298},{&quot;water&quot;:&quot;Austria&quot;,&quot;X1992&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Bahamas&quot;,&quot;X1992&quot;:0.0013,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Bahrain&quot;,&quot;X1992&quot;:0.0441,&quot;X2002&quot;:0.0441,&quot;X2007&quot;:0.1024},{&quot;water&quot;:&quot;Barbados&quot;,&quot;X2007&quot;:0.0146},{&quot;water&quot;:&quot;British Virgin Islands&quot;,&quot;X2007&quot;:0.0042},{&quot;water&quot;:&quot;Canada&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cape Verde&quot;,&quot;X1992&quot;:0.002,&quot;X1997&quot;:0.0017},{&quot;water&quot;:&quot;Cayman Islands&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Central African Rep.&quot;},{&quot;water&quot;:&quot;Chile&quot;,&quot;X1992&quot;:0.0048,&quot;X2002&quot;:0.0048},{&quot;water&quot;:&quot;Colombia&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cuba&quot;,&quot;X1992&quot;:0.0069,&quot;X1997&quot;:0.0069,&quot;X2002&quot;:0.0069},{&quot;water&quot;:&quot;Cyprus&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.0335},{&quot;water&quot;:&quot;Czech Rep.&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Denmark&quot;,&quot;X1992&quot;:0.015,&quot;X2002&quot;:0.015},{&quot;water&quot;:&quot;Djibouti&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Ecuador&quot;,&quot;X1992&quot;:0.0022,&quot;X1997&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Egypt&quot;,&quot;X1992&quot;:0.025,&quot;X1997&quot;:0.025,&quot;X2002&quot;:0.1},{&quot;water&quot;:&quot;El Salvador&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Finland&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;France&quot;,&quot;X1992&quot;:0.0117,&quot;X2002&quot;:0.0117},{&quot;water&quot;:&quot;Gibraltar&quot;,&quot;X1992&quot;:0.0077},{&quot;water&quot;:&quot;Greece&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01},{&quot;water&quot;:&quot;Honduras&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Hungary&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;India&quot;,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Indonesia&quot;,&quot;X1992&quot;:0.0187,&quot;X2002&quot;:0.0187},{&quot;water&quot;:&quot;Iran&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.003,&quot;X2007&quot;:0.2},{&quot;water&quot;:&quot;Iraq&quot;,&quot;X1997&quot;:0.0074,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Ireland&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Israel&quot;,&quot;X1992&quot;:0.0256,&quot;X2002&quot;:0.0256,&quot;X2007&quot;:0.14},{&quot;water&quot;:&quot;Italy&quot;,&quot;X1992&quot;:0.0973,&quot;X2002&quot;:0.0973},{&quot;water&quot;:&quot;Jamaica&quot;,&quot;X1992&quot;:0.0005,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Japan&quot;,&quot;X1997&quot;:0.04,&quot;X2002&quot;:0.04},{&quot;water&quot;:&quot;Jordan&quot;,&quot;X1997&quot;:0.002,&quot;X2007&quot;:0.0098},{&quot;water&quot;:&quot;Kazakhstan&quot;,&quot;X1997&quot;:1.328,&quot;X2002&quot;:1.328},{&quot;water&quot;:&quot;Kuwait&quot;,&quot;X1992&quot;:0.507,&quot;X1997&quot;:0.231,&quot;X2002&quot;:0.4202},{&quot;water&quot;:&quot;Lebanon&quot;,&quot;X2007&quot;:0.0473},{&quot;water&quot;:&quot;Libya&quot;,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Malaysia&quot;,&quot;X1992&quot;:0.0043,&quot;X2002&quot;:0.0043},{&quot;water&quot;:&quot;Maldives&quot;,&quot;X1992&quot;:0.0004},{&quot;water&quot;:&quot;Malta&quot;,&quot;X1992&quot;:0.024,&quot;X1997&quot;:0.031,&quot;X2002&quot;:0.031},{&quot;water&quot;:&quot;Marshall Islands&quot;,&quot;X1992&quot;:0.0007},{&quot;water&quot;:&quot;Mauritania&quot;,&quot;X1992&quot;:0.002,&quot;X2002&quot;:0.002},{&quot;water&quot;:&quot;Mexico&quot;,&quot;X1992&quot;:0.0307,&quot;X2002&quot;:0.0307},{&quot;water&quot;:&quot;Morocco&quot;,&quot;X1992&quot;:0.0034,&quot;X1997&quot;:0.0034,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Namibia&quot;,&quot;X1992&quot;:0.0003,&quot;X2002&quot;:0.0003},{&quot;water&quot;:&quot;Netherlands Antilles&quot;,&quot;X1992&quot;:0.063},{&quot;water&quot;:&quot;Nicaragua&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Nigeria&quot;,&quot;X1992&quot;:0.003,&quot;X2002&quot;:0.003},{&quot;water&quot;:&quot;Norway&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Oman&quot;,&quot;X1997&quot;:0.034,&quot;X2002&quot;:0.034,&quot;X2007&quot;:0.109},{&quot;water&quot;:&quot;Peru&quot;,&quot;X1992&quot;:0.0054,&quot;X2002&quot;:0.0054},{&quot;water&quot;:&quot;Poland&quot;,&quot;X1992&quot;:0.007,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Portugal&quot;,&quot;X1992&quot;:0.0016,&quot;X2002&quot;:0.0016},{&quot;water&quot;:&quot;Qatar&quot;,&quot;X1992&quot;:0.065,&quot;X1997&quot;:0.099,&quot;X2002&quot;:0.099,&quot;X2007&quot;:0.18},{&quot;water&quot;:&quot;Saudi Arabia&quot;,&quot;X1992&quot;:0.683,&quot;X1997&quot;:0.727,&quot;X2002&quot;:0.863,&quot;X2007&quot;:1.033},{&quot;water&quot;:&quot;Senegal&quot;,&quot;X1992&quot;:0,&quot;X2002&quot;:0},{&quot;water&quot;:&quot;Somalia&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;South Africa&quot;,&quot;X1992&quot;:0.018,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Spain&quot;,&quot;X1992&quot;:0.1002,&quot;X2002&quot;:0.1002},{&quot;water&quot;:&quot;Sudan&quot;,&quot;X1992&quot;:0.0004,&quot;X1997&quot;:0.0004,&quot;X2002&quot;:0.0004},{&quot;water&quot;:&quot;Sweden&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Trinidad and Tobago&quot;,&quot;X2007&quot;:0.036},{&quot;water&quot;:&quot;Tunisia&quot;,&quot;X1992&quot;:0.008,&quot;X2002&quot;:0.013},{&quot;water&quot;:&quot;Turkey&quot;,&quot;X1992&quot;:0.0005,&quot;X2002&quot;:0.0005,&quot;X2007&quot;:0.0005},{&quot;water&quot;:&quot;United Arab Emirates&quot;,&quot;X1992&quot;:0.163,&quot;X1997&quot;:0.385,&quot;X2007&quot;:0.95},{&quot;water&quot;:&quot;United Kingdom&quot;,&quot;X1992&quot;:0.0333,&quot;X2002&quot;:0.0333},{&quot;water&quot;:&quot;United States&quot;,&quot;X1992&quot;:0.58,&quot;X2002&quot;:0.58},{&quot;water&quot;:&quot;Venezuela&quot;,&quot;X1992&quot;:0.0052,&quot;X2002&quot;:0.0052},{&quot;water&quot;:&quot;Yemen, Rep.&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01}] # Convert mtcars to a pretty JSON: pretty_json pretty_json &lt;- toJSON(mtcars, pretty = TRUE) # Print pretty_json pretty_json ## [ ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.62, ## &quot;qsec&quot;: 16.46, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4&quot; ## }, ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.875, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4 Wag&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 108, ## &quot;hp&quot;: 93, ## &quot;drat&quot;: 3.85, ## &quot;wt&quot;: 2.32, ## &quot;qsec&quot;: 18.61, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Datsun 710&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 258, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.215, ## &quot;qsec&quot;: 19.44, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Hornet 4 Drive&quot; ## }, ## { ## &quot;mpg&quot;: 18.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Hornet Sportabout&quot; ## }, ## { ## &quot;mpg&quot;: 18.1, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 225, ## &quot;hp&quot;: 105, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.46, ## &quot;qsec&quot;: 20.22, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Valiant&quot; ## }, ## { ## &quot;mpg&quot;: 14.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.21, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 15.84, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Duster 360&quot; ## }, ## { ## &quot;mpg&quot;: 24.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 146.7, ## &quot;hp&quot;: 62, ## &quot;drat&quot;: 3.69, ## &quot;wt&quot;: 3.19, ## &quot;qsec&quot;: 20, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 240D&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 140.8, ## &quot;hp&quot;: 95, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.15, ## &quot;qsec&quot;: 22.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 230&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.3, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280&quot; ## }, ## { ## &quot;mpg&quot;: 17.8, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280C&quot; ## }, ## { ## &quot;mpg&quot;: 16.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 4.07, ## &quot;qsec&quot;: 17.4, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SE&quot; ## }, ## { ## &quot;mpg&quot;: 17.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.73, ## &quot;qsec&quot;: 17.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SL&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.78, ## &quot;qsec&quot;: 18, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SLC&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 472, ## &quot;hp&quot;: 205, ## &quot;drat&quot;: 2.93, ## &quot;wt&quot;: 5.25, ## &quot;qsec&quot;: 17.98, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Cadillac Fleetwood&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 460, ## &quot;hp&quot;: 215, ## &quot;drat&quot;: 3, ## &quot;wt&quot;: 5.424, ## &quot;qsec&quot;: 17.82, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Lincoln Continental&quot; ## }, ## { ## &quot;mpg&quot;: 14.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 440, ## &quot;hp&quot;: 230, ## &quot;drat&quot;: 3.23, ## &quot;wt&quot;: 5.345, ## &quot;qsec&quot;: 17.42, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Chrysler Imperial&quot; ## }, ## { ## &quot;mpg&quot;: 32.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 78.7, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 2.2, ## &quot;qsec&quot;: 19.47, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat 128&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 75.7, ## &quot;hp&quot;: 52, ## &quot;drat&quot;: 4.93, ## &quot;wt&quot;: 1.615, ## &quot;qsec&quot;: 18.52, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Honda Civic&quot; ## }, ## { ## &quot;mpg&quot;: 33.9, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 71.1, ## &quot;hp&quot;: 65, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 1.835, ## &quot;qsec&quot;: 19.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corolla&quot; ## }, ## { ## &quot;mpg&quot;: 21.5, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.1, ## &quot;hp&quot;: 97, ## &quot;drat&quot;: 3.7, ## &quot;wt&quot;: 2.465, ## &quot;qsec&quot;: 20.01, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corona&quot; ## }, ## { ## &quot;mpg&quot;: 15.5, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 318, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.52, ## &quot;qsec&quot;: 16.87, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Dodge Challenger&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 304, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.435, ## &quot;qsec&quot;: 17.3, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;AMC Javelin&quot; ## }, ## { ## &quot;mpg&quot;: 13.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 350, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.73, ## &quot;wt&quot;: 3.84, ## &quot;qsec&quot;: 15.41, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Camaro Z28&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 400, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.845, ## &quot;qsec&quot;: 17.05, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Pontiac Firebird&quot; ## }, ## { ## &quot;mpg&quot;: 27.3, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 79, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 1.935, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat X1-9&quot; ## }, ## { ## &quot;mpg&quot;: 26, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.3, ## &quot;hp&quot;: 91, ## &quot;drat&quot;: 4.43, ## &quot;wt&quot;: 2.14, ## &quot;qsec&quot;: 16.7, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Porsche 914-2&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 95.1, ## &quot;hp&quot;: 113, ## &quot;drat&quot;: 3.77, ## &quot;wt&quot;: 1.513, ## &quot;qsec&quot;: 16.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Lotus Europa&quot; ## }, ## { ## &quot;mpg&quot;: 15.8, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 351, ## &quot;hp&quot;: 264, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 3.17, ## &quot;qsec&quot;: 14.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Ford Pantera L&quot; ## }, ## { ## &quot;mpg&quot;: 19.7, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 145, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.62, ## &quot;wt&quot;: 2.77, ## &quot;qsec&quot;: 15.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 6, ## &quot;_row&quot;: &quot;Ferrari Dino&quot; ## }, ## { ## &quot;mpg&quot;: 15, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 301, ## &quot;hp&quot;: 335, ## &quot;drat&quot;: 3.54, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 14.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 8, ## &quot;_row&quot;: &quot;Maserati Bora&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 121, ## &quot;hp&quot;: 109, ## &quot;drat&quot;: 4.11, ## &quot;wt&quot;: 2.78, ## &quot;qsec&quot;: 18.6, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Volvo 142E&quot; ## } ## ] # Minify pretty_json: mini_json mini_json &lt;- minify(pretty_json) # Print mini_json mini_json ## [{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.62,&quot;qsec&quot;:16.46,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4&quot;},{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.875,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4 Wag&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:108,&quot;hp&quot;:93,&quot;drat&quot;:3.85,&quot;wt&quot;:2.32,&quot;qsec&quot;:18.61,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Datsun 710&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:6,&quot;disp&quot;:258,&quot;hp&quot;:110,&quot;drat&quot;:3.08,&quot;wt&quot;:3.215,&quot;qsec&quot;:19.44,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Hornet 4 Drive&quot;},{&quot;mpg&quot;:18.7,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:175,&quot;drat&quot;:3.15,&quot;wt&quot;:3.44,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Hornet Sportabout&quot;},{&quot;mpg&quot;:18.1,&quot;cyl&quot;:6,&quot;disp&quot;:225,&quot;hp&quot;:105,&quot;drat&quot;:2.76,&quot;wt&quot;:3.46,&quot;qsec&quot;:20.22,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Valiant&quot;},{&quot;mpg&quot;:14.3,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:245,&quot;drat&quot;:3.21,&quot;wt&quot;:3.57,&quot;qsec&quot;:15.84,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Duster 360&quot;},{&quot;mpg&quot;:24.4,&quot;cyl&quot;:4,&quot;disp&quot;:146.7,&quot;hp&quot;:62,&quot;drat&quot;:3.69,&quot;wt&quot;:3.19,&quot;qsec&quot;:20,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 240D&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:140.8,&quot;hp&quot;:95,&quot;drat&quot;:3.92,&quot;wt&quot;:3.15,&quot;qsec&quot;:22.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 230&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.3,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280&quot;},{&quot;mpg&quot;:17.8,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280C&quot;},{&quot;mpg&quot;:16.4,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:4.07,&quot;qsec&quot;:17.4,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SE&quot;},{&quot;mpg&quot;:17.3,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.73,&quot;qsec&quot;:17.6,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SL&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.78,&quot;qsec&quot;:18,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SLC&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:472,&quot;hp&quot;:205,&quot;drat&quot;:2.93,&quot;wt&quot;:5.25,&quot;qsec&quot;:17.98,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Cadillac Fleetwood&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:460,&quot;hp&quot;:215,&quot;drat&quot;:3,&quot;wt&quot;:5.424,&quot;qsec&quot;:17.82,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Lincoln Continental&quot;},{&quot;mpg&quot;:14.7,&quot;cyl&quot;:8,&quot;disp&quot;:440,&quot;hp&quot;:230,&quot;drat&quot;:3.23,&quot;wt&quot;:5.345,&quot;qsec&quot;:17.42,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Chrysler Imperial&quot;},{&quot;mpg&quot;:32.4,&quot;cyl&quot;:4,&quot;disp&quot;:78.7,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:2.2,&quot;qsec&quot;:19.47,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat 128&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:75.7,&quot;hp&quot;:52,&quot;drat&quot;:4.93,&quot;wt&quot;:1.615,&quot;qsec&quot;:18.52,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Honda Civic&quot;},{&quot;mpg&quot;:33.9,&quot;cyl&quot;:4,&quot;disp&quot;:71.1,&quot;hp&quot;:65,&quot;drat&quot;:4.22,&quot;wt&quot;:1.835,&quot;qsec&quot;:19.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corolla&quot;},{&quot;mpg&quot;:21.5,&quot;cyl&quot;:4,&quot;disp&quot;:120.1,&quot;hp&quot;:97,&quot;drat&quot;:3.7,&quot;wt&quot;:2.465,&quot;qsec&quot;:20.01,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corona&quot;},{&quot;mpg&quot;:15.5,&quot;cyl&quot;:8,&quot;disp&quot;:318,&quot;hp&quot;:150,&quot;drat&quot;:2.76,&quot;wt&quot;:3.52,&quot;qsec&quot;:16.87,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Dodge Challenger&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:304,&quot;hp&quot;:150,&quot;drat&quot;:3.15,&quot;wt&quot;:3.435,&quot;qsec&quot;:17.3,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;AMC Javelin&quot;},{&quot;mpg&quot;:13.3,&quot;cyl&quot;:8,&quot;disp&quot;:350,&quot;hp&quot;:245,&quot;drat&quot;:3.73,&quot;wt&quot;:3.84,&quot;qsec&quot;:15.41,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Camaro Z28&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:8,&quot;disp&quot;:400,&quot;hp&quot;:175,&quot;drat&quot;:3.08,&quot;wt&quot;:3.845,&quot;qsec&quot;:17.05,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Pontiac Firebird&quot;},{&quot;mpg&quot;:27.3,&quot;cyl&quot;:4,&quot;disp&quot;:79,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:1.935,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat X1-9&quot;},{&quot;mpg&quot;:26,&quot;cyl&quot;:4,&quot;disp&quot;:120.3,&quot;hp&quot;:91,&quot;drat&quot;:4.43,&quot;wt&quot;:2.14,&quot;qsec&quot;:16.7,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Porsche 914-2&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:95.1,&quot;hp&quot;:113,&quot;drat&quot;:3.77,&quot;wt&quot;:1.513,&quot;qsec&quot;:16.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Lotus Europa&quot;},{&quot;mpg&quot;:15.8,&quot;cyl&quot;:8,&quot;disp&quot;:351,&quot;hp&quot;:264,&quot;drat&quot;:4.22,&quot;wt&quot;:3.17,&quot;qsec&quot;:14.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Ford Pantera L&quot;},{&quot;mpg&quot;:19.7,&quot;cyl&quot;:6,&quot;disp&quot;:145,&quot;hp&quot;:175,&quot;drat&quot;:3.62,&quot;wt&quot;:2.77,&quot;qsec&quot;:15.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:6,&quot;_row&quot;:&quot;Ferrari Dino&quot;},{&quot;mpg&quot;:15,&quot;cyl&quot;:8,&quot;disp&quot;:301,&quot;hp&quot;:335,&quot;drat&quot;:3.54,&quot;wt&quot;:3.57,&quot;qsec&quot;:14.6,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:8,&quot;_row&quot;:&quot;Maserati Bora&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:4,&quot;disp&quot;:121,&quot;hp&quot;:109,&quot;drat&quot;:4.11,&quot;wt&quot;:2.78,&quot;qsec&quot;:18.6,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Volvo 142E&quot;}] 2.5 Importing from other statistical software Common software packages include SAS, STATA and SPSS. Two packages useful for importing data from these packages are: haven: by Hadley Wickham and is under active development. It aims to be more consistent, easier and faster than foreign. It can read SAS, Stata and SPSS and will read in the file as an D dataframe. foreign: is an older package by the R Core Team. Foreign support more data formats than haven including Weka and Systat # Load the haven package library(haven) # Import sales.sas7bdat: sales sales &lt;- read_sas(&quot;sales.sas7bdat&quot;) # Display the structure of sales str(sales) # Import the data from the URL: sugar sugar &lt;- read_dta(&quot;http://assets.datacamp.com/production/course_1478/datasets/trade.dta&quot;) # Structure of sugar str(sugar) # Convert values in Date column to dates sugar$Date &lt;- as.Date(as_factor(sugar$Date)) # Structure of sugar again str(sugar) # Import person.sav: traits traits &lt;- read_sav(&quot;person.sav&quot;) # Summarize traits summary(traits) # Print out a subset subset(traits, Extroversion &gt; 40 &amp; Agreeableness &gt; 40) When using SPSS files, it is often the case that the variable labels are also imported, it is best to change these in to standard R factors. # Import SPSS data from the URL: work work &lt;- read_sav(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav&quot;) # Display summary of work$GENDER summary(work$GENDER) # Convert work$GENDER to a factor work$GENDER &lt;- as_factor(work$GENDER) # Display summary of work$GENDER again summary(work$GENDER) Foreign cannot use single SAS datafiles like haven, it works with SAS library files .xport. Foreign tends to use dots in the function names rather than underscores in haven e.g. read.dta() vs read_dta(). Foreign does not provide consistency with it’s functions i.e. read.dta() has different arguments than read.spss(), however foreign provides more control over the data importing, such as dealing with multiple types of missing data which are often present in survey data, more comprehensively than haven. Although haven is still being developed. # Load the foreign package library(foreign) # Specify the file path using file.path(): path path &lt;- file.path(&quot;worldbank&quot;, &quot;edequality.dta&quot;) # Create and print structure of edu_equal_1 edu_equal_1 &lt;- read.dta(path) str(edu_equal_1) # Create and print structure of edu_equal_2 edu_equal_2 &lt;- read.dta(path, convert.factors = FALSE) str(edu_equal_2) # Create and print structure of edu_equal_3 edu_equal_3 &lt;- read.dta(path, convert.underscore = TRUE) str(edu_equal_3) # Import international.sav as a data frame: demo demo &lt;- read.spss(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/international.sav&quot;, to.data.frame = TRUE) # Create boxplot of gdp variable of demo boxplot(demo$gdp) "],
["references.html", "References", " References "],
["writing-functions-in-r.html", "3 Writing Functions in R 3.1 Function overview 3.2 When and how you should write a function 3.3 Functional Programming 3.4 Advanced Inputs and Outputs 3.5 Robust Functions", " 3 Writing Functions in R Notes taken during/inspired by the Datacamp course ‘Writing Functions in R’ by Hadley and Charlotte Wickham. R can create functions using a basic ‘receipe’ as Hadley calls it my_fun &lt;- function(arg1, arg2) { body } Unlike in other languages, there is no special syntax for naming a function. Once created, there is no difference between the functions you create and the functions created by other developers such as those in base R. Every function has three components The formal arguments The body of the function The environment - usually invisible but determines where the function looks for variables The environemnt is where the function was defined, if just coding the function in this will be the global environment. The output from a function is usually the last expression evaluated. However, it is possible to use a return statement to return(value) which will stop the function at that point. It is possible to have functions without a name, so called ‘anonymous’ functions, which are covered later in this chapter. Anonymous functions can be called, but have to be done on the same line. Course slides: * Part 1 - Refresher * Part 2 - When and how you should write a function * Part 3 - Functional programming * Part 4 - Advanced inputs and outputs * Part 5 - Robust functions Other useful info: Hadley’s Non-standard evaluation functions vignette 3.1 Function overview # Define ratio() function ratio &lt;- function(x, y) { x / y } # Call ratio() with arguments 3 and 4 ratio(3, 4) ## [1] 0.75 There are two ways to specify the arguments - ratio(3, 4), which relies on matching by position, or ratio(x = 3, y = 4), which relies on matching by name. For functions you and others use often, it’s okay to use positional matching for the first one or two arguments. These are usually the data to be computed on. Good examples are the x argument to the summary functions (mean(), sd(), etc.) and the x and y arguments to plotting functions. However, beyond the first couple of arguments you should always use matching by name. It makes your code much easier for you and others to read. This is particularly important if the argument is optional, because it has a default. When overriding a default value, it’s good practice to use the name. Notice that when you call a function, you should place a space around = in function calls, and always put a space after a comma, not before (just like in regular English). Using whitespace makes it easier to skim the function for the important components. In the follwing example we tidy up the function to follow best practice and make it easier to understand and repeat if needed in the future. # Original mean(0.1,x=c(1:9, NA),TRUE) ## [1] 5 # Rewrite the call to follow best practices mean(c(1:9, NA), trim = 0.1, na.rm = TRUE) ## [1] 5 3.1.1 Scoping Scoping describes how R looks up values when given a name. When creating a function, R will look within that function first for a name e.g. x. If it doesn’t exist in that environment, it will look one level up, so if you’ve defined x but not y in a function, R will look for y in the next environment up - if within a single function, this next level up will be the gobal environment. Every time you cann a function, it will get a ‘clean’ environment. 3.1.2 Data Structures There are two main typers of data structure in R - atomic vectors and lists (which are multiple vectors). Lists are useful because they can be used to have different data ypes within them, they have nested vectors in effect. Lists can be referenced either with a single brackedt [], a double braket [[]] which will strip out a level of hierachy or the usual dollar notation $. (#fig:Subsetting Lists)Subsetting List flavours There are a few ways to subset a list. Throughout the course we’ll mostly use double bracket ([[]]) subsetting by index and by name. That is, my_list[[1]] extracts the first element of the list my_list, and my_list[[“name”]] extracts the element in my_list that is called name. If the list is nested you can travel down the heirarchy by recursive subsetting. For example, mylist[[1]][[“name”]] is the element called name inside the first element of my_list. A data frame is just a special kind of list, so you can use double bracket subsetting on data frames too. my_df[[1]] will extract the first column of a data frame and my_df[[“name”]] will extract the column named name from the data frame. # 2nd element in tricky_list typeof(tricky_list[[2]]) # Element called x in tricky_list typeof(tricky_list[[&quot;x&quot;]]) # 2nd element inside the element called x in tricky_list typeof(tricky_list[[&quot;x&quot;]][[2]]) Sometimes the output of models can be quite challenging to get out, with many lists inside a single list. # Guess where the regression model is stored names(tricky_list) # Use names() and str() on the model element names(tricky_list[[&quot;model&quot;]]) str(tricky_list[[&quot;model&quot;]]) # Subset the coefficients element tricky_list[[&quot;model&quot;]][[&quot;coefficients&quot;]] # Subset the wt element tricky_list[[&quot;model&quot;]][[&quot;coefficients&quot;]][[&quot;wt&quot;]] 3.1.3 For loops For loops are used for iteraction. There are a number of parts within a loop: 1: The sequence - describes the name of an object with indexes an iteration e.g. i, and the values that this index should iterate over 2: The body - between the curly braces {} and describes the operations to iterate over, referring back to the index (e.g. i) 3: The output - where should the results of the loop go? OFten this might print to the screen rather than saving the output If you want to repeat a function for each column where the data frame is empty, rather than use the sequence for (i in 1:ncol(df)), it is better to use seq_along(df), since our sequence is now the somewhat non-sensical: 1, 0. You might think you wouldn’t be silly enough to use a for loop with an empty data frame, but once you start writing your own functions, there’s no telling what the input will be. df &lt;- data.frame() 1:ncol(df) # doesn&#39;t handle the empty data well for (i in 1:ncol(df)) { print(median(df[[i]])) } # Replace the 1:ncol(df) sequence for (i in seq_along(df)) { print(median(df[[i]])) } Our for loop does a good job displaying the column medians, but we might want to store these medians in a vector for future use. Before you start the loop, you must always allocate sufficient space for the output, let’s say an object called output. This is very important for efficiency: if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow. A general way of creating an empty vector of given length is the vector() function. It has two arguments: the type of the vector (“logical”, “integer”, “double”, “character”, etc.) and the length of the vector. Then, at each iteration of the loop you must store the output in the corresponding entry of the output vector, i.e. assign the result to output[[i]]. (You might ask why we are using double brackets here when output is a vector. It’s primarily for generalizability: this subsetting will work whether output is a vector or a list.) Let’s edit our loop to store the medians, rather than printing them to the console. # Create new double vector: output output &lt;- vector(&quot;double&quot;, ncol(df)) # Alter the loop for (i in seq_along(df)) { # Change code to store result in output output[i] &lt;- median(df[[i]]) } # Print output output 3.2 When and how you should write a function Why might you write a function? Doing the same thing many times is not efficient and it can lead to errors - you copy the same function/formula from a previous exercise, but forget to change a variable name to fit your new instance. If you have copied and pasted twice, meaning you now have three copies, it is time to write a function As it takes less effort to check the intent of the code - there is just one function to check - more time can be spent on checking the validity or quality of the data and outputs. In addition, since we have created a function, we can use other packages such as the map function in purrr, to repeat (map) our functions repeatadly. 3.2.1 Rescale example We have a snippet of code that successfully rescales a column to be between 0 and 1: (df\\(a - min(df\\)a, na.rm = TRUE)) / (max(df\\(a, na.rm = TRUE) - min(df\\)a, na.rm = TRUE)) Our goal over the next few exercises is to turn this snippet, written to work on the a column in the data frame df, into a general purpose rescale01() function that we can apply to any vector. The first step of turning a snippet into a function is to examine the snippet and decide how many inputs there are, then rewrite the snippet to refer to these inputs using temporary names. These inputs will become the arguments to our function, so choosing good names for them is important. (We’ll talk more about naming arguments in a later exercise.) In this snippet, there is one input: the numeric vector to be rescaled (currently df$a). What would be a good name for this input? It’s quite common in R to refer to a vector of data simply as x (like in the mean function), so we will follow that convention here. # Define example vector x x &lt;- seq(1:10) # Rewrite this snippet to refer to x (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 Our next step is to examine our snippet and see if we can write it more clearly. Take a close look at our rewritten snippet. Do you see any duplication? (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)) One obviously duplicated statement is min(x, na.rm = TRUE). It makes more sense for us just to calculate it once, store the result, and then refer to it when needed. In fact, since we also need the maximum value of x, it would be even better to calculate the range once, then refer to the first and second elements when they are needed. What should we call this intermediate variable? You’ll soon get the message that using good names is an important part of writing clear code! I suggest we call it rng (for “range”). # Define example vector x x &lt;- 1:10 # Define rng rng &lt;- range(x, na.rm = TRUE) # Rewrite this snippet to refer to the elements of rng (x - rng[1]) / (rng[2] - rng[1]) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 What do you need to write a function? You need a name for the function, you need to know the arguments to the function, and you need code that forms the body of the function. # Define example vector x x &lt;- 1:10 # Use the function template to create the rescale01 function rescale01 &lt;- function(x) { # body rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } # Test your function, call rescale01 using the vector x as the argument rescale01(x) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 3.2.2 Write a function step by step Typically we go through a similar process in order to write a function: Start with a simiple problem Get a working snippet of code Rewrite the code to use temporary variables See if there is any duplication that we can minimise and rewrite to remove this With this clearly working version, we can turn it in to a function, by wrapping the code in a function In this next section we are going to go through these steps in order to write a function - both_na() - that counts at how many positions two vectors, x and y, both have a missing value. So first, we start writing some simple code to see how we get something workable. # Define example vectors x and y x &lt;- c( 1, 2, NA, 3, NA) y &lt;- c(NA, 3, NA, 3, 4) # Count how many elements are missing in both x and y sum(is.na(x) &amp; is.na(y)) ## [1] 1 So the function appears to be doing what we want, the function is already quite simple so we don’t need to simplify further, so we proceed to wrap the function. # Turn this snippet into a function: both_na() both_na &lt;- function(x, y) { sum(is.na(x) &amp; is.na(y)) } Now we can test whether the function operates as intended by using some other examples. # Define x, y1 and y2 x &lt;- c(NA, NA, NA) y1 &lt;- c( 1, NA, NA) y2 &lt;- c( 1, NA, NA, NA) # Call both_na on x, y1 both_na(x, y1) ## [1] 2 # Call both_na on x, y2 both_na(x, y2) ## Warning in is.na(x) &amp; is.na(y): longer object length is not a multiple of ## shorter object length ## [1] 3 3.2.3 How can you write a good function? It should have the following criteria fulfilled It should be correct It should be understandable to other people Correct + understandable = obvioulsy correct Good names can help make the data more understandable. Good naming applies to all aspects in R - objects, functions or arguments. Some examples are below, it doesn’t matter which convention you follow although it is important to stick to the approach i.e. be consistent. Long names - these should be lower case and seperated with underscores e.g. row_maxes Other names - don’t override existing variables or functions e.g. T &lt;- FALSE or c &lt;- 10 Function names - use a verb as a name, since the function does something Argument names - use nouns or naming words Argument length - use short names when appropriate e.g. x,y or z Data frames - usually referred to as df Numeric indices - use i and j typically rows and columns Others - n for the number of rows, p for the number of columns Argument order - usually we placed data objects first, like x or df, then things that control the computation next (detail) which should be given default values Also think about having an R style guide. 3.3 Functional Programming Writing for loops in R is not best practice. They are like detailed recipe books which outline very detailed step in a recipe, they don’t rely on any pre-existing knowledge so as a consequence, become very long. This can make it hard to understand and see differences and similarities between different recipes (for loops). For loops tend to relegate the verbs by hiding them in a sea of nouns. Using functional programming allows you to create ‘meta recipes’ which helps to identify what is different and what is the same, by simplifying some of the steps. In the following example we will create a data frame, then try to calculate a median for each column. This could be achieved by repeating median(df[[p]]) for each column p in the df, but it would be a lot of repetition. The following shows how you would use a for loop # Create the dataframe df &lt;- data.frame( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) # Initialize output vector output &lt;- vector(&quot;double&quot;, ncol(df)) # Fill in the body of the for loop for (i in seq_along(df)) { output[i] &lt;- median(df[[i]]) } # View the result output ## [1] 0.30104359 0.07076497 -0.58144482 -0.04331344 Now if we had two more data frames, df2 and df3 we would have something similar to the following # Create the dataframe2 df2 &lt;- data.frame( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) # Create the dataframe3 df3 &lt;- data.frame( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) output &lt;- vector(&quot;double&quot;, ncol(df2)) for (i in seq_along(df2)) { output[[i]] &lt;- median(df2[[i]]) } output ## [1] -0.0641349 0.2422641 -0.2854498 -0.2563127 output &lt;- vector(&quot;double&quot;, ncol(df3)) for (i in seq_along(df3)) { output[[i]] &lt;- median(df3[[i]]) } output ## [1] 0.4657658 0.1358885 -0.2084402 -0.1577409 It would be easier to write a function in this instance. # Turn this code into col_median() col_median &lt;- function(df) { output &lt;- vector(&quot;double&quot;, ncol(df)) for (i in seq_along(df)) { output[[i]] &lt;- median(df[[i]]) } output } col_median(df2) ## [1] -0.0641349 0.2422641 -0.2854498 -0.2563127 col_median(df3) ## [1] 0.4657658 0.1358885 -0.2084402 -0.1577409 And if we wanted means instead of medians we could write a similar function. # Create col_mean() function to find column means col_mean &lt;- function(df) { output &lt;- numeric(length(df)) for (i in seq_along(df)) { output[[i]] &lt;- mean(df[[i]]) } output } col_mean(df2) ## [1] 0.1970742 0.1477954 -0.1617852 -0.1566563 col_mean(df3) ## [1] 0.45339778 0.34380317 0.12075255 0.06709137 And if we wanted standard deviations # Define col_sd() function col_sd &lt;- function(df) { output &lt;- numeric(length(df)) for (i in seq_along(df)) { output[[i]] &lt;- sd(df[[i]]) } output } col_sd(df2) ## [1] 1.0386878 0.6958183 1.3089331 0.8902624 col_sd(df3) ## [1] 1.1144186 1.3909175 1.2245477 0.8780557 We have now copied our median function twice - to create mean and sd functions. It would be writter to write a function which will take column summaries for any summary function we provide. f &lt;- function(x, power) { # Edit the body to return absolute deviations raised to power abs(x - mean(x)) ^ power } We can also use functions as arguments, by replacing the mean or median elements with fun, where fun = the summary function desired. col_summary &lt;- function(df, fun) { output &lt;- vector(&quot;numeric&quot;, length(df)) for (i in seq_along(df)) { output[[i]] &lt;- fun(df[[i]]) } output } 3.3.1 Using purrr Passing functions as arguments is quite a common task in R, we can use the map function in purrr to achieve this. Every function in the purrr package takes a vector to begin with (.x) and loops over it to do something (.f) e.g. map_dbl(.x, .f, …). The types of map functions are: map() returns a list map_dbl() returns a double vector map_lgl() returns a logical vector map_int() returns a integer vector map_chr() returns a character vector Arguments to map() are .x and .f and not x and f because .x and .f are very unlikely to be argument names you might pass through the …, thereby preventing confusion about whether an argument belongs to map() or to the function being mapped. # load the package and examine the data library(purrr) library(nycflights13) ## Warning: package &#39;nycflights13&#39; was built under R version 3.4.2 library(dplyr) # to select just the numeric columns of our dataset ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union str(planes) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3322 obs. of 9 variables: ## $ tailnum : chr &quot;N10156&quot; &quot;N102UW&quot; &quot;N103US&quot; &quot;N104UW&quot; ... ## $ year : int 2004 1998 1999 1999 2002 1999 1999 1999 1999 1999 ... ## $ type : chr &quot;Fixed wing multi engine&quot; &quot;Fixed wing multi engine&quot; &quot;Fixed wing multi engine&quot; &quot;Fixed wing multi engine&quot; ... ## $ manufacturer: chr &quot;EMBRAER&quot; &quot;AIRBUS INDUSTRIE&quot; &quot;AIRBUS INDUSTRIE&quot; &quot;AIRBUS INDUSTRIE&quot; ... ## $ model : chr &quot;EMB-145XR&quot; &quot;A320-214&quot; &quot;A320-214&quot; &quot;A320-214&quot; ... ## $ engines : int 2 2 2 2 2 2 2 2 2 2 ... ## $ seats : int 55 182 182 182 55 182 182 182 182 182 ... ## $ speed : int NA NA NA NA NA NA NA NA NA NA ... ## $ engine : chr &quot;Turbo-fan&quot; &quot;Turbo-fan&quot; &quot;Turbo-fan&quot; &quot;Turbo-fan&quot; ... planes2 &lt;- select_if(planes, is.numeric) # Find the mean of each column map_dbl(planes2, mean) ## year engines seats speed ## NA 1.995184 154.316376 NA # Find the mean of each column, excluding missing values map_dbl(planes2, mean, na.rm = T) ## year engines seats speed ## 2000.484010 1.995184 154.316376 236.782609 # Find the 5th percentile of each column, excluding missing values map_dbl(planes2, quantile, probs = .05, na.rm = T) ## year engines seats speed ## 1988.0 2.0 55.0 90.5 # Find the columns that are numeric map_lgl(df3, is.numeric) ## a b c d ## TRUE TRUE TRUE TRUE # Find the type of each column map_chr(df3, typeof) ## a b c d ## &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; # Find a summary of each column map(df3, summary) ## $a ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.3180 -0.3523 0.4658 0.4534 1.4880 1.9203 ## ## $b ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.2480 -0.5321 0.1359 0.3438 0.7812 3.5138 ## ## $c ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.7018 -0.4845 -0.2084 0.1208 0.4456 2.7308 ## ## $d ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.84672 -0.56942 -0.15774 0.06709 0.57799 1.87096 3.3.2 Shortcuts You can define an anonymous function on the fly like map(df, function(x) sum(is.na(x))) Which will count how many missing values are in x. This allows you to create and map your own functions. In the next set of exercises we will use the mtcars data, but split the dataset in to a set for each number of cylinders in the engine e.g. 4, 6 and 8 cylinders. Our goal is to fit a separate linear regression of miles per gallon (mpg) against weight (wt) for each group of cars in our list of data frames, where each data frame in our list represents a different group. How should we get started? First, let’s confirm the structure of this list of data frames. Then, we’ll solve a simpler problem first: fit the regression to the first group of cars. # Split the data cyl &lt;- split(mtcars, mtcars$cyl) # Examine the structure of cyl str(cyl) ## List of 3 ## $ 4:&#39;data.frame&#39;: 11 obs. of 11 variables: ## ..$ mpg : num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ... ## ..$ cyl : num [1:11] 4 4 4 4 4 4 4 4 4 4 ... ## ..$ disp: num [1:11] 108 146.7 140.8 78.7 75.7 ... ## ..$ hp : num [1:11] 93 62 95 66 52 65 97 66 91 113 ... ## ..$ drat: num [1:11] 3.85 3.69 3.92 4.08 4.93 4.22 3.7 4.08 4.43 3.77 ... ## ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ... ## ..$ qsec: num [1:11] 18.6 20 22.9 19.5 18.5 ... ## ..$ vs : num [1:11] 1 1 1 1 1 1 1 1 0 1 ... ## ..$ am : num [1:11] 1 0 0 1 1 1 0 1 1 1 ... ## ..$ gear: num [1:11] 4 4 4 4 4 4 3 4 5 5 ... ## ..$ carb: num [1:11] 1 2 2 1 2 1 1 1 2 2 ... ## $ 6:&#39;data.frame&#39;: 7 obs. of 11 variables: ## ..$ mpg : num [1:7] 21 21 21.4 18.1 19.2 17.8 19.7 ## ..$ cyl : num [1:7] 6 6 6 6 6 6 6 ## ..$ disp: num [1:7] 160 160 258 225 168 ... ## ..$ hp : num [1:7] 110 110 110 105 123 123 175 ## ..$ drat: num [1:7] 3.9 3.9 3.08 2.76 3.92 3.92 3.62 ## ..$ wt : num [1:7] 2.62 2.88 3.21 3.46 3.44 ... ## ..$ qsec: num [1:7] 16.5 17 19.4 20.2 18.3 ... ## ..$ vs : num [1:7] 0 0 1 1 1 1 0 ## ..$ am : num [1:7] 1 1 0 0 0 0 1 ## ..$ gear: num [1:7] 4 4 3 3 4 4 5 ## ..$ carb: num [1:7] 4 4 1 1 4 4 6 ## $ 8:&#39;data.frame&#39;: 14 obs. of 11 variables: ## ..$ mpg : num [1:14] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 ... ## ..$ cyl : num [1:14] 8 8 8 8 8 8 8 8 8 8 ... ## ..$ disp: num [1:14] 360 360 276 276 276 ... ## ..$ hp : num [1:14] 175 245 180 180 180 205 215 230 150 150 ... ## ..$ drat: num [1:14] 3.15 3.21 3.07 3.07 3.07 2.93 3 3.23 2.76 3.15 ... ## ..$ wt : num [1:14] 3.44 3.57 4.07 3.73 3.78 ... ## ..$ qsec: num [1:14] 17 15.8 17.4 17.6 18 ... ## ..$ vs : num [1:14] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ am : num [1:14] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ gear: num [1:14] 3 3 3 3 3 3 3 3 3 3 ... ## ..$ carb: num [1:14] 2 4 3 3 3 4 4 4 2 2 ... # Extract the first element into four_cyls four_cyls &lt;- cyl[[1]] # Fit a linear regression of mpg on wt using four_cyls lm(wt ~ mpg, four_cyls) ## ## Call: ## lm(formula = wt ~ mpg, data = four_cyls) ## ## Coefficients: ## (Intercept) mpg ## 4.68734 -0.09007 We now have a snippet of code that performs the operation we want on one data frame. One option would be to turn this into a function, for example: fit_reg &lt;- function(df) { lm(mpg ~ wt, data = df) } Then pass this function into map(): map(cyl, fit_reg) But it seems a bit much to define a function for such a specific model when we only want to do this once. Instead of defining the function in the global environment, we will just use the function anonymously inside our call to map(). What does this mean? Instead of referring to our function by name in map(), we define it on the fly in the .f argument to map() # Rewrite to call an anonymous function map(cyl, function(df) lm(mpg ~ wt, data = df)) ## $`4` ## ## Call: ## lm(formula = mpg ~ wt, data = df) ## ## Coefficients: ## (Intercept) wt ## 39.571 -5.647 ## ## ## $`6` ## ## Call: ## lm(formula = mpg ~ wt, data = df) ## ## Coefficients: ## (Intercept) wt ## 28.41 -2.78 ## ## ## $`8` ## ## Call: ## lm(formula = mpg ~ wt, data = df) ## ## Coefficients: ## (Intercept) wt ## 23.868 -2.192 Writing anonymous functions takes a lot of extra key strokes, so purrr provides a shortcut that allows you to write an anonymous function as a one-sided formula instead. In R, a one-sided formula starts with a ~, followed by an R expression. In purrr’s map functions, the R expression can refer to an element of the .x argument using the . character. Let’s take a look at an example. Imagine, instead of a regression on each data frame in cyl, we wanted to know the mean displacement for each data frame. One way to do this would be to use an anonymous function: map_dbl(cyl, function(df) mean(df$disp)) To perform the same operation using the formula shortcut, we replace the function definition (function(df)) with the ~, then when we need to refer to the element of cyl the function operates on (in this case df), we use a .. map_dbl(cyl, ~ mean(.$disp)) Much less typing. It also saves you from coming up with an argument name. Lets rewrite our previous anonymous function using this formula shortcut instead. # Rewrite to use the formula shortcut instead map(cyl, ~ lm(mpg ~ wt, data = .)) ## $`4` ## ## Call: ## lm(formula = mpg ~ wt, data = .) ## ## Coefficients: ## (Intercept) wt ## 39.571 -5.647 ## ## ## $`6` ## ## Call: ## lm(formula = mpg ~ wt, data = .) ## ## Coefficients: ## (Intercept) wt ## 28.41 -2.78 ## ## ## $`8` ## ## Call: ## lm(formula = mpg ~ wt, data = .) ## ## Coefficients: ## (Intercept) wt ## 23.868 -2.192 There are also some useful shortcuts that come in handy when you want to subset each element of the .x argument. If the .f argument to a map function is set equal to a string, let’s say “name”, then purrr extracts the “name” element from every element of .x. This is a really common situation you find yourself in when you work with nested lists. For example, if we have a list of where every element contains an a and b element: list_of_results &lt;- list( list(a = 1, b = “A”), list(a = 2, b = “C”), list(a = 3, b = “D”) ) We might want to pull out the a element from every entry. We could do it with the string shortcut like this: map(list_of_results, “a”) Now take our list of regresssion models: map(cyl, ~ lm(mpg ~ wt, data = .)) It might be nice to extract the slope coefficient from each model. You’ll do this in a few steps: first fit the models, then get the coefficients from each model using the coef() function, then pull out the wt estimate using the string shortcut. # Save the result from the previous exercise to the variable models models &lt;- map(cyl, ~ lm(mpg ~ wt, data = .)) # Use map and coef to get the coefficients for each model: coefs coefs &lt;- map(models, ~ coef(.)) # Use string shortcut to extract the wt coefficient map(coefs, &quot;wt&quot;) ## $`4` ## [1] -5.647025 ## ## $`6` ## [1] -2.780106 ## ## $`8` ## [1] -2.192438 Another useful shortcut for subsetting is to pass a numeric vector as the .f argument. This works just like passing a string but subsets by index rather than name. For example, with your previous list_of_results: list_of_results &lt;- list( list(a = 1, b = “A”), list(a = 2, b = “C”), list(a = 3, b = “D”) ) Another way to pull out the a element from each list, is to pull out the first element: map(list_of_results, 1) Let’s pull out the slopes from our models again, but this time using numeric subsetting. Also, since we are pulling out a single numeric value from each element, let’s use map_dbl(). # use map_dbl with the numeric shortcut to pull out the second element map_dbl(coefs, 1) ## 4 6 8 ## 39.57120 28.40884 23.86803 purrr also includes a pipe operator: %&gt;%. The pipe operator is another shortcut that saves typing, but also increases readability. The example below pulls out the R2 from each model. Rewrite the last two lines to use a pipe instead. # Define models (don&#39;t change) models &lt;- mtcars %&gt;% split(mtcars$cyl) %&gt;% map(~ lm(mpg ~ wt, data = .)) # Original code # summaries &lt;- map(models, summary) # map_dbl(summaries, &quot;r.squared&quot;) # Re-writen to single command with pipes models %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) ## 4 6 8 ## 0.5086326 0.4645102 0.4229655 3.4 Advanced Inputs and Outputs When using purrr, problems like errors with a single item within the map function, for instance char instead of numeric data in a list of lists, can cause errors with all the map functions. We don’t know which element caused the error and we cant access any of the succesful runs. Purrr comes with a function called safely, which will return both successful elements and errors. It will take a function and return a variaton of that function which will never give an error - it will return two results, those successful (result) and those not (errors). This is one adverb for dealing with unusual output: safely() captures the successful result or the error, always returns a list possibly() always succeeds, you give it a default value to return when there is an error quietly() captures printed output, messages, and warnings instead of capturing errors # Create safe_readLines() by passing readLines() to safely() safe_readLines &lt;- safely(readLines) # Call safe_readLines() on &quot;http://example.org&quot; safe_readLines(&quot;http://example.org&quot;) ## $result ## [1] &quot;&lt;!doctype html&gt;&quot; ## [2] &quot;&lt;html&gt;&quot; ## [3] &quot;&lt;head&gt;&quot; ## [4] &quot; &lt;title&gt;Example Domain&lt;/title&gt;&quot; ## [5] &quot;&quot; ## [6] &quot; &lt;meta charset=\\&quot;utf-8\\&quot; /&gt;&quot; ## [7] &quot; &lt;meta http-equiv=\\&quot;Content-type\\&quot; content=\\&quot;text/html; charset=utf-8\\&quot; /&gt;&quot; ## [8] &quot; &lt;meta name=\\&quot;viewport\\&quot; content=\\&quot;width=device-width, initial-scale=1\\&quot; /&gt;&quot; ## [9] &quot; &lt;style type=\\&quot;text/css\\&quot;&gt;&quot; ## [10] &quot; body {&quot; ## [11] &quot; background-color: #f0f0f2;&quot; ## [12] &quot; margin: 0;&quot; ## [13] &quot; padding: 0;&quot; ## [14] &quot; font-family: \\&quot;Open Sans\\&quot;, \\&quot;Helvetica Neue\\&quot;, Helvetica, Arial, sans-serif;&quot; ## [15] &quot; &quot; ## [16] &quot; }&quot; ## [17] &quot; div {&quot; ## [18] &quot; width: 600px;&quot; ## [19] &quot; margin: 5em auto;&quot; ## [20] &quot; padding: 50px;&quot; ## [21] &quot; background-color: #fff;&quot; ## [22] &quot; border-radius: 1em;&quot; ## [23] &quot; }&quot; ## [24] &quot; a:link, a:visited {&quot; ## [25] &quot; color: #38488f;&quot; ## [26] &quot; text-decoration: none;&quot; ## [27] &quot; }&quot; ## [28] &quot; @media (max-width: 700px) {&quot; ## [29] &quot; body {&quot; ## [30] &quot; background-color: #fff;&quot; ## [31] &quot; }&quot; ## [32] &quot; div {&quot; ## [33] &quot; width: auto;&quot; ## [34] &quot; margin: 0 auto;&quot; ## [35] &quot; border-radius: 0;&quot; ## [36] &quot; padding: 1em;&quot; ## [37] &quot; }&quot; ## [38] &quot; }&quot; ## [39] &quot; &lt;/style&gt; &quot; ## [40] &quot;&lt;/head&gt;&quot; ## [41] &quot;&quot; ## [42] &quot;&lt;body&gt;&quot; ## [43] &quot;&lt;div&gt;&quot; ## [44] &quot; &lt;h1&gt;Example Domain&lt;/h1&gt;&quot; ## [45] &quot; &lt;p&gt;This domain is established to be used for illustrative examples in documents. You may use this&quot; ## [46] &quot; domain in examples without prior coordination or asking for permission.&lt;/p&gt;&quot; ## [47] &quot; &lt;p&gt;&lt;a href=\\&quot;http://www.iana.org/domains/example\\&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;&quot; ## [48] &quot;&lt;/div&gt;&quot; ## [49] &quot;&lt;/body&gt;&quot; ## [50] &quot;&lt;/html&gt;&quot; ## ## $error ## NULL # Call safe_readLines() on &quot;http://asdfasdasdkfjlda&quot; safe_readLines(&quot;http://asdfasdasdkfjlda&quot;) ## Warning in file(con, &quot;r&quot;): InternetOpenUrl failed: &#39;The server name or ## address could not be resolved&#39; ## $result ## NULL ## ## $error ## &lt;simpleError in file(con, &quot;r&quot;): cannot open the connection&gt; Safely also works with map functions. # Define safe_readLines() safe_readLines &lt;- safely(readLines) urls &lt;- list( example = &quot;http://example.org&quot;, rproj = &quot;http://www.r-project.org&quot;, asdf = &quot;http://asdfasdasdkfjlda&quot; ) # Use the safe_readLines() function with map(): html html &lt;- map(urls, safe_readLines) ## Warning in file(con, &quot;r&quot;): InternetOpenUrl failed: &#39;The server name or ## address could not be resolved&#39; # Call str() on html str(html) ## List of 3 ## $ example:List of 2 ## ..$ result: chr [1:50] &quot;&lt;!doctype html&gt;&quot; &quot;&lt;html&gt;&quot; &quot;&lt;head&gt;&quot; &quot; &lt;title&gt;Example Domain&lt;/title&gt;&quot; ... ## ..$ error : NULL ## $ rproj :List of 2 ## ..$ result: chr [1:122] &quot;&lt;!DOCTYPE html&gt;&quot; &quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot; &quot; &lt;head&gt;&quot; &quot; &lt;meta charset=\\&quot;utf-8\\&quot;&gt;&quot; ... ## ..$ error : NULL ## $ asdf :List of 2 ## ..$ result: NULL ## ..$ error :List of 2 ## .. ..$ message: chr &quot;cannot open the connection&quot; ## .. ..$ call : language file(con, &quot;r&quot;) ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; # Extract the result from one of the successful elements html[[1]] ## $result ## [1] &quot;&lt;!doctype html&gt;&quot; ## [2] &quot;&lt;html&gt;&quot; ## [3] &quot;&lt;head&gt;&quot; ## [4] &quot; &lt;title&gt;Example Domain&lt;/title&gt;&quot; ## [5] &quot;&quot; ## [6] &quot; &lt;meta charset=\\&quot;utf-8\\&quot; /&gt;&quot; ## [7] &quot; &lt;meta http-equiv=\\&quot;Content-type\\&quot; content=\\&quot;text/html; charset=utf-8\\&quot; /&gt;&quot; ## [8] &quot; &lt;meta name=\\&quot;viewport\\&quot; content=\\&quot;width=device-width, initial-scale=1\\&quot; /&gt;&quot; ## [9] &quot; &lt;style type=\\&quot;text/css\\&quot;&gt;&quot; ## [10] &quot; body {&quot; ## [11] &quot; background-color: #f0f0f2;&quot; ## [12] &quot; margin: 0;&quot; ## [13] &quot; padding: 0;&quot; ## [14] &quot; font-family: \\&quot;Open Sans\\&quot;, \\&quot;Helvetica Neue\\&quot;, Helvetica, Arial, sans-serif;&quot; ## [15] &quot; &quot; ## [16] &quot; }&quot; ## [17] &quot; div {&quot; ## [18] &quot; width: 600px;&quot; ## [19] &quot; margin: 5em auto;&quot; ## [20] &quot; padding: 50px;&quot; ## [21] &quot; background-color: #fff;&quot; ## [22] &quot; border-radius: 1em;&quot; ## [23] &quot; }&quot; ## [24] &quot; a:link, a:visited {&quot; ## [25] &quot; color: #38488f;&quot; ## [26] &quot; text-decoration: none;&quot; ## [27] &quot; }&quot; ## [28] &quot; @media (max-width: 700px) {&quot; ## [29] &quot; body {&quot; ## [30] &quot; background-color: #fff;&quot; ## [31] &quot; }&quot; ## [32] &quot; div {&quot; ## [33] &quot; width: auto;&quot; ## [34] &quot; margin: 0 auto;&quot; ## [35] &quot; border-radius: 0;&quot; ## [36] &quot; padding: 1em;&quot; ## [37] &quot; }&quot; ## [38] &quot; }&quot; ## [39] &quot; &lt;/style&gt; &quot; ## [40] &quot;&lt;/head&gt;&quot; ## [41] &quot;&quot; ## [42] &quot;&lt;body&gt;&quot; ## [43] &quot;&lt;div&gt;&quot; ## [44] &quot; &lt;h1&gt;Example Domain&lt;/h1&gt;&quot; ## [45] &quot; &lt;p&gt;This domain is established to be used for illustrative examples in documents. You may use this&quot; ## [46] &quot; domain in examples without prior coordination or asking for permission.&lt;/p&gt;&quot; ## [47] &quot; &lt;p&gt;&lt;a href=\\&quot;http://www.iana.org/domains/example\\&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;&quot; ## [48] &quot;&lt;/div&gt;&quot; ## [49] &quot;&lt;/body&gt;&quot; ## [50] &quot;&lt;/html&gt;&quot; ## ## $error ## NULL # Extract the error from the element that was unsuccessful html[[3]] ## $result ## NULL ## ## $error ## &lt;simpleError in file(con, &quot;r&quot;): cannot open the connection&gt; We now have output that contains the HTML for each of the two URLs on which readLines() was successful and the error for the other. But the output isn’t that easy to work with, since the results and errors are buried in the inner-most level of the list. purrr provides a function transpose() that reshapes a list so the inner-most level becomes the outer-most level. In otherwords, it turns a list-of-lists “inside-out”. Consider the following list: nested_list &lt;- list( x1 = list(a = 1, b = 2), x2 = list(a = 3, b = 4) ) If I need to extract the a element in x1, I could do nested_list[[“x1”]][[“a”]]. However, if I transpose the list first, the order of subsetting reverses. That is, to extract the same element I could also do transpose(nested_list)[[“a”]][[“x1”]]. This is really handy for safe output, since we can grab all the results or all the errors really easily. # Define save_readLines() and html safe_readLines &lt;- safely(readLines) html &lt;- map(urls, safe_readLines) ## Warning in file(con, &quot;r&quot;): InternetOpenUrl failed: &#39;The server name or ## address could not be resolved&#39; # Examine the structure of transpose(html) str(transpose(html)) ## List of 2 ## $ result:List of 3 ## ..$ example: chr [1:50] &quot;&lt;!doctype html&gt;&quot; &quot;&lt;html&gt;&quot; &quot;&lt;head&gt;&quot; &quot; &lt;title&gt;Example Domain&lt;/title&gt;&quot; ... ## ..$ rproj : chr [1:122] &quot;&lt;!DOCTYPE html&gt;&quot; &quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot; &quot; &lt;head&gt;&quot; &quot; &lt;meta charset=\\&quot;utf-8\\&quot;&gt;&quot; ... ## ..$ asdf : NULL ## $ error :List of 3 ## ..$ example: NULL ## ..$ rproj : NULL ## ..$ asdf :List of 2 ## .. ..$ message: chr &quot;cannot open the connection&quot; ## .. ..$ call : language file(con, &quot;r&quot;) ## .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; # Extract the results: res res &lt;- transpose(html)[[&quot;result&quot;]] # Extract the errors: errs errs &lt;- transpose(html)[[&quot;error&quot;]] What you do with the errors and results is up to you. But, commonly you’ll want to collect all the results for the elements that were successful and examine the inputs for all those that weren’t. # Initialize some objects safe_readLines &lt;- safely(readLines) html &lt;- map(urls, safe_readLines) ## Warning in file(con, &quot;r&quot;): InternetOpenUrl failed: &#39;The server name or ## address could not be resolved&#39; res &lt;- transpose(html)[[&quot;result&quot;]] errs &lt;- transpose(html)[[&quot;error&quot;]] # Create a logical vector is_ok is_ok &lt;- map_lgl(errs, is_null) # Extract the successful results res[is_ok] ## $example ## [1] &quot;&lt;!doctype html&gt;&quot; ## [2] &quot;&lt;html&gt;&quot; ## [3] &quot;&lt;head&gt;&quot; ## [4] &quot; &lt;title&gt;Example Domain&lt;/title&gt;&quot; ## [5] &quot;&quot; ## [6] &quot; &lt;meta charset=\\&quot;utf-8\\&quot; /&gt;&quot; ## [7] &quot; &lt;meta http-equiv=\\&quot;Content-type\\&quot; content=\\&quot;text/html; charset=utf-8\\&quot; /&gt;&quot; ## [8] &quot; &lt;meta name=\\&quot;viewport\\&quot; content=\\&quot;width=device-width, initial-scale=1\\&quot; /&gt;&quot; ## [9] &quot; &lt;style type=\\&quot;text/css\\&quot;&gt;&quot; ## [10] &quot; body {&quot; ## [11] &quot; background-color: #f0f0f2;&quot; ## [12] &quot; margin: 0;&quot; ## [13] &quot; padding: 0;&quot; ## [14] &quot; font-family: \\&quot;Open Sans\\&quot;, \\&quot;Helvetica Neue\\&quot;, Helvetica, Arial, sans-serif;&quot; ## [15] &quot; &quot; ## [16] &quot; }&quot; ## [17] &quot; div {&quot; ## [18] &quot; width: 600px;&quot; ## [19] &quot; margin: 5em auto;&quot; ## [20] &quot; padding: 50px;&quot; ## [21] &quot; background-color: #fff;&quot; ## [22] &quot; border-radius: 1em;&quot; ## [23] &quot; }&quot; ## [24] &quot; a:link, a:visited {&quot; ## [25] &quot; color: #38488f;&quot; ## [26] &quot; text-decoration: none;&quot; ## [27] &quot; }&quot; ## [28] &quot; @media (max-width: 700px) {&quot; ## [29] &quot; body {&quot; ## [30] &quot; background-color: #fff;&quot; ## [31] &quot; }&quot; ## [32] &quot; div {&quot; ## [33] &quot; width: auto;&quot; ## [34] &quot; margin: 0 auto;&quot; ## [35] &quot; border-radius: 0;&quot; ## [36] &quot; padding: 1em;&quot; ## [37] &quot; }&quot; ## [38] &quot; }&quot; ## [39] &quot; &lt;/style&gt; &quot; ## [40] &quot;&lt;/head&gt;&quot; ## [41] &quot;&quot; ## [42] &quot;&lt;body&gt;&quot; ## [43] &quot;&lt;div&gt;&quot; ## [44] &quot; &lt;h1&gt;Example Domain&lt;/h1&gt;&quot; ## [45] &quot; &lt;p&gt;This domain is established to be used for illustrative examples in documents. You may use this&quot; ## [46] &quot; domain in examples without prior coordination or asking for permission.&lt;/p&gt;&quot; ## [47] &quot; &lt;p&gt;&lt;a href=\\&quot;http://www.iana.org/domains/example\\&quot;&gt;More information...&lt;/a&gt;&lt;/p&gt;&quot; ## [48] &quot;&lt;/div&gt;&quot; ## [49] &quot;&lt;/body&gt;&quot; ## [50] &quot;&lt;/html&gt;&quot; ## ## $rproj ## [1] &quot;&lt;!DOCTYPE html&gt;&quot; ## [2] &quot;&lt;html lang=\\&quot;en\\&quot;&gt;&quot; ## [3] &quot; &lt;head&gt;&quot; ## [4] &quot; &lt;meta charset=\\&quot;utf-8\\&quot;&gt;&quot; ## [5] &quot; &lt;meta http-equiv=\\&quot;X-UA-Compatible\\&quot; content=\\&quot;IE=edge\\&quot;&gt;&quot; ## [6] &quot; &lt;meta name=\\&quot;viewport\\&quot; content=\\&quot;width=device-width, initial-scale=1\\&quot;&gt;&quot; ## [7] &quot; &lt;title&gt;R: The R Project for Statistical Computing&lt;/title&gt;&quot; ## [8] &quot;&quot; ## [9] &quot; &lt;link rel=\\&quot;icon\\&quot; type=\\&quot;image/png\\&quot; href=\\&quot;/favicon-32x32.png\\&quot; sizes=\\&quot;32x32\\&quot; /&gt;&quot; ## [10] &quot; &lt;link rel=\\&quot;icon\\&quot; type=\\&quot;image/png\\&quot; href=\\&quot;/favicon-16x16.png\\&quot; sizes=\\&quot;16x16\\&quot; /&gt;&quot; ## [11] &quot;&quot; ## [12] &quot; &lt;!-- Bootstrap --&gt;&quot; ## [13] &quot; &lt;link href=\\&quot;/css/bootstrap.min.css\\&quot; rel=\\&quot;stylesheet\\&quot;&gt;&quot; ## [14] &quot; &lt;link href=\\&quot;/css/R.css\\&quot; rel=\\&quot;stylesheet\\&quot;&gt;&quot; ## [15] &quot;&quot; ## [16] &quot; &lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt;&quot; ## [17] &quot; &lt;!-- WARNING: Respond.js doesn&#39;t work if you view the page via file:// --&gt;&quot; ## [18] &quot; &lt;!--[if lt IE 9]&gt;&quot; ## [19] &quot; &lt;script src=\\&quot;https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\\&quot;&gt;&lt;/script&gt;&quot; ## [20] &quot; &lt;script src=\\&quot;https://oss.maxcdn.com/respond/1.4.2/respond.min.js\\&quot;&gt;&lt;/script&gt;&quot; ## [21] &quot; &lt;![endif]--&gt;&quot; ## [22] &quot; &lt;/head&gt;&quot; ## [23] &quot; &lt;body&gt;&quot; ## [24] &quot; &lt;div class=\\&quot;container page\\&quot;&gt;&quot; ## [25] &quot; &lt;div class=\\&quot;row\\&quot;&gt;&quot; ## [26] &quot; &lt;div class=\\&quot;col-xs-12 col-sm-offset-1 col-sm-2 sidebar\\&quot; role=\\&quot;navigation\\&quot;&gt;&quot; ## [27] &quot;&lt;div class=\\&quot;row\\&quot;&gt;&quot; ## [28] &quot;&lt;div class=\\&quot;col-xs-6 col-sm-12\\&quot;&gt;&quot; ## [29] &quot;&lt;p&gt;&lt;a href=\\&quot;/\\&quot;&gt;&lt;img src=\\&quot;/Rlogo.png\\&quot; width=\\&quot;100\\&quot; height=\\&quot;78\\&quot; alt = \\&quot;R\\&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&quot; ## [30] &quot;&lt;p&gt;&lt;small&gt;&lt;a href=\\&quot;/\\&quot;&gt;[Home]&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;&quot; ## [31] &quot;&lt;h2 id=\\&quot;download\\&quot;&gt;Download&lt;/h2&gt;&quot; ## [32] &quot;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/mirrors.html\\&quot;&gt;CRAN&lt;/a&gt;&lt;/p&gt;&quot; ## [33] &quot;&lt;h2 id=\\&quot;r-project\\&quot;&gt;R Project&lt;/h2&gt;&quot; ## [34] &quot;&lt;ul&gt;&quot; ## [35] &quot;&lt;li&gt;&lt;a href=\\&quot;/about.html\\&quot;&gt;About R&lt;/a&gt;&lt;/li&gt;&quot; ## [36] &quot;&lt;li&gt;&lt;a href=\\&quot;/logo/\\&quot;&gt;Logo&lt;/a&gt;&lt;/li&gt;&quot; ## [37] &quot;&lt;li&gt;&lt;a href=\\&quot;/contributors.html\\&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt;&quot; ## [38] &quot;&lt;li&gt;&lt;a href=\\&quot;/news.html\\&quot;&gt;Whatâs New?&lt;/a&gt;&lt;/li&gt;&quot; ## [39] &quot;&lt;li&gt;&lt;a href=\\&quot;/bugs.html\\&quot;&gt;Reporting Bugs&lt;/a&gt;&lt;/li&gt;&quot; ## [40] &quot;&lt;li&gt;&lt;a href=\\&quot;http://developer.R-project.org\\&quot;&gt;Development Site&lt;/a&gt;&lt;/li&gt;&quot; ## [41] &quot;&lt;li&gt;&lt;a href=\\&quot;/conferences.html\\&quot;&gt;Conferences&lt;/a&gt;&lt;/li&gt;&quot; ## [42] &quot;&lt;li&gt;&lt;a href=\\&quot;/search.html\\&quot;&gt;Search&lt;/a&gt;&lt;/li&gt;&quot; ## [43] &quot;&lt;/ul&gt;&quot; ## [44] &quot;&lt;/div&gt;&quot; ## [45] &quot;&lt;div class=\\&quot;col-xs-6 col-sm-12\\&quot;&gt;&quot; ## [46] &quot;&lt;h2 id=\\&quot;r-foundation\\&quot;&gt;R Foundation&lt;/h2&gt;&quot; ## [47] &quot;&lt;ul&gt;&quot; ## [48] &quot;&lt;li&gt;&lt;a href=\\&quot;/foundation/\\&quot;&gt;Foundation&lt;/a&gt;&lt;/li&gt;&quot; ## [49] &quot;&lt;li&gt;&lt;a href=\\&quot;/foundation/board.html\\&quot;&gt;Board&lt;/a&gt;&lt;/li&gt;&quot; ## [50] &quot;&lt;li&gt;&lt;a href=\\&quot;/foundation/members.html\\&quot;&gt;Members&lt;/a&gt;&lt;/li&gt;&quot; ## [51] &quot;&lt;li&gt;&lt;a href=\\&quot;/foundation/donors.html\\&quot;&gt;Donors&lt;/a&gt;&lt;/li&gt;&quot; ## [52] &quot;&lt;li&gt;&lt;a href=\\&quot;/foundation/donations.html\\&quot;&gt;Donate&lt;/a&gt;&lt;/li&gt;&quot; ## [53] &quot;&lt;/ul&gt;&quot; ## [54] &quot;&lt;h2 id=\\&quot;help-with-r\\&quot;&gt;Help With R&lt;/h2&gt;&quot; ## [55] &quot;&lt;ul&gt;&quot; ## [56] &quot;&lt;li&gt;&lt;a href=\\&quot;/help.html\\&quot;&gt;Getting Help&lt;/a&gt;&lt;/li&gt;&quot; ## [57] &quot;&lt;/ul&gt;&quot; ## [58] &quot;&lt;h2 id=\\&quot;documentation\\&quot;&gt;Documentation&lt;/h2&gt;&quot; ## [59] &quot;&lt;ul&gt;&quot; ## [60] &quot;&lt;li&gt;&lt;a href=\\&quot;http://cran.r-project.org/manuals.html\\&quot;&gt;Manuals&lt;/a&gt;&lt;/li&gt;&quot; ## [61] &quot;&lt;li&gt;&lt;a href=\\&quot;http://cran.r-project.org/faqs.html\\&quot;&gt;FAQs&lt;/a&gt;&lt;/li&gt;&quot; ## [62] &quot;&lt;li&gt;&lt;a href=\\&quot;http://journal.r-project.org\\&quot;&gt;The R Journal&lt;/a&gt;&lt;/li&gt;&quot; ## [63] &quot;&lt;li&gt;&lt;a href=\\&quot;/doc/bib/R-books.html\\&quot;&gt;Books&lt;/a&gt;&lt;/li&gt;&quot; ## [64] &quot;&lt;li&gt;&lt;a href=\\&quot;/certification.html\\&quot;&gt;Certification&lt;/a&gt;&lt;/li&gt;&quot; ## [65] &quot;&lt;li&gt;&lt;a href=\\&quot;/other-docs.html\\&quot;&gt;Other&lt;/a&gt;&lt;/li&gt;&quot; ## [66] &quot;&lt;/ul&gt;&quot; ## [67] &quot;&lt;h2 id=\\&quot;links\\&quot;&gt;Links&lt;/h2&gt;&quot; ## [68] &quot;&lt;ul&gt;&quot; ## [69] &quot;&lt;li&gt;&lt;a href=\\&quot;http://www.bioconductor.org\\&quot;&gt;Bioconductor&lt;/a&gt;&lt;/li&gt;&quot; ## [70] &quot;&lt;li&gt;&lt;a href=\\&quot;/other-projects.html\\&quot;&gt;Related Projects&lt;/a&gt;&lt;/li&gt;&quot; ## [71] &quot;&lt;li&gt;&lt;a href=\\&quot;/gsoc.html\\&quot;&gt;GSoC&lt;/a&gt;&lt;/li&gt;&quot; ## [72] &quot;&lt;/ul&gt;&quot; ## [73] &quot;&lt;/div&gt;&quot; ## [74] &quot;&lt;/div&gt;&quot; ## [75] &quot; &lt;/div&gt;&quot; ## [76] &quot; &lt;div class=\\&quot;col-xs-12 col-sm-7\\&quot;&gt;&quot; ## [77] &quot; &lt;h1&gt;The R Project for Statistical Computing&lt;/h1&gt;&quot; ## [78] &quot;&lt;h2 id=\\&quot;getting-started\\&quot;&gt;Getting Started&lt;/h2&gt;&quot; ## [79] &quot;&lt;p&gt;R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. To &lt;strong&gt;&lt;a href=\\&quot;http://cran.r-project.org/mirrors.html\\&quot;&gt;download R&lt;/a&gt;&lt;/strong&gt;, please choose your preferred &lt;a href=\\&quot;http://cran.r-project.org/mirrors.html\\&quot;&gt;CRAN mirror&lt;/a&gt;.&lt;/p&gt;&quot; ## [80] &quot;&lt;p&gt;If you have questions about R like how to download and install the software, or what the license terms are, please read our &lt;a href=\\&quot;http://cran.R-project.org/faqs.html\\&quot;&gt;answers to frequently asked questions&lt;/a&gt; before you send an email.&lt;/p&gt;&quot; ## [81] &quot;&lt;h2 id=\\&quot;news\\&quot;&gt;News&lt;/h2&gt;&quot; ## [82] &quot;&lt;ul&gt;&quot; ## [83] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/src/base-prerelease\\&quot;&gt;&lt;strong&gt;R version 3.4.3 (Kite-Eating Tree) prerelease versions&lt;/strong&gt;&lt;/a&gt; will appear starting Monday 2017-11-20. Final release is scheduled for Thursday 2017-11-30.&lt;/p&gt;&lt;/li&gt;&quot; ## [84] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/src/base/R-3\\&quot;&gt;&lt;strong&gt;R version 3.4.2 (Short Summer)&lt;/strong&gt;&lt;/a&gt; has been released on Thursday 2017-09-28.&lt;/p&gt;&lt;/li&gt;&quot; ## [85] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;https://journal.r-project.org/archive/2017-1\\&quot;&gt;&lt;strong&gt;The R Journal Volume 9/1&lt;/strong&gt;&lt;/a&gt; is available.&lt;/p&gt;&lt;/li&gt;&quot; ## [86] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/src/base/R-3\\&quot;&gt;&lt;strong&gt;R version 3.3.3 (Another Canoe)&lt;/strong&gt;&lt;/a&gt; has been released on Monday 2017-03-06.&lt;/p&gt;&lt;/li&gt;&quot; ## [87] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;https://journal.r-project.org/archive/2016-2\\&quot;&gt;&lt;strong&gt;The R Journal Volume 8/2&lt;/strong&gt;&lt;/a&gt; is available.&lt;/p&gt;&lt;/li&gt;&quot; ## [88] &quot;&lt;li&gt;&lt;p&gt;&lt;strong&gt;useR! 2017&lt;/strong&gt; (July 4 - 7 in Brussels) has opened registration and more at http://user2017.brussels/&lt;/p&gt;&lt;/li&gt;&quot; ## [89] &quot;&lt;li&gt;&lt;p&gt;Tomas Kalibera has joined the R core team.&lt;/p&gt;&lt;/li&gt;&quot; ## [90] &quot;&lt;li&gt;&lt;p&gt;The R Foundation welcomes five new ordinary members: Jennifer Bryan, Dianne Cook, Julie Josse, Tomas Kalibera, and Balasubramanian Narasimhan.&lt;/p&gt;&lt;/li&gt;&quot; ## [91] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://journal.r-project.org\\&quot;&gt;&lt;strong&gt;The R Journal Volume 8/1&lt;/strong&gt;&lt;/a&gt; is available.&lt;/p&gt;&lt;/li&gt;&quot; ## [92] &quot;&lt;li&gt;&lt;p&gt;The &lt;strong&gt;useR! 2017&lt;/strong&gt; conference will take place in Brussels, July 4 - 7, 2017.&lt;/p&gt;&lt;/li&gt;&quot; ## [93] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/src/base/R-3\\&quot;&gt;&lt;strong&gt;R version 3.2.5 (Very, Very Secure Dishes)&lt;/strong&gt;&lt;/a&gt; has been released on 2016-04-14. This is a rebadging of the quick-fix release 3.2.4-revised.&lt;/p&gt;&lt;/li&gt;&quot; ## [94] &quot;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Notice XQuartz users (Mac OS X)&lt;/strong&gt; A security issue has been detected with the Sparkle update mechanism used by XQuartz. Avoid updating over insecure channels.&lt;/p&gt;&lt;/li&gt;&quot; ## [95] &quot;&lt;li&gt;&lt;p&gt;The &lt;a href=\\&quot;http://www.r-project.org/logo\\&quot;&gt;&lt;strong&gt;R Logo&lt;/strong&gt;&lt;/a&gt; is available for download in high-resolution PNG or SVG formats.&lt;/p&gt;&lt;/li&gt;&quot; ## [96] &quot;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=\\&quot;http://www.r-project.org/useR-2016\\&quot;&gt;useR! 2016&lt;/a&gt;&lt;/strong&gt;, has taken place at Stanford University, CA, USA, June 27 - June 30, 2016.&lt;/p&gt;&lt;/li&gt;&quot; ## [97] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://journal.r-project.org\\&quot;&gt;&lt;strong&gt;The R Journal Volume 7/2&lt;/strong&gt;&lt;/a&gt; is available.&lt;/p&gt;&lt;/li&gt;&quot; ## [98] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/src/base/R-3\\&quot;&gt;&lt;strong&gt;R version 3.2.3 (Wooden Christmas-Tree)&lt;/strong&gt;&lt;/a&gt; has been released on 2015-12-10.&lt;/p&gt;&lt;/li&gt;&quot; ## [99] &quot;&lt;li&gt;&lt;p&gt;&lt;a href=\\&quot;http://cran.r-project.org/src/base/R-3\\&quot;&gt;&lt;strong&gt;R version 3.1.3 (Smooth Sidewalk)&lt;/strong&gt;&lt;/a&gt; has been released on 2015-03-09.&lt;/p&gt;&lt;/li&gt;&quot; ## [100] &quot;&lt;/ul&gt;&quot; ## [101] &quot;&lt;!--- (Boilerplate for release run-in)&quot; ## [102] &quot;- [**R version 3.1.3 (Smooth Sidewalk) prerelease versions**](http://cran.r-project.org/src/base-prerelease) will appear starting February 28. Final release is scheduled for 2015-03-09.&quot; ## [103] &quot;--&gt;&quot; ## [104] &quot; &lt;/div&gt;&quot; ## [105] &quot; &lt;/div&gt;&quot; ## [106] &quot; &lt;div class=\\&quot;raw footer\\&quot;&gt;&quot; ## [107] &quot; &amp;copy; The R Foundation. For queries about this web site, please contact&quot; ## [108] &quot;\\t&lt;script type=&#39;text/javascript&#39;&gt;&quot; ## [109] &quot;&lt;!--&quot; ## [110] &quot;var s=\\&quot;=b!isfg&gt;#nbjmup;xfcnbtufsAs.qspkfdu/psh#?uif!xfcnbtufs=0b?\\&quot;;&quot; ## [111] &quot;m=\\&quot;\\&quot;; for (i=0; i&lt;s.length; i++) {if(s.charCodeAt(i) == 28){m+= &#39;&amp;&#39;;} else if (s.charCodeAt(i) == 23) {m+= &#39;!&#39;;} else {m+=String.fromCharCode(s.charCodeAt(i)-1);}}document.write(m);//--&gt;&quot; ## [112] &quot;\\t&lt;/script&gt;;&quot; ## [113] &quot; for queries about R itself, please consult the &quot; ## [114] &quot; &lt;a href=\\&quot;help.html\\&quot;&gt;Getting Help&lt;/a&gt; section.&quot; ## [115] &quot; &lt;/div&gt;&quot; ## [116] &quot; &lt;/div&gt;&quot; ## [117] &quot; &lt;!-- jQuery (necessary for Bootstrap&#39;s JavaScript plugins) --&gt;&quot; ## [118] &quot; &lt;script src=\\&quot;https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js\\&quot;&gt;&lt;/script&gt;&quot; ## [119] &quot; &lt;!-- Include all compiled plugins (below), or include individual files as needed --&gt;&quot; ## [120] &quot; &lt;script src=\\&quot;/js/bootstrap.min.js\\&quot;&gt;&lt;/script&gt;&quot; ## [121] &quot; &lt;/body&gt;&quot; ## [122] &quot;&lt;/html&gt;&quot; # Extract the input from the unsuccessful results urls[!is_ok] ## $asdf ## [1] &quot;http://asdfasdasdkfjlda&quot; 3.4.1 Maps over multiple arguments If we want to add multiple arguments - for instance creating a normal distribution using rnorm() with two arguments, one for the number of rows (n) and one for the desired mean - we can use the map2 function which now takes the form map2(.x, .y, .f …) # iterate over two arguments But if we wanted to extend this, by adding saying the standard deviation (sd) rather than use map3 and so on for more arguments, purr has a pmap function that takes lists as inputs pmap(.l, .f, …) # iterate over many arguments Or if we wanted to do a similar exercise but use different functions, say to run distributions for expontial as well as rnorm, we use the invoke map() functions invoke_map(.f, .x = list(NULL), …) # iterate over functions and arguments This next section will use the random number generator rnorm(). # Create a list n containing the values: 5, 10, and 20 n &lt;- list(5, 10, 20) # Call map() on n with rnorm() to simulate three samples map(n, rnorm) ## [[1]] ## [1] -0.4366686 1.4324114 -1.5956013 -1.1505808 0.2122502 ## ## [[2]] ## [1] 0.3243319 -0.9960460 -0.7376161 0.3113217 -0.2849793 -0.2588673 ## [7] -0.3798126 -0.9622933 2.3591024 -1.3414916 ## ## [[3]] ## [1] 2.0575641 -0.2063629 1.5997894 -0.9344443 -1.0462768 0.4878357 ## [7] 1.2556464 1.4165219 -0.8103948 -1.1889049 0.9819992 -0.1521257 ## [13] 1.3140112 1.2322927 -1.0607637 -0.3193866 0.1884801 0.3432868 ## [19] 0.8239909 -0.8821097 If we also want to vary the mean, the mean can be specified in rnorm() by the argument mean. Now there are two arguments to rnorm() we want to vary: n and mean. The map2() function is designed exactly for this purpose; it allows iteration over two objects. The first two arguments to map2() are the objects to iterate over and the third argument .f is the function to apply. Let’s use map2() to simulate three samples with different sample sizes and different means. # Initialize n n &lt;- list(5, 10, 20) # Create a list mu containing the values: 1, 5, and 10 mu &lt;- list(1, 5, 10) # Edit to call map2() on n and mu with rnorm() to simulate three samples map2(n, mu, rnorm) ## [[1]] ## [1] 1.720083 1.656689 0.798450 2.075659 -1.213459 ## ## [[2]] ## [1] 6.059364 4.866746 5.195483 5.948219 5.237326 5.880230 5.762817 ## [8] 2.770638 4.808516 5.295286 ## ## [[3]] ## [1] 11.466687 10.622913 9.661050 9.517950 9.780029 8.685184 12.568060 ## [8] 11.306317 9.724562 11.219939 9.858983 9.636000 9.934999 10.446160 ## [15] 9.322075 8.903951 11.063303 10.899909 9.619289 10.003181 We might want to vary: sd, the standard deviation of the Normal distribution. You might think there is a map3() function, but there isn’t. Instead purrr provides a pmap() function that iterates over 2 or more arguments. First, let’s take a look at pmap() for the situation we just solved: iterating over two arguments. Instead of providing each item to iterate over as arguments, pmap() takes a list of arguments as its input. For example, we could replicate our previous example, iterating over both n and mu with the following: n &lt;- list(5, 10, 20) mu &lt;- list(1, 5, 10) pmap(list(n, mu), rnorm) Notice how we had to put our two items to iterate over (n and mu) into a list. Let’s expand this code to iterate over varying standard deviations too. # Initialize n and mu n &lt;- list(5, 10, 20) mu &lt;- list(1, 5, 10) # Create a sd list with the values: 0.1, 1 and 0.1 sd &lt;- list(0.1, 1, 0.1) # Edit this call to pmap() to iterate over the sd list as well pmap(list(n, mu, sd), rnorm) ## [[1]] ## [1] 0.8698591 1.0009957 0.9710751 0.9496946 1.0495229 ## ## [[2]] ## [1] 6.472870 5.854239 4.528828 4.874714 4.466589 6.146899 4.184024 ## [8] 2.707230 4.097509 6.853301 ## ## [[3]] ## [1] 10.296885 10.152149 9.871139 10.070173 10.187305 10.126474 10.100678 ## [8] 9.930911 10.030108 9.962779 10.104376 10.128644 9.952670 10.190224 ## [15] 9.992032 9.890755 9.976874 10.139623 9.957864 10.028125 By default pmap() matches the elements of the list to the arguments in the function by position. Instead of relying on this positional matching, a safer alternative is to provide names in our list. The name of each element should be the argument name we want to match it to. # Name the elements so they are read correctly pmap(list(mean = mu, n = n , sd = sd), rnorm) ## [[1]] ## [1] 0.9576075 0.9636535 1.0238025 1.0513216 1.2082554 ## ## [[2]] ## [1] 3.185946 6.555173 5.636418 4.991143 6.161679 4.698976 4.847455 ## [8] 5.882055 4.035050 5.180936 ## ## [[3]] ## [1] 10.185981 10.095582 10.090515 10.070867 9.965686 9.851724 9.991329 ## [8] 9.780453 10.113041 10.093998 10.227955 9.987986 10.035544 9.980724 ## [15] 9.989603 9.911811 9.943535 9.954504 9.944317 9.955925 Sometimes it’s not the arguments to a function you want to iterate over, but a set of functions themselves. Imagine that instead of varying the parameters to rnorm() we want to simulate from different distributions, say, using rnorm(), runif(), and rexp(). How do we iterate over calling these functions? In purrr, this is handled by the invoke_map() function. The first argument is a list of functions. In our example, something like: f &lt;- list(“rnorm”, “runif”, “rexp”) The second argument specifies the arguments to the functions. In the simplest case, all the functions take the same argument, and we can specify it directly, relying on … to pass it to each function. In this case, call each function with the argument n = 5: invoke_map(f, n = 5) In more complicated cases, the functions may take different arguments, or we may want to pass different values to each function. In this case, we need to supply invoke_map() with a list, where each element specifies the arguments to the corresponding function. # Define list of functions f &lt;- list(&quot;rnorm&quot;, &quot;runif&quot;, &quot;rexp&quot;) # Parameter list for rnorm() rnorm_params &lt;- list(mean = 10) # Add a min element with value 0 and max element with value 5 runif_params &lt;- list(min =0, max = 5) # Add a rate element with value 5 rexp_params &lt;- list(rate = 5) # Define params for each function params &lt;- list( rnorm_params, runif_params, rexp_params ) # Call invoke_map() on f supplying params as the second argument invoke_map(f, params, n = 5) ## [[1]] ## [1] 10.443370 8.959307 10.637096 10.571040 9.406870 ## ## [[2]] ## [1] 1.4220193 2.9846494 0.9510743 4.6481254 4.5609494 ## ## [[3]] ## [1] 0.32896695 0.36738067 0.06013094 0.12338600 0.11387700 3.4.2 Side effect functions In R we have functions that are so called ‘side effect’ functions, for example saving files or producing plots. They don’t manipulate data per se. In purrr we have a function, walk(), which is designed for use with these side effect functions rather than map(). You can also walk over 2 or more functions, like map 2 and pmap we saw earlier. Walk can be used in pipe operations also. First, let’s check that our simulated samples are in fact what we think they are by plotting a histogram for each one. # Define list of functions f &lt;- list(Normal = &quot;rnorm&quot;, Uniform = &quot;runif&quot;, Exp = &quot;rexp&quot;) # Define params params &lt;- list( Normal = list(mean = 10), Uniform = list(min = 0, max = 5), Exp = list(rate = 5) ) # Assign the simulated samples to sims sims &lt;- invoke_map(f, params, n = 50) # Use walk() to make a histogram of each element in sims walk(sims, hist) We need better breaks for the bins on the x-axis. That means we need to vary two arguments to hist(): x and breaks. Remember map2()? That allowed us to iterate over two arguments. Guess what? There is a walk2(), too! Let’s use walk2() to improve those histograms with better breaks. # Replace &quot;Sturges&quot; with reasonable breaks for each sample breaks_list &lt;- list( Normal = seq(6, 16, 0.5), Uniform = seq(0, 5, 0.25), Exp = seq(0, 1.5, 0.1) ) # Use walk2() to make histograms with the right breaks walk2(sims, breaks_list, hist) An extension of these hard coded break values for the bins, would be to use a function to calculate them whenever our data changes. # Variable rather than hard coded range finder find_breaks &lt;- function(x){ rng &lt;- range(x, na.rm = TRUE) seq(rng[1], rng[2], length.out = 30) } # Call find_breaks() on sims[[1]] find_breaks(sims[[1]]) ## [1] 7.859287 7.995212 8.131137 8.267062 8.402987 8.538912 8.674837 ## [8] 8.810762 8.946687 9.082612 9.218537 9.354462 9.490387 9.626312 ## [15] 9.762237 9.898162 10.034087 10.170012 10.305937 10.441862 10.577787 ## [22] 10.713712 10.849637 10.985562 11.121487 11.257412 11.393337 11.529262 ## [29] 11.665187 11.801112 We can now use these breaks for our histograms. # Use map() to iterate find_breaks() over sims: nice_breaks nice_breaks &lt;- map(sims, find_breaks) # Use nice_breaks as the second argument to walk2() walk2(sims, nice_breaks, hist) Next we want to tidy the labels. We can use the … argument to any of the map() or walk() functions to pass in further arguments to the function .f. In this case, we might decide we don’t want any labels on the x-axis, in which case we need to pass an empty string to the xlab argument of hist(): walk2(sims, nice_breaks, hist, xlab = “”) For the titles, we don’t want them to be the same for each plot. How can we iterate over the arguments x, breaks and main? There is a pwalk() function that works just like pmap(). Note that in the following example, xlab = “” is kept outside of the list of arguments being iterated over since it’s the same value for all three histograms. # Increase sample size to 1000 sims &lt;- invoke_map(f, params, n = 1000) # Compute nice_breaks (don&#39;t change this) nice_breaks &lt;- map(sims, find_breaks) # Create a vector nice_titles nice_titles &lt;- list( &quot;Normal(10, 1)&quot;, &quot;Uniform(0, 5)&quot;, &quot;Exp(5)&quot; ) # Use pwalk() instead of walk2() pwalk(list(x = sims, breaks = nice_breaks, main = nice_titles), hist, xlab = &quot;&quot;) One of the nice things about the walk() functions is that they return the object you passed to them. This means they can easily be used in pipelines (a pipeline is just a short way of saying “a statement with lots of pipes”). WE can, for instance, calculate summary statistics as shown below. # Pipe this along to map(), using summary() as .f sims %&gt;% walk(hist) %&gt;% map(summary) ## $Normal ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6.544 9.387 9.972 10.010 10.684 13.890 ## ## $Uniform ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000235 1.230563 2.507549 2.525777 3.772409 4.999888 ## ## $Exp ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0001179 0.0636020 0.1467002 0.2097436 0.2804284 1.5812353 3.5 Robust Functions Robust functions either work or give error messages that can be understood. R is both an interactive analytical environment (helpful) and programming environment (strict). Interactive functions are designed to iterate over things quickly to help with analysis, the programming/robust functions on the other hand want to get things right. Interactive functions try to guess at the type of things you want to achieve, which can cause problems later on. There are 3 main types of problem functions: TYpe-unstable functions - they return different types of things. For example, with one type of input they might return a vector, but with another type they return a dataframe or a matrix. Non standard evaluation - ‘an important part of R’s magic’ says Hadley. It allows you to use succinct APIs like ggplot and dpylr, but it can introduce ambiguity when programming Hidden arguments - R has global options that can affect the operation of some functions. The most notorious of these are strings as factors. # Define troublesome x and y x &lt;- c(NA, NA, NA) y &lt;- c( 1, NA, NA, NA) both_na &lt;- function(x, y) { # Add stopifnot() to check length of x and y stopifnot(length(x) == length(y)) sum(is.na(x) &amp; is.na(y)) } # Call both_na() on x and y both_na(x, y) We might want a more informative error message. To do so, we use the stop() rather than the stopifnot() function, which has more options including specifying the error message. # Define troublesome x and y x &lt;- c(NA, NA, NA) y &lt;- c( 1, NA, NA, NA) both_na &lt;- function(x, y) { if (length(x) != length(y)) { stop(&quot;x and y must have the same length&quot;, call. = FALSE) } sum(is.na(x) &amp; is.na(y)) } # Call both_na() both_na(x, y) 3.5.1 Unstable types An unstable type is a function whose output can’t be known without being fully aware of what it’s inputs are. They can be hard to program with, since you need to write cases which handle all the different input types. Most of the time this doesn’t matter, but problems can arise when type inconsistent functions are burried within your own functions. Where possible you should try to avoid writing type unstable functions. The first step in this is to avoid using such functions inside your own functions, which comes from knowing what the common type-inconsistent functions are e.g. sapply. You also need to know a set of type consistent functions that can be used, which include those from the purrr package. 3.5.2 Non standard evaluation (NSE) These are functions that don’t use the usual lookup rules. One example is the subset function, which takes a dataset then a control function, such as disp &gt; 400, to subset the data e.g. subset(mtcars, disp &gt; 400). Because disp only exists within the mtcars data frame, we can only evaluation disp &gt; 400 inside the mtcars data, not in the global environment. Non standard evaulation functions are great as they can save a lot of typing, but they can cause problems inside your own functions. Another example of NSE is within ggplot, where we can call a data frame initially, but don’t have to apply the $ sign notation for all the aesthetic mappings within the ggplot call, we only need to specify the data frame once. Dplyr also has a number of NSE functions. BUT these functions can call objects from the global enironment also, for instance a grid search / control object or vector for ML can be called within a model (e.g. caret package). These speed benefits can make programming with such functions difficult. To get around NSE, you can avoid using them in your own functions, or try to protect against where they cause problems. As within unstable types, the majority of the time (95%) such things may not cause problems and it is the edge cases we need to be aware of. More information is available in Hadley’s NSE vignette. For example, if we had the following NSE function (filter from dplyr) within our own function big_x &lt;- function(df, threshold) { dplyr::filter(df, x &gt; threshold) } There are two ways this could fail The x column doesn’t exist in df - but may exist in the global environment, causing the function to find a value there There is a threshold column in df - causing dplyr to look inside the dataframe first and finding the incorrect value, rather our desired value as an argument to the custom/user defined function. We could we write our function previously outlined with stop controls to handle these two instances. big_x &lt;- function(df, threshold) { # Write a check for x not being in df if (!&quot;x&quot; %in% names(df)) { stop(&quot;df must contain variable called x&quot;, call. = FALSE) } # Write a check for threshold being in df if (&quot;threshold&quot; %in% names(df)) { stop(&quot;df must not contain variable called threshold&quot;, call. = FALSE) } dplyr::filter(df, x &gt; threshold) } 3.5.3 Hidden arguments Pure functions only have outputs that depend on their inputs and also don’t affect the outside world, except through their return value. Hidden arguments are function inputs that may be different for different users or sessions. This can commonly occur when argument defaults depend on global options. You can view global options in R by typing options(). Some packages can change their options, they can vary dependent on the OS or other custom user defined setups. Such options include the number of digits to be displayed, how warning and error messages are handled and what to do with missing values. The help page provides more information ?options. A classic example of a hidden dependence is the stringsAsFactors argument to the read.csv() function (and a few other data frame functions.) When you see the following code, you don’t know exactly what the result will be: pools &lt;- read.csv(“swimming_pools.csv”) That’s because if the argument stringsAsFactors isn’t specified, it inherits its value from getOption(“stringsAsFactors”), a global option that a user may change. "],
["joining-data-in-r-with-dplyr.html", "4 Joining Data in R with dplyr 4.1 Mutating joins 4.2 Filtering joins and set operations 4.3 Set Operations 4.4 Bind in Dplyr 4.5 Advanced Joining 4.6 Joining mutiple tables 4.7 Other implentations 4.8 Case Study - Lahman DB", " 4 Joining Data in R with dplyr Notes taken during/inspired by the Datacamp course ‘Joining Data in R with dplyr’ by Garrett Grolemund. Other useful info: R for Data Science Book Data Wrangling Cheatsheet dplyr two table verbs vignette dbplyr vignette Course Slides: Part1 - Mutating Joins Part2 - Filtering joins and set operations Part3 - Assembling data Part4 - Advanced joining Part5 - Case Study 4.1 Mutating joins Data is best used in R when in a single data table. This course introduces a number of techniques to achieve this. Dplyr also has connectors to a range of different databases, so can be used to both extract and manipulate data in databases. 4.1.1 Keys We often want to join two tables together, adding a set of values or variables from a second table(s). For this to happen, we need a key, whereby we have a key in the initial table (a primary key) that is uniquely identifies rows in that within that table or dataset (we don’t have duplicates) and we then use this key to add in data from a secondary table (the foreign key to that table). The foreign key in the secondary table may be duplicated or not appear at all. Sometimes no single variable acts as a primary key in a dataset. Instead, it takes a combination of variables to uniquely identify each row, for example a table of addresses with different columns representing sections of the address - house number, street name, postcode/zip code. When working with dplyr, it works with the following tables for the purposes of joining data Tables in dplyr are one of the following: data frames tibbles (tbl_df) - similar to data frame but only what fits in to your R console window will be displayed but you can use View() all the table if needed tbl references (#fig:Dplyr Joins)Joins available in Dplyr ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union 4.1.2 Left and right joins left_join() is the basic join function in dplyr. You can use it whenever you want to augment a data frame with information from another data frame. For example, left_join(x, y) joins y to x. The second dataset you specify is joined to the first dataset. In right_join() the order of the datasets reversed # Join artists to bands bands2 &lt;- left_join(bands, artists, by = c(&quot;first&quot;, &quot;last&quot;)) # Examine the results bands2 # Recreate bands3 with a right join bands2 &lt;- left_join(bands, artists, by = c(&quot;first&quot;, &quot;last&quot;)) bands3 &lt;- right_join(artists, bands, by = c(&quot;first&quot;, &quot;last&quot;)) # Check that bands3 is equal to bands2 setequal(bands2, bands3) 4.1.3 Inner and full joins Left_join and right_join are half of a class of ‘mutating joins’ with the name coming from dplyrs mutate() function which returns a copy of the dataset with one or more columns of the data added to it. The other two functions are: inner_join: only retains rows from both/all datasets full_join: retains any row from both/any data set %&gt;%: Can be used to string joins or other functions together # Create goal2 using full_join() and inner_join() goal2 &lt;- artists %&gt;% full_join(bands, by = c(&quot;first&quot;,&quot;last&quot;)) %&gt;% inner_join(songs, by = c(&quot;first&quot;,&quot;last&quot;)) # Create one table that combines all information artists %&gt;% full_join(bands, by = c(&quot;first&quot;,&quot;last&quot;)) %&gt;% full_join(songs, by = c(&quot;first&quot;,&quot;last&quot;)) %&gt;% full_join(albums, by = c(&quot;album&quot;, &quot;band&quot;)) 4.2 Filtering joins and set operations Filtering joins returns a copy of the origianl data set rather than an augmented version of the original dataset. (#fig:Filtering Joins)Filtering and Mutating Joins Semi_join() is one of the filtering join functions, it can be used to check which rows in one table match the rows in another table, perhaps before a mutating join. Semi_join is sometimes easier than many seperate functions together, for instance # View the output of semi_join() artists %&gt;% semi_join(songs, by = c(&quot;first&quot;, &quot;last&quot;)) # Create the same result artists %&gt;% right_join(songs, by = c(&quot;first&quot;, &quot;last&quot;)) %&gt;% filter(!is.na(instrument)) %&gt;% select(first, last, instrument) Semi-joins provide a useful way to explore the connections between multiple tables of data. For example, you can use a semi-join to determine the number of albums in the albums dataset that were made by a band in the bands dataset. albums %&gt;% # Collect the albums made by a band semi_join(bands, by = &quot;band&quot;) %&gt;% # Count the albums made by a band nrow() Anti-joins shows records which are in the primary table but do not have matches in the second data table. This can be useful for checking spelling or key value errors. # Return rows of artists that don&#39;t have bands info artists %&gt;% anti_join(bands, by = c(&quot;first&quot;, &quot;last&quot;)) # Check whether album names in labels are mis-entered labels %&gt;% anti_join(albums, by = &quot;album&quot;) Joins can also be used to count the number of records. # Determine which key joins labels and songs labels songs songs %&gt;% # Find the rows of songs that match a row in labels semi_join(labels, by = &quot;album&quot;) %&gt;% # Number of matches between labels and songs nrow() 4.3 Set Operations When two datasets contain the same variables, it can be useful to combine the datasets with set operations (rather than joins). The three set operations can be used to combine observations from two datasets in to a single dataset. (#fig:Set Ops)Set Operations For instance we can count the number of unique songs in two data tables. # Import files aerosmith &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/aerosmith.csv&quot;, stringsAsFactors = FALSE) greatest_hits &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/greatest_hits.csv&quot;, stringsAsFactors = FALSE) aerosmith %&gt;% # Create the new dataset using a set operation union(greatest_hits) %&gt;% # Count the total number of songs nrow() ## [1] 24 Or use it to see which is in both (where the dataset has the exact same variables) - this is similar to the semi_join function. # Create the new dataset using a set operation aerosmith %&gt;% intersect(greatest_hits) ## song length ## 1 Dream On 4:28 Or use it to identify which are in one dataset but not the other. Here, we also match on just the variable we are interested in that matches across both datasets. # Import file live &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/live.csv&quot;, stringsAsFactors = FALSE) # Select the song names from live live_songs &lt;- live %&gt;% select(song) # Select the song names from greatest_hits greatest_songs &lt;- greatest_hits %&gt;% select(song) # Create the new dataset using a set operation - songs in live that are not in greatest_hits live_songs %&gt;% setdiff(greatest_songs) ## song ## 1 Lord of the Thighs ## 2 Toys in the Attic ## 3 Sick as a Dog ## 4 Sight for Sore Eyes ## 5 S.O.S. (Too Bad) ## 6 I Ain&#39;t Got You ## 7 Mother Popcorn/Draw the Line ## 8 Train Kept A-Rollin&#39;/Strangers in the Night There is no set operation to find rows that appear in one data frame or another, but not both. However, you can accomplish this by combining set operators. live_songs &lt;- live %&gt;% select(song) greatest_songs &lt;- greatest_hits %&gt;% select(song) # Return the songs that only exist in one dataset live_songs %&gt;% setdiff(greatest_songs) %&gt;% union(greatest_songs %&gt;% setdiff(live_songs)) ## song ## 1 S.O.S. (Too Bad) ## 2 Draw the Line ## 3 Remember (Walking in the Sand) ## 4 Sight for Sore Eyes ## 5 Kings and Queens ## 6 Seasons of Winter ## 7 Big Ten Inch Record ## 8 Same Old Song and Dance ## 9 One Way Street (live) ## 10 Toys in the Attic ## 11 Sweet Emotion (remix) ## 12 Lord of the Thighs ## 13 Train Kept A-Rollin&#39;/Strangers in the Night ## 14 Sick as a Dog ## 15 I Ain&#39;t Got You ## 16 Lightning Strikes ## 17 Mother Popcorn/Draw the Line Note: The union() function removes duplicate rows, even if a duplicate is desired (perhaps a different record such as someone with the same name). Its common to want to know if one data set is the same as another dataset dplyr’s setequal will do this easily base R’s identical is will only return true if the datasets have the exact same rows in the exact same order Recap: Mutating Joins: left_join right_join inner_join full_join Filtering Joins: semi_join *anti_join Set Operations: union intersect setdiff Comparisions: setequal The definitive and complete contain the songs that appear in competing Led Zeppelin anthologies: The Definitive Collection and The Complete Studio Recordings, respectively. Both anthologies claim to contain the complete studio recordings of Led Zeppelin, but do the anthologies contain the same exact songs? # Import files complete &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/complete.csv&quot;, stringsAsFactors = FALSE) definitive &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/definitive.csv&quot;, stringsAsFactors = FALSE) # Check if same order: definitive and complete identical(definitive, complete) ## [1] FALSE # Check if any order: definitive and complete setequal(definitive, complete) ## FALSE: Different number of rows # Songs in definitive but not complete complete %&gt;% setdiff(definitive) ## [1] song album ## &lt;0 rows&gt; (or 0-length row.names) # Songs in complete but not definitive definitive %&gt;% setdiff(complete) ## song album ## 1 Rock and Roll The Song Remains the Same ## 2 Celebration Day The Song Remains the Same ## 3 Black Dog The Song Remains the Same ## 4 Over the Hills and Far Away The Song Remains the Same ## 5 Misty Mountain Hop The Song Remains the Same ## 6 Since I&#39;ve Been Loving You The Song Remains the Same ## 7 No Quarter The Song Remains the Same ## 8 The Song Remains the Same The Song Remains the Same ## 9 The Rain Song The Song Remains the Same ## 10 The Ocean The Song Remains the Same ## 11 Dazed and Confused The Song Remains the Same ## 12 Stairway to Heaven The Song Remains the Same ## 13 Moby Dick The Song Remains the Same ## 14 Heartbreaker The Song Remains the Same ## 15 Whole Lotta Love The Song Remains the Same # Return songs in definitive that are not in complete definitive %&gt;% anti_join(complete, by = c(&quot;song&quot;, &quot;album&quot;)) ## song album ## 1 Rock and Roll The Song Remains the Same ## 2 Celebration Day The Song Remains the Same ## 3 Black Dog The Song Remains the Same ## 4 Over the Hills and Far Away The Song Remains the Same ## 5 Misty Mountain Hop The Song Remains the Same ## 6 Since I&#39;ve Been Loving You The Song Remains the Same ## 7 No Quarter The Song Remains the Same ## 8 The Song Remains the Same The Song Remains the Same ## 9 The Rain Song The Song Remains the Same ## 10 The Ocean The Song Remains the Same ## 11 Dazed and Confused The Song Remains the Same ## 12 Stairway to Heaven The Song Remains the Same ## 13 Moby Dick The Song Remains the Same ## 14 Heartbreaker The Song Remains the Same ## 15 Whole Lotta Love The Song Remains the Same # Return songs in complete that are not in definitive complete %&gt;% anti_join(definitive, by = c(&quot;song&quot;, &quot;album&quot;)) ## [1] song album ## &lt;0 rows&gt; (or 0-length row.names) It appears that The Definitive Collection contains songs from the soundtrack of The Song Remains the Same, a movie filmed during a live Led Zeppelin concert. Is this the only difference between The Definitive Collection and The Complete Studio Recordings? Remember: base R’s identical is will only return true if the datasets have the exact same rows in the exact same order. # Import file sounddtrack soundtrack &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/soundtrack.csv&quot;, stringsAsFactors = FALSE) # Check if same order: definitive and union of complete and soundtrack complete %&gt;% union(soundtrack) %&gt;% identical(definitive) ## [1] FALSE # Check if any order: definitive and union of complete and soundtrack complete %&gt;% union(soundtrack) %&gt;% setequal(definitive) ## TRUE 4.4 Bind in Dplyr Whilst base R has rbrind and cbind, dplyr has bind_rows and bind_cols as equivalents. Bind_rows adds the second dataset underneath the first, bind_cols assumes the datasets are in the exact same order and can be thought of as a ‘lazy join’. The benefits of dplyr binds are: Faster Return a tibble Can handle lists of data frames .id The last one (.id) will return a name to indicate which source the data in the new data frame (tibble) the data came from, particularly useful for things like ggplot and keeping track of your data overall. # Examine side_one and side_two side_one side_two # Bind side_one and side_two into a single dataset side_one %&gt;% bind_rows(side_two) discography and jimi contain all of the information you need to create an anthology dataset for the band The Jimi Hendrix Experience. discography contains a data frame of each album by The Jimi Hendrix Experience and the year of the album. jimi contains a list of data frames of album tracks, one for each album released by The Jimi Hendrix Experience. You can pass bind_rows() a list of data frames like jimi to bind together into a single data frame. # Examine discography and jimi discography jimi jimi %&gt;% # Bind jimi into a single data frame bind_rows(.id = &quot;album&quot;) %&gt;% # Make a complete data frame left_join(discography) # Import file sounddtrack hank_years &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/hank_years.csv&quot;, stringsAsFactors = FALSE) hank_charts &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/hank_charts.csv&quot;, stringsAsFactors = FALSE) # Examine hank_years and hank_charts hank_years ## year song ## 1 1947 Move It On Over ## 2 1947 My Love for You (Has Turned to Hate) ## 3 1947 Never Again (Will I Knock on Your Door) ## 4 1947 On the Banks of the Old Ponchartrain ## 5 1947 Pan American ## 6 1947 Wealth Won&#39;t Save Your Soul ## 7 1948 A Mansion on the Hill ## 8 1948 Honky Tonkin&#39; ## 9 1948 I Saw the Light ## 10 1948 I&#39;m a Long Gone Daddy ## 11 1948 My Sweet Love Ain&#39;t Around ## 12 1949 I&#39;m So Lonesome I Could Cry ## 13 1949 Lost Highway ## 14 1949 Lovesick Blues ## 15 1949 Mind Your Own Business ## 16 1949 My Bucket&#39;s Got a Hole in It ## 17 1949 Never Again (Will I Knock on Your Door) ## 18 1949 Wedding Bells ## 19 1949 You&#39;re Gonna Change (Or I&#39;m Gonna Leave) ## 20 1950 I Just Don&#39;t Like This Kind of Living ## 21 1950 Long Gone Lonesome Blues ## 22 1950 Moanin&#39; the Blues ## 23 1950 My Son Calls Another Man Daddy ## 24 1950 Nobody&#39;s Lonesome for Me ## 25 1950 They&#39;ll Never Take Her Love from Me ## 26 1950 Why Don&#39;t You Love Me ## 27 1950 Why Should We Try Anymore ## 28 1951 (I Heard That) Lonesome Whistle ## 29 1951 Baby, We&#39;re Really in Love ## 30 1951 Cold, Cold Heart ## 31 1951 Crazy Heart ## 32 1951 Dear John ## 33 1951 Hey Good Lookin&#39; ## 34 1951 Howlin&#39; At the Moon ## 35 1951 I Can&#39;t Help It (If I&#39;m Still in Love With You) ## 36 1952 Half as Much ## 37 1952 Honky Tonk Blues ## 38 1952 I&#39;ll Never Get Out of This World Alive ## 39 1952 Jambalaya (On the Bayou) ## 40 1952 Settin&#39; the Woods on Fire ## 41 1952 You Win Again ## 42 1953 Calling You ## 43 1953 I Won&#39;t Be Home No More ## 44 1953 Kaw-Liga ## 45 1953 Take These Chains from My Heart ## 46 1953 Weary Blues from Waitin&#39; ## 47 1953 Your Cheatin&#39; Heart ## 48 1954 (I&#39;m Gonna) Sing, Sing, Sing ## 49 1954 How Can You Refuse Him Now ## 50 1954 I&#39;m Satisfied with You ## 51 1954 You Better Keep It on Your Mind ## 52 1955 A Teardrop on a Rose ## 53 1955 At the First Fall of Snow ## 54 1955 Mother Is Gone ## 55 1955 Please Don&#39;t Let Me Love You ## 56 1955 Thank God ## 57 1956 A Home in Heaven ## 58 1956 California Zephyr ## 59 1956 Singing Waterfall ## 60 1956 There&#39;s No Room in My Heart for the Blues ## 61 1957 Leave Me Alone with the Blues ## 62 1957 Ready to Go Home ## 63 1957 The Waltz of the Wind ## 64 1958 Just Waitin&#39; ## 65 1965 The Pale Horse and His Rider ## 66 1966 Kaw-Liga ## 67 1989 There&#39;s a Tear in My Beer hank_charts ## song peak ## 1 (I Heard That) Lonesome Whistle 9 ## 2 (I&#39;m Gonna) Sing, Sing, Sing NA ## 3 A Home in Heaven NA ## 4 A Mansion on the Hill 12 ## 5 A Teardrop on a Rose NA ## 6 At the First Fall of Snow NA ## 7 Baby, We&#39;re Really in Love 4 ## 8 California Zephyr NA ## 9 Calling You NA ## 10 Cold, Cold Heart 1 ## 11 Crazy Heart 4 ## 12 Dear John 8 ## 13 Half as Much 2 ## 14 Hey Good Lookin&#39; 1 ## 15 Honky Tonk Blues 2 ## 16 Honky Tonkin&#39; 14 ## 17 How Can You Refuse Him Now NA ## 18 Howlin&#39; At the Moon 3 ## 19 I Can&#39;t Help It (If I&#39;m Still in Love With You) 2 ## 20 I Just Don&#39;t Like This Kind of Living 5 ## 21 I Saw the Light NA ## 22 I Won&#39;t Be Home No More 4 ## 23 I&#39;ll Never Get Out of This World Alive 1 ## 24 I&#39;m a Long Gone Daddy 6 ## 25 I&#39;m Satisfied with You NA ## 26 I&#39;m So Lonesome I Could Cry 2 ## 27 Jambalaya (On the Bayou) 1 ## 28 Just Waitin&#39; NA ## 29 Kaw-Liga 1 ## 30 Kaw-Liga NA ## 31 Leave Me Alone with the Blues NA ## 32 Long Gone Lonesome Blues 1 ## 33 Lost Highway 12 ## 34 Lovesick Blues 1 ## 35 Mind Your Own Business 5 ## 36 Moanin&#39; the Blues 1 ## 37 Mother Is Gone NA ## 38 Move It On Over 4 ## 39 My Bucket&#39;s Got a Hole in It 2 ## 40 My Love for You (Has Turned to Hate) NA ## 41 My Son Calls Another Man Daddy 9 ## 42 My Sweet Love Ain&#39;t Around NA ## 43 Never Again (Will I Knock on Your Door) NA ## 44 Never Again (Will I Knock on Your Door) 6 ## 45 Nobody&#39;s Lonesome for Me 9 ## 46 On the Banks of the Old Ponchartrain NA ## 47 Pan American NA ## 48 Please Don&#39;t Let Me Love You 9 ## 49 Ready to Go Home NA ## 50 Settin&#39; the Woods on Fire 2 ## 51 Singing Waterfall NA ## 52 Take These Chains from My Heart 1 ## 53 Thank God NA ## 54 The Pale Horse and His Rider NA ## 55 The Waltz of the Wind NA ## 56 There&#39;s a Tear in My Beer 7 ## 57 There&#39;s No Room in My Heart for the Blues NA ## 58 They&#39;ll Never Take Her Love from Me 5 ## 59 Wealth Won&#39;t Save Your Soul NA ## 60 Weary Blues from Waitin&#39; 7 ## 61 Wedding Bells 2 ## 62 Why Don&#39;t You Love Me 1 ## 63 Why Should We Try Anymore 9 ## 64 You Better Keep It on Your Mind NA ## 65 You Win Again 10 ## 66 You&#39;re Gonna Change (Or I&#39;m Gonna Leave) 4 ## 67 Your Cheatin&#39; Heart 1 hank_years %&gt;% # Reorder hank_years alphabetically by song title arrange(song) %&gt;% # Select just the year column select(year) %&gt;% # Bind the year column bind_cols(hank_charts) %&gt;% # Arrange the finished dataset arrange(year, song) ## year song peak ## 1 1947 Move It On Over 4 ## 2 1947 My Love for You (Has Turned to Hate) NA ## 3 1947 Never Again (Will I Knock on Your Door) NA ## 4 1947 On the Banks of the Old Ponchartrain NA ## 5 1947 Pan American NA ## 6 1947 Wealth Won&#39;t Save Your Soul NA ## 7 1948 A Mansion on the Hill 12 ## 8 1948 Honky Tonkin&#39; 14 ## 9 1948 I&#39;m Satisfied with You NA ## 10 1948 I Just Don&#39;t Like This Kind of Living 5 ## 11 1948 My Sweet Love Ain&#39;t Around NA ## 12 1949 I Won&#39;t Be Home No More 4 ## 13 1949 Lost Highway 12 ## 14 1949 Lovesick Blues 1 ## 15 1949 Mind Your Own Business 5 ## 16 1949 My Bucket&#39;s Got a Hole in It 2 ## 17 1949 Never Again (Will I Knock on Your Door) 6 ## 18 1949 Wedding Bells 2 ## 19 1949 You Better Keep It on Your Mind NA ## 20 1950 I&#39;m a Long Gone Daddy 6 ## 21 1950 Long Gone Lonesome Blues 1 ## 22 1950 Moanin&#39; the Blues 1 ## 23 1950 My Son Calls Another Man Daddy 9 ## 24 1950 Nobody&#39;s Lonesome for Me 9 ## 25 1950 They&#39;ll Never Take Her Love from Me 5 ## 26 1950 Why Don&#39;t You Love Me 1 ## 27 1950 Why Should We Try Anymore 9 ## 28 1951 (I&#39;m Gonna) Sing, Sing, Sing NA ## 29 1951 Baby, We&#39;re Really in Love 4 ## 30 1951 Cold, Cold Heart 1 ## 31 1951 Crazy Heart 4 ## 32 1951 Dear John 8 ## 33 1951 Hey Good Lookin&#39; 1 ## 34 1951 Howlin&#39; At the Moon 3 ## 35 1951 I&#39;ll Never Get Out of This World Alive 1 ## 36 1952 Half as Much 2 ## 37 1952 Honky Tonk Blues 2 ## 38 1952 I Can&#39;t Help It (If I&#39;m Still in Love With You) 2 ## 39 1952 Jambalaya (On the Bayou) 1 ## 40 1952 Settin&#39; the Woods on Fire 2 ## 41 1952 You&#39;re Gonna Change (Or I&#39;m Gonna Leave) 4 ## 42 1953 Calling You NA ## 43 1953 I&#39;m So Lonesome I Could Cry 2 ## 44 1953 Kaw-Liga 1 ## 45 1953 Take These Chains from My Heart 1 ## 46 1953 Weary Blues from Waitin&#39; 7 ## 47 1953 Your Cheatin&#39; Heart 1 ## 48 1954 (I Heard That) Lonesome Whistle 9 ## 49 1954 How Can You Refuse Him Now NA ## 50 1954 I Saw the Light NA ## 51 1954 You Win Again 10 ## 52 1955 A Teardrop on a Rose NA ## 53 1955 At the First Fall of Snow NA ## 54 1955 Mother Is Gone NA ## 55 1955 Please Don&#39;t Let Me Love You 9 ## 56 1955 Thank God NA ## 57 1956 A Home in Heaven NA ## 58 1956 California Zephyr NA ## 59 1956 Singing Waterfall NA ## 60 1956 There&#39;s No Room in My Heart for the Blues NA ## 61 1957 Leave Me Alone with the Blues NA ## 62 1957 Ready to Go Home NA ## 63 1957 The Waltz of the Wind NA ## 64 1958 Just Waitin&#39; NA ## 65 1965 The Pale Horse and His Rider NA ## 66 1966 Kaw-Liga NA ## 67 1989 There&#39;s a Tear in My Beer 7 Unfortunately, there is usually no clear way to tell whether or not the rows in two datasets align unless the datasets contain a mutual key. In that scenario, you can use a mutating join to bind the datasets in a foolproof way. 4.4.1 Data frames data.frame() defaults Changes strings to factors Adds row names Changes unusual column names However sometimes we do not want this behaviour, so we can use data_frame() from dplyr data_frame() will not… Change the data type of vectors (e.g. strings to factors) Add row names Change column names Recycle vectors greater than length one # Make combined data frame using data_frame() data_frame(year = hank_year, song = hank_song, peak = hank_peak) %&gt;% # Extract songs where peak equals 1 filter(peak == &quot;1&quot;) # Or if the data was a list of vectors # Convert the hank list into a data frame as_data_frame(hank) %&gt;% # Extract songs where peak equals 1 filter(peak == &quot;1&quot;) # Or if we had nested data tables as a list of vectors bind_rows(michael, .id = &quot;album&quot;) %&gt;% group_by(album) %&gt;% mutate(rank = min_rank(peak)) %&gt;% filter(rank == 1) %&gt;% select(-rank, -peak) 4.4.2 Data Types Usually R will do sensible things when working with data, linke 1 + 1 = 2, but “one” + “one” = error. You should be aware of some of the data types when working in R. Every piece of data in R is a vector, even if it only has a single value in it. Unless the data is in a list, all elements in the vector are going to be in one of six data types - known as atomic data types. You can use typeof() to identfy what is in a vector. Table 4.1: Atomic Data Types in R Type Output Atomic data type &gt;typeof(TRUE) [1] “logical” Logical &gt;typeof(“hello”) [1] “character” Character (i.e. string) &gt;typeof(3.14) [1] “double” Double (i.e. numeric w/ decimal) &gt;typeof(1L) [1] “integer” Integer (i.e. numeric w/o decimal) &gt;typeof(1 + 2i) [1] “complex” Complex &gt;typeof(raw(1)) [1] “raw” Raw New classes of data, such as factors which are used for categorical variables, are created from one of the six types above giving it a class attributing and other metadata stored as attributes. A factor is a an integer vector with a factor class atribute, a levels attribute and sometimes a level attribute. Whilst they contain a sequence of integers, they are dispalyed as the asssocicated labels. In R, each column in a data frame must be of a single type or class of data because each column is stored as a single vector. If mutliple tables or columns are being combined, R uses coercion rules to decide what to do. If any variable being combined has a character, it stores it as a character string Doubles being combined with Integers or logicals gets stored as a double (T = 1, F = 0) If integers are being combined with a logical, it gets stored as a integer (T = 1, F = 0) factors with charecters, the factor lables gets converted to strings (A = “A”, B = “B”) factors with doubles or integers, the factor gets converted to their numeric values (A = 1, B = 2) Note pay particular attention to factor data with numeric labels - e.g. if 4, 5 and 6 values have factor levels of 1, 2 and 3, when combining or coercing with or to a numeric, the result will be 4, 5 and 6 rather than the factor lables. To get the lables which are numeric values, convert the factor to a character string then convert those to a double or integer e.g. as.numeric(as.character(x)). Dplyr won’t try and coerce different data but will throw an error and let you manually determine what you want to do with the data. If combining factors, dplyr will convert them to charecters then give a warning message. For example, sixties contains the top selling albums in the US in the 1960s. It stores year as a numeric (double). When you combine it with seventies, which stores year as a factor, bind_rows() returns an error. You can fix this by coercing seventies$year to a numeric. But if you seventies %&gt;% mutate(year = as.numeric(year)) will not return the correct year. We need to convert the factor to a string. seventies %&gt;% # Coerce seventies$year into a useful numeric mutate(year = as.numeric(as.character(year))) %&gt;% # Bind the updated version of seventies to sixties bind_rows(sixties) %&gt;% arrange(year) 4.5 Advanced Joining Sometimes joins can fail for one of two principle reasons - missing keys or duplicate keys. These can be either related to the values or the columns. One common problem can be that column names are missing and that the rows are called by their actual value. R’s data frames can store important information in the row.names attribute. This is not a tidy way to store data, but it does happen quite commonly. If the primary key of your dataset is stored in row.names, you will have trouble joining it to other datasets. For example, stage_songs contains information about songs that appear in musicals. However, it stores the primary key (song name) in the row.names attribute. As a result, you cannot access the key with a join function. One way to remedy this problem is to use the function rownames_to_column() from the tibble package. rownames_to_column() returns a copy of a dataset with the row names added to the data as a column. # Import the data stage_songs &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/stage_songs.csv&quot;, stringsAsFactors = FALSE) stage_writers &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/stage_writers.csv&quot;, stringsAsFactors = FALSE) # And view stage_songs ## X musical year ## 1 Children Will Listen Into the Woods 1986 ## 2 Maria West Side Story 1957 ## 3 Memory Cats 1981 ## 4 The Music of the Night Phantom of the Opera 1986 stage_writers ## song composer ## 1 Children Will Listen Stephen Sondheim ## 2 Maria Leonard Bernstein ## 3 Memory Andrew Lloyd Webber ## 4 The Music of the Night Andrew Lloyd Webber # Load the tibble package library(tibble) stage_songs %&gt;% # Add row names as a column named song rownames_to_column(var = &quot;song&quot;) %&gt;% # Left join stage_writers to stage_songs left_join(stage_writers) ## Joining, by = &quot;song&quot; ## song X musical year composer ## 1 1 Children Will Listen Into the Woods 1986 &lt;NA&gt; ## 2 2 Maria West Side Story 1957 &lt;NA&gt; ## 3 3 Memory Cats 1981 &lt;NA&gt; ## 4 4 The Music of the Night Phantom of the Opera 1986 &lt;NA&gt; If there are duplicate rows in the primary table, there will be duplicate joins to the second table, resulting in a mutiple of new rows. Equally, if there are multiple rows in the secondary table there will be duplicate entries. This can be made worse if there are what appear to be duplicate key values in both sets of tables resulting in n^2 rows. In the case of missing data, it is best to remove the data with a filter first, before joining. # Remove NA&#39;s from key before joining two_songs %&gt;% filter(!is.na(movie)) %&gt;% inner_join(singers, by = &quot;movie&quot;) It is not always needed to add the by = argument when joining, dplyr when compare both tables looking for a common field. However, where the column names differ (but have the same contents) when can explicitly state the columns to join by. Equally, sometimes you may have to specify the by = argument when column names are the same, but contain different data, so you specify which column to actually join on. # Import the data movie_studios &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/movie_studios.csv&quot;, stringsAsFactors = FALSE) movie_years &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/movie_years.csv&quot;, stringsAsFactors = FALSE) # View the data movie_studios ## movie name ## 1 The Road to Morocco Paramount Pictures ## 2 Going My Way Paramount Pictures ## 3 Anchors Aweigh Metro-Goldwyn-Mayer ## 4 Till the Clouds Roll By Metro-Goldwyn-Mayer ## 5 White Christmas Paramount Pictures ## 6 The Tender Trap Metro-Goldwyn-Mayer ## 7 High Society Metro-Goldwyn-Mayer ## 8 The Joker is Wild Paramount Pictures ## 9 Pal Joey Columbia Pictures ## 10 Can-Can Twentieth-Century Fox movie_years ## movie name year ## 1 The Road to Morocco Bing Crosby 1942 ## 2 Going My Way Bing Crosby 1944 ## 3 Anchors Aweigh Frank Sinatra 1945 ## 4 Till the Clouds Roll By Frank Sinatra 1946 ## 5 White Christmas Bing Crosby 1954 ## 6 The Tender Trap Frank Sinatra 1955 ## 7 High Society Bing Crosby 1956 ## 8 The Joker is Wild Frank Sinatra 1957 ## 9 Pal Joey Frank Sinatra 1957 ## 10 Can-Can Frank Sinatra 1960 In this instance, name refers to the studio and the name of the actor in each data table. By default, dplyr will ignore duplicate column names if you set the by argument and do not include the duplicated name in the argument. When you do this, dplyr will treat the columns in the normal fashion, but it will add .x and .y to the duplicated names to help you tell the columns apart. We can use rename(data, new_name = old_name) renames old_name to new_name in data to something else if desired. movie_years %&gt;% # Left join movie_studios to movie_years left_join(movie_studios, by = &quot;movie&quot;) %&gt;% # Rename the columns: artist and studio rename(artist = name.x, studio = name.y) ## movie artist year studio ## 1 The Road to Morocco Bing Crosby 1942 Paramount Pictures ## 2 Going My Way Bing Crosby 1944 Paramount Pictures ## 3 Anchors Aweigh Frank Sinatra 1945 Metro-Goldwyn-Mayer ## 4 Till the Clouds Roll By Frank Sinatra 1946 Metro-Goldwyn-Mayer ## 5 White Christmas Bing Crosby 1954 Paramount Pictures ## 6 The Tender Trap Frank Sinatra 1955 Metro-Goldwyn-Mayer ## 7 High Society Bing Crosby 1956 Metro-Goldwyn-Mayer ## 8 The Joker is Wild Frank Sinatra 1957 Paramount Pictures ## 9 Pal Joey Frank Sinatra 1957 Columbia Pictures ## 10 Can-Can Frank Sinatra 1960 Twentieth-Century Fox In the next example, name refers to different objects. To make the join, set by to a named vector. The names of the vector will refer to column names in the primary dataset (x). The values of the vector will correspond to the column names in the secondary dataset (y), e.g. x %&gt;% left_join(y, by = c(“x.name1” = “y.name2”)) # Import the data elvis_songs &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/elvis_songs.csv&quot;, stringsAsFactors = FALSE) elvis_movies &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/elvis_movies.csv&quot;, stringsAsFactors = FALSE) # And view elvis_songs ## name movie ## 1 (You&#39;re So Square) Baby I Don&#39;t Care Jailhouse Rock ## 2 I Can&#39;t Help Falling in Love Blue Hawaii ## 3 Jailhouse Rock Jailhouse Rock ## 4 Viva Las Vegas Viva Las Vegas ## 5 You Don&#39;t Know Me Clambake elvis_movies ## name year ## 1 Jailhouse Rock 1957 ## 2 Blue Hawaii 1961 ## 3 Viva Las Vegas 1963 ## 4 Clambake 1967 elvis_movies %&gt;% # Left join elvis_songs to elvis_movies by this column left_join(elvis_songs, by = c(&quot;name&quot; = &quot;movie&quot;)) %&gt;% # Rename columns rename(movie = name, song = name.y) ## movie year song ## 1 Jailhouse Rock 1957 (You&#39;re So Square) Baby I Don&#39;t Care ## 2 Jailhouse Rock 1957 Jailhouse Rock ## 3 Blue Hawaii 1961 I Can&#39;t Help Falling in Love ## 4 Viva Las Vegas 1963 Viva Las Vegas ## 5 Clambake 1967 You Don&#39;t Know Me In the following example, the two datasets in question have the same key variable, however the has a different column name that connects the datasets. We also rename one of the columns within the select statement to aid understanding in the dplyr pipeline. # Import the data movie_directors &lt;- read.csv(&quot;D:/CloudStation/Documents/2017/RData/movie_directors.csv&quot;, stringsAsFactors = FALSE) # Identify the key columns movie_directors ## name director studio ## 1 Anchors Aweigh George Sidney Metro-Goldwyn-Mayer ## 2 Can-Can Walter Lang Twentieth-Century Fox ## 3 Going My Way Leo McCarey Paramount Pictures ## 4 High Society Charles Walters Metro-Goldwyn-Mayer ## 5 Pal Joey George Sidney Columbia Pictures ## 6 The Joker is Wild Charles Vidor Paramount Pictures ## 7 The Road to Morocco David Butler Paramount Pictures ## 8 The Tender Trap Charles Walters Metro-Goldwyn-Mayer ## 9 Till the Clouds Roll By Richard Whorf Metro-Goldwyn-Mayer ## 10 White Christmas Michael Curtiz Paramount Pictures movie_years ## movie name year ## 1 The Road to Morocco Bing Crosby 1942 ## 2 Going My Way Bing Crosby 1944 ## 3 Anchors Aweigh Frank Sinatra 1945 ## 4 Till the Clouds Roll By Frank Sinatra 1946 ## 5 White Christmas Bing Crosby 1954 ## 6 The Tender Trap Frank Sinatra 1955 ## 7 High Society Bing Crosby 1956 ## 8 The Joker is Wild Frank Sinatra 1957 ## 9 Pal Joey Frank Sinatra 1957 ## 10 Can-Can Frank Sinatra 1960 movie_years %&gt;% # Left join movie_directors to movie_years left_join(movie_directors, by = c(&quot;movie&quot; = &quot;name&quot;)) %&gt;% # Arrange the columns using select() select(year, movie, artist = name, director, studio) ## year movie artist director ## 1 1942 The Road to Morocco Bing Crosby David Butler ## 2 1944 Going My Way Bing Crosby Leo McCarey ## 3 1945 Anchors Aweigh Frank Sinatra George Sidney ## 4 1946 Till the Clouds Roll By Frank Sinatra Richard Whorf ## 5 1954 White Christmas Bing Crosby Michael Curtiz ## 6 1955 The Tender Trap Frank Sinatra Charles Walters ## 7 1956 High Society Bing Crosby Charles Walters ## 8 1957 The Joker is Wild Frank Sinatra Charles Vidor ## 9 1957 Pal Joey Frank Sinatra George Sidney ## 10 1960 Can-Can Frank Sinatra Walter Lang ## studio ## 1 Paramount Pictures ## 2 Paramount Pictures ## 3 Metro-Goldwyn-Mayer ## 4 Metro-Goldwyn-Mayer ## 5 Paramount Pictures ## 6 Metro-Goldwyn-Mayer ## 7 Metro-Goldwyn-Mayer ## 8 Paramount Pictures ## 9 Columbia Pictures ## 10 Twentieth-Century Fox 4.6 Joining mutiple tables Whilst it would be possible to join multiple tables iteratively, for instance df1 %&gt;% left_join(df2) %&gt;% left_join(df3) %&gt;% left_join(df4) We can use the purrr package instead. We list the dataframes as a vector first, then pass the vector as the first argument to the reduce() function, the second argument should be the dplyr function (e.g. left_join without brackets), the third argument should be the by = var to specify what to join on. # Load the purrr library library(purrr) # Place supergroups, more_bands, and more_artists into a list list(supergroups, more_bands, more_artists) %&gt;% # Use reduce to join together the contents of the list reduce(left_join, by = c(&quot;first&quot;, &quot;last&quot;)) # Or to list just those appear in all three tables list(more_artists, more_bands, supergroups) %&gt;% # Return rows of more_artists in all three datasets reduce(semi_join, by = c(&quot;first&quot;, &quot;last&quot;)) 4.7 Other implentations Dplyr join functions are similar to SQL statements, as shown below. (#fig:SQL functions Dplyr)Joins available in Dplyr With dplyr it is possible to create a connection to a database using the DBI package: src_sqlite(): for SQLite datbases src_mysql: mySQL and MariaDB src_postgres: PostgreSQL 4.8 Case Study - Lahman DB In this seciton we will use the Sean Lahman baseball statistics data. # Load names and the package lahmanNames &lt;- readRDS(&quot;D:/CloudStation/Documents/2017/RData/lahmanNames.rds&quot;) library(purrr) library(Lahman) # Find variables in common reduce(lahmanNames, intersect) ## # A tibble: 0 x 1 ## # ... with 1 variables: var &lt;chr&gt; There are no common (intersecting) variables across the datasets. But perhaps some variables are in more than one table. lahmanNames %&gt;% # Bind the data frames in lahmanNames bind_rows() %&gt;% # Group the result by var group_by(var) %&gt;% # Tally the number of appearances tally() %&gt;% # Filter the data filter(n &gt; 1) %&gt;% # Arrange the results arrange(desc(n)) ## # A tibble: 59 x 2 ## var n ## &lt;chr&gt; &lt;int&gt; ## 1 yearID 21 ## 2 playerID 19 ## 3 lgID 17 ## 4 teamID 13 ## 5 G 10 ## 6 L 6 ## 7 W 6 ## 8 BB 5 ## 9 CS 5 ## 10 GS 5 ## # ... with 49 more rows So PlayerID appears regularly, but in which tables? lahmanNames %&gt;% # Bind the data frames bind_rows(.id = &#39;dataframe&#39;) %&gt;% # Filter the results filter(var == &quot;playerID&quot;) %&gt;% # Extract the dataframe variable `$`(dataframe) ## [1] &quot;AllstarFull&quot; &quot;Appearances&quot; &quot;AwardsManagers&quot; ## [4] &quot;AwardsPlayers&quot; &quot;AwardsShareManagers&quot; &quot;AwardsSharePlayers&quot; ## [7] &quot;Batting&quot; &quot;BattingPost&quot; &quot;CollegePlaying&quot; ## [10] &quot;Fielding&quot; &quot;FieldingOF&quot; &quot;FieldingPost&quot; ## [13] &quot;HallOfFame&quot; &quot;Managers&quot; &quot;ManagersHalf&quot; ## [16] &quot;Master&quot; &quot;Pitching&quot; &quot;PitchingPost&quot; ## [19] &quot;Salaries&quot; Next we can begin to look at the salaries data. First, let’s begin by ensuring that we have salary information for each player in the database, or at least no systematic holes in our coverage. Our new table will be concise and players contains only one row for each distinct player. players &lt;- Master %&gt;% # Return one row for each distinct player distinct(playerID, nameFirst, nameLast) Next, how many missing values do we have? players %&gt;% # Find all players who do not appear in Salaries anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # Count them count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 13958 The answer - a lot! Is it possible that these players somehow did not play (and hence did not earn a salary)? We can check with the Appearances data frame. Appearances contains information about every game played in major league baseball. That is, if a player played a game, it would show up as a row in Appearances. players %&gt;% anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # How many unsalaried players appear in Appearances? semi_join(Appearances, by = &quot;playerID&quot;) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 13765 So a large number of players played a game but are missing salary information. Interestingly, 193 players neither played a game nor have a recorded salary. Perhaps the unsalaried players only played one or two games, and hence did not earn a full salary. players %&gt;% # Find all players who do not appear in Salaries anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # Join them to Appearances left_join(Appearances, by = &quot;playerID&quot;) %&gt;% # Calculate total_games for each player group_by(playerID) %&gt;% summarise(total_games = sum(G_all, na.rm = T)) %&gt;% # Arrange in descending order by total_games arrange(desc(total_games)) ## # A tibble: 13,958 x 2 ## playerID total_games ## &lt;chr&gt; &lt;int&gt; ## 1 yastrca01 3308 ## 2 aaronha01 3298 ## 3 cobbty01 3034 ## 4 musiast01 3026 ## 5 mayswi01 2992 ## 6 robinbr01 2896 ## 7 kalinal01 2834 ## 8 collied01 2824 ## 9 robinfr02 2808 ## 10 wagneho01 2795 ## # ... with 13,948 more rows Here we some some players played thousands of games, so the idea that some didn’t play enough games doesn’t seem to hold. Is it possible that the unsalaried players did not actually play in the games that they appeared in? One way to check would be to determine if the players had an at-bat (i.e. batted) in the games that they appeared in. players %&gt;% # Find unsalaried players anti_join(Salaries, by = &quot;playerID&quot;) %&gt;% # Join Batting to the unsalaried players left_join(Batting, by = &quot;playerID&quot;) %&gt;% # Group by player group_by(playerID) %&gt;% # Sum at-bats for each player summarise(total_at_bat = sum(AB, na.rm = T)) %&gt;% # Arrange in descending order arrange(desc(total_at_bat)) ## # A tibble: 13,958 x 2 ## playerID total_at_bat ## &lt;chr&gt; &lt;int&gt; ## 1 aaronha01 12364 ## 2 yastrca01 11988 ## 3 cobbty01 11434 ## 4 musiast01 10972 ## 5 mayswi01 10881 ## 6 robinbr01 10654 ## 7 wagneho01 10430 ## 8 brocklo01 10332 ## 9 ansonca01 10277 ## 10 aparilu01 10230 ## # ... with 13,948 more rows The unpaid players definitely participated in the games. The highest number of at bats is Hank Aaron so it looks like we are dealing with missing data here and not unsalaried players. Next, lets look at the Hall of Fame players # Find the distinct players that appear in HallOfFame nominated &lt;- HallOfFame %&gt;% distinct(playerID) nominated %&gt;% # Count the number of players in nominated count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 1260 nominated_full &lt;- nominated %&gt;% # Join to Master left_join(Master, by = &quot;playerID&quot;) %&gt;% # Return playerID, nameFirst, nameLast select(playerID, nameFirst, nameLast) Next, let’s find out how many of those nominated are now inducted in to the HoF # Find distinct players in HallOfFame with inducted == &quot;Y&quot; inducted &lt;- HallOfFame %&gt;% filter(inducted == &quot;Y&quot;) %&gt;% distinct(playerID) inducted %&gt;% # Count the number of players in inducted count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 317 inducted_full &lt;- inducted %&gt;% # Join to Master left_join(Master, by = &quot;playerID&quot;) %&gt;% # Return playerID, nameFirst, nameLast select(playerID, nameFirst, nameLast) Now that we know who was inducted and who was nominated, let’s examine what separates the nominees who were inducted from the nominees who were not. Let’s start with a simple question: Did nominees who were inducted get more awards than nominees who were not inducted? # Tally the number of awards in AwardsPlayers by playerID nAwards &lt;- AwardsPlayers %&gt;% group_by(playerID) %&gt;% tally() nAwards %&gt;% # Filter to just the players in inducted semi_join(inducted, by = &quot;playerID&quot;) %&gt;% # Calculate the mean number of awards per player summarize(avg_n = mean(n, na.rm = T)) ## # A tibble: 1 x 1 ## avg_n ## &lt;dbl&gt; ## 1 12.14583 nAwards %&gt;% # Filter to just the players in nominated semi_join(nominated, by = &quot;playerID&quot;) %&gt;% # Filter to players NOT in inducted anti_join(inducted, by = &quot;playerID&quot;) %&gt;% # Calculate the mean number of awards per player summarize(avg_n = mean(n, na.rm = T)) ## # A tibble: 1 x 1 ## avg_n ## &lt;dbl&gt; ## 1 4.231054 The answer is yes - it looks like about 3 times the number. Was the salary much higher for those who were inducted? # Find the players who are in nominated, but not inducted notInducted &lt;- nominated %&gt;% setdiff(inducted) Salaries %&gt;% # Find the players who are in notInducted semi_join(notInducted, by = &quot;playerID&quot;) %&gt;% # Calculate the max salary by player group_by(playerID) %&gt;% summarize(max_salary = max(salary, na.rm = T)) %&gt;% # Calculate the average of the max salaries summarize(avg_salary = mean(max_salary, na.rm = T)) ## # A tibble: 1 x 1 ## avg_salary ## &lt;dbl&gt; ## 1 5124653 # Repeat for players who were inducted Salaries %&gt;% semi_join(inducted, by = &quot;playerID&quot;) %&gt;% # Calculate the max salary by player group_by(playerID) %&gt;% summarize(max_salary = max(salary, na.rm = T)) %&gt;% # Calculate the average of the max salaries summarize(avg_salary = mean(max_salary, na.rm = T)) ## # A tibble: 1 x 1 ## avg_salary ## &lt;dbl&gt; ## 1 6092038 So the salaries of the players who were inducted was higher. Were any players nominated 5 years before they retired? players &lt;- Appearances %&gt;% # Filter Appearances against nominated semi_join(nominated, by = &quot;playerID&quot;) %&gt;% # Find last year played by player group_by(playerID) %&gt;% summarize(last_year = max(yearID, na.rm = T)) %&gt;% # Join to full HallOfFame left_join(HallOfFame, by = &quot;playerID&quot;) %&gt;% # Filter for unusual observations filter((yearID - last_year) &lt; 5 ) players ## # A tibble: 194 x 10 ## playerID last_year yearID votedBy ballots needed votes inducted ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fctr&gt; ## 1 altroni01 1933 1937 BBWAA 201 151 3 N ## 2 applilu01 1950 1953 BBWAA 264 198 2 N ## 3 bartedi01 1946 1948 BBWAA 121 91 1 N ## 4 beckro01 2004 2008 BBWAA 543 408 2 N ## 5 boudrlo01 1952 1956 BBWAA 193 145 2 N ## 6 camildo01 1945 1948 BBWAA 121 91 1 N ## 7 chandsp01 1947 1950 BBWAA 168 126 2 N ## 8 chandsp01 1947 1951 BBWAA 226 170 1 N ## 9 chapmbe01 1946 1949 BBWAA 153 115 1 N ## 10 cissebi01 1938 1937 BBWAA 201 151 1 N ## # ... with 184 more rows, and 2 more variables: category &lt;fctr&gt;, ## # needed_note &lt;chr&gt; players %&gt;% group_by(playerID) %&gt;% tally() %&gt;% # Arrange the results arrange(desc(n)) ## # A tibble: 92 x 2 ## playerID n ## &lt;chr&gt; &lt;int&gt; ## 1 deandi01 9 ## 2 dickebi01 7 ## 3 foxxji01 6 ## 4 lyonste01 6 ## 5 greenha01 5 ## 6 ruffire01 5 ## 7 cronijo01 4 ## 8 dimagjo01 4 ## 9 gehrich01 4 ## 10 hackst01 4 ## # ... with 82 more rows So we get a list of 194 players who were nominated within 5 years of having last played, with some players being nominated many times. And of those, how many were nominated whilst still playing? Appearances %&gt;% # Filter Appearances against nominated semi_join(nominated, by = &quot;playerID&quot;) %&gt;% # Find last year played by player group_by(playerID) %&gt;% summarize(last_year = max(yearID, na.rm = T)) %&gt;% # Join to full HallOfFame left_join(HallOfFame, by = &quot;playerID&quot;) %&gt;% # Filter for unusual observations filter(yearID &lt;= last_year) %&gt;% # look for the most recent incident of nomination before reirement arrange(desc(yearID)) ## # A tibble: 39 x 10 ## playerID last_year yearID votedBy ballots needed votes inducted ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fctr&gt; ## 1 francju02 2014 2013 BBWAA 569 427 6 N ## 2 rijojo01 2002 2001 BBWAA 515 387 1 N ## 3 stephjo03 2002 1979 BBWAA 432 324 0 N ## 4 minosmi01 1980 1969 BBWAA 340 255 6 N ## 5 spahnwa01 1965 1958 BBWAA 266 200 1 N ## 6 rizzuph01 1956 1956 BBWAA 193 145 1 N ## 7 paigesa01 1965 1951 BBWAA 226 170 1 N ## 8 waltebu01 1950 1950 BBWAA 168 126 4 N ## 9 medwijo01 1948 1948 BBWAA 121 91 1 N ## 10 deandi01 1947 1947 BBWAA 161 121 88 N ## # ... with 29 more rows, and 2 more variables: category &lt;fctr&gt;, ## # needed_note &lt;chr&gt; So around 40 players were nominated whilst still plaing "],
["cleaning-data.html", "5 Cleaning Data 5.1 Tidying data 5.2 Preparing data for analysis 5.3 String manipulation 5.4 Missing, Specials and Outliers 5.5 Examples", " 5 Cleaning Data Notes taken during/inspired by the Datacamp course ‘Cleaning Data in R’ by Nick Carchedi. 5.1 Tidying data In Hadley’s paper on tidy data, he talked about how columns in a data frame should be variables or attributes and rows should be observations - this somestimes does not happen if there are things like dummy variables as columns, that could be collpased in to a single column. The entire data table (data frame) should be about one particular set of data i.e. we have countries without an embedded table about cats. Hadley introduced the tidyr package to try and help clean some data. There are two fundamental verbs of data tidying: gather() takes multiple columns, and gathers them into key-value pairs: it makes “wide” data longer spread() takes two columns (key &amp; value) and spreads in to multiple columns, it makes “long” data wider gather(data, key, value …) data: is a data frame key: the name of the new key column value: the name of the new value column …: names of columns to gather or not (if not, state -col e.g. -time to not include the time column in the gathered table) spread(data, key, value) data: is a data frame key: the name containing the key column value: the name containing the value column # Apply gather() to bmi and save the result as bmi_long bmi_long &lt;- gather(bmi, year, bmi_value, -Country) # Apply spread() to bmi_long bmi_wide &lt;- spread(bmi_long, year, bmi_val) Another useful feature is separate(). This takes a single variable and separates it into two separate columns or variable, for instance converting a year-month (2015-10) into a separate column for year and month. separate(data, col, into, sep = “”) data: a data frame col: bare name of column to separate into: charecter vector of new column names Optional sep = “”: in the separate command you can designate on what item (/, @ etc) to break the data by. This is optional and can depend on the column type (numeric vs char) # separate year-mo into two columns separate(treatments, year_mo, c(&quot;year&quot;, &quot;month&quot;)) We can also use the unite function to combine two columns together unite(data, col, …) data: a data frame col: name of the new column …: columns to unite The default seperator within the new column is an underscore, however we can specify something different Optional sep = “-”: would add the seperator as a hyphen head(bmi_cc) Country_ISO year bmi_val 1 Afghanistan/AF Y1980 21.48678 2 Albania/AL Y1980 25.22533 3 Algeria/DZ Y1980 22.25703 4 Andorra/AD Y1980 25.66652 5 Angola/AO Y1980 20.94876 6 Antigua and Barbuda/AG Y1980 23.31424 So to separate Country_ISO into two columns # Apply separate() to bmi_cc bmi_cc_clean &lt;- separate(bmi_cc, col = Country_ISO, into = c(&quot;Country&quot;, &quot;ISO&quot;), sep = &quot;/&quot;) # Apply unite() to bmi_cc_clean aand reverse bmi_cc &lt;- unite(bmi_cc_clean, Country_ISO, Country, ISO, sep = &quot;-&quot;) 5.2 Preparing data for analysis Often we need to convert, or in the case of raw data, create the appropriate data type for each variable prior to analysis. Some common data types include character: “treatment”, “123”, “A” numeric: 23.44, 120, NaN, Inf integer: 4L, 1123L factor: factor(“Hello”), factor(8) logical: TRUE, FALSE, NA We can use the class() function to detmine the variable type, or we can also include a value to determine the appropriate type e.g. class(77L) will return [1] “integer”. We can also use the coercion functions to change the types, such as as.numeric, as.factor() and as.character(). For dates and times, we can use the lubridate package. # Load the lubridate package library(lubridate) # Parse as date dmy(&quot;17 Sep 2015&quot;) # Parse as date and time (with no seconds!) mdy_hm(&quot;July 15, 2012 12:56&quot;) # Coerce dob to a date (with no time) students2$dob &lt;- ymd(students2$dob) # Coerce nurse_visit to a date and time students2$nurse_visit &lt;- ymd_hms(students2$nurse_visit) 5.3 String manipulation Another useful package is stringr, which like lubridate and other Hadley packages has a consistent interface, providing a range of functions for dealing with strings. Some functions include str_trim() - Trim leading and trailing white space str_pad() - Pad with additional characters str_detect() - Detect a pattern str_replace() - Find and replace a pattern # Load the stringr package library(stringr) # Trim all leading and trailing whitespace str_trim(c(&quot; Filip &quot;, &quot;Nick &quot;, &quot; Jonathan&quot;)) # Pad these strings with leading zeros str_pad(c(&quot;23485W&quot;, &quot;8823453Q&quot;, &quot;994Z&quot;), width = 9, side = &quot;left&quot;, pad = 0) # Detect all dates of birth (dob) in 1997 str_detect(students2$dob, &quot;1997&quot;) # In the sex column, replace &quot;F&quot; with &quot;Female&quot;... students2$sex &lt;- str_replace(students2$sex, &quot;F&quot;, &quot;Female&quot;) # ...And &quot;M&quot; with &quot;Male&quot; students2$sex &lt;- str_replace(students2$sex, &quot;M&quot;, &quot;Male&quot;) R {base} also has some handy features for strings, including toupper() and tolower(). 5.4 Missing, Specials and Outliers Generally missing values in R are represented by NA. However, if the data has been imported from other systems, the values can be different, such as a . (dot) if imported from SPSS. We can use the is.na(df) to return a TRUE/FALSE array of where there are NA values in a data frame. Or, for large datasets, we can use the any(is.na(df)) to return a true or false if there is an NA anywhere in the data frame. Alternatively we can use the sum(is.na(df)) to count how many NAs are in the dataframe. Use complete.cases() to see which rows have no missing values. Special values include inf for infinite value, NaN for Not a number. Outliers are best detected by measures such as the IQR or other nuemrical measures (see the EDA section), by using a boxplot or a histogram/density plot. There are a number of likely reasons for an outlier: Valid measurements Variability in measurement Experimental error Data entry error May be discarded or retained depending on cause. In some instances we may want to cap, or put a limit on, the maximum number the outlier can. Looking at the actual values and considering possible values can help, for instance negative age values or a perons age above 200 are not plausible values. However, they may be data entry errors or in the case of negative numbers, represent a deliberately coded missing value. 5.5 Examples The weather dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. head(weather) X year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 1 2014 12 Max.TemperatureF 64 42 51 43 42 45 38 29 49 48 39 39 42 45 2 2014 12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39 43 36 35 37 39 3 2014 12 Min.TemperatureF 39 33 37 30 26 38 21 18 29 38 32 31 32 33 4 2014 12 Max.Dew.PointF 46 40 49 24 37 45 36 28 49 45 37 28 28 29 # Load the tidyr package library(tidyr) # Gather the columns weather2 &lt;- gather(weather, day, value, X1:X31, na.rm = TRUE) becomes X year month measure day value 1 2014 12 Max.TemperatureF X1 64 2 2014 12 Mean.TemperatureF X1 52 3 2014 12 Min.TemperatureF X1 39 4 2014 12 Max.Dew.PointF X1 46 5 2014 12 MeanDew.PointF X1 40 6 2014 12 Min.DewpointF X1 26 Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. WE also have an additional column (X) which is not needed as it is just the row number. # First remove column of row names weather2 &lt;- weather2[, -1] # Spread the data weather3 &lt;- spread(weather2, measure, value) Table 4.1: year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH 2014 12 X1 6 Rain 46 29 2014 12 X2 7 Rain-Snow 40 29 2014 12 X3 8 Rain 49 38 2014 12 X4 3 24 33 2014 12 X5 5 Rain 37 26 2014 12 X6 8 Rain 45 25 … Now that the weather dataset adheres to tidy data principles, the next step is to prepare it for analysis. We’ll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. # Remove X&#39;s from day column weather3$day &lt;- str_replace(weather3$day, &quot;X&quot;, &quot;&quot;) # Unite the year, month, and day columns weather4 &lt;- unite(weather3, date, year, month, day, sep = &quot;-&quot;) # Convert date column to proper date format using lubridates&#39;s ymd() weather4$date &lt;- ymd(weather4$date) # Rearrange columns using dplyr&#39;s select() weather5 &lt;- select(weather4, date, Events, CloudCover:WindDirDegrees) It’s important for analysis that variables are coded appropriately. This is not yet the case with our weather data. Recall that functions such as as.numeric() and as.character() can be used to coerce variables into different types. It’s important to keep in mind that coercions are not always successful, particularly if there’s some data in a column that you don’t expect. For example, the following will cause problems: as.numeric(c(4, 6.44, “some string”, 222)) So you can use the str_replace function to change character values to something else. If we have missing data, we can use indices and is.na function to identify then only see those rows with NA values on a variable of interest. # Count missing values sum(is.na(weather6)) # Find missing values summary(weather6) # Find indices of NAs in Max.Gust.SpeedMPH ind &lt;- which(is.na(weather6$Max.Gust.SpeedMPH)) # Look at the full rows for records missing Max.Gust.SpeedMPH weather6[ind, ] Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with summary(). Once implausible values are identified, they must be dealt with in an intelligent and informed way. Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data. # Find row with Max.Humidity of 1000 ind &lt;- which(weather6$Max.Humidity == 1000) # Look at the data for that day weather6[ind, ] # Change 1000 to 100 weather6$Max.Humidity[ind] &lt;- 100 Before officially calling our weather data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we generally recommend using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. "],
["importing-cleaning-data-in-r-case-studies.html", "6 Importing &amp; Cleaning Data in R: Case Studies 6.1 Ticket Sales Data 6.2 Working with dates 6.3 MBTA Ridership Data 6.4 World Food Facts 6.5 School Attendance Data", " 6 Importing &amp; Cleaning Data in R: Case Studies Notes taken during/inspired by the Datacamp course ‘Importing &amp; Cleaning Data in R: Case Studies’ by Nick Carchedi. 6.1 Ticket Sales Data # Import sales.csv: sales sales &lt;- read.csv(&quot;https://assets.datacamp.com/production/course_1294/datasets/sales.csv&quot;, stringsAsFactors = FALSE) # View dimensions of sales dim(sales) ## [1] 5000 46 # Inspect first 6 rows of sales head(sales, n = 6) ## X event_id primary_act_id secondary_act_id ## 1 1 abcaf1adb99a935fc661 43f0436b905bfa7c2eec b85143bf51323b72e53c ## 2 2 6c56d7f08c95f2aa453c 1a3e9aecd0617706a794 f53529c5679ea6ca5a48 ## 3 3 c7ab4524a121f9d687d2 4b677c3f5bec71eec8d1 b85143bf51323b72e53c ## 4 4 394cb493f893be9b9ed1 b1ccea01ad6ef8522796 b85143bf51323b72e53c ## 5 5 55b5f67e618557929f48 91c03a34b562436efa3c b85143bf51323b72e53c ## 6 6 4f10fd8b9f550352bd56 ac4b847b3fde66f2117e 63814f3d63317f1b56c4 ## purch_party_lkup_id ## 1 7dfa56dd7d5956b17587 ## 2 4f9e6fc637eaf7b736c2 ## 3 6c2545703bd527a7144d ## 4 527d6b1eaffc69ddd882 ## 5 8bd62c394a35213bdf52 ## 6 3b3a628f83135acd0676 ## event_name ## 1 Xfinity Center Mansfield Premier Parking: Florida Georgia Line ## 2 Gorge Camping - dave matthews band - sept 3-7 ## 3 Dodge Theatre Adams Street Parking - benise ## 4 Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow ## 5 Premier Parking - motley crue ## 6 Fast Lane Access: Journey ## primary_act_name secondary_act_name ## 1 XFINITY Center Mansfield Premier Parking NULL ## 2 Gorge Camping Dave Matthews Band ## 3 Parking Event NULL ## 4 Gexa Energy Pavilion VIP Parking NULL ## 5 White River Amphitheatre Premier Parking NULL ## 6 Fast Lane Access Journey ## major_cat_name minor_cat_name la_event_type_cat ## 1 MISC PARKING PARKING ## 2 MISC CAMPING INVALID ## 3 MISC PARKING PARKING ## 4 MISC PARKING PARKING ## 5 MISC PARKING PARKING ## 6 MISC SPECIAL ENTRY (UPSELL) UPSELL ## event_disp_name ## 1 Xfinity Center Mansfield Premier Parking: Florida Georgia Line ## 2 Gorge Camping - dave matthews band - sept 3-7 ## 3 Dodge Theatre Adams Street Parking - benise ## 4 Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow ## 5 Premier Parking - motley crue ## 6 Fast Lane Access: Journey ## ticket_text ## 1 THIS TICKET IS VALID FOR PARKING ONLY GOOD THIS DAY ONLY PREMIER PARKING PASS XFINITY CENTER,LOTS 4 PM SAT SEP 12 2015 7:30 PM ## 2 %OVERNIGHT C A M P I N G%* * * * * *%GORGE CAMPGROUND%* GOOD THIS DATE ONLY *%SEP 3 - 6, 2009 ## 3 ADAMS STREET GARAGE%PARKING FOR 4/21/06 ONLY%DODGE THEATRE PARKING PASS%ENTRANCE ON ADAMS STREET%BENISE%GARAGE OPENS AT 6:00PM ## 4 THIS TICKET IS VALID FOR PARKING ONLY GOOD FOR THIS DATE ONLY VIP PARKING PASS GEXA ENERGY PAVILION FRI SEP 02 2011 7:00 PM ## 5 THIS TICKET IS VALID%FOR PARKING ONLY%GOOD THIS DATE ONLY%PREMIER PARKING PASS%WHITE RIVER AMPHITHEATRE%SAT JUL 30, 2005 6:00PM ## 6 FAST LANE JOURNEY FAST LANE EVENT THIS IS NOT A TICKET SAN MANUEL AMPHITHEATER SAT JUL 21 2012 7:00 PM ## tickets_purchased_qty trans_face_val_amt delivery_type_cd ## 1 1 45 eTicket ## 2 1 75 TicketFast ## 3 1 5 TicketFast ## 4 1 20 Mail ## 5 1 20 Mail ## 6 2 10 TicketFast ## event_date_time event_dt presale_dt onsale_dt ## 1 2015-09-12 23:30:00 2015-09-12 NULL 2015-05-15 ## 2 2009-09-05 01:00:00 2009-09-04 NULL 2009-03-13 ## 3 2006-04-22 01:30:00 2006-04-21 NULL 2006-02-25 ## 4 2011-09-03 00:00:00 2011-09-02 NULL 2011-04-22 ## 5 2005-07-31 01:00:00 2005-07-30 2005-03-02 2005-03-04 ## 6 2012-07-22 02:00:00 2012-07-21 NULL 2012-04-11 ## sales_ord_create_dttm sales_ord_tran_dt print_dt timezn_nm ## 1 2015-09-11 18:17:45 2015-09-11 2015-09-12 EST ## 2 2009-07-06 00:00:00 2009-07-05 2009-09-01 PST ## 3 2006-04-05 00:00:00 2006-04-05 2006-04-05 MST ## 4 2011-07-01 17:38:50 2011-07-01 2011-07-06 CST ## 5 2005-06-18 00:00:00 2005-06-18 2005-06-28 PST ## 6 2012-07-21 17:20:18 2012-07-21 2012-07-21 PST ## venue_city venue_state venue_postal_cd_sgmt_1 ## 1 MANSFIELD MASSACHUSETTS 02048 ## 2 QUINCY WASHINGTON 98848 ## 3 PHOENIX ARIZONA 85003 ## 4 DALLAS TEXAS 75210 ## 5 AUBURN WASHINGTON 98092 ## 6 SAN BERNARDINO CALIFORNIA 92407 ## sales_platform_cd print_flg la_valid_tkt_event_flg fin_mkt_nm ## 1 www.concerts.livenation.com T N Boston ## 2 NULL T N Seattle ## 3 NULL T N Arizona ## 4 NULL T N Dallas ## 5 NULL T N Seattle ## 6 www.livenation.com T N Los Angeles ## web_session_cookie_val gndr_cd age_yr income_amt edu_val ## 1 7dfa56dd7d5956b17587 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 4f9e6fc637eaf7b736c2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 6c2545703bd527a7144d &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 527d6b1eaffc69ddd882 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 8bd62c394a35213bdf52 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 3b3a628f83135acd0676 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## edu_1st_indv_val edu_2nd_indv_val adults_in_hh_num married_ind ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## child_present_ind home_owner_ind occpn_val occpn_1st_val occpn_2nd_val ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## dist_to_ven ## 1 NA ## 2 59 ## 3 NA ## 4 NA ## 5 NA ## 6 NA # View column names of sales names(sales) ## [1] &quot;X&quot; &quot;event_id&quot; ## [3] &quot;primary_act_id&quot; &quot;secondary_act_id&quot; ## [5] &quot;purch_party_lkup_id&quot; &quot;event_name&quot; ## [7] &quot;primary_act_name&quot; &quot;secondary_act_name&quot; ## [9] &quot;major_cat_name&quot; &quot;minor_cat_name&quot; ## [11] &quot;la_event_type_cat&quot; &quot;event_disp_name&quot; ## [13] &quot;ticket_text&quot; &quot;tickets_purchased_qty&quot; ## [15] &quot;trans_face_val_amt&quot; &quot;delivery_type_cd&quot; ## [17] &quot;event_date_time&quot; &quot;event_dt&quot; ## [19] &quot;presale_dt&quot; &quot;onsale_dt&quot; ## [21] &quot;sales_ord_create_dttm&quot; &quot;sales_ord_tran_dt&quot; ## [23] &quot;print_dt&quot; &quot;timezn_nm&quot; ## [25] &quot;venue_city&quot; &quot;venue_state&quot; ## [27] &quot;venue_postal_cd_sgmt_1&quot; &quot;sales_platform_cd&quot; ## [29] &quot;print_flg&quot; &quot;la_valid_tkt_event_flg&quot; ## [31] &quot;fin_mkt_nm&quot; &quot;web_session_cookie_val&quot; ## [33] &quot;gndr_cd&quot; &quot;age_yr&quot; ## [35] &quot;income_amt&quot; &quot;edu_val&quot; ## [37] &quot;edu_1st_indv_val&quot; &quot;edu_2nd_indv_val&quot; ## [39] &quot;adults_in_hh_num&quot; &quot;married_ind&quot; ## [41] &quot;child_present_ind&quot; &quot;home_owner_ind&quot; ## [43] &quot;occpn_val&quot; &quot;occpn_1st_val&quot; ## [45] &quot;occpn_2nd_val&quot; &quot;dist_to_ven&quot; Luckily, the rows and columns appear to be arranged in a meaningful way: each row represents an observation and each column a variable, or piece of information about that observation. In R, there are a great many tools at your disposal to help get a feel for your data. Besides the three you used in the previous exercise, the functions str() and summary() can be very helpful. The dplyr package, introduced in Cleaning Data in R, offers the glimpse() function, which can also be used for this purpose. The package is already installed on DataCamp; you just need to load it. # Look at structure of sales str(sales) ## &#39;data.frame&#39;: 5000 obs. of 46 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ event_id : chr &quot;abcaf1adb99a935fc661&quot; &quot;6c56d7f08c95f2aa453c&quot; &quot;c7ab4524a121f9d687d2&quot; &quot;394cb493f893be9b9ed1&quot; ... ## $ primary_act_id : chr &quot;43f0436b905bfa7c2eec&quot; &quot;1a3e9aecd0617706a794&quot; &quot;4b677c3f5bec71eec8d1&quot; &quot;b1ccea01ad6ef8522796&quot; ... ## $ secondary_act_id : chr &quot;b85143bf51323b72e53c&quot; &quot;f53529c5679ea6ca5a48&quot; &quot;b85143bf51323b72e53c&quot; &quot;b85143bf51323b72e53c&quot; ... ## $ purch_party_lkup_id : chr &quot;7dfa56dd7d5956b17587&quot; &quot;4f9e6fc637eaf7b736c2&quot; &quot;6c2545703bd527a7144d&quot; &quot;527d6b1eaffc69ddd882&quot; ... ## $ event_name : chr &quot;Xfinity Center Mansfield Premier Parking: Florida Georgia Line&quot; &quot;Gorge Camping - dave matthews band - sept 3-7&quot; &quot;Dodge Theatre Adams Street Parking - benise&quot; &quot;Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow&quot; ... ## $ primary_act_name : chr &quot;XFINITY Center Mansfield Premier Parking&quot; &quot;Gorge Camping&quot; &quot;Parking Event&quot; &quot;Gexa Energy Pavilion VIP Parking&quot; ... ## $ secondary_act_name : chr &quot;NULL&quot; &quot;Dave Matthews Band&quot; &quot;NULL&quot; &quot;NULL&quot; ... ## $ major_cat_name : chr &quot;MISC&quot; &quot;MISC&quot; &quot;MISC&quot; &quot;MISC&quot; ... ## $ minor_cat_name : chr &quot;PARKING&quot; &quot;CAMPING&quot; &quot;PARKING&quot; &quot;PARKING&quot; ... ## $ la_event_type_cat : chr &quot;PARKING&quot; &quot;INVALID&quot; &quot;PARKING&quot; &quot;PARKING&quot; ... ## $ event_disp_name : chr &quot;Xfinity Center Mansfield Premier Parking: Florida Georgia Line&quot; &quot;Gorge Camping - dave matthews band - sept 3-7&quot; &quot;Dodge Theatre Adams Street Parking - benise&quot; &quot;Gexa Energy Pavilion Vip Parking : kid rock with sheryl crow&quot; ... ## $ ticket_text : chr &quot; THIS TICKET IS VALID FOR PARKING ONLY GOOD THIS DAY ONLY PREMIER PARKING PASS XFINIT&quot;| __truncated__ &quot;%OVERNIGHT C A M P I N G%* * * * * *%GORGE CAMPGROUND%* GOOD THIS DATE ONLY *%SEP 3 - 6, 2009&quot; &quot;ADAMS STREET GARAGE%PARKING FOR 4/21/06 ONLY%DODGE THEATRE PARKING PASS%ENTRANCE ON ADAMS STREET%BENISE%GARAGE OPENS AT 6:00PM&quot; &quot; THIS TICKET IS VALID FOR PARKING ONLY GOOD FOR THIS DATE ONLY VIP PARKING PASS GEXA&quot;| __truncated__ ... ## $ tickets_purchased_qty : int 1 1 1 1 1 2 1 1 1 1 ... ## $ trans_face_val_amt : num 45 75 5 20 20 10 30 28 20 25 ... ## $ delivery_type_cd : chr &quot;eTicket&quot; &quot;TicketFast&quot; &quot;TicketFast&quot; &quot;Mail&quot; ... ## $ event_date_time : chr &quot;2015-09-12 23:30:00&quot; &quot;2009-09-05 01:00:00&quot; &quot;2006-04-22 01:30:00&quot; &quot;2011-09-03 00:00:00&quot; ... ## $ event_dt : chr &quot;2015-09-12&quot; &quot;2009-09-04&quot; &quot;2006-04-21&quot; &quot;2011-09-02&quot; ... ## $ presale_dt : chr &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; ... ## $ onsale_dt : chr &quot;2015-05-15&quot; &quot;2009-03-13&quot; &quot;2006-02-25&quot; &quot;2011-04-22&quot; ... ## $ sales_ord_create_dttm : chr &quot;2015-09-11 18:17:45&quot; &quot;2009-07-06 00:00:00&quot; &quot;2006-04-05 00:00:00&quot; &quot;2011-07-01 17:38:50&quot; ... ## $ sales_ord_tran_dt : chr &quot;2015-09-11&quot; &quot;2009-07-05&quot; &quot;2006-04-05&quot; &quot;2011-07-01&quot; ... ## $ print_dt : chr &quot;2015-09-12&quot; &quot;2009-09-01&quot; &quot;2006-04-05&quot; &quot;2011-07-06&quot; ... ## $ timezn_nm : chr &quot;EST&quot; &quot;PST&quot; &quot;MST&quot; &quot;CST&quot; ... ## $ venue_city : chr &quot;MANSFIELD&quot; &quot;QUINCY&quot; &quot;PHOENIX&quot; &quot;DALLAS&quot; ... ## $ venue_state : chr &quot;MASSACHUSETTS&quot; &quot;WASHINGTON&quot; &quot;ARIZONA&quot; &quot;TEXAS&quot; ... ## $ venue_postal_cd_sgmt_1: chr &quot;02048&quot; &quot;98848&quot; &quot;85003&quot; &quot;75210&quot; ... ## $ sales_platform_cd : chr &quot;www.concerts.livenation.com&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; ... ## $ print_flg : chr &quot;T &quot; &quot;T &quot; &quot;T &quot; &quot;T &quot; ... ## $ la_valid_tkt_event_flg: chr &quot;N &quot; &quot;N &quot; &quot;N &quot; &quot;N &quot; ... ## $ fin_mkt_nm : chr &quot;Boston&quot; &quot;Seattle&quot; &quot;Arizona&quot; &quot;Dallas&quot; ... ## $ web_session_cookie_val: chr &quot;7dfa56dd7d5956b17587&quot; &quot;4f9e6fc637eaf7b736c2&quot; &quot;6c2545703bd527a7144d&quot; &quot;527d6b1eaffc69ddd882&quot; ... ## $ gndr_cd : chr NA NA NA NA ... ## $ age_yr : chr NA NA NA NA ... ## $ income_amt : chr NA NA NA NA ... ## $ edu_val : chr NA NA NA NA ... ## $ edu_1st_indv_val : chr NA NA NA NA ... ## $ edu_2nd_indv_val : chr NA NA NA NA ... ## $ adults_in_hh_num : chr NA NA NA NA ... ## $ married_ind : chr NA NA NA NA ... ## $ child_present_ind : chr NA NA NA NA ... ## $ home_owner_ind : chr NA NA NA NA ... ## $ occpn_val : chr NA NA NA NA ... ## $ occpn_1st_val : chr NA NA NA NA ... ## $ occpn_2nd_val : chr NA NA NA NA ... ## $ dist_to_ven : int NA 59 NA NA NA NA NA NA NA NA ... # View a summary of sales summary(sales) ## X event_id primary_act_id secondary_act_id ## Min. : 1 Length:5000 Length:5000 Length:5000 ## 1st Qu.:1251 Class :character Class :character Class :character ## Median :2500 Mode :character Mode :character Mode :character ## Mean :2500 ## 3rd Qu.:3750 ## Max. :5000 ## ## purch_party_lkup_id event_name primary_act_name ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## secondary_act_name major_cat_name minor_cat_name ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## la_event_type_cat event_disp_name ticket_text ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## tickets_purchased_qty trans_face_val_amt delivery_type_cd ## Min. :1.000 Min. : 1.00 Length:5000 ## 1st Qu.:1.000 1st Qu.: 20.00 Class :character ## Median :1.000 Median : 30.00 Mode :character ## Mean :1.639 Mean : 77.08 ## 3rd Qu.:2.000 3rd Qu.: 85.00 ## Max. :8.000 Max. :1520.88 ## ## event_date_time event_dt presale_dt ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## onsale_dt sales_ord_create_dttm sales_ord_tran_dt ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## print_dt timezn_nm venue_city ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## venue_state venue_postal_cd_sgmt_1 sales_platform_cd ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## print_flg la_valid_tkt_event_flg fin_mkt_nm ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## web_session_cookie_val gndr_cd age_yr ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## income_amt edu_val edu_1st_indv_val ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## edu_2nd_indv_val adults_in_hh_num married_ind ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## child_present_ind home_owner_ind occpn_val ## Length:5000 Length:5000 Length:5000 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## occpn_1st_val occpn_2nd_val dist_to_ven ## Length:5000 Length:5000 Min. : 0.0 ## Class :character Class :character 1st Qu.: 12.0 ## Mode :character Mode :character Median : 26.0 ## Mean : 158.2 ## 3rd Qu.: 77.5 ## Max. :2548.0 ## NA&#39;s :4677 # Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Get a glimpse of sales glimpse(sales) ## Observations: 5,000 ## Variables: 46 ## $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ... ## $ event_id &lt;chr&gt; &quot;abcaf1adb99a935fc661&quot;, &quot;6c56d7f08c95f2... ## $ primary_act_id &lt;chr&gt; &quot;43f0436b905bfa7c2eec&quot;, &quot;1a3e9aecd06177... ## $ secondary_act_id &lt;chr&gt; &quot;b85143bf51323b72e53c&quot;, &quot;f53529c5679ea6... ## $ purch_party_lkup_id &lt;chr&gt; &quot;7dfa56dd7d5956b17587&quot;, &quot;4f9e6fc637eaf7... ## $ event_name &lt;chr&gt; &quot;Xfinity Center Mansfield Premier Parki... ## $ primary_act_name &lt;chr&gt; &quot;XFINITY Center Mansfield Premier Parki... ## $ secondary_act_name &lt;chr&gt; &quot;NULL&quot;, &quot;Dave Matthews Band&quot;, &quot;NULL&quot;, &quot;... ## $ major_cat_name &lt;chr&gt; &quot;MISC&quot;, &quot;MISC&quot;, &quot;MISC&quot;, &quot;MISC&quot;, &quot;MISC&quot;,... ## $ minor_cat_name &lt;chr&gt; &quot;PARKING&quot;, &quot;CAMPING&quot;, &quot;PARKING&quot;, &quot;PARKI... ## $ la_event_type_cat &lt;chr&gt; &quot;PARKING&quot;, &quot;INVALID&quot;, &quot;PARKING&quot;, &quot;PARKI... ## $ event_disp_name &lt;chr&gt; &quot;Xfinity Center Mansfield Premier Parki... ## $ ticket_text &lt;chr&gt; &quot; THIS TICKET IS VALID FOR PAR... ## $ tickets_purchased_qty &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 4, ... ## $ trans_face_val_amt &lt;dbl&gt; 45, 75, 5, 20, 20, 10, 30, 28, 20, 25, ... ## $ delivery_type_cd &lt;chr&gt; &quot;eTicket&quot;, &quot;TicketFast&quot;, &quot;TicketFast&quot;, ... ## $ event_date_time &lt;chr&gt; &quot;2015-09-12 23:30:00&quot;, &quot;2009-09-05 01:0... ## $ event_dt &lt;chr&gt; &quot;2015-09-12&quot;, &quot;2009-09-04&quot;, &quot;2006-04-21... ## $ presale_dt &lt;chr&gt; &quot;NULL&quot;, &quot;NULL&quot;, &quot;NULL&quot;, &quot;NULL&quot;, &quot;2005-0... ## $ onsale_dt &lt;chr&gt; &quot;2015-05-15&quot;, &quot;2009-03-13&quot;, &quot;2006-02-25... ## $ sales_ord_create_dttm &lt;chr&gt; &quot;2015-09-11 18:17:45&quot;, &quot;2009-07-06 00:0... ## $ sales_ord_tran_dt &lt;chr&gt; &quot;2015-09-11&quot;, &quot;2009-07-05&quot;, &quot;2006-04-05... ## $ print_dt &lt;chr&gt; &quot;2015-09-12&quot;, &quot;2009-09-01&quot;, &quot;2006-04-05... ## $ timezn_nm &lt;chr&gt; &quot;EST&quot;, &quot;PST&quot;, &quot;MST&quot;, &quot;CST&quot;, &quot;PST&quot;, &quot;PST... ## $ venue_city &lt;chr&gt; &quot;MANSFIELD&quot;, &quot;QUINCY&quot;, &quot;PHOENIX&quot;, &quot;DALL... ## $ venue_state &lt;chr&gt; &quot;MASSACHUSETTS&quot;, &quot;WASHINGTON&quot;, &quot;ARIZONA... ## $ venue_postal_cd_sgmt_1 &lt;chr&gt; &quot;02048&quot;, &quot;98848&quot;, &quot;85003&quot;, &quot;75210&quot;, &quot;98... ## $ sales_platform_cd &lt;chr&gt; &quot;www.concerts.livenation.com&quot;, &quot;NULL&quot;, ... ## $ print_flg &lt;chr&gt; &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T &quot;, &quot;T ... ## $ la_valid_tkt_event_flg &lt;chr&gt; &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N &quot;, &quot;N ... ## $ fin_mkt_nm &lt;chr&gt; &quot;Boston&quot;, &quot;Seattle&quot;, &quot;Arizona&quot;, &quot;Dallas... ## $ web_session_cookie_val &lt;chr&gt; &quot;7dfa56dd7d5956b17587&quot;, &quot;4f9e6fc637eaf7... ## $ gndr_cd &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;M&quot;, NA, NA, NA... ## $ age_yr &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;28&quot;, NA, NA, N... ## $ income_amt &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;112500&quot;, NA, N... ## $ edu_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;High School&quot;, ... ## $ edu_1st_indv_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;High School&quot;, ... ## $ edu_2nd_indv_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;NULL&quot;, NA, NA,... ## $ adults_in_hh_num &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;4&quot;, NA, NA, NA... ## $ married_ind &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;0&quot;, NA, NA, NA... ## $ child_present_ind &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;1&quot;, NA, NA, NA... ## $ home_owner_ind &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;0&quot;, NA, NA, NA... ## $ occpn_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;NULL&quot;, NA, NA,... ## $ occpn_1st_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;Craftsman Blue... ## $ occpn_2nd_val &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;NULL&quot;, NA, NA,... ## $ dist_to_ven &lt;int&gt; NA, 59, NA, NA, NA, NA, NA, NA, NA, NA,... 6.1.1 Removing redundant info The first column of data is just a duplication of the row numbers. Not very useful. Go ahead and delete that column. Remember that nrow() and ncol() return the number of rows and columns in a data frame, respectively. Also, recall that you can use square brackets to subset a data frame as follows: my_df[1:5, ] # First 5 rows of my_df my_df[, 4] # Fourth column of my_df Alternatively, you can remove rows and columns using negative indices. For example: my_df[-(1:5), ] # Omit first 5 rows of my_df my_df[, -4] # Omit fourth column of my_df # Remove the first column of sales: sales2 sales2 &lt;- sales[, -1] Many of the columns have information that’s of no use to us. For example, the first four columns contain internal codes representing particular events. The last fifteen columns also aren’t worth keeping; there are too many missing values to make them worthwhile. An easy way to get rid of unnecessary columns is to create a vector containing the column indices you want to keep, then subset the data based on that vector using single bracket subsetting. # Define a vector of column indices: keep keep &lt;- c(5:30) # Subset sales2 using keep: sales3 sales3 &lt;- sales2[keep] Some of the columns in your data frame include multiple pieces of information that should be in separate columns. In this exercise, you will separate such a column into two: one for date and one for time. You will use the separate() function from the tidyr package (already installed for you). For isntance the event_date_time column has a date and time separated by a space. Therefore, you’ll use sep = &quot; &quot; as an argument to separate(). head(sales3$event_date_time) ## [1] &quot;2015-09-12 23:30:00&quot; &quot;2009-09-05 01:00:00&quot; &quot;2006-04-22 01:30:00&quot; ## [4] &quot;2011-09-03 00:00:00&quot; &quot;2005-07-31 01:00:00&quot; &quot;2012-07-22 02:00:00&quot; head(sales3$sales_ord_create_dttm) ## [1] &quot;2015-09-11 18:17:45&quot; &quot;2009-07-06 00:00:00&quot; &quot;2006-04-05 00:00:00&quot; ## [4] &quot;2011-07-01 17:38:50&quot; &quot;2005-06-18 00:00:00&quot; &quot;2012-07-21 17:20:18&quot; # Load tidyr library(tidyr) # Split event_date_time: sales4 sales4 &lt;- separate(sales3, event_date_time, into = c(&quot;event_dt&quot;, &quot;event_time&quot;), sep = &quot; &quot;) Looks like that second call to separate() threw a warning. Not to worry; warnings aren’t as bad as error messages. It’s not saying that the command didn’t execute; it’s just a heads-up that something unusual happened. The warning says Too few values at 4 locations. You may be able to guess already what the issue is, but it’s still good to take a look. sales3$sales_ord_create_dttm[c(2516, 3863, 4082, 4183)] ## [1] &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; # Define an issues vector issues &lt;- c(2516, 3863, 4082, 4183) # Print values of sales_ord_create_dttm at these indices sales3$sales_ord_create_dttm[issues] ## [1] &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; &quot;NULL&quot; # Print a well-behaved value of sales_ord_create_dttm sales3$sales_ord_create_dttm[2517] ## [1] &quot;2013-08-04 23:07:19&quot; 6.2 Working with dates Some of the columns in your dataset contain dates of different events. Right now, they are stored as character strings. That’s fine if all you want to do is look up the date associated with an event, but if you want to do any comparisons or math with the dates, it’s MUCH easier to store them as Date objects. Luckily, all of the date columns in this dataset have the substring “dt” in their name, so you can use the str_detect() function of the stringr package to find the date columns. Then you can coerce them to Date objects using a function from the lubridate package. You’ll use lapply() to apply the appropriate lubridate function to all of the columns that contain dates. Recall the following syntax for lapply() applied to some data frame columns of interest: lapply(my_data_frame[, cols], function_name) Also recall that function names in lubridate combine the letters y, m, d, h, m, and s depending on the format of the date/time string being read in. # Load stringr library(stringr) # Find columns of sales5 containing &quot;dt&quot;: date_cols date_cols &lt;- str_detect(names(sales4),&quot;dt&quot;) # Load lubridate library(lubridate) ## Loading required package: methods ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date # Coerce date columns into Date objects sales4[, date_cols] &lt;- lapply(sales4[, date_cols], ymd) ## Warning: 2892 failed to parse. ## Warning: 101 failed to parse. ## Warning: All formats failed to parse. No formats found. ## Warning: 424 failed to parse. Some of the calls to ymd() caused a failure to parse warning. That’s probably because of more missing data, but again, it’s good to check to be sure. ## stringr is loaded # Find date columns (don&#39;t change) date_cols &lt;- str_detect(names(sales4), &quot;dt&quot;) # Create logical vectors indicating missing values (don&#39;t change) missing &lt;- lapply(sales4[, date_cols], is.na) # Create a numerical vector that counts missing values: num_missing num_missing &lt;- sapply(missing, sum) # Print num_missing num_missing ## event_dt event_dt.1 presale_dt ## 0 0 2892 ## onsale_dt sales_ord_create_dttm sales_ord_tran_dt ## 101 5000 0 ## print_dt ## 424 The number of NAs in each column match the numbers from the warning messages, so missing data is the culprit. How to proceed depends on your desired analysis. If you really need complete sets of date/time information, you might delete the rows or columns containing NAs. 6.3 MBTA Ridership Data The Massachusetts Bay Transportation Authority (“MBTA” or just “the T” for short) manages America’s oldest subway, as well as Greater Boston’s commuter rail, ferry, and bus systems. It’s your first day on the job as the T’s data analyst and you’ve been tasked with analyzing average ridership through time. You’re in luck, because this chapter of the course will guide you through cleaning a set of MBTA ridership data! The dataset is stored as an Excel spreadsheet called mbta.xlsx. You’ll use the read_excel() function from Hadley Wickham’s readxl package to import it. The first time you import a dataset, you might not know how many rows need to be skipped. In this case, the first row is a title (see this Excel screenshot), so you’ll need to skip the first row. # Load readxl library(readxl) # Import mbta.xlsx and skip first row: mbta mbta &lt;- read_excel(&quot;D:/CloudStation/Documents/2017/RData/mbta.xlsx&quot;, skip = 1) # View the structure of mbta str(mbta) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 11 obs. of 60 variables: ## $ X__1 : num 1 2 3 4 5 6 7 8 9 10 ... ## $ mode : chr &quot;All Modes by Qtr&quot; &quot;Boat&quot; &quot;Bus&quot; &quot;Commuter Rail&quot; ... ## $ 2007-01: chr &quot;NA&quot; &quot;4&quot; &quot;335.819&quot; &quot;142.2&quot; ... ## $ 2007-02: chr &quot;NA&quot; &quot;3.6&quot; &quot;338.675&quot; &quot;138.5&quot; ... ## $ 2007-03: num 1188 40 340 138 459 ... ## $ 2007-04: chr &quot;NA&quot; &quot;4.3&quot; &quot;352.162&quot; &quot;139.5&quot; ... ## $ 2007-05: chr &quot;NA&quot; &quot;4.9&quot; &quot;354.367&quot; &quot;139&quot; ... ## $ 2007-06: num 1246 5.8 350.5 143 477 ... ## $ 2007-07: chr &quot;NA&quot; &quot;6.521&quot; &quot;357.519&quot; &quot;142.391&quot; ... ## $ 2007-08: chr &quot;NA&quot; &quot;6.572&quot; &quot;355.479&quot; &quot;142.364&quot; ... ## $ 2007-09: num 1256.57 5.47 372.6 143.05 499.57 ... ## $ 2007-10: chr &quot;NA&quot; &quot;5.145&quot; &quot;368.847&quot; &quot;146.542&quot; ... ## $ 2007-11: chr &quot;NA&quot; &quot;3.763&quot; &quot;330.826&quot; &quot;145.089&quot; ... ## $ 2007-12: num 1216.89 2.98 312.92 141.59 448.27 ... ## $ 2008-01: chr &quot;NA&quot; &quot;3.175&quot; &quot;340.324&quot; &quot;142.145&quot; ... ## $ 2008-02: chr &quot;NA&quot; &quot;3.111&quot; &quot;352.905&quot; &quot;142.607&quot; ... ## $ 2008-03: num 1253.52 3.51 361.15 137.45 494.05 ... ## $ 2008-04: chr &quot;NA&quot; &quot;4.164&quot; &quot;368.189&quot; &quot;140.389&quot; ... ## $ 2008-05: chr &quot;NA&quot; &quot;4.015&quot; &quot;363.903&quot; &quot;142.585&quot; ... ## $ 2008-06: num 1314.82 5.19 362.96 142.06 518.35 ... ## $ 2008-07: chr &quot;NA&quot; &quot;6.016&quot; &quot;370.921&quot; &quot;145.731&quot; ... ## $ 2008-08: chr &quot;NA&quot; &quot;5.8&quot; &quot;361.057&quot; &quot;144.565&quot; ... ## $ 2008-09: num 1307.04 4.59 389.54 141.91 517.32 ... ## $ 2008-10: chr &quot;NA&quot; &quot;4.285&quot; &quot;357.974&quot; &quot;151.957&quot; ... ## $ 2008-11: chr &quot;NA&quot; &quot;3.488&quot; &quot;345.423&quot; &quot;152.952&quot; ... ## $ 2008-12: num 1232.65 3.01 325.77 140.81 446.74 ... ## $ 2009-01: chr &quot;NA&quot; &quot;3.014&quot; &quot;338.532&quot; &quot;141.448&quot; ... ## $ 2009-02: chr &quot;NA&quot; &quot;3.196&quot; &quot;360.412&quot; &quot;143.529&quot; ... ## $ 2009-03: num 1209.79 3.33 353.69 142.89 467.22 ... ## $ 2009-04: chr &quot;NA&quot; &quot;4.049&quot; &quot;359.38&quot; &quot;142.34&quot; ... ## $ 2009-05: chr &quot;NA&quot; &quot;4.119&quot; &quot;354.75&quot; &quot;144.225&quot; ... ## $ 2009-06: num 1233.1 4.9 347.9 142 473.1 ... ## $ 2009-07: chr &quot;NA&quot; &quot;6.444&quot; &quot;339.477&quot; &quot;137.691&quot; ... ## $ 2009-08: chr &quot;NA&quot; &quot;5.903&quot; &quot;332.661&quot; &quot;139.158&quot; ... ## $ 2009-09: num 1230.5 4.7 374.3 139.1 500.4 ... ## $ 2009-10: chr &quot;NA&quot; &quot;4.212&quot; &quot;385.868&quot; &quot;137.104&quot; ... ## $ 2009-11: chr &quot;NA&quot; &quot;3.576&quot; &quot;366.98&quot; &quot;129.343&quot; ... ## $ 2009-12: num 1207.85 3.11 332.39 126.07 440.93 ... ## $ 2010-01: chr &quot;NA&quot; &quot;3.207&quot; &quot;362.226&quot; &quot;130.91&quot; ... ## $ 2010-02: chr &quot;NA&quot; &quot;3.195&quot; &quot;361.138&quot; &quot;131.918&quot; ... ## $ 2010-03: num 1208.86 3.48 373.44 131.25 483.4 ... ## $ 2010-04: chr &quot;NA&quot; &quot;4.452&quot; &quot;378.611&quot; &quot;131.722&quot; ... ## $ 2010-05: chr &quot;NA&quot; &quot;4.415&quot; &quot;380.171&quot; &quot;128.8&quot; ... ## $ 2010-06: num 1244.41 5.41 363.27 129.14 490.26 ... ## $ 2010-07: chr &quot;NA&quot; &quot;6.513&quot; &quot;353.04&quot; &quot;122.935&quot; ... ## $ 2010-08: chr &quot;NA&quot; &quot;6.269&quot; &quot;343.688&quot; &quot;129.732&quot; ... ## $ 2010-09: num 1225.5 4.7 381.6 132.9 521.1 ... ## $ 2010-10: chr &quot;NA&quot; &quot;4.402&quot; &quot;384.987&quot; &quot;131.033&quot; ... ## $ 2010-11: chr &quot;NA&quot; &quot;3.731&quot; &quot;367.955&quot; &quot;130.889&quot; ... ## $ 2010-12: num 1216.26 3.16 326.34 121.42 450.43 ... ## $ 2011-01: chr &quot;NA&quot; &quot;3.14&quot; &quot;334.958&quot; &quot;128.396&quot; ... ## $ 2011-02: chr &quot;NA&quot; &quot;3.284&quot; &quot;346.234&quot; &quot;125.463&quot; ... ## $ 2011-03: num 1223.45 3.67 380.4 134.37 516.73 ... ## $ 2011-04: chr &quot;NA&quot; &quot;4.251&quot; &quot;380.446&quot; &quot;134.169&quot; ... ## $ 2011-05: chr &quot;NA&quot; &quot;4.431&quot; &quot;385.289&quot; &quot;136.14&quot; ... ## $ 2011-06: num 1302.41 5.47 376.32 135.58 529.53 ... ## $ 2011-07: chr &quot;NA&quot; &quot;6.581&quot; &quot;361.585&quot; &quot;132.41&quot; ... ## $ 2011-08: chr &quot;NA&quot; &quot;6.733&quot; &quot;353.793&quot; &quot;130.616&quot; ... ## $ 2011-09: num 1291 5 388 137 550 ... ## $ 2011-10: chr &quot;NA&quot; &quot;4.484&quot; &quot;398.456&quot; &quot;128.72&quot; ... # View the first 6 rows of mbta head(mbta, n = 6) ## # A tibble: 6 x 60 ## X__1 mode `2007-01` `2007-02` `2007-03` `2007-04` `2007-05` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 All Modes by Qtr NA NA 1187.653 NA NA ## 2 2 Boat 4 3.6 40.000 4.3 4.9 ## 3 3 Bus 335.819 338.675 339.867 352.162 354.367 ## 4 4 Commuter Rail 142.2 138.5 137.700 139.5 139 ## 5 5 Heavy Rail 435.294 448.271 458.583 472.201 474.579 ## 6 6 Light Rail 227.231 240.262 241.444 255.557 248.262 ## # ... with 53 more variables: `2007-06` &lt;dbl&gt;, `2007-07` &lt;chr&gt;, ## # `2007-08` &lt;chr&gt;, `2007-09` &lt;dbl&gt;, `2007-10` &lt;chr&gt;, `2007-11` &lt;chr&gt;, ## # `2007-12` &lt;dbl&gt;, `2008-01` &lt;chr&gt;, `2008-02` &lt;chr&gt;, `2008-03` &lt;dbl&gt;, ## # `2008-04` &lt;chr&gt;, `2008-05` &lt;chr&gt;, `2008-06` &lt;dbl&gt;, `2008-07` &lt;chr&gt;, ## # `2008-08` &lt;chr&gt;, `2008-09` &lt;dbl&gt;, `2008-10` &lt;chr&gt;, `2008-11` &lt;chr&gt;, ## # `2008-12` &lt;dbl&gt;, `2009-01` &lt;chr&gt;, `2009-02` &lt;chr&gt;, `2009-03` &lt;dbl&gt;, ## # `2009-04` &lt;chr&gt;, `2009-05` &lt;chr&gt;, `2009-06` &lt;dbl&gt;, `2009-07` &lt;chr&gt;, ## # `2009-08` &lt;chr&gt;, `2009-09` &lt;dbl&gt;, `2009-10` &lt;chr&gt;, `2009-11` &lt;chr&gt;, ## # `2009-12` &lt;dbl&gt;, `2010-01` &lt;chr&gt;, `2010-02` &lt;chr&gt;, `2010-03` &lt;dbl&gt;, ## # `2010-04` &lt;chr&gt;, `2010-05` &lt;chr&gt;, `2010-06` &lt;dbl&gt;, `2010-07` &lt;chr&gt;, ## # `2010-08` &lt;chr&gt;, `2010-09` &lt;dbl&gt;, `2010-10` &lt;chr&gt;, `2010-11` &lt;chr&gt;, ## # `2010-12` &lt;dbl&gt;, `2011-01` &lt;chr&gt;, `2011-02` &lt;chr&gt;, `2011-03` &lt;dbl&gt;, ## # `2011-04` &lt;chr&gt;, `2011-05` &lt;chr&gt;, `2011-06` &lt;dbl&gt;, `2011-07` &lt;chr&gt;, ## # `2011-08` &lt;chr&gt;, `2011-09` &lt;dbl&gt;, `2011-10` &lt;chr&gt; # View a summary of mbta summary(mbta) ## X__1 mode 2007-01 2007-02 ## Min. : 1.0 Length:11 Length:11 Length:11 ## 1st Qu.: 3.5 Class :character Class :character Class :character ## Median : 6.0 Mode :character Mode :character Mode :character ## Mean : 6.0 ## 3rd Qu.: 8.5 ## Max. :11.0 ## 2007-03 2007-04 2007-05 ## Min. : 0.114 Length:11 Length:11 ## 1st Qu.: 9.278 Class :character Class :character ## Median : 137.700 Mode :character Mode :character ## Mean : 330.293 ## 3rd Qu.: 399.225 ## Max. :1204.725 ## 2007-06 2007-07 2007-08 ## Min. : 0.096 Length:11 Length:11 ## 1st Qu.: 5.700 Class :character Class :character ## Median : 143.000 Mode :character Mode :character ## Mean : 339.846 ## 3rd Qu.: 413.788 ## Max. :1246.129 ## 2007-09 2007-10 2007-11 ## Min. : -0.007 Length:11 Length:11 ## 1st Qu.: 5.539 Class :character Class :character ## Median : 143.051 Mode :character Mode :character ## Mean : 352.554 ## 3rd Qu.: 436.082 ## Max. :1310.764 ## 2007-12 2008-01 2008-02 ## Min. : -0.060 Length:11 Length:11 ## 1st Qu.: 4.385 Class :character Class :character ## Median : 141.585 Mode :character Mode :character ## Mean : 321.588 ## 3rd Qu.: 380.594 ## Max. :1216.890 ## 2008-03 2008-04 2008-05 ## Min. : 0.058 Length:11 Length:11 ## 1st Qu.: 5.170 Class :character Class :character ## Median : 137.453 Mode :character Mode :character ## Mean : 345.604 ## 3rd Qu.: 427.601 ## Max. :1274.031 ## 2008-06 2008-07 2008-08 ## Min. : 0.060 Length:11 Length:11 ## 1st Qu.: 5.742 Class :character Class :character ## Median : 142.057 Mode :character Mode :character ## Mean : 359.667 ## 3rd Qu.: 440.656 ## Max. :1320.728 ## 2008-09 2008-10 2008-11 ## Min. : 0.021 Length:11 Length:11 ## 1st Qu.: 5.691 Class :character Class :character ## Median : 141.907 Mode :character Mode :character ## Mean : 362.099 ## 3rd Qu.: 453.430 ## Max. :1338.015 ## 2008-12 2009-01 2009-02 ## Min. : -0.015 Length:11 Length:11 ## 1st Qu.: 4.689 Class :character Class :character ## Median : 140.810 Mode :character Mode :character ## Mean : 319.882 ## 3rd Qu.: 386.255 ## Max. :1232.655 ## 2009-03 2009-04 2009-05 ## Min. : -0.050 Length:11 Length:11 ## 1st Qu.: 5.003 Class :character Class :character ## Median : 142.893 Mode :character Mode :character ## Mean : 330.142 ## 3rd Qu.: 410.455 ## Max. :1210.912 ## 2009-06 2009-07 2009-08 ## Min. : -0.079 Length:11 Length:11 ## 1st Qu.: 5.845 Class :character Class :character ## Median : 142.006 Mode :character Mode :character ## Mean : 333.194 ## 3rd Qu.: 410.482 ## Max. :1233.085 ## 2009-09 2009-10 2009-11 ## Min. : -0.035 Length:11 Length:11 ## 1st Qu.: 5.693 Class :character Class :character ## Median : 139.087 Mode :character Mode :character ## Mean : 346.687 ## 3rd Qu.: 437.332 ## Max. :1291.564 ## 2009-12 2010-01 2010-02 ## Min. : -0.022 Length:11 Length:11 ## 1st Qu.: 4.784 Class :character Class :character ## Median : 126.066 Mode :character Mode :character ## Mean : 312.962 ## 3rd Qu.: 386.659 ## Max. :1207.845 ## 2010-03 2010-04 2010-05 ## Min. : 0.012 Length:11 Length:11 ## 1st Qu.: 5.274 Class :character Class :character ## Median : 131.252 Mode :character Mode :character ## Mean : 332.726 ## 3rd Qu.: 428.420 ## Max. :1225.556 ## 2010-06 2010-07 2010-08 ## Min. : 0.008 Length:11 Length:11 ## 1st Qu.: 6.436 Class :character Class :character ## Median : 129.144 Mode :character Mode :character ## Mean : 335.964 ## 3rd Qu.: 426.769 ## Max. :1244.409 ## 2010-09 2010-10 2010-11 ## Min. : 0.001 Length:11 Length:11 ## 1st Qu.: 5.567 Class :character Class :character ## Median : 132.892 Mode :character Mode :character ## Mean : 346.524 ## 3rd Qu.: 451.361 ## Max. :1293.117 ## 2010-12 2011-01 2011-02 ## Min. : -0.004 Length:11 Length:11 ## 1st Qu.: 4.466 Class :character Class :character ## Median : 121.422 Mode :character Mode :character ## Mean : 312.917 ## 3rd Qu.: 388.385 ## Max. :1216.262 ## 2011-03 2011-04 2011-05 ## Min. : 0.05 Length:11 Length:11 ## 1st Qu.: 6.03 Class :character Class :character ## Median : 134.37 Mode :character Mode :character ## Mean : 345.17 ## 3rd Qu.: 448.56 ## Max. :1286.66 ## 2011-06 2011-07 2011-08 ## Min. : 0.054 Length:11 Length:11 ## 1st Qu.: 6.926 Class :character Class :character ## Median : 135.581 Mode :character Mode :character ## Mean : 353.331 ## 3rd Qu.: 452.923 ## Max. :1302.414 ## 2011-09 2011-10 ## Min. : 0.043 Length:11 ## 1st Qu.: 6.660 Class :character ## Median : 136.901 Mode :character ## Mean : 362.555 ## 3rd Qu.: 469.204 ## Max. :1348.754 The data are organized with observations stored as columns rather than as rows. First, though, you can address the missing data. All of the NA values are stored in the All Modes by Qtr row. This row really belongs in a different data frame; it is a quarterly average of weekday MBTA ridership. Since this dataset tracks monthly average ridership, you’ll remove that row. Similarly, the 7th row (Pct Chg / Yr) and the 11th row (TOTAL) are not really observations as much as they are analysis. Go ahead and remove the 7th and 11th rows as well. The first column also needs to be removed because it’s just listing the row numbers. # Remove rows 1, 7, and 11 of mbta: mbta2 keep &lt;- !(mbta$mode %in% c(&#39;All Modes by Qtr&#39;, &#39;Pct Chg / Yr&#39;, &#39;TOTAL&#39;)) mbta2 &lt;- mbta[keep,] # Remove the first column of mbta2: mbta3 mbta3 &lt;- mbta2[,-1] Our next problem is variables are stored in rows instead of columns. The different modes of transportation (commuter rail, bus, subway, ferry, …) are variables, providing information about each month’s average ridership. The months themselves are observations. You can tell which is which because as you go through time, the month changes, but the modes of transport offered by the T do not. As is customary, you want to represent variables in columns rather than rows. The first step is to use the gather() function from the tidyr package, which will gather columns into key-value pairs. # Load tidyr library(tidyr) # Gather columns of mbta3: mbta4 mbta4 &lt;- gather(mbta3, month, thou_riders, -mode) # View the head of mbta4 head(mbta4) ## # A tibble: 6 x 3 ## mode month thou_riders ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Boat 2007-01 4 ## 2 Bus 2007-01 335.819 ## 3 Commuter Rail 2007-01 142.2 ## 4 Heavy Rail 2007-01 435.294 ## 5 Light Rail 2007-01 227.231 ## 6 Private Bus 2007-01 4.772 The thousand riders coloumn is still charecter data, so lets change that. # Coerce thou_riders to numeric mbta4$thou_riders &lt;- as.numeric(mbta4$thou_riders) Now, you can finish the job you started earlier: getting variables into columns. Right now, variables are stored as “keys” in the mode column. You’ll use the tidyr function spread() to make them into columns containing average weekday ridership for the given month and mode of transport. # Spread the contents of mbta4: mbta5 mbta5 &lt;- spread(mbta4, mode, thou_riders) # View the head of mbta5 head(mbta5) ## # A tibble: 6 x 9 ## month Boat Bus `Commuter Rail` `Heavy Rail` `Light Rail` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007-01 4.0 335.819 142.2 435.294 227.231 ## 2 2007-02 3.6 338.675 138.5 448.271 240.262 ## 3 2007-03 40.0 339.867 137.7 458.583 241.444 ## 4 2007-04 4.3 352.162 139.5 472.201 255.557 ## 5 2007-05 4.9 354.367 139.0 474.579 248.262 ## 6 2007-06 5.8 350.543 143.0 477.032 246.108 ## # ... with 3 more variables: `Private Bus` &lt;dbl&gt;, RIDE &lt;dbl&gt;, `Trackless ## # Trolley` &lt;dbl&gt; If we want to look at the data by year, we can seperate the month field out in to month and year. # View the head of mbta5 head(mbta5) ## # A tibble: 6 x 9 ## month Boat Bus `Commuter Rail` `Heavy Rail` `Light Rail` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007-01 4.0 335.819 142.2 435.294 227.231 ## 2 2007-02 3.6 338.675 138.5 448.271 240.262 ## 3 2007-03 40.0 339.867 137.7 458.583 241.444 ## 4 2007-04 4.3 352.162 139.5 472.201 255.557 ## 5 2007-05 4.9 354.367 139.0 474.579 248.262 ## 6 2007-06 5.8 350.543 143.0 477.032 246.108 ## # ... with 3 more variables: `Private Bus` &lt;dbl&gt;, RIDE &lt;dbl&gt;, `Trackless ## # Trolley` &lt;dbl&gt; # Split month column into month and year: mbta6 mbta6 &lt;- separate(mbta5, month, into = c(&quot;month&quot;, &quot;year&quot;), sep =&quot;-&quot;) # View the head of mbta6 head(mbta6) ## # A tibble: 6 x 10 ## month year Boat Bus `Commuter Rail` `Heavy Rail` `Light Rail` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007 01 4.0 335.819 142.2 435.294 227.231 ## 2 2007 02 3.6 338.675 138.5 448.271 240.262 ## 3 2007 03 40.0 339.867 137.7 458.583 241.444 ## 4 2007 04 4.3 352.162 139.5 472.201 255.557 ## 5 2007 05 4.9 354.367 139.0 474.579 248.262 ## 6 2007 06 5.8 350.543 143.0 477.032 246.108 ## # ... with 3 more variables: `Private Bus` &lt;dbl&gt;, RIDE &lt;dbl&gt;, `Trackless ## # Trolley` &lt;dbl&gt; Looks like some of the data might be a bit out, which you can check using different functions, histogram being one such function. # View a summary of mbta6 summary(mbta6) ## month year Boat Bus ## Length:58 Length:58 Min. : 2.985 Min. :312.9 ## Class :character Class :character 1st Qu.: 3.494 1st Qu.:345.6 ## Mode :character Mode :character Median : 4.293 Median :359.9 ## Mean : 5.068 Mean :358.6 ## 3rd Qu.: 5.356 3rd Qu.:372.2 ## Max. :40.000 Max. :398.5 ## Commuter Rail Heavy Rail Light Rail Private Bus ## Min. :121.4 Min. :435.3 Min. :194.4 Min. :2.213 ## 1st Qu.:131.4 1st Qu.:471.1 1st Qu.:220.6 1st Qu.:2.641 ## Median :138.8 Median :487.3 Median :231.9 Median :2.820 ## Mean :137.4 Mean :489.3 Mean :233.0 Mean :3.352 ## 3rd Qu.:142.4 3rd Qu.:511.3 3rd Qu.:244.5 3rd Qu.:4.167 ## Max. :153.0 Max. :554.9 Max. :271.1 Max. :4.878 ## RIDE Trackless Trolley ## Min. :4.900 Min. : 5.777 ## 1st Qu.:5.965 1st Qu.:11.679 ## Median :6.615 Median :12.598 ## Mean :6.604 Mean :12.125 ## 3rd Qu.:7.149 3rd Qu.:13.320 ## Max. :8.598 Max. :15.109 # Generate a histogram of Boat ridership hist(mbta6$Boat) Looks like we may have an input or typo on the value close to 40 - perhaps should have been a 4.0 or just a 4. Because it’s an error, you don’t want this value influencing your analysis. In this exercise, you’ll locate the incorrect value and change it to 4. # Find the row number of the incorrect value: i i &lt;- which(mbta6$Boat == 40) # Replace the incorrect value with 4 mbta6$Boat[i] &lt;- 4 # Generate a histogram of Boat column hist(mbta6$Boat) library(ggplot2) # Look at all T ridership over time (example plot) ggplot(mbta4, aes(x = month, y = thou_riders, col = mode)) + geom_point() + scale_x_discrete(name = &quot;Month&quot;, breaks = c(200701, 200801, 200901, 201001, 201101)) + scale_y_continuous(name = &quot;Avg Weekday Ridership (thousands)&quot;) 6.4 World Food Facts library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, ## week, yday, year ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last # Import sales.csv: food food &lt;- fread(&quot;https://assets.datacamp.com/production/course_1294/datasets/food.csv&quot;, stringsAsFactors = FALSE) # Convert food to a data frame food &lt;- data.frame(food) # View summary of food summary(food) ## V1 code url creator ## Min. : 1.0 Min. :100030 Length:1500 Length:1500 ## 1st Qu.: 375.8 1st Qu.:124975 Class :character Class :character ## Median : 750.5 Median :149514 Mode :character Mode :character ## Mean : 750.5 Mean :149613 ## 3rd Qu.:1125.2 3rd Qu.:174506 ## Max. :1500.0 Max. :199880 ## ## created_t created_datetime last_modified_t ## Min. :1.332e+09 Length:1500 Min. :1.340e+09 ## 1st Qu.:1.394e+09 Class :character 1st Qu.:1.424e+09 ## Median :1.425e+09 Mode :character Median :1.437e+09 ## Mean :1.414e+09 Mean :1.430e+09 ## 3rd Qu.:1.436e+09 3rd Qu.:1.446e+09 ## Max. :1.453e+09 Max. :1.453e+09 ## ## last_modified_datetime product_name generic_name ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## quantity packaging packaging_tags ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## brands brands_tags categories ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## categories_tags categories_en origins ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## origins_tags manufacturing_places manufacturing_places_tags ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## labels labels_tags labels_en ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## emb_codes emb_codes_tags first_packaging_code_geo ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## cities cities_tags purchase_places stores ## Mode:logical Length:1500 Length:1500 Length:1500 ## NA&#39;s:1500 Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## countries countries_tags countries_en ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## ingredients_text allergens allergens_en traces ## Length:1500 Length:1500 Mode:logical Length:1500 ## Class :character Class :character NA&#39;s:1500 Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## traces_tags traces_en serving_size no_nutriments ## Length:1500 Length:1500 Length:1500 Mode:logical ## Class :character Class :character Class :character NA&#39;s:1500 ## Mode :character Mode :character Mode :character ## ## ## ## ## additives_n additives additives_tags additives_en ## Min. : 0.000 Length:1500 Length:1500 Length:1500 ## 1st Qu.: 0.000 Class :character Class :character Class :character ## Median : 1.000 Mode :character Mode :character Mode :character ## Mean : 1.846 ## 3rd Qu.: 3.000 ## Max. :17.000 ## NA&#39;s :514 ## ingredients_from_palm_oil_n ingredients_from_palm_oil ## Min. :0.0000 Mode:logical ## 1st Qu.:0.0000 NA&#39;s:1500 ## Median :0.0000 ## Mean :0.0487 ## 3rd Qu.:0.0000 ## Max. :1.0000 ## NA&#39;s :514 ## ingredients_from_palm_oil_tags ingredients_that_may_be_from_palm_oil_n ## Length:1500 Min. :0.0000 ## Class :character 1st Qu.:0.0000 ## Mode :character Median :0.0000 ## Mean :0.1379 ## 3rd Qu.:0.0000 ## Max. :4.0000 ## NA&#39;s :514 ## ingredients_that_may_be_from_palm_oil ## Mode:logical ## NA&#39;s:1500 ## ## ## ## ## ## ingredients_that_may_be_from_palm_oil_tags nutrition_grade_uk ## Length:1500 Mode:logical ## Class :character NA&#39;s:1500 ## Mode :character ## ## ## ## ## nutrition_grade_fr pnns_groups_1 pnns_groups_2 ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## states states_tags states_en ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## main_category main_category_en image_url ## Length:1500 Length:1500 Length:1500 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## image_small_url energy_100g energy_from_fat_100g fat_100g ## Length:1500 Min. : 0.0 Min. : 0.00 Min. : 0.00 ## Class :character 1st Qu.: 369.8 1st Qu.: 35.98 1st Qu.: 0.90 ## Mode :character Median : 966.5 Median : 237.00 Median : 6.00 ## Mean :1083.2 Mean : 668.41 Mean : 13.39 ## 3rd Qu.:1641.5 3rd Qu.: 974.00 3rd Qu.: 20.00 ## Max. :3700.0 Max. :2900.00 Max. :100.00 ## NA&#39;s :700 NA&#39;s :1486 NA&#39;s :708 ## saturated_fat_100g butyric_acid_100g caproic_acid_100g caprylic_acid_100g ## Min. : 0.000 Mode:logical Mode:logical Mode:logical ## 1st Qu.: 0.200 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## Median : 1.700 ## Mean : 4.874 ## 3rd Qu.: 6.500 ## Max. :57.000 ## NA&#39;s :797 ## capric_acid_100g lauric_acid_100g myristic_acid_100g palmitic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## stearic_acid_100g arachidic_acid_100g behenic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## lignoceric_acid_100g cerotic_acid_100g montanic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## melissic_acid_100g monounsaturated_fat_100g polyunsaturated_fat_100g ## Mode:logical Min. : 0.00 Min. : 0.400 ## NA&#39;s:1500 1st Qu.: 3.87 1st Qu.: 1.653 ## Median : 9.50 Median : 3.900 ## Mean :19.77 Mean : 9.986 ## 3rd Qu.:29.00 3rd Qu.:12.700 ## Max. :75.00 Max. :46.200 ## NA&#39;s :1465 NA&#39;s :1464 ## omega_3_fat_100g alpha_linolenic_acid_100g eicosapentaenoic_acid_100g ## Min. : 0.033 Min. :0.0800 Min. :0.721 ## 1st Qu.: 1.300 1st Qu.:0.0905 1st Qu.:0.721 ## Median : 3.000 Median :0.1010 Median :0.721 ## Mean : 3.726 Mean :0.1737 Mean :0.721 ## 3rd Qu.: 3.200 3rd Qu.:0.2205 3rd Qu.:0.721 ## Max. :12.400 Max. :0.3400 Max. :0.721 ## NA&#39;s :1491 NA&#39;s :1497 NA&#39;s :1499 ## docosahexaenoic_acid_100g omega_6_fat_100g linoleic_acid_100g ## Min. :1.09 Min. :0.25 Min. :0.5000 ## 1st Qu.:1.09 1st Qu.:0.25 1st Qu.:0.5165 ## Median :1.09 Median :0.25 Median :0.5330 ## Mean :1.09 Mean :0.25 Mean :0.5330 ## 3rd Qu.:1.09 3rd Qu.:0.25 3rd Qu.:0.5495 ## Max. :1.09 Max. :0.25 Max. :0.5660 ## NA&#39;s :1499 NA&#39;s :1499 NA&#39;s :1498 ## arachidonic_acid_100g gamma_linolenic_acid_100g ## Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## dihomo_gamma_linolenic_acid_100g omega_9_fat_100g oleic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## elaidic_acid_100g gondoic_acid_100g mead_acid_100g erucic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## nervonic_acid_100g trans_fat_100g cholesterol_100g carbohydrates_100g ## Mode:logical Min. :0.0000 Min. :0.0000 Min. : 0.000 ## NA&#39;s:1500 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.: 3.792 ## Median :0.0000 Median :0.0000 Median : 13.500 ## Mean :0.0105 Mean :0.0265 Mean : 27.958 ## 3rd Qu.:0.0000 3rd Qu.:0.0026 3rd Qu.: 55.000 ## Max. :0.1000 Max. :0.4300 Max. :100.000 ## NA&#39;s :1481 NA&#39;s :1477 NA&#39;s :708 ## sugars_100g sucrose_100g glucose_100g fructose_100g ## Min. : 0.00 Mode:logical Mode:logical Min. :100 ## 1st Qu.: 1.00 NA&#39;s:1500 NA&#39;s:1500 1st Qu.:100 ## Median : 4.05 Median :100 ## Mean : 12.66 Mean :100 ## 3rd Qu.: 14.70 3rd Qu.:100 ## Max. :100.00 Max. :100 ## NA&#39;s :788 NA&#39;s :1499 ## lactose_100g maltose_100g maltodextrins_100g starch_100g ## Min. :0.000 Mode:logical Mode:logical Min. : 0.00 ## 1st Qu.:0.250 NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 9.45 ## Median :0.500 Median :39.50 ## Mean :2.933 Mean :30.73 ## 3rd Qu.:4.400 3rd Qu.:42.85 ## Max. :8.300 Max. :71.00 ## NA&#39;s :1497 NA&#39;s :1493 ## polyols_100g fiber_100g proteins_100g casein_100g ## Min. : 8.60 Min. : 0.000 Min. : 0.000 Min. :1.1 ## 1st Qu.:59.10 1st Qu.: 0.500 1st Qu.: 1.500 1st Qu.:1.1 ## Median :67.00 Median : 1.750 Median : 6.000 Median :1.1 ## Mean :56.06 Mean : 2.823 Mean : 7.563 Mean :1.1 ## 3rd Qu.:69.80 3rd Qu.: 3.500 3rd Qu.:10.675 3rd Qu.:1.1 ## Max. :70.00 Max. :46.700 Max. :61.000 Max. :1.1 ## NA&#39;s :1491 NA&#39;s :994 NA&#39;s :710 NA&#39;s :1499 ## serum_proteins_100g nucleotides_100g salt_100g sodium_100g ## Mode:logical Mode:logical Min. : 0.0000 Min. : 0.0000 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 0.0438 1st Qu.: 0.0172 ## Median : 0.4498 Median : 0.1771 ## Mean : 1.1205 Mean : 0.4409 ## 3rd Qu.: 1.1938 3rd Qu.: 0.4700 ## Max. :102.0000 Max. :40.0000 ## NA&#39;s :780 NA&#39;s :780 ## alcohol_100g vitamin_a_100g beta_carotene_100g vitamin_d_100g ## Min. : 0.00 Min. :0.0000 Mode:logical Min. :0e+00 ## 1st Qu.: 0.00 1st Qu.:0.0000 NA&#39;s:1500 1st Qu.:0e+00 ## Median : 5.50 Median :0.0001 Median :0e+00 ## Mean :10.07 Mean :0.0003 Mean :0e+00 ## 3rd Qu.:13.00 3rd Qu.:0.0006 3rd Qu.:0e+00 ## Max. :50.00 Max. :0.0013 Max. :1e-04 ## NA&#39;s :1433 NA&#39;s :1477 NA&#39;s :1485 ## vitamin_e_100g vitamin_k_100g vitamin_c_100g vitamin_b1_100g ## Min. :0.0005 Min. :0 Min. :0.000 Min. :0.0001 ## 1st Qu.:0.0021 1st Qu.:0 1st Qu.:0.002 1st Qu.:0.0003 ## Median :0.0044 Median :0 Median :0.019 Median :0.0004 ## Mean :0.0069 Mean :0 Mean :0.025 Mean :0.0006 ## 3rd Qu.:0.0097 3rd Qu.:0 3rd Qu.:0.030 3rd Qu.:0.0010 ## Max. :0.0320 Max. :0 Max. :0.217 Max. :0.0013 ## NA&#39;s :1478 NA&#39;s :1498 NA&#39;s :1459 NA&#39;s :1478 ## vitamin_b2_100g vitamin_pp_100g vitamin_b6_100g vitamin_b9_100g ## Min. :0.0002 Min. :0.0006 Min. :0.0001 Min. :0e+00 ## 1st Qu.:0.0003 1st Qu.:0.0033 1st Qu.:0.0002 1st Qu.:0e+00 ## Median :0.0009 Median :0.0069 Median :0.0008 Median :1e-04 ## Mean :0.0011 Mean :0.0086 Mean :0.0112 Mean :1e-04 ## 3rd Qu.:0.0013 3rd Qu.:0.0140 3rd Qu.:0.0012 3rd Qu.:2e-04 ## Max. :0.0066 Max. :0.0160 Max. :0.2000 Max. :2e-04 ## NA&#39;s :1483 NA&#39;s :1484 NA&#39;s :1481 NA&#39;s :1483 ## vitamin_b12_100g biotin_100g pantothenic_acid_100g silica_100g ## Min. :0 Min. :0 Min. :0.0000 Min. :8e-04 ## 1st Qu.:0 1st Qu.:0 1st Qu.:0.0007 1st Qu.:8e-04 ## Median :0 Median :0 Median :0.0020 Median :8e-04 ## Mean :0 Mean :0 Mean :0.0027 Mean :8e-04 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0.0051 3rd Qu.:8e-04 ## Max. :0 Max. :0 Max. :0.0060 Max. :8e-04 ## NA&#39;s :1489 NA&#39;s :1498 NA&#39;s :1486 NA&#39;s :1499 ## bicarbonate_100g potassium_100g chloride_100g calcium_100g ## Min. :0.0006 Min. :0.0000 Min. :0.0003 Min. :0.0000 ## 1st Qu.:0.0678 1st Qu.:0.0650 1st Qu.:0.0006 1st Qu.:0.0450 ## Median :0.1350 Median :0.1940 Median :0.0009 Median :0.1200 ## Mean :0.1692 Mean :0.3288 Mean :0.0144 Mean :0.2040 ## 3rd Qu.:0.2535 3rd Qu.:0.3670 3rd Qu.:0.0214 3rd Qu.:0.1985 ## Max. :0.3720 Max. :1.4300 Max. :0.0420 Max. :1.0000 ## NA&#39;s :1497 NA&#39;s :1487 NA&#39;s :1497 NA&#39;s :1449 ## phosphorus_100g iron_100g magnesium_100g zinc_100g ## Min. :0.0430 Min. :0.0000 Min. :0.0000 Min. :0.0005 ## 1st Qu.:0.1938 1st Qu.:0.0012 1st Qu.:0.0670 1st Qu.:0.0009 ## Median :0.3185 Median :0.0042 Median :0.1040 Median :0.0017 ## Mean :0.3777 Mean :0.0045 Mean :0.1066 Mean :0.0016 ## 3rd Qu.:0.4340 3rd Qu.:0.0077 3rd Qu.:0.1300 3rd Qu.:0.0022 ## Max. :1.1550 Max. :0.0137 Max. :0.3330 Max. :0.0026 ## NA&#39;s :1488 NA&#39;s :1463 NA&#39;s :1479 NA&#39;s :1493 ## copper_100g manganese_100g fluoride_100g selenium_100g ## Min. :0e+00 Min. :0 Min. :0 Min. :0 ## 1st Qu.:1e-04 1st Qu.:0 1st Qu.:0 1st Qu.:0 ## Median :1e-04 Median :0 Median :0 Median :0 ## Mean :1e-04 Mean :0 Mean :0 Mean :0 ## 3rd Qu.:1e-04 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 ## Max. :1e-04 Max. :0 Max. :0 Max. :0 ## NA&#39;s :1498 NA&#39;s :1499 NA&#39;s :1498 NA&#39;s :1499 ## chromium_100g molybdenum_100g iodine_100g caffeine_100g ## Mode:logical Mode:logical Min. :0 Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:0 NA&#39;s:1500 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## NA&#39;s :1499 ## taurine_100g ph_100g fruits_vegetables_nuts_100g ## Mode:logical Mode:logical Min. : 2.00 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:11.25 ## Median :42.00 ## Mean :36.88 ## 3rd Qu.:52.25 ## Max. :80.00 ## NA&#39;s :1470 ## collagen_meat_protein_ratio_100g cocoa_100g chlorophyl_100g ## Min. :12.00 Min. :30 Mode:logical ## 1st Qu.:13.50 1st Qu.:47 NA&#39;s:1500 ## Median :15.00 Median :60 ## Mean :15.67 Mean :57 ## 3rd Qu.:17.50 3rd Qu.:70 ## Max. :20.00 Max. :81 ## NA&#39;s :1497 NA&#39;s :1491 ## carbon_footprint_100g nutrition_score_fr_100g nutrition_score_uk_100g ## Min. : 12.00 Min. :-12.000 Min. :-12.000 ## 1st Qu.: 97.42 1st Qu.: 1.000 1st Qu.: 0.000 ## Median :182.85 Median : 7.000 Median : 6.000 ## Mean :131.18 Mean : 7.941 Mean : 7.631 ## 3rd Qu.:190.78 3rd Qu.: 15.000 3rd Qu.: 16.000 ## Max. :198.70 Max. : 28.000 Max. : 28.000 ## NA&#39;s :1497 NA&#39;s :825 NA&#39;s :825 # View head of food head(food) ## V1 code ## 1 1 100030 ## 2 2 100050 ## 3 3 100079 ## 4 4 100094 ## 5 5 100124 ## 6 6 100136 ## url ## 1 http://world-en.openfoodfacts.org/product/3222475745867/confiture-de-fraise-fraise-des-bois-au-sucre-de-canne-casino-delices ## 2 http://world-en.openfoodfacts.org/product/5410976880110/guylian-sea-shells-selection ## 3 http://world-en.openfoodfacts.org/product/3264750423503/pates-de-fruits-aromatisees-jacquot ## 4 http://world-en.openfoodfacts.org/product/8006040247001/nata-vegetal-a-base-de-soja-valsoia ## 5 http://world-en.openfoodfacts.org/product/8480000340764/semillas-de-girasol-con-cascara-tostadas-aguasal-hacendado ## 6 http://world-en.openfoodfacts.org/product/0087703177727/soft-drink ## creator created_t created_datetime last_modified_t ## 1 sebleouf 1424747544 2015-02-24T03:12:24Z 1438445887 ## 2 foodorigins 1450316429 2015-12-17T01:40:29Z 1450817956 ## 3 domdom26 1428674916 2015-04-10T14:08:36Z 1428739289 ## 4 javichu 1420416591 2015-01-05T00:09:51Z 1420417876 ## 5 javichu 1420501121 2015-01-05T23:38:41Z 1445700917 ## 6 foodorigins 1437983923 2015-07-27T07:58:43Z 1445577476 ## last_modified_datetime ## 1 2015-08-01T16:18:07Z ## 2 2015-12-22T20:59:16Z ## 3 2015-04-11T08:01:29Z ## 4 2015-01-05T00:31:16Z ## 5 2015-10-24T15:35:17Z ## 6 2015-10-23T05:17:56Z ## product_name ## 1 Confiture de fraise fraise des bois au sucre de canne ## 2 Guylian Sea Shells Selection ## 3 PÃ¢tes de fruits aromatisÃ©es ## 4 Nata vegetal a base de soja &amp;quot;Valsoia&amp;quot; ## 5 Semillas de girasol con cÃ¡scara tostadas aguasal ## 6 Soft Drink ## generic_name quantity ## 1 265 g ## 2 375g ## 3 PÃ¢tes de fruits 1 kg ## 4 Nata vegetal a base de soja 200 ml ## 5 Semillas de girasol con cÃ¡scara tostadas aguasal 200 g ## 6 ## packaging ## 1 Bocal,Verre ## 2 Plastic,Box ## 3 Carton,plastique ## 4 Tetra Brik ## 5 Bolsa de plÃ¡stico,Envasado en atmÃ³sfera protectora ## 6 ## packaging_tags ## 1 bocal,verre ## 2 plastic,box ## 3 carton,plastique ## 4 tetra-brik ## 5 bolsa-de-plastico,envasado-en-atmosfera-protectora ## 6 ## brands ## 1 Casino DÃ©lices ## 2 Guylian ## 3 Jacquot ## 4 Valsoia,//Propiedad de://,Valsoia S.p.A. ## 5 Hacendado,//Propiedad de://,Mercadona S.A. ## 6 ## brands_tags ## 1 casino-delices ## 2 guylian ## 3 jacquot ## 4 valsoia,propiedad-de,valsoia-s-p-a ## 5 hacendado,propiedad-de,mercadona-s-a ## 6 ## categories ## 1 Aliments et boissons Ã base de vÃ©gÃ©taux,Aliments d&#39;origine vÃ©gÃ©tale,Aliments Ã base de fruits et de lÃ©gumes,Petit-dÃ©jeuners,Produits Ã tartiner,Fruits et produits dÃ©rivÃ©s,PÃ¢tes Ã tartiner vÃ©gÃ©taux,Produits Ã tartiner sucrÃ©s,Confitures et marmelades,Confitures,Confitures de fruits,Confitures de fruits rouges,Confitures de fraises ## 2 Chocolate ## 3 pÃ¢tes de fruits ## 4 Alimentos y bebidas de origen vegetal,Alimentos de origen vegetal,Natas vegetales,Natas vegetales a base de soja para cocinar,Natas vegetales para cocinar ## 5 Semillas de girasol y derivados, Semillas, Semillas de girasol, Semillas de girasol con cÃ¡scara, Semillas de girasol tostadas, Semillas de girasol con cÃ¡scara tostadas, Semillas de girasol con cÃ¡scara tostadas aguasal ## 6 ## categories_tags ## 1 en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:breakfasts,en:spreads,en:fruits-based-foods,en:plant-based-spreads,en:sweet-spreads,en:fruit-preserves,en:jams,en:fruit-jams,en:berry-jams,en:strawberry-jams ## 2 en:sugary-snacks,en:chocolates ## 3 en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:sugary-snacks,en:confectioneries,en:fruits-based-foods,en:fruit-pastes ## 4 en:plant-based-foods-and-beverages,en:plant-based-foods,en:plant-based-creams,en:plant-based-creams-for-cooking,en:soy-based-creams-for-cooking ## 5 en:plant-based-foods-and-beverages,en:plant-based-foods,en:seeds,en:sunflower-seeds-and-their-products,en:sunflower-seeds,en:roasted-sunflower-seeds,en:unshelled-sunflower-seeds,en:roasted-unshelled-sunflower-seeds,es:semillas-de-girasol-con-cascara-tostadas-aguasal ## 6 ## categories_en ## 1 Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Breakfasts,Spreads,Fruits based foods,Plant-based spreads,Sweet spreads,Fruit preserves,Jams,Fruit jams,Berry jams,Strawberry jams ## 2 Sugary snacks,Chocolates ## 3 Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Sugary snacks,Confectioneries,Fruits based foods,Fruit pastes ## 4 Plant-based foods and beverages,Plant-based foods,Plant-based creams,Plant-based creams for cooking,Soy-based creams for cooking ## 5 Plant-based foods and beverages,Plant-based foods,Seeds,Sunflower seeds and their products,Sunflower seeds,Roasted sunflower seeds,Unshelled sunflower seeds,Roasted unshelled sunflower seeds,es:Semillas-de-girasol-con-cascara-tostadas-aguasal ## 6 ## origins origins_tags ## 1 ## 2 ## 3 ## 4 ## 5 Argentina argentina ## 6 South Korea south-korea ## manufacturing_places ## 1 France ## 2 Belgium ## 3 ## 4 Italia ## 5 Beniparrell,Valencia (provincia),Comunidad Valenciana,EspaÃ±a ## 6 South Korea ## manufacturing_places_tags ## 1 france ## 2 belgium ## 3 ## 4 italia ## 5 beniparrell,valencia-provincia,comunidad-valenciana,espana ## 6 south-korea ## labels ## 1 ## 2 ## 3 ## 4 Vegetariano,Vegano,Sin gluten,Sin OMG,Sin lactosa ## 5 Vegetariano,Vegano,Sin gluten ## 6 ## labels_tags ## 1 ## 2 ## 3 ## 4 en:vegetarian,en:vegan,en:gluten-free,en:no-gmos,en:no-lactose ## 5 en:vegetarian,en:vegan,en:gluten-free ## 6 ## labels_en ## 1 ## 2 ## 3 ## 4 Vegetarian,Vegan,Gluten-free,No GMOs,No lactose ## 5 Vegetarian,Vegan,Gluten-free ## 6 ## emb_codes ## 1 EMB 78015 ## 2 ## 3 ## 4 ## 5 ES 21.016540/V EC,ENVASADOR:,IMPORTACO S.A. ## 6 ## emb_codes_tags first_packaging_code_geo ## 1 emb-78015 48.983333,2.066667 ## 2 ## 3 ## 4 ## 5 es-21-016540-v-ec,envasador,importaco-s-a ## 6 ## cities cities_tags purchase_places stores ## 1 NA andresy-yvelines-france Lyon,France Casino ## 2 NA NSW,Australia ## 3 NA France ## 4 NA Madrid,EspaÃ±a El Corte InglÃ©s ## 5 NA Madrid,EspaÃ±a Mercadona ## 6 NA ## countries countries_tags countries_en ## 1 France en:france France ## 2 Australia en:australia Australia ## 3 France en:france France ## 4 EspaÃ±a en:spain Spain ## 5 EspaÃ±a en:spain Spain ## 6 Australia en:australia Australia ## ingredients_text ## 1 Sucre de canne, fraises 40 g, fraises des bois 14 g, gÃ©lifiant : pectines de fruits, jus de citron concentrÃ©. PrÃ©parÃ©e avec 54 g de fruits pour 100 g de produit fini. ## 2 ## 3 Pulpe de pommes 50% , sucre, sirop de glucose, gÃ©lifiant : pectine, acidifiant : acide citrique, arÃ´mes, colorants naturels : extrait de paprika â complexes cuivreâchlorophyllines â curcumine â antnocyanes ## 4 Extracto de soja (78%) (agua, semillas de soja 8,3%), grasas vegetales, jarabe de glucosa, dextrosa, emulsionante: mono- y diglicÃ©ridos de Ã¡cidos grasos (E-471), sal marina, estabilizantes: goma xantana (E-415), carragenatos (E-407), goma guar (E-412); aromas, antioxidante: extractos de tocoferoles (de soja) (E-306). (Nota: el envase en italiano del paquete -que puede verse en el enlace-, especifica que el producto es 100% vegetal. Por tanto los mono- y diglicÃ©ridos de Ã¡cidos grasos (E-471) son de origen no animal). ## 5 Pipas de girasol y sal. ## 6 ## allergens allergens_en traces traces_tags ## 1 NA Lait,Fruits Ã coque en:milk,en:nuts ## 2 NA ## 3 NA ## 4 NA ## 5 NA Frutos de cÃ¡scara,Cacahuetes en:nuts,en:peanuts ## 6 NA ## traces_en serving_size no_nutriments additives_n ## 1 Milk,Nuts 15 g NA 1 ## 2 NA NA ## 3 NA 2 ## 4 NA 5 ## 5 Nuts,Peanuts NA 0 ## 6 NA NA ## additives ## 1 [ sucre-de-canne -&gt; fr:sucre-de-canne ] [ sucre-de -&gt; fr:sucre-de ] [ sucre -&gt; fr:sucre ] [ fraises-40-g -&gt; fr:fraises-40-g ] [ fraises-40 -&gt; fr:fraises-40 ] [ fraises -&gt; fr:fraises ] [ fraises-des-bois-14-g -&gt; fr:fraises-des-bois-14-g ] [ fraises-des-bois-14 -&gt; fr:fraises-des-bois-14 ] [ fraises-des-bois -&gt; fr:fraises-des-bois ] [ fraises-des -&gt; fr:fraises-des ] [ fraises -&gt; fr:fraises ] [ pectines-de-fruits -&gt; fr:pectines-de-fruits ] [ pectines-de -&gt; fr:pectines-de ] [ pectines -&gt; en:e440 -&gt; exists ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit-fini -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit-fini ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de-produit ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g-de ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100-g ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100 -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour-100 ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits-pour ] [ jus-de-citron-concentre-preparee-avec-54-g-de-fruits -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de-fruits ] [ jus-de-citron-concentre-preparee-avec-54-g-de -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g-de ] [ jus-de-citron-concentre-preparee-avec-54-g -&gt; fr:jus-de-citron-concentre-preparee-avec-54-g ] [ jus-de-citron-concentre-preparee-avec-54 -&gt; fr:jus-de-citron-concentre-preparee-avec-54 ] [ jus-de-citron-concentre-preparee-avec -&gt; fr:jus-de-citron-concentre-preparee-avec ] [ jus-de-citron-concentre-preparee -&gt; fr:jus-de-citron-concentre-preparee ] [ jus-de-citron-concentre -&gt; fr:jus-de-citron-concentre ] [ jus-de-citron -&gt; fr:jus-de-citron ] [ jus-de -&gt; fr:jus-de ] [ jus -&gt; fr:jus ] ## 2 ## 3 [ pulpe-de-pommes-50 -&gt; fr:pulpe-de-pommes-50 ] [ pulpe-de-pommes -&gt; fr:pulpe-de-pommes ] [ pulpe-de -&gt; fr:pulpe-de ] [ pulpe -&gt; fr:pulpe ] [ sucre -&gt; fr:sucre ] [ sirop-de-glucose -&gt; fr:sirop-de-glucose ] [ sirop-de -&gt; fr:sirop-de ] [ sirop -&gt; fr:sirop ] [ pectine -&gt; en:e440 -&gt; exists ] [ acide-citrique -&gt; en:e330 -&gt; exists ] [ aromes -&gt; fr:aromes ] [ naturels -&gt; fr:naturels ] [ extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine-antnocyanes -&gt; fr:extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine-antnocyanes ] [ extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine -&gt; fr:extrait-de-paprika-complexes-cuivre-chlorophyllines-curcumine ] [ extrait-de-paprika-complexes-cuivre-chlorophyllines -&gt; fr:extrait-de-paprika-complexes-cuivre-chlorophyllines ] [ extrait-de-paprika-complexes-cuivre -&gt; fr:extrait-de-paprika-complexes-cuivre ] [ extrait-de-paprika-complexes -&gt; fr:extrait-de-paprika-complexes ] [ extrait-de-paprika -&gt; fr:extrait-de-paprika ] [ extrait-de -&gt; fr:extrait-de ] [ extrait -&gt; fr:extrait ] ## 4 [ extracto-de-soja -&gt; es:extracto-de-soja ] [ 78 -&gt; es:78 ] [ agua -&gt; es:agua ] [ semillas-de-soja-8 -&gt; es:semillas-de-soja-8 ] [ 3 -&gt; en:fd-c ] [ grasas-vegetales -&gt; es:grasas-vegetales ] [ jarabe-de-glucosa -&gt; es:jarabe-de-glucosa ] [ dextrosa -&gt; es:dextrosa ] [ emulsionante -&gt; es:emulsionante ] [ mono-y-digliceridos-de-acidos-grasos -&gt; en:e471 -&gt; exists ] [ e471 -&gt; en:e471 ] [ sal-marina -&gt; es:sal-marina ] [ estabilizantes -&gt; es:estabilizantes ] [ goma-xantana -&gt; en:e415 -&gt; exists ] [ e415 -&gt; en:e415 ] [ carragenatos -&gt; en:e407 -&gt; exists ] [ e407 -&gt; en:e407 ] [ goma-guar -&gt; en:e412 -&gt; exists ] [ e412 -&gt; en:e412 ] [ aromas -&gt; es:aromas ] [ antioxidante -&gt; es:antioxidante ] [ extractos-de-tocoferoles -&gt; es:extractos-de-tocoferoles ] [ de-soja -&gt; es:de-soja ] [ e306 -&gt; en:e306 -&gt; exists ] [ nota -&gt; es:nota ] [ el-envase-en-italiano-del-paquete-que-puede-verse-en-el-enlace -&gt; es:el-envase-en-italiano-del-paquete-que-puede-verse-en-el-enlace ] [ especifica-que-el-producto-es-100-vegetal-por-tanto-los-mono-y-digliceridos-de-acidos-grasos -&gt; es:especifica-que-el-producto-es-100-vegetal-por-tanto-los-mono-y-digliceridos-de-acidos-grasos ] [ e471 -&gt; en:e471 ] [ son-de-origen-no-animal -&gt; es:son-de-origen-no-animal ] [ -&gt; es: ] ## 5 [ pipas-de-girasol-y-sal -&gt; es:pipas-de-girasol-y-sal ] ## 6 ## additives_tags ## 1 en:e440 ## 2 ## 3 en:e440,en:e330 ## 4 en:e471,en:e415,en:e407,en:e412,en:e306 ## 5 ## 6 ## additives_en ## 1 E440 - Pectins ## 2 ## 3 E440 - Pectins,E330 - Citric acid ## 4 E471 - Mono- and diglycerides of fatty acids,E415 - Xanthan gum,E407 - Carrageenan,E412 - Guar gum,E306 - Tocopherol-rich extract ## 5 ## 6 ## ingredients_from_palm_oil_n ingredients_from_palm_oil ## 1 0 NA ## 2 NA NA ## 3 0 NA ## 4 0 NA ## 5 0 NA ## 6 NA NA ## ingredients_from_palm_oil_tags ingredients_that_may_be_from_palm_oil_n ## 1 0 ## 2 NA ## 3 0 ## 4 1 ## 5 0 ## 6 NA ## ingredients_that_may_be_from_palm_oil ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA ## ingredients_that_may_be_from_palm_oil_tags nutrition_grade_uk ## 1 NA ## 2 NA ## 3 NA ## 4 e471-mono-et-diglycerides-d-acides-gras-alimentaires NA ## 5 NA ## 6 NA ## nutrition_grade_fr pnns_groups_1 pnns_groups_2 ## 1 d Sugary snacks Sweets ## 2 Sugary snacks Chocolate products ## 3 Fruits and vegetables Fruits ## 4 d unknown unknown ## 5 d unknown unknown ## 6 unknown unknown ## states ## 1 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 2 en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-be-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 3 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 4 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 5 en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-completed, en:characteristics-completed, en:photos-validated, en:photos-uploaded ## 6 en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-be-completed, en:characteristics-to-be-completed, en:categories-to-be-completed, en:brands-to-be-completed, en:packaging-to-be-completed, en:quantity-to-be-completed, en:photos-to-be-validated, en:photos-uploaded ## states_tags ## 1 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 2 en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 3 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 4 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 5 en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-completed,en:characteristics-completed,en:photos-validated,en:photos-uploaded ## 6 en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-completed,en:characteristics-to-be-completed,en:categories-to-be-completed,en:brands-to-be-completed,en:packaging-to-be-completed,en:quantity-to-be-completed,en:photos-to-be-validated,en:photos-uploaded ## states_en ## 1 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characteristics completed,Photos validated,Photos uploaded ## 2 To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Characteristics completed,Photos validated,Photos uploaded ## 3 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characteristics completed,Photos validated,Photos uploaded ## 4 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date completed,Characteristics completed,Photos validated,Photos uploaded ## 5 To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date completed,Characteristics completed,Photos validated,Photos uploaded ## 6 To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Characteristics to be completed,Categories to be completed,Brands to be completed,Packaging to be completed,Quantity to be completed,Photos to be validated,Photos uploaded ## main_category main_category_en ## 1 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 2 en:sugary-snacks Sugary snacks ## 3 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 4 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 5 en:plant-based-foods-and-beverages Plant-based foods and beverages ## 6 ## image_url ## 1 http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.400.jpg ## 2 http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.400.jpg ## 3 http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.400.jpg ## 4 http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.400.jpg ## 5 http://en.openfoodfacts.org/images/products/848/000/034/0764/front.6.400.jpg ## 6 http://en.openfoodfacts.org/images/products/008/770/317/7727/front.8.400.jpg ## image_small_url ## 1 http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.200.jpg ## 2 http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.200.jpg ## 3 http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.200.jpg ## 4 http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.200.jpg ## 5 http://en.openfoodfacts.org/images/products/848/000/034/0764/front.6.200.jpg ## 6 http://en.openfoodfacts.org/images/products/008/770/317/7727/front.8.200.jpg ## energy_100g energy_from_fat_100g fat_100g saturated_fat_100g ## 1 918 NA 0.0 0.0 ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 766 NA 16.7 9.9 ## 5 2359 NA 45.5 5.2 ## 6 NA NA NA NA ## butyric_acid_100g caproic_acid_100g caprylic_acid_100g capric_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## lauric_acid_100g myristic_acid_100g palmitic_acid_100g stearic_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## arachidic_acid_100g behenic_acid_100g lignoceric_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## cerotic_acid_100g montanic_acid_100g melissic_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## monounsaturated_fat_100g polyunsaturated_fat_100g omega_3_fat_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 2.9 3.9 NA ## 5 9.5 32.8 NA ## 6 NA NA NA ## alpha_linolenic_acid_100g eicosapentaenoic_acid_100g ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## docosahexaenoic_acid_100g omega_6_fat_100g linoleic_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## arachidonic_acid_100g gamma_linolenic_acid_100g ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA ## dihomo_gamma_linolenic_acid_100g omega_9_fat_100g oleic_acid_100g ## 1 NA NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## elaidic_acid_100g gondoic_acid_100g mead_acid_100g erucic_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## nervonic_acid_100g trans_fat_100g cholesterol_100g carbohydrates_100g ## 1 NA NA NA 54.0 ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA 2e-04 5.7 ## 5 NA NA NA 17.3 ## 6 NA NA NA NA ## sugars_100g sucrose_100g glucose_100g fructose_100g lactose_100g ## 1 54.0 NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 4.2 NA NA NA NA ## 5 2.7 NA NA NA NA ## 6 NA NA NA NA NA ## maltose_100g maltodextrins_100g starch_100g polyols_100g fiber_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA 0.2 ## 5 NA NA NA NA 9.0 ## 6 NA NA NA NA NA ## proteins_100g casein_100g serum_proteins_100g nucleotides_100g salt_100g ## 1 0.0 NA NA NA 0.0000 ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 2.9 NA NA NA 0.0508 ## 5 18.2 NA NA NA 3.9878 ## 6 NA NA NA NA NA ## sodium_100g alcohol_100g vitamin_a_100g beta_carotene_100g ## 1 0.00 NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 0.02 NA NA NA ## 5 1.57 NA NA NA ## 6 NA NA NA NA ## vitamin_d_100g vitamin_e_100g vitamin_k_100g vitamin_c_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## vitamin_b1_100g vitamin_b2_100g vitamin_pp_100g vitamin_b6_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## vitamin_b9_100g vitamin_b12_100g biotin_100g pantothenic_acid_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## silica_100g bicarbonate_100g potassium_100g chloride_100g calcium_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## phosphorus_100g iron_100g magnesium_100g zinc_100g copper_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 1.155 0.0038 0.129 NA NA ## 6 NA NA NA NA NA ## manganese_100g fluoride_100g selenium_100g chromium_100g molybdenum_100g ## 1 NA NA NA NA NA ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA NA NA NA ## iodine_100g caffeine_100g taurine_100g ph_100g ## 1 NA NA NA NA ## 2 NA NA NA NA ## 3 NA NA NA NA ## 4 NA NA NA NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## fruits_vegetables_nuts_100g collagen_meat_protein_ratio_100g cocoa_100g ## 1 54 NA NA ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA NA ## 5 NA NA NA ## 6 NA NA NA ## chlorophyl_100g carbon_footprint_100g nutrition_score_fr_100g ## 1 NA NA 11 ## 2 NA NA NA ## 3 NA NA NA ## 4 NA NA 11 ## 5 NA NA 17 ## 6 NA NA NA ## nutrition_score_uk_100g ## 1 11 ## 2 NA ## 3 NA ## 4 11 ## 5 17 ## 6 NA # View structure of food str(food) ## &#39;data.frame&#39;: 1500 obs. of 160 variables: ## $ V1 : int 1 2 3 4 5 6 7 8 9 10 ... ## $ code : int 100030 100050 100079 100094 100124 100136 100194 100221 100257 100258 ... ## $ url : chr &quot;http://world-en.openfoodfacts.org/product/3222475745867/confiture-de-fraise-fraise-des-bois-au-sucre-de-canne-casino-delices&quot; &quot;http://world-en.openfoodfacts.org/product/5410976880110/guylian-sea-shells-selection&quot; &quot;http://world-en.openfoodfacts.org/product/3264750423503/pates-de-fruits-aromatisees-jacquot&quot; &quot;http://world-en.openfoodfacts.org/product/8006040247001/nata-vegetal-a-base-de-soja-valsoia&quot; ... ## $ creator : chr &quot;sebleouf&quot; &quot;foodorigins&quot; &quot;domdom26&quot; &quot;javichu&quot; ... ## $ created_t : int 1424747544 1450316429 1428674916 1420416591 1420501121 1437983923 1442420988 1435686217 1436991777 1400516512 ... ## $ created_datetime : chr &quot;2015-02-24T03:12:24Z&quot; &quot;2015-12-17T01:40:29Z&quot; &quot;2015-04-10T14:08:36Z&quot; &quot;2015-01-05T00:09:51Z&quot; ... ## $ last_modified_t : int 1438445887 1450817956 1428739289 1420417876 1445700917 1445577476 1442420988 1451405288 1436991779 1437236856 ... ## $ last_modified_datetime : chr &quot;2015-08-01T16:18:07Z&quot; &quot;2015-12-22T20:59:16Z&quot; &quot;2015-04-11T08:01:29Z&quot; &quot;2015-01-05T00:31:16Z&quot; ... ## $ product_name : chr &quot;Confiture de fraise fraise des bois au sucre de canne&quot; &quot;Guylian Sea Shells Selection&quot; &quot;PÃ¢tes de fruits aromatisÃ©es&quot; &quot;Nata vegetal a base de soja &amp;quot;Valsoia&amp;quot;&quot; ... ## $ generic_name : chr &quot;&quot; &quot;&quot; &quot;PÃ¢tes de fruits&quot; &quot;Nata vegetal a base de soja&quot; ... ## $ quantity : chr &quot;265 g&quot; &quot;375g&quot; &quot;1 kg&quot; &quot;200 ml&quot; ... ## $ packaging : chr &quot;Bocal,Verre&quot; &quot;Plastic,Box&quot; &quot;Carton,plastique&quot; &quot;Tetra Brik&quot; ... ## $ packaging_tags : chr &quot;bocal,verre&quot; &quot;plastic,box&quot; &quot;carton,plastique&quot; &quot;tetra-brik&quot; ... ## $ brands : chr &quot;Casino DÃ©lices&quot; &quot;Guylian&quot; &quot;Jacquot&quot; &quot;Valsoia,//Propiedad de://,Valsoia S.p.A.&quot; ... ## $ brands_tags : chr &quot;casino-delices&quot; &quot;guylian&quot; &quot;jacquot&quot; &quot;valsoia,propiedad-de,valsoia-s-p-a&quot; ... ## $ categories : chr &quot;Aliments et boissons Ã base de vÃ©gÃ©taux,Aliments d&#39;origine vÃ©gÃ©tale,Aliments Ã base de fruits et de lÃ©gu&quot;| __truncated__ &quot;Chocolate&quot; &quot;pÃ¢tes de fruits&quot; &quot;Alimentos y bebidas de origen vegetal,Alimentos de origen vegetal,Natas vegetales,Natas vegetales a base de soj&quot;| __truncated__ ... ## $ categories_tags : chr &quot;en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:breakfasts,en:s&quot;| __truncated__ &quot;en:sugary-snacks,en:chocolates&quot; &quot;en:plant-based-foods-and-beverages,en:plant-based-foods,en:fruits-and-vegetables-based-foods,en:sugary-snacks,e&quot;| __truncated__ &quot;en:plant-based-foods-and-beverages,en:plant-based-foods,en:plant-based-creams,en:plant-based-creams-for-cooking&quot;| __truncated__ ... ## $ categories_en : chr &quot;Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Breakfasts,Spreads,Fruits b&quot;| __truncated__ &quot;Sugary snacks,Chocolates&quot; &quot;Plant-based foods and beverages,Plant-based foods,Fruits and vegetables based foods,Sugary snacks,Confectioneri&quot;| __truncated__ &quot;Plant-based foods and beverages,Plant-based foods,Plant-based creams,Plant-based creams for cooking,Soy-based c&quot;| __truncated__ ... ## $ origins : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ origins_tags : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ manufacturing_places : chr &quot;France&quot; &quot;Belgium&quot; &quot;&quot; &quot;Italia&quot; ... ## $ manufacturing_places_tags : chr &quot;france&quot; &quot;belgium&quot; &quot;&quot; &quot;italia&quot; ... ## $ labels : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;Vegetariano,Vegano,Sin gluten,Sin OMG,Sin lactosa&quot; ... ## $ labels_tags : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;en:vegetarian,en:vegan,en:gluten-free,en:no-gmos,en:no-lactose&quot; ... ## $ labels_en : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;Vegetarian,Vegan,Gluten-free,No GMOs,No lactose&quot; ... ## $ emb_codes : chr &quot;EMB 78015&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ emb_codes_tags : chr &quot;emb-78015&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ first_packaging_code_geo : chr &quot;48.983333,2.066667&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ cities : logi NA NA NA NA NA NA ... ## $ cities_tags : chr &quot;andresy-yvelines-france&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ purchase_places : chr &quot;Lyon,France&quot; &quot;NSW,Australia&quot; &quot;France&quot; &quot;Madrid,EspaÃ±a&quot; ... ## $ stores : chr &quot;Casino&quot; &quot;&quot; &quot;&quot; &quot;El Corte InglÃ©s&quot; ... ## $ countries : chr &quot;France&quot; &quot;Australia&quot; &quot;France&quot; &quot;EspaÃ±a&quot; ... ## $ countries_tags : chr &quot;en:france&quot; &quot;en:australia&quot; &quot;en:france&quot; &quot;en:spain&quot; ... ## $ countries_en : chr &quot;France&quot; &quot;Australia&quot; &quot;France&quot; &quot;Spain&quot; ... ## $ ingredients_text : chr &quot;Sucre de canne, fraises 40 g, fraises des bois 14 g, gÃ©lifiant : pectines de fruits, jus de citron concentrÃ©.&quot;| __truncated__ &quot;&quot; &quot;Pulpe de pommes 50% , sucre, sirop de glucose, gÃ©lifiant : pectine, acidifiant : acide citrique, arÃ´mes, colo&quot;| __truncated__ &quot;Extracto de soja (78%) (agua, semillas de soja 8,3%), grasas vegetales, jarabe de glucosa, dextrosa, emulsionan&quot;| __truncated__ ... ## $ allergens : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ allergens_en : logi NA NA NA NA NA NA ... ## $ traces : chr &quot;Lait,Fruits Ã coque&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ traces_tags : chr &quot;en:milk,en:nuts&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ traces_en : chr &quot;Milk,Nuts&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ serving_size : chr &quot;15 g&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ no_nutriments : logi NA NA NA NA NA NA ... ## $ additives_n : int 1 NA 2 5 0 NA NA 0 NA 1 ... ## $ additives : chr &quot;[ sucre-de-canne -&gt; fr:sucre-de-canne ] [ sucre-de -&gt; fr:sucre-de ] [ sucre -&gt; fr:sucre ] [ fraises-40-g &quot;| __truncated__ &quot;&quot; &quot;[ pulpe-de-pommes-50 -&gt; fr:pulpe-de-pommes-50 ] [ pulpe-de-pommes -&gt; fr:pulpe-de-pommes ] [ pulpe-de -&gt; fr:&quot;| __truncated__ &quot;[ extracto-de-soja -&gt; es:extracto-de-soja ] [ 78 -&gt; es:78 ] [ agua -&gt; es:agua ] [ semillas-de-soja-8 -&gt; e&quot;| __truncated__ ... ## $ additives_tags : chr &quot;en:e440&quot; &quot;&quot; &quot;en:e440,en:e330&quot; &quot;en:e471,en:e415,en:e407,en:e412,en:e306&quot; ... ## $ additives_en : chr &quot;E440 - Pectins&quot; &quot;&quot; &quot;E440 - Pectins,E330 - Citric acid&quot; &quot;E471 - Mono- and diglycerides of fatty acids,E415 - Xanthan gum,E407 - Carrageenan,E412 - Guar gum,E306 - Tocop&quot;| __truncated__ ... ## $ ingredients_from_palm_oil_n : int 0 NA 0 0 0 NA NA 0 NA 0 ... ## $ ingredients_from_palm_oil : logi NA NA NA NA NA NA ... ## $ ingredients_from_palm_oil_tags : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ ingredients_that_may_be_from_palm_oil_n : int 0 NA 0 1 0 NA NA 0 NA 0 ... ## $ ingredients_that_may_be_from_palm_oil : logi NA NA NA NA NA NA ... ## $ ingredients_that_may_be_from_palm_oil_tags: chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;e471-mono-et-diglycerides-d-acides-gras-alimentaires&quot; ... ## $ nutrition_grade_uk : logi NA NA NA NA NA NA ... ## $ nutrition_grade_fr : chr &quot;d&quot; &quot;&quot; &quot;&quot; &quot;d&quot; ... ## $ pnns_groups_1 : chr &quot;Sugary snacks&quot; &quot;Sugary snacks&quot; &quot;Fruits and vegetables&quot; &quot;unknown&quot; ... ## $ pnns_groups_2 : chr &quot;Sweets&quot; &quot;Chocolate products&quot; &quot;Fruits&quot; &quot;unknown&quot; ... ## $ states : chr &quot;en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be&quot;| __truncated__ &quot;en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-b&quot;| __truncated__ &quot;en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be&quot;| __truncated__ &quot;en:to-be-checked, en:complete, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-compl&quot;| __truncated__ ... ## $ states_tags : chr &quot;en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-com&quot;| __truncated__ &quot;en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-c&quot;| __truncated__ &quot;en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-com&quot;| __truncated__ &quot;en:to-be-checked,en:complete,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-completed&quot;| __truncated__ ... ## $ states_en : chr &quot;To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characte&quot;| __truncated__ &quot;To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Cha&quot;| __truncated__ &quot;To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Characte&quot;| __truncated__ &quot;To be checked,Complete,Nutrition facts completed,Ingredients completed,Expiration date completed,Characteristic&quot;| __truncated__ ... ## $ main_category : chr &quot;en:plant-based-foods-and-beverages&quot; &quot;en:sugary-snacks&quot; &quot;en:plant-based-foods-and-beverages&quot; &quot;en:plant-based-foods-and-beverages&quot; ... ## $ main_category_en : chr &quot;Plant-based foods and beverages&quot; &quot;Sugary snacks&quot; &quot;Plant-based foods and beverages&quot; &quot;Plant-based foods and beverages&quot; ... ## $ image_url : chr &quot;http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.400.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.400.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.400.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.400.jpg&quot; ... ## $ image_small_url : chr &quot;http://en.openfoodfacts.org/images/products/322/247/574/5867/front.8.200.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/541/097/688/0110/front.7.200.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/326/475/042/3503/front.6.200.jpg&quot; &quot;http://en.openfoodfacts.org/images/products/800/604/024/7001/front.7.200.jpg&quot; ... ## $ energy_100g : num 918 NA NA 766 2359 ... ## $ energy_from_fat_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ fat_100g : num 0 NA NA 16.7 45.5 NA NA 25 NA 4 ... ## $ saturated_fat_100g : num 0 NA NA 9.9 5.2 NA NA 17 NA 0.54 ... ## $ butyric_acid_100g : logi NA NA NA NA NA NA ... ## $ caproic_acid_100g : logi NA NA NA NA NA NA ... ## $ caprylic_acid_100g : logi NA NA NA NA NA NA ... ## $ capric_acid_100g : logi NA NA NA NA NA NA ... ## $ lauric_acid_100g : logi NA NA NA NA NA NA ... ## $ myristic_acid_100g : logi NA NA NA NA NA NA ... ## $ palmitic_acid_100g : logi NA NA NA NA NA NA ... ## $ stearic_acid_100g : logi NA NA NA NA NA NA ... ## $ arachidic_acid_100g : logi NA NA NA NA NA NA ... ## $ behenic_acid_100g : logi NA NA NA NA NA NA ... ## $ lignoceric_acid_100g : logi NA NA NA NA NA NA ... ## $ cerotic_acid_100g : logi NA NA NA NA NA NA ... ## $ montanic_acid_100g : logi NA NA NA NA NA NA ... ## $ melissic_acid_100g : logi NA NA NA NA NA NA ... ## $ monounsaturated_fat_100g : num NA NA NA 2.9 9.5 NA NA NA NA NA ... ## $ polyunsaturated_fat_100g : num NA NA NA 3.9 32.8 NA NA NA NA NA ... ## $ omega_3_fat_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ alpha_linolenic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ eicosapentaenoic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ docosahexaenoic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ omega_6_fat_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ linoleic_acid_100g : num NA NA NA NA NA NA NA NA NA NA ... ## $ arachidonic_acid_100g : logi NA NA NA NA NA NA ... ## $ gamma_linolenic_acid_100g : logi NA NA NA NA NA NA ... ## $ dihomo_gamma_linolenic_acid_100g : logi NA NA NA NA NA NA ... ## $ omega_9_fat_100g : logi NA NA NA NA NA NA ... ## $ oleic_acid_100g : logi NA NA NA NA NA NA ... ## $ elaidic_acid_100g : logi NA NA NA NA NA NA ... ## $ gondoic_acid_100g : logi NA NA NA NA NA NA ... ## $ mead_acid_100g : logi NA NA NA NA NA NA ... ## $ erucic_acid_100g : logi NA NA NA NA NA NA ... ## [list output truncated] This is a large dataset and it is difficult to see what is going on. So let’s try dplyr. # Load dplyr library(dplyr) # View a glimpse of food glimpse(food) ## Observations: 1,500 ## Variables: 160 ## $ V1 &lt;int&gt; 1, 2, 3, 4, 5, 6, 7... ## $ code &lt;int&gt; 100030, 100050, 100... ## $ url &lt;chr&gt; &quot;http://world-en.op... ## $ creator &lt;chr&gt; &quot;sebleouf&quot;, &quot;foodor... ## $ created_t &lt;int&gt; 1424747544, 1450316... ## $ created_datetime &lt;chr&gt; &quot;2015-02-24T03:12:2... ## $ last_modified_t &lt;int&gt; 1438445887, 1450817... ## $ last_modified_datetime &lt;chr&gt; &quot;2015-08-01T16:18:0... ## $ product_name &lt;chr&gt; &quot;Confiture de frais... ## $ generic_name &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;PÃ¢tes de ... ## $ quantity &lt;chr&gt; &quot;265 g&quot;, &quot;375g&quot;, &quot;1... ## $ packaging &lt;chr&gt; &quot;Bocal,Verre&quot;, &quot;Pla... ## $ packaging_tags &lt;chr&gt; &quot;bocal,verre&quot;, &quot;pla... ## $ brands &lt;chr&gt; &quot;Casino DÃ©lices&quot;, ... ## $ brands_tags &lt;chr&gt; &quot;casino-delices&quot;, &quot;... ## $ categories &lt;chr&gt; &quot;Aliments et boisso... ## $ categories_tags &lt;chr&gt; &quot;en:plant-based-foo... ## $ categories_en &lt;chr&gt; &quot;Plant-based foods ... ## $ origins &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Ar... ## $ origins_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;ar... ## $ manufacturing_places &lt;chr&gt; &quot;France&quot;, &quot;Belgium&quot;... ## $ manufacturing_places_tags &lt;chr&gt; &quot;france&quot;, &quot;belgium&quot;... ## $ labels &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Vegeta... ## $ labels_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;en:veg... ## $ labels_en &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Vegeta... ## $ emb_codes &lt;chr&gt; &quot;EMB 78015&quot;, &quot;&quot;, &quot;&quot;... ## $ emb_codes_tags &lt;chr&gt; &quot;emb-78015&quot;, &quot;&quot;, &quot;&quot;... ## $ first_packaging_code_geo &lt;chr&gt; &quot;48.983333,2.066667... ## $ cities &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ cities_tags &lt;chr&gt; &quot;andresy-yvelines-f... ## $ purchase_places &lt;chr&gt; &quot;Lyon,France&quot;, &quot;NSW... ## $ stores &lt;chr&gt; &quot;Casino&quot;, &quot;&quot;, &quot;&quot;, &quot;... ## $ countries &lt;chr&gt; &quot;France&quot;, &quot;Australi... ## $ countries_tags &lt;chr&gt; &quot;en:france&quot;, &quot;en:au... ## $ countries_en &lt;chr&gt; &quot;France&quot;, &quot;Australi... ## $ ingredients_text &lt;chr&gt; &quot;Sucre de canne, fr... ## $ allergens &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,... ## $ allergens_en &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ traces &lt;chr&gt; &quot;Lait,Fruits Ã coq... ## $ traces_tags &lt;chr&gt; &quot;en:milk,en:nuts&quot;, ... ## $ traces_en &lt;chr&gt; &quot;Milk,Nuts&quot;, &quot;&quot;, &quot;&quot;... ## $ serving_size &lt;chr&gt; &quot;15 g&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,... ## $ no_nutriments &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ additives_n &lt;int&gt; 1, NA, 2, 5, 0, NA,... ## $ additives &lt;chr&gt; &quot;[ sucre-de-canne -... ## $ additives_tags &lt;chr&gt; &quot;en:e440&quot;, &quot;&quot;, &quot;en:... ## $ additives_en &lt;chr&gt; &quot;E440 - Pectins&quot;, &quot;... ## $ ingredients_from_palm_oil_n &lt;int&gt; 0, NA, 0, 0, 0, NA,... ## $ ingredients_from_palm_oil &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ ingredients_from_palm_oil_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,... ## $ ingredients_that_may_be_from_palm_oil_n &lt;int&gt; 0, NA, 0, 1, 0, NA,... ## $ ingredients_that_may_be_from_palm_oil &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ ingredients_that_may_be_from_palm_oil_tags &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;e471-m... ## $ nutrition_grade_uk &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ nutrition_grade_fr &lt;chr&gt; &quot;d&quot;, &quot;&quot;, &quot;&quot;, &quot;d&quot;, &quot;... ## $ pnns_groups_1 &lt;chr&gt; &quot;Sugary snacks&quot;, &quot;S... ## $ pnns_groups_2 &lt;chr&gt; &quot;Sweets&quot;, &quot;Chocolat... ## $ states &lt;chr&gt; &quot;en:to-be-checked, ... ## $ states_tags &lt;chr&gt; &quot;en:to-be-checked,e... ## $ states_en &lt;chr&gt; &quot;To be checked,Comp... ## $ main_category &lt;chr&gt; &quot;en:plant-based-foo... ## $ main_category_en &lt;chr&gt; &quot;Plant-based foods ... ## $ image_url &lt;chr&gt; &quot;http://en.openfood... ## $ image_small_url &lt;chr&gt; &quot;http://en.openfood... ## $ energy_100g &lt;dbl&gt; 918, NA, NA, 766, 2... ## $ energy_from_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ fat_100g &lt;dbl&gt; 0.00, NA, NA, 16.70... ## $ saturated_fat_100g &lt;dbl&gt; 0.000, NA, NA, 9.90... ## $ butyric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ caproic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ caprylic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ capric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ lauric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ myristic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ palmitic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ stearic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ arachidic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ behenic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ lignoceric_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ cerotic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ montanic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ melissic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ monounsaturated_fat_100g &lt;dbl&gt; NA, NA, NA, 2.9, 9.... ## $ polyunsaturated_fat_100g &lt;dbl&gt; NA, NA, NA, 3.9, 32... ## $ omega_3_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ alpha_linolenic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ eicosapentaenoic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ docosahexaenoic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ omega_6_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ linoleic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ arachidonic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ gamma_linolenic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ dihomo_gamma_linolenic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ omega_9_fat_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ oleic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ elaidic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ gondoic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ mead_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ erucic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ nervonic_acid_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ trans_fat_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ cholesterol_100g &lt;dbl&gt; NA, NA, NA, 0.00020... ## $ carbohydrates_100g &lt;dbl&gt; 54.00, NA, NA, 5.70... ## $ sugars_100g &lt;dbl&gt; 54.00, NA, NA, 4.20... ## $ sucrose_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ glucose_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ fructose_100g &lt;int&gt; NA, NA, NA, NA, NA,... ## $ lactose_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ maltose_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ maltodextrins_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ starch_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ polyols_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ fiber_100g &lt;dbl&gt; NA, NA, NA, 0.2, 9.... ## $ proteins_100g &lt;dbl&gt; 0.00, NA, NA, 2.90,... ## $ casein_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ serum_proteins_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ nucleotides_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ salt_100g &lt;dbl&gt; 0.0000000, NA, NA, ... ## $ sodium_100g &lt;dbl&gt; 0.0000000, NA, NA, ... ## $ alcohol_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_a_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ beta_carotene_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_d_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_e_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_k_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_c_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b1_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b2_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_pp_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b6_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b9_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ vitamin_b12_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ biotin_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ pantothenic_acid_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ silica_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ bicarbonate_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ potassium_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ chloride_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ calcium_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ phosphorus_100g &lt;dbl&gt; NA, NA, NA, NA, 1.1... ## $ iron_100g &lt;dbl&gt; NA, NA, NA, NA, 0.0... ## $ magnesium_100g &lt;dbl&gt; NA, NA, NA, NA, 0.1... ## $ zinc_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ copper_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ manganese_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ fluoride_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ selenium_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ chromium_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ molybdenum_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ iodine_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ caffeine_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ taurine_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ ph_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ fruits_vegetables_nuts_100g &lt;dbl&gt; 54, NA, NA, NA, NA,... ## $ collagen_meat_protein_ratio_100g &lt;int&gt; NA, NA, NA, NA, NA,... ## $ cocoa_100g &lt;int&gt; NA, NA, NA, NA, NA,... ## $ chlorophyl_100g &lt;lgl&gt; NA, NA, NA, NA, NA,... ## $ carbon_footprint_100g &lt;dbl&gt; NA, NA, NA, NA, NA,... ## $ nutrition_score_fr_100g &lt;int&gt; 11, NA, NA, 11, 17,... ## $ nutrition_score_uk_100g &lt;int&gt; 11, NA, NA, 11, 17,... # View column names of food names(food) ## [1] &quot;V1&quot; ## [2] &quot;code&quot; ## [3] &quot;url&quot; ## [4] &quot;creator&quot; ## [5] &quot;created_t&quot; ## [6] &quot;created_datetime&quot; ## [7] &quot;last_modified_t&quot; ## [8] &quot;last_modified_datetime&quot; ## [9] &quot;product_name&quot; ## [10] &quot;generic_name&quot; ## [11] &quot;quantity&quot; ## [12] &quot;packaging&quot; ## [13] &quot;packaging_tags&quot; ## [14] &quot;brands&quot; ## [15] &quot;brands_tags&quot; ## [16] &quot;categories&quot; ## [17] &quot;categories_tags&quot; ## [18] &quot;categories_en&quot; ## [19] &quot;origins&quot; ## [20] &quot;origins_tags&quot; ## [21] &quot;manufacturing_places&quot; ## [22] &quot;manufacturing_places_tags&quot; ## [23] &quot;labels&quot; ## [24] &quot;labels_tags&quot; ## [25] &quot;labels_en&quot; ## [26] &quot;emb_codes&quot; ## [27] &quot;emb_codes_tags&quot; ## [28] &quot;first_packaging_code_geo&quot; ## [29] &quot;cities&quot; ## [30] &quot;cities_tags&quot; ## [31] &quot;purchase_places&quot; ## [32] &quot;stores&quot; ## [33] &quot;countries&quot; ## [34] &quot;countries_tags&quot; ## [35] &quot;countries_en&quot; ## [36] &quot;ingredients_text&quot; ## [37] &quot;allergens&quot; ## [38] &quot;allergens_en&quot; ## [39] &quot;traces&quot; ## [40] &quot;traces_tags&quot; ## [41] &quot;traces_en&quot; ## [42] &quot;serving_size&quot; ## [43] &quot;no_nutriments&quot; ## [44] &quot;additives_n&quot; ## [45] &quot;additives&quot; ## [46] &quot;additives_tags&quot; ## [47] &quot;additives_en&quot; ## [48] &quot;ingredients_from_palm_oil_n&quot; ## [49] &quot;ingredients_from_palm_oil&quot; ## [50] &quot;ingredients_from_palm_oil_tags&quot; ## [51] &quot;ingredients_that_may_be_from_palm_oil_n&quot; ## [52] &quot;ingredients_that_may_be_from_palm_oil&quot; ## [53] &quot;ingredients_that_may_be_from_palm_oil_tags&quot; ## [54] &quot;nutrition_grade_uk&quot; ## [55] &quot;nutrition_grade_fr&quot; ## [56] &quot;pnns_groups_1&quot; ## [57] &quot;pnns_groups_2&quot; ## [58] &quot;states&quot; ## [59] &quot;states_tags&quot; ## [60] &quot;states_en&quot; ## [61] &quot;main_category&quot; ## [62] &quot;main_category_en&quot; ## [63] &quot;image_url&quot; ## [64] &quot;image_small_url&quot; ## [65] &quot;energy_100g&quot; ## [66] &quot;energy_from_fat_100g&quot; ## [67] &quot;fat_100g&quot; ## [68] &quot;saturated_fat_100g&quot; ## [69] &quot;butyric_acid_100g&quot; ## [70] &quot;caproic_acid_100g&quot; ## [71] &quot;caprylic_acid_100g&quot; ## [72] &quot;capric_acid_100g&quot; ## [73] &quot;lauric_acid_100g&quot; ## [74] &quot;myristic_acid_100g&quot; ## [75] &quot;palmitic_acid_100g&quot; ## [76] &quot;stearic_acid_100g&quot; ## [77] &quot;arachidic_acid_100g&quot; ## [78] &quot;behenic_acid_100g&quot; ## [79] &quot;lignoceric_acid_100g&quot; ## [80] &quot;cerotic_acid_100g&quot; ## [81] &quot;montanic_acid_100g&quot; ## [82] &quot;melissic_acid_100g&quot; ## [83] &quot;monounsaturated_fat_100g&quot; ## [84] &quot;polyunsaturated_fat_100g&quot; ## [85] &quot;omega_3_fat_100g&quot; ## [86] &quot;alpha_linolenic_acid_100g&quot; ## [87] &quot;eicosapentaenoic_acid_100g&quot; ## [88] &quot;docosahexaenoic_acid_100g&quot; ## [89] &quot;omega_6_fat_100g&quot; ## [90] &quot;linoleic_acid_100g&quot; ## [91] &quot;arachidonic_acid_100g&quot; ## [92] &quot;gamma_linolenic_acid_100g&quot; ## [93] &quot;dihomo_gamma_linolenic_acid_100g&quot; ## [94] &quot;omega_9_fat_100g&quot; ## [95] &quot;oleic_acid_100g&quot; ## [96] &quot;elaidic_acid_100g&quot; ## [97] &quot;gondoic_acid_100g&quot; ## [98] &quot;mead_acid_100g&quot; ## [99] &quot;erucic_acid_100g&quot; ## [100] &quot;nervonic_acid_100g&quot; ## [101] &quot;trans_fat_100g&quot; ## [102] &quot;cholesterol_100g&quot; ## [103] &quot;carbohydrates_100g&quot; ## [104] &quot;sugars_100g&quot; ## [105] &quot;sucrose_100g&quot; ## [106] &quot;glucose_100g&quot; ## [107] &quot;fructose_100g&quot; ## [108] &quot;lactose_100g&quot; ## [109] &quot;maltose_100g&quot; ## [110] &quot;maltodextrins_100g&quot; ## [111] &quot;starch_100g&quot; ## [112] &quot;polyols_100g&quot; ## [113] &quot;fiber_100g&quot; ## [114] &quot;proteins_100g&quot; ## [115] &quot;casein_100g&quot; ## [116] &quot;serum_proteins_100g&quot; ## [117] &quot;nucleotides_100g&quot; ## [118] &quot;salt_100g&quot; ## [119] &quot;sodium_100g&quot; ## [120] &quot;alcohol_100g&quot; ## [121] &quot;vitamin_a_100g&quot; ## [122] &quot;beta_carotene_100g&quot; ## [123] &quot;vitamin_d_100g&quot; ## [124] &quot;vitamin_e_100g&quot; ## [125] &quot;vitamin_k_100g&quot; ## [126] &quot;vitamin_c_100g&quot; ## [127] &quot;vitamin_b1_100g&quot; ## [128] &quot;vitamin_b2_100g&quot; ## [129] &quot;vitamin_pp_100g&quot; ## [130] &quot;vitamin_b6_100g&quot; ## [131] &quot;vitamin_b9_100g&quot; ## [132] &quot;vitamin_b12_100g&quot; ## [133] &quot;biotin_100g&quot; ## [134] &quot;pantothenic_acid_100g&quot; ## [135] &quot;silica_100g&quot; ## [136] &quot;bicarbonate_100g&quot; ## [137] &quot;potassium_100g&quot; ## [138] &quot;chloride_100g&quot; ## [139] &quot;calcium_100g&quot; ## [140] &quot;phosphorus_100g&quot; ## [141] &quot;iron_100g&quot; ## [142] &quot;magnesium_100g&quot; ## [143] &quot;zinc_100g&quot; ## [144] &quot;copper_100g&quot; ## [145] &quot;manganese_100g&quot; ## [146] &quot;fluoride_100g&quot; ## [147] &quot;selenium_100g&quot; ## [148] &quot;chromium_100g&quot; ## [149] &quot;molybdenum_100g&quot; ## [150] &quot;iodine_100g&quot; ## [151] &quot;caffeine_100g&quot; ## [152] &quot;taurine_100g&quot; ## [153] &quot;ph_100g&quot; ## [154] &quot;fruits_vegetables_nuts_100g&quot; ## [155] &quot;collagen_meat_protein_ratio_100g&quot; ## [156] &quot;cocoa_100g&quot; ## [157] &quot;chlorophyl_100g&quot; ## [158] &quot;carbon_footprint_100g&quot; ## [159] &quot;nutrition_score_fr_100g&quot; ## [160] &quot;nutrition_score_uk_100g&quot; There is a lot of information there, there’s some information on what and when information was added (1:9), meta information about food (10:17, 22:27), where it came from (18:21, 28:34), what it’s made of (35:52), nutrition grades (53:54), some unclear (55:63), and some nutritional information (64:159). There are also some duplicates, different pairs of columns that contain duplicate information. There are many columns containing information that you just can’t use. # Define vector of duplicate cols duplicates &lt;- c(4, 6, 11, 13, 15, 17, 18, 20, 22, 24, 25, 28, 32, 34, 36, 38, 40, 44, 46, 48, 51, 54, 65, 158) # Remove duplicates from food: food2 food2 &lt;- food[, -duplicates] # Define useless vector useless &lt;- c(1, 2, 3, 32:41) # Remove useless columns from food2: food3 food3 &lt;- food2[, -useless] Earlier on we saw that there are many columns containing nutritional information in them, identified with a ‘100g’ label in the column name. If we want to use the nutritional information, we can therefore use this to identify those columns. library(stringr) # Create vector of column indices: nutrition nutrition &lt;- str_detect(names(food3), &quot;100g&quot;) # View the number of columns it applies to summary(nutrition) ## Mode FALSE TRUE ## logical 29 94 # View a summary of nutrition columns summary(food3[, nutrition]) ## energy_from_fat_100g fat_100g saturated_fat_100g ## Min. : 0.00 Min. : 0.00 Min. : 0.000 ## 1st Qu.: 35.98 1st Qu.: 0.90 1st Qu.: 0.200 ## Median : 237.00 Median : 6.00 Median : 1.700 ## Mean : 668.41 Mean : 13.39 Mean : 4.874 ## 3rd Qu.: 974.00 3rd Qu.: 20.00 3rd Qu.: 6.500 ## Max. :2900.00 Max. :100.00 Max. :57.000 ## NA&#39;s :1486 NA&#39;s :708 NA&#39;s :797 ## butyric_acid_100g caproic_acid_100g caprylic_acid_100g capric_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## lauric_acid_100g myristic_acid_100g palmitic_acid_100g stearic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## arachidic_acid_100g behenic_acid_100g lignoceric_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## cerotic_acid_100g montanic_acid_100g melissic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## monounsaturated_fat_100g polyunsaturated_fat_100g omega_3_fat_100g ## Min. : 0.00 Min. : 0.400 Min. : 0.033 ## 1st Qu.: 3.87 1st Qu.: 1.653 1st Qu.: 1.300 ## Median : 9.50 Median : 3.900 Median : 3.000 ## Mean :19.77 Mean : 9.986 Mean : 3.726 ## 3rd Qu.:29.00 3rd Qu.:12.700 3rd Qu.: 3.200 ## Max. :75.00 Max. :46.200 Max. :12.400 ## NA&#39;s :1465 NA&#39;s :1464 NA&#39;s :1491 ## alpha_linolenic_acid_100g eicosapentaenoic_acid_100g ## Min. :0.0800 Min. :0.721 ## 1st Qu.:0.0905 1st Qu.:0.721 ## Median :0.1010 Median :0.721 ## Mean :0.1737 Mean :0.721 ## 3rd Qu.:0.2205 3rd Qu.:0.721 ## Max. :0.3400 Max. :0.721 ## NA&#39;s :1497 NA&#39;s :1499 ## docosahexaenoic_acid_100g omega_6_fat_100g linoleic_acid_100g ## Min. :1.09 Min. :0.25 Min. :0.5000 ## 1st Qu.:1.09 1st Qu.:0.25 1st Qu.:0.5165 ## Median :1.09 Median :0.25 Median :0.5330 ## Mean :1.09 Mean :0.25 Mean :0.5330 ## 3rd Qu.:1.09 3rd Qu.:0.25 3rd Qu.:0.5495 ## Max. :1.09 Max. :0.25 Max. :0.5660 ## NA&#39;s :1499 NA&#39;s :1499 NA&#39;s :1498 ## arachidonic_acid_100g gamma_linolenic_acid_100g ## Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## dihomo_gamma_linolenic_acid_100g omega_9_fat_100g oleic_acid_100g ## Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## elaidic_acid_100g gondoic_acid_100g mead_acid_100g erucic_acid_100g ## Mode:logical Mode:logical Mode:logical Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 NA&#39;s:1500 ## ## ## ## ## ## nervonic_acid_100g trans_fat_100g cholesterol_100g carbohydrates_100g ## Mode:logical Min. :0.0000 Min. :0.0000 Min. : 0.000 ## NA&#39;s:1500 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.: 3.792 ## Median :0.0000 Median :0.0000 Median : 13.500 ## Mean :0.0105 Mean :0.0265 Mean : 27.958 ## 3rd Qu.:0.0000 3rd Qu.:0.0026 3rd Qu.: 55.000 ## Max. :0.1000 Max. :0.4300 Max. :100.000 ## NA&#39;s :1481 NA&#39;s :1477 NA&#39;s :708 ## sugars_100g sucrose_100g glucose_100g fructose_100g ## Min. : 0.00 Mode:logical Mode:logical Min. :100 ## 1st Qu.: 1.00 NA&#39;s:1500 NA&#39;s:1500 1st Qu.:100 ## Median : 4.05 Median :100 ## Mean : 12.66 Mean :100 ## 3rd Qu.: 14.70 3rd Qu.:100 ## Max. :100.00 Max. :100 ## NA&#39;s :788 NA&#39;s :1499 ## lactose_100g maltose_100g maltodextrins_100g starch_100g ## Min. :0.000 Mode:logical Mode:logical Min. : 0.00 ## 1st Qu.:0.250 NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 9.45 ## Median :0.500 Median :39.50 ## Mean :2.933 Mean :30.73 ## 3rd Qu.:4.400 3rd Qu.:42.85 ## Max. :8.300 Max. :71.00 ## NA&#39;s :1497 NA&#39;s :1493 ## polyols_100g fiber_100g proteins_100g casein_100g ## Min. : 8.60 Min. : 0.000 Min. : 0.000 Min. :1.1 ## 1st Qu.:59.10 1st Qu.: 0.500 1st Qu.: 1.500 1st Qu.:1.1 ## Median :67.00 Median : 1.750 Median : 6.000 Median :1.1 ## Mean :56.06 Mean : 2.823 Mean : 7.563 Mean :1.1 ## 3rd Qu.:69.80 3rd Qu.: 3.500 3rd Qu.:10.675 3rd Qu.:1.1 ## Max. :70.00 Max. :46.700 Max. :61.000 Max. :1.1 ## NA&#39;s :1491 NA&#39;s :994 NA&#39;s :710 NA&#39;s :1499 ## serum_proteins_100g nucleotides_100g salt_100g sodium_100g ## Mode:logical Mode:logical Min. : 0.0000 Min. : 0.0000 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.: 0.0438 1st Qu.: 0.0172 ## Median : 0.4498 Median : 0.1771 ## Mean : 1.1205 Mean : 0.4409 ## 3rd Qu.: 1.1938 3rd Qu.: 0.4700 ## Max. :102.0000 Max. :40.0000 ## NA&#39;s :780 NA&#39;s :780 ## alcohol_100g vitamin_a_100g beta_carotene_100g vitamin_d_100g ## Min. : 0.00 Min. :0.0000 Mode:logical Min. :0e+00 ## 1st Qu.: 0.00 1st Qu.:0.0000 NA&#39;s:1500 1st Qu.:0e+00 ## Median : 5.50 Median :0.0001 Median :0e+00 ## Mean :10.07 Mean :0.0003 Mean :0e+00 ## 3rd Qu.:13.00 3rd Qu.:0.0006 3rd Qu.:0e+00 ## Max. :50.00 Max. :0.0013 Max. :1e-04 ## NA&#39;s :1433 NA&#39;s :1477 NA&#39;s :1485 ## vitamin_e_100g vitamin_k_100g vitamin_c_100g vitamin_b1_100g ## Min. :0.0005 Min. :0 Min. :0.000 Min. :0.0001 ## 1st Qu.:0.0021 1st Qu.:0 1st Qu.:0.002 1st Qu.:0.0003 ## Median :0.0044 Median :0 Median :0.019 Median :0.0004 ## Mean :0.0069 Mean :0 Mean :0.025 Mean :0.0006 ## 3rd Qu.:0.0097 3rd Qu.:0 3rd Qu.:0.030 3rd Qu.:0.0010 ## Max. :0.0320 Max. :0 Max. :0.217 Max. :0.0013 ## NA&#39;s :1478 NA&#39;s :1498 NA&#39;s :1459 NA&#39;s :1478 ## vitamin_b2_100g vitamin_pp_100g vitamin_b6_100g vitamin_b9_100g ## Min. :0.0002 Min. :0.0006 Min. :0.0001 Min. :0e+00 ## 1st Qu.:0.0003 1st Qu.:0.0033 1st Qu.:0.0002 1st Qu.:0e+00 ## Median :0.0009 Median :0.0069 Median :0.0008 Median :1e-04 ## Mean :0.0011 Mean :0.0086 Mean :0.0112 Mean :1e-04 ## 3rd Qu.:0.0013 3rd Qu.:0.0140 3rd Qu.:0.0012 3rd Qu.:2e-04 ## Max. :0.0066 Max. :0.0160 Max. :0.2000 Max. :2e-04 ## NA&#39;s :1483 NA&#39;s :1484 NA&#39;s :1481 NA&#39;s :1483 ## vitamin_b12_100g biotin_100g pantothenic_acid_100g silica_100g ## Min. :0 Min. :0 Min. :0.0000 Min. :8e-04 ## 1st Qu.:0 1st Qu.:0 1st Qu.:0.0007 1st Qu.:8e-04 ## Median :0 Median :0 Median :0.0020 Median :8e-04 ## Mean :0 Mean :0 Mean :0.0027 Mean :8e-04 ## 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0.0051 3rd Qu.:8e-04 ## Max. :0 Max. :0 Max. :0.0060 Max. :8e-04 ## NA&#39;s :1489 NA&#39;s :1498 NA&#39;s :1486 NA&#39;s :1499 ## bicarbonate_100g potassium_100g chloride_100g calcium_100g ## Min. :0.0006 Min. :0.0000 Min. :0.0003 Min. :0.0000 ## 1st Qu.:0.0678 1st Qu.:0.0650 1st Qu.:0.0006 1st Qu.:0.0450 ## Median :0.1350 Median :0.1940 Median :0.0009 Median :0.1200 ## Mean :0.1692 Mean :0.3288 Mean :0.0144 Mean :0.2040 ## 3rd Qu.:0.2535 3rd Qu.:0.3670 3rd Qu.:0.0214 3rd Qu.:0.1985 ## Max. :0.3720 Max. :1.4300 Max. :0.0420 Max. :1.0000 ## NA&#39;s :1497 NA&#39;s :1487 NA&#39;s :1497 NA&#39;s :1449 ## phosphorus_100g iron_100g magnesium_100g zinc_100g ## Min. :0.0430 Min. :0.0000 Min. :0.0000 Min. :0.0005 ## 1st Qu.:0.1938 1st Qu.:0.0012 1st Qu.:0.0670 1st Qu.:0.0009 ## Median :0.3185 Median :0.0042 Median :0.1040 Median :0.0017 ## Mean :0.3777 Mean :0.0045 Mean :0.1066 Mean :0.0016 ## 3rd Qu.:0.4340 3rd Qu.:0.0077 3rd Qu.:0.1300 3rd Qu.:0.0022 ## Max. :1.1550 Max. :0.0137 Max. :0.3330 Max. :0.0026 ## NA&#39;s :1488 NA&#39;s :1463 NA&#39;s :1479 NA&#39;s :1493 ## copper_100g manganese_100g fluoride_100g selenium_100g ## Min. :0e+00 Min. :0 Min. :0 Min. :0 ## 1st Qu.:1e-04 1st Qu.:0 1st Qu.:0 1st Qu.:0 ## Median :1e-04 Median :0 Median :0 Median :0 ## Mean :1e-04 Mean :0 Mean :0 Mean :0 ## 3rd Qu.:1e-04 3rd Qu.:0 3rd Qu.:0 3rd Qu.:0 ## Max. :1e-04 Max. :0 Max. :0 Max. :0 ## NA&#39;s :1498 NA&#39;s :1499 NA&#39;s :1498 NA&#39;s :1499 ## chromium_100g molybdenum_100g iodine_100g caffeine_100g ## Mode:logical Mode:logical Min. :0 Mode:logical ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:0 NA&#39;s:1500 ## Median :0 ## Mean :0 ## 3rd Qu.:0 ## Max. :0 ## NA&#39;s :1499 ## taurine_100g ph_100g fruits_vegetables_nuts_100g ## Mode:logical Mode:logical Min. : 2.00 ## NA&#39;s:1500 NA&#39;s:1500 1st Qu.:11.25 ## Median :42.00 ## Mean :36.88 ## 3rd Qu.:52.25 ## Max. :80.00 ## NA&#39;s :1470 ## collagen_meat_protein_ratio_100g cocoa_100g chlorophyl_100g ## Min. :12.00 Min. :30 Mode:logical ## 1st Qu.:13.50 1st Qu.:47 NA&#39;s:1500 ## Median :15.00 Median :60 ## Mean :15.67 Mean :57 ## 3rd Qu.:17.50 3rd Qu.:70 ## Max. :20.00 Max. :81 ## NA&#39;s :1497 NA&#39;s :1491 ## nutrition_score_fr_100g nutrition_score_uk_100g ## Min. :-12.000 Min. :-12.000 ## 1st Qu.: 1.000 1st Qu.: 0.000 ## Median : 7.000 Median : 6.000 ## Mean : 7.941 Mean : 7.631 ## 3rd Qu.: 15.000 3rd Qu.: 16.000 ## Max. : 28.000 Max. : 28.000 ## NA&#39;s :825 NA&#39;s :825 We can see there are a large number of missing (NA) values. For some variables however, NA is sometimes left as the default where the actual number is zero. This is the case with the sugars_100g column. # Find indices of sugar NA values: missing missing &lt;- is.na(food3$sugars_100g) # Replace NA values with 0 food3$sugars_100g[missing] &lt;- 0 # Create first histogram hist(food3$sugars_100g, breaks = 100) # Create food4 food4 &lt;- food3[food3$sugars_100g != 0, ] # Create second histogram hist(food4$sugars_100g, breaks = 100) Your dataset has information about packaging, but there’s a bit of a problem: it’s stored in several different languages (Spanish, French, and English). The root word for plastic is same in English (plastic), French (plastique), and Spanish (plastico). To get a general idea of how many of these foods are packaged in plastic, you can look through the packaging_tags column for the string “plasti”. # Find entries containing &quot;plasti&quot;: plastic plastic &lt;- str_detect(food3$packaging_tags, &quot;plasti&quot;) # Print the sum of plastic sum(plastic) ## [1] 0 6.5 School Attendance Data In this section we will work with attendance data from public schools in the US, organized by school level and state, during the 2007-2008 academic year. The data contain information on average daily attendance (ADA) as a percentage of total enrollment, school day length, and school year length. # Load the gdata package library(gdata) # Import the spreadsheet: att. NOTE: Requires perl to be installed url &lt;- &#39;http://s3.amazonaws.com/assets.datacamp.com/production/course_1294/datasets/attendance.xls&#39; att &lt;- read.xls(url) # Print the column names names(att) ## [1] &quot;Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08&quot; ## [2] &quot;X&quot; ## [3] &quot;X.1&quot; ## [4] &quot;X.2&quot; ## [5] &quot;X.3&quot; ## [6] &quot;X.4&quot; ## [7] &quot;X.5&quot; ## [8] &quot;X.6&quot; ## [9] &quot;X.7&quot; ## [10] &quot;X.8&quot; ## [11] &quot;X.9&quot; ## [12] &quot;X.10&quot; ## [13] &quot;X.11&quot; ## [14] &quot;X.12&quot; ## [15] &quot;X.13&quot; ## [16] &quot;X.14&quot; ## [17] &quot;X.15&quot; # Print the first 6 rows head(att) ## Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08 ## 1 ## 2 ## 3 1 ## 4 United States ........ ## 5 Alabama ................. ## 6 Alaska .................. ## X ## 1 Total elementary, secondary, and combined elementary/secondary schools ## 2 ADA as percent of enrollment ## 3 2 ## 4 93.1 ## 5 93.8 ## 6 89.9 ## X.1 X.2 X.3 X.4 ## 1 ## 2 Average hours in school day Average days in school year ## 3 3 4 ## 4 (0.22) 6.6 (0.02) 180 ## 5 (1.24) 7.0 (0.07) 180 ## 6 (1.22) 6.5 (0.05) 180 ## X.5 X.6 X.7 X.8 ## 1 Elementary schools ## 2 Average hours in school year ADA as percent of enrollment ## 3 5 6 ## 4 (0.1) 1,193 (3.1) 94.0 ## 5 (0.8) 1,267 (12.3) 93.8 ## 6 (3.4) 1,163 (22.9) 91.3 ## X.9 X.10 X.11 X.12 ## 1 Secondary schools ## 2 Average hours in school day ADA as percent of enrollment ## 3 7 8 ## 4 (0.27) 6.7 (0.02) 91.1 ## 5 (1.84) 7.0 (0.08) 94.6 ## 6 (1.56) 6.5 (0.05) 93.2 ## X.13 X.14 X.15 ## 1 ## 2 Average hours in school day ## 3 9 ## 4 (0.43) 6.6 (0.04) ## 5 (0.38) 7.1 (0.17) ## 6 (1.57) 6.2 (0.15) # Print the last 6 rows tail(att) ## Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08 ## 54 Wisconsin ............... ## 55 Wyoming ................. ## 56 â Not applicable. ## 57 â¡Reporting standards not met (too few cases). ## 58 NOTE: Averages reflect data reported by schools rather than state requirements. School-reported length of day may exceed state requirements, and there is a range of statistical error in reported estimates. Standard errors appear in parentheses. ## 59 SOURCE: U.S. Department of Education, National Center for Education Statistics, Schools and Staffing Survey (SASS), \\\\Public School Questionnaire ## X X.1 X.2 ## 54 95.0 (0.57) 6.9 ## 55 92.4 (1.15) 6.9 ## 56 ## 57 ## 58 ## 59 \\\\ 2003-04 and 2007-08. (This table was prepared June 2011.) ## X.3 X.4 X.5 X.6 X.7 X.8 X.9 X.10 X.11 X.12 X.13 X.14 ## 54 (0.04) 180 (0.7) 1,246 (8.6) 95.4 (0.41) 6.9 (0.05) 93.0 (1.91) 7.0 ## 55 (0.05) 175 (1.3) 1,201 (8.3) 92.2 (1.65) 6.9 (0.05) 92.4 (0.75) 7.0 ## 56 ## 57 ## 58 ## 59 ## X.15 ## 54 (0.14) ## 55 (0.07) ## 56 ## 57 ## 58 ## 59 # Print the structure str(att) ## &#39;data.frame&#39;: 59 obs. of 17 variables: ## $ Table.43..Average.daily.attendance..ADA..as.a.percentage.of.total.enrollment..school.day.length..and.school.year.length.in.public.schools..by.school.level.and.state..2007.08: Factor w/ 58 levels &quot;&quot;,&quot; United States ........&quot;,..: 1 1 3 2 6 7 8 9 10 11 ... ## $ X : Factor w/ 42 levels &quot;&quot;,&quot;\\\\ 2003-04 and 2007-08. (This table was prepared June 2011.)&quot;,..: 42 41 3 22 28 8 6 14 23 29 ... ## $ X.1 : Factor w/ 45 levels &quot;&quot;,&quot;(0.22)&quot;,&quot;(0.23)&quot;,..: 1 1 1 2 22 21 41 27 14 6 ... ## $ X.2 : Factor w/ 14 levels &quot;&quot;,&quot;3&quot;,&quot;6.2&quot;,&quot;6.3&quot;,..: 1 14 2 7 11 6 5 10 3 11 ... ## $ X.3 : Factor w/ 14 levels &quot;&quot;,&quot;(0.02)&quot;,&quot;(0.03)&quot;,..: 1 1 1 2 7 5 9 6 7 5 ... ## $ X.4 : Factor w/ 15 levels &quot;&quot;,&quot;171&quot;,&quot;172&quot;,..: 1 15 14 10 10 10 11 9 11 2 ... ## $ X.5 : Factor w/ 22 levels &quot;&quot;,&quot;(0.0)&quot;,&quot;(0.1)&quot;,..: 1 1 1 3 10 21 19 4 6 12 ... ## $ X.6 : Factor w/ 48 levels &quot;&quot;,&quot;1,102&quot;,&quot;1,117&quot;,..: 1 48 47 26 45 15 13 36 5 28 ... ## $ X.7 : Factor w/ 48 levels &quot;&quot;,&quot;(10.1)&quot;,&quot;(10.3)&quot;,..: 1 1 1 36 11 35 22 6 13 48 ... ## $ X.8 : Factor w/ 40 levels &quot;&quot;,&quot;6&quot;,&quot;81.0&quot;,..: 40 39 2 22 20 9 5 13 27 25 ... ## $ X.9 : Factor w/ 51 levels &quot;&quot;,&quot;(0.24)&quot;,&quot;(0.25)&quot;,..: 1 1 1 4 32 25 48 35 20 13 ... ## $ X.10 : Factor w/ 14 levels &quot;&quot;,&quot;6.2&quot;,&quot;6.3&quot;,..: 1 14 10 7 11 5 4 9 3 11 ... ## $ X.11 : Factor w/ 13 levels &quot;&quot;,&quot;(0.02)&quot;,&quot;(0.03)&quot;,..: 1 1 1 2 8 5 10 8 5 7 ... ## $ X.12 : Factor w/ 41 levels &quot;&quot;,&quot;8&quot;,&quot;85.8&quot;,..: 41 40 2 18 34 29 7 17 8 19 ... ## $ X.13 : Factor w/ 47 levels &quot;&quot;,&quot;(0.35)&quot;,&quot;(0.37)&quot;,..: 1 1 1 6 4 24 42 34 22 19 ... ## $ X.14 : Factor w/ 17 levels &quot;&quot;,&quot;5.9&quot;,&quot;6.1&quot;,..: 1 17 15 8 13 4 6 10 3 12 ... ## $ X.15 : Factor w/ 29 levels &quot;&quot;,&quot;(0.03)&quot;,&quot;(0.04)&quot;,..: 1 1 1 3 14 12 20 7 17 8 ... In the table there is some metadata placed at the bottom of the table, we can remove this. Also some of the columns don’t contain attendance figures, they contain daily hours in the even odd number columns 3-17, so we can remove these too. # Create remove for the rows remove &lt;- c(3,56:59) # Create att2 att2 &lt;- att[-remove, ] # Create remove for the odd columns remove &lt;- seq(3,17,2) # Create att3 att3 &lt;- att2[, -remove] In this data frame, columns 1, 6, and 7 represent attendance data for US elementary schools, columns 1, 8, and 9 represent data for secondary schools, and columns 1 through 5 represent data for all schools in the US. Each of these should be stored as its own separate data frame and split accordingly. # Subset just elementary schools: att_elem att_elem &lt;- att3[,c(1,6,7)] # Subset just secondary schools: att_sec att_sec &lt;- att3[,c(1,8,9)] # Subset all schools: att4 att4 &lt;- att3[,1:5] Next we can assign column names to the variables, then remove the now un-neccessary first two columns # Define cnames vector cnames &lt;- c(&quot;state&quot;, &quot;avg_attend_pct&quot;, &quot;avg_hr_per_day&quot;, &quot;avg_day_per_yr&quot;, &quot;avg_hr_per_yr&quot;) # Assign column names of att4 colnames(att4) &lt;- cnames # Remove first two rows of att4: att5 att5 &lt;- att4[-c(1,2), ] # View the names of att5 names(att5) ## [1] &quot;state&quot; &quot;avg_attend_pct&quot; &quot;avg_hr_per_day&quot; &quot;avg_day_per_yr&quot; ## [5] &quot;avg_hr_per_yr&quot; Next the state variable has periods (.) for spaces and trailing charecters to pad the length for the field. We can tidy this up. # View the head of att5 head(att5) ## state avg_attend_pct avg_hr_per_day avg_day_per_yr ## 4 United States ........ 93.1 6.6 180 ## 5 Alabama ................. 93.8 7.0 180 ## 6 Alaska .................. 89.9 6.5 180 ## 7 Arizona ................. 89.0 6.4 181 ## 8 Arkansas ................ 91.8 6.9 179 ## 9 California .............. 93.2 6.2 181 ## avg_hr_per_yr ## 4 1,193 ## 5 1,267 ## 6 1,163 ## 7 1,159 ## 8 1,229 ## 9 1,129 # Remove all periods in state column att5$state &lt;- str_replace_all(att5$state, &quot;\\\\.&quot;, &quot;&quot;) # Remove white space around state names att5$state &lt;- str_trim(att5$state) # View the head of att5 head(att5) ## state avg_attend_pct avg_hr_per_day avg_day_per_yr avg_hr_per_yr ## 4 United States 93.1 6.6 180 1,193 ## 5 Alabama 93.8 7.0 180 1,267 ## 6 Alaska 89.9 6.5 180 1,163 ## 7 Arizona 89.0 6.4 181 1,159 ## 8 Arkansas 91.8 6.9 179 1,229 ## 9 California 93.2 6.2 181 1,129 Looking at the first few lines we can see that some of the data types are incorrect - upon import, numerical data has come in as character strings and is currently as factor variables. # View the structure str(att5) ## &#39;data.frame&#39;: 52 obs. of 5 variables: ## $ state : chr &quot;United States&quot; &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; ... ## $ avg_attend_pct: Factor w/ 42 levels &quot;&quot;,&quot;\\\\ 2003-04 and 2007-08. (This table was prepared June 2011.)&quot;,..: 22 28 8 6 14 23 29 5 7 11 ... ## $ avg_hr_per_day: Factor w/ 14 levels &quot;&quot;,&quot;3&quot;,&quot;6.2&quot;,&quot;6.3&quot;,..: 7 11 6 5 10 3 11 6 8 10 ... ## $ avg_day_per_yr: Factor w/ 15 levels &quot;&quot;,&quot;171&quot;,&quot;172&quot;,..: 10 10 10 11 9 11 2 11 11 11 ... ## $ avg_hr_per_yr : Factor w/ 48 levels &quot;&quot;,&quot;1,102&quot;,&quot;1,117&quot;,..: 26 45 15 13 36 5 28 19 31 42 ... # Change columns to numeric using dplyr (one way to acheive numeric conversion) library(dplyr) example &lt;- mutate_at(att5, c(2:5), funs(as.numeric)) # Define vector containing numerical columns: cols cols &lt;- c(2:5) # Use sapply to coerce cols to numeric (another way to convert) att5[, cols] &lt;- sapply(att5[,cols], as.numeric) # View the structure str(att5) ## &#39;data.frame&#39;: 52 obs. of 5 variables: ## $ state : chr &quot;United States&quot; &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; ... ## $ avg_attend_pct: num 22 28 8 6 14 23 29 5 7 11 ... ## $ avg_hr_per_day: num 7 11 6 5 10 3 11 6 8 10 ... ## $ avg_day_per_yr: num 10 10 10 11 9 11 2 11 11 11 ... ## $ avg_hr_per_yr : num 26 45 15 13 36 5 28 19 31 42 ... "],
["data-visualization.html", "7 Data Visualization 7.1 Base R Graphics 7.2 Different Plot Types 7.3 Adding details to plots 7.4 Adding text 7.5 How much is too much 7.6 Advanced Plot Customisation 7.7 Other graphics systems", " 7 Data Visualization Notes taken during/inspired by the Datacamp course ‘Data Visualization in R’ by Ronald Pearson. 7.1 Base R Graphics Two basic types of data visualizations: Exploratory visualizations help us understand the data Explanatory visualizations help us share ourunderstanding with others There are four graphics systems in R: Base graphics: Easiest to learn and focus of this course Grid graphics: powerful set of modules for building othertools Lattice graphics: general purpose system based on grid graphics ggplot2: “the grammar of graphics” Grid graphics allows more control of the output but is more complex, typically it is used by package developers and both lattice and ggplot2 build on this. # Load MASS package library(MASS) # Plot whiteside data plot(whiteside) # Plot Gas vs. Temp plot(whiteside$Temp, whiteside$Gas, ylab = &quot;Heating gas consumption&quot;, xlab = &quot;Outside temperature&quot;) # Apply the plot() function to Insul - R will automatically change the plot type to suit the data (a factor) plot(whiteside$Insul) Adding additional details to your explanatory plots can help emphasize certain aspects of your data. For example, by specifying the pch and col arguments to the plot() function, you can add different point shapes and colors to show how different variables or subsets of your data relate to each other. In addition, you can add a new set of points to your existing scatterplot with the points() function, and add reference lines with the abline() function. # Plot Max.Price vs. Price as red triangles plot(Cars93$Price, Cars93$Max.Price, pch = 17, col = &quot;red&quot;) # Add Min.Price vs. Price as blue circles points(Cars93$Price, Cars93$Min.Price, pch = 16, col = &quot;blue&quot;) # Add an equality reference line with abline() - lty=2 makes a dashed line abline(a = 0, b = 1, lty = 2) You can plot multiple graphs on a single pane using the par() function with its mfrow parameter. For example, par(mfrow = c(1, 2)) creates a plot array with 1 row and 2 columns, allowing you to view two graphs side-by-side. This way, you can compare and contrast different datasets or different views of the same dataset. This exercise asks you to compare two views of the Animals2 dataset from the robustbase package, differing in how its variables are represented. The objective of this exercise is to emphasize that the original representation of the variables that we have in a dataset is not always the best one for visualization or analysis. By representing the original variables in log scale, for example, we can better see and understand the data. # Load the robustbase package library(robustbase) # Set up the side-by-side plot array par(mfrow = c(1, 2)) # First plot: brain vs. body in its original form plot(Animals2$body, Animals2$brain) # Add the first title title(&quot;Original representation&quot;) # Second plot: log-log plot of brain vs. body plot(Animals2$body, Animals2$brain, log = &quot;xy&quot;) # Add the second title title(&quot;Log-log plot&quot;) Sometimes, log-transforming the x and y variables by specifying log = “xy” inside the plot() function can make it a lot easier to see the direct relationship between the two variables. Similarly, you can specify log = “y” to log-transform only the y variable and log = “x” to log-transform only the x variable. 7.1.1 Avoiding pie charts The same dataset can be displayed or summarized in many different ways, but some are much more suitable than others. Despite their general popularity, pie charts are often a poor choice. Though R allows pie charts with the pie() function, even the help file for this function argues against their use. Specifically, the help file includes a “Note” that begins with the words: “Pie charts are a very bad way of displaying information.” Bar charts are a recommended alternative. # Load the insuranceData package library(insuranceData) # Use the data() function to get the dataCar data frame data(dataCar) # Set up a side-by-side plot array par(mfrow = c(1, 2)) # Create a table of veh_body record counts and sort tbl &lt;- sort(table(dataCar$veh_body), decreasing = TRUE) # Create the pie chart and give it a title pie(tbl) + title(&quot;Pie chart&quot;) ## integer(0) # Create the barplot with perpendicular, half-sized labels barplot(tbl, las = 2, cex.names = 0.5) # Add a title title(&quot;Bar chart&quot;) 7.2 Different Plot Types Histograms are probably the best-known way of looking at how the values of a numerical variable are distributed over their range, and R provides several different histogram implementations. The purpose of this exercise is to introduce two of these: hist() is part of base R and its default option yields a histogram based on the number of times a record falls into each of the bins on which the histogram is based. truehist() is from the MASS package and scales these counts to give an estimate of the probability density. # Set up a side-by-side plot array par(mfrow = c(1, 2)) # Create a histogram of counts with hist() hist(Cars93$Horsepower, main = &quot;hist() plot&quot;) # Create a normalized histogram with truehist() truehist(Cars93$Horsepower, main = &quot;truehist() plot&quot;) While they are probably not as well known as the histogram, density estimates may be regarded as smoothed histograms, designed to give a better estimate of the density function for a random variable. In this exercise, you’ll use the ChickWeight dataset, which contains a collection of chicks’ weights. You will first select for the chicks that are 16 weeks old. Then, you’ll create a histogram using the truehist() function, and add its density plot on top, using the lines() and density() functions with their default options. The density plot of this type of variable is often expected to conform approximately to the bell-shaped curve, otherwise known as the Gaussian distribution. Let’s find out whether that’s the case for this dataset. # Create index16, pointing to 16-week chicks index16 &lt;- which(ChickWeight$Time == 16) # Get the 16-week chick weights weights &lt;- ChickWeight$weight[index16] # Plot the normalized histogram truehist(weights) # Add the density curve to the histogram lines(density(weights)) A practical limitation of both histograms and density estimates is that, if we want to know whether the Gaussian distribution assumption is reasonable for our data, it is difficult to tell. The quantile-quantile plot, or QQ-plot, is a useful alternative: we sort our data, plot it against a specially-designed x-axis based on our reference distribution (e.g., the Gaussian “bell curve”), and look to see whether the points lie approximately on a straight line. In R, several QQ-plot implementations are available, but the most convenient one is the qqPlot() function in the car package. The first part of this exercise applies this function to the 16-week chick weight data considered in the last exercise, to show that the Gaussian distribution appears to be reasonable here. The second part of the exercise applies this function to another variable where the Gaussian distribution is obviously a poor fit, but the results also show the presence of repeated values (flat stretches in the plot) and portions of the data range where there are no observations (vertical “jumps” in the plot). # Load the car package to make qqPlot() available library(car) ## Warning: package &#39;car&#39; was built under R version 3.4.2 # Show the normal QQ-plot of the chick weights qqPlot(weights) # Show the normal QQ-plot of the Boston$tax data qqPlot(Boston$tax) The points lie in a relatively straight line only if the data is normally distributed (ie. Its density plot is approximately a Gaussian distribution). So looking at the second QQ-plot, you can see that the tax variable in the Boston data frame is definitely not normally distributed. A scatterplot represents each (x, y) pair in a dataset by a single point. If some of these pairs are repeated (i.e. if the same combination of x and y values appears more than once and thus lie on top of each other), we can’t see this in a scatterplot. Several approaches have been developed to deal with this problem, including jittering, which adds small random values to each x and y value, so repeated points will appear as clusters of nearby points. A useful alternative that is equally effective in representing repeated data points is the sunflowerplot, which represents each repeated point by a “sunflower,” with one “petal” for each repetition of a data point. Note the large cluster in the upper left corner of the plot, indicating that records with the maximum rad value and zero zn value occur many times. # Set up a side-by-side plot array par(mfrow = c(1, 2)) # Create the standard scatterplot plot(rad ~ zn, data = Boston) # Add the title title(&quot;Standard scatterplot&quot;) # Create the sunflowerplot sunflowerplot(rad ~ zn, data = Boston) # Add the title title(&quot;Sunflower plot&quot;) The boxplot() function shows how the distribution of a numerical variable y differs across the unique levels of a second variable, x. To be effective, this second variable should not have too many unique levels (e.g., 10 or fewer is good; many more than this makes the plot difficult to interpret). The boxplot() function also has a number of optional parameters and this exercise asks you to use three of them to obtain a more informative plot: varwidth allows for variable-width boxplots that show the different sizes of the data subsets. log allows for log-transformed y-values. las allows for more readable axis labels. This exercise also illustrates the use of the formula interface: y ~ x indicates that we want a boxplot of the y variable across the different levels of the x variable. You can see from the wider boxplots for rad = 4, 5, and 24 how much larger these subsets are than the others. # Create a variable-width boxplot with log y-axis &amp; horizontal labels boxplot(crim ~ rad, data = Boston, varwidth = TRUE, log = &quot;y&quot;, las = 1 ) # Add a title title(&quot;Crime rate vs. radial highway index&quot;) A mosaic plot may be viewed as a scatterplot between categorical variables and it is supported in R with the mosaicplot() function. As this example shows, in addition to categorical variables, this plot can also be useful in understanding the relationship between numerical variables, either integer- or real-valued, that take only a few distinct values. More specifically, this exercise asks you to construct a mosaic plot showing the relationship between the numerical carb and cyl variables from the mtcars data frame, variables that exhibit 6 and 3 unique values, respectively. # Create a mosaic plot using the formula interface mosaicplot(carb ~ cyl, data = mtcars) A single box plot gives a graphical representation of the range of variation in a numerical variable, based on five numbers: The minimum and maximum values The median (or “middle”) value Two intermediate values called the lower and upper quartiles In addition, the standard box plot computes a nominal data range from three of these numbers and flags points falling outside this range as outliers, representing them as distinct points. The bag plot extends this representation to two numerical variables, showing their relationship, both within two-dimensional “bags” corresponding to the “box” in the standard boxplot, and indicating outlying points outside these limits. # Create a side-by-side boxplot summary boxplot(Cars93$Min.Price, Cars93$Max.Price) # Load aplpack to make the bagplot() function available library(aplpack) ## Loading required package: tcltk # Create a bagplot for the same two variables bagplot(Cars93$Min.Price, Cars93$Max.Price, cex = 1.2) # Add an equality reference line abline(a = 0, b = 1, lty = 2) Correlation matrices were introduced in the video as a useful tool for obtaining a preliminary view of the relationships between multiple numerical variables. This exercise asks you to use the corrplot() function from the corrplot package to visualize this correlation matrix for the numerical variables from the UScereal data frame in the MASS package. Recall that in this version of these plots, ellipses that are thin and elongated indicate a large correlation value between the indicated variables, while ellipses that are nearly circular indicate correlations near zero. # Load the corrplot library for the corrplot() function library(corrplot) # Extract the numerical variables from UScereal numericalVars &lt;- UScereal[, 2:10] # Compute the correlation matrix for these variables corrMat &lt;- cor(numericalVars) # Generate the correlation ellipse plot corrplot(corrMat, method = &quot;ellipse&quot;) Decision trees represent a popular form of predictive model because they are easy to visualize and explain, rpart package is probably the most popular of several R packages that can be used to build and visualize these models. # Load the rpart library library(rpart) # Fit an rpart model to predict medv from all other Boston variables tree_model &lt;- rpart(medv ~., data = Boston) # Plot the structure of this decision tree model plot(tree_model) # Add labels to this plot text(tree_model, cex = 0.7) 7.3 Adding details to plots You already saw how the mfrow parameter to the par() function could be used to plot multiple graphs in one pane. The par() function also allows you to set many other graphics parameters, whose values will remain in effect until they are reset by a subsequent par() call. Specifically, a call to the par() function with no parameters specified will return a list whose element names each specify a graphics parameter and whose element values specify the corresponding default values of these parameters. These parameters may be set by a call in the form par(name = value) where name is the name of the parameter to be set and value is the value to be assigned to this parameter. The following section shows the number of functions contained within the base R par function. # Assign the return value from the par() function to plot_pars plot_pars &lt;- par() # Display the names of the par() function&#39;s list elements names(plot_pars) ## [1] &quot;xlog&quot; &quot;ylog&quot; &quot;adj&quot; &quot;ann&quot; &quot;ask&quot; ## [6] &quot;bg&quot; &quot;bty&quot; &quot;cex&quot; &quot;cex.axis&quot; &quot;cex.lab&quot; ## [11] &quot;cex.main&quot; &quot;cex.sub&quot; &quot;cin&quot; &quot;col&quot; &quot;col.axis&quot; ## [16] &quot;col.lab&quot; &quot;col.main&quot; &quot;col.sub&quot; &quot;cra&quot; &quot;crt&quot; ## [21] &quot;csi&quot; &quot;cxy&quot; &quot;din&quot; &quot;err&quot; &quot;family&quot; ## [26] &quot;fg&quot; &quot;fig&quot; &quot;fin&quot; &quot;font&quot; &quot;font.axis&quot; ## [31] &quot;font.lab&quot; &quot;font.main&quot; &quot;font.sub&quot; &quot;lab&quot; &quot;las&quot; ## [36] &quot;lend&quot; &quot;lheight&quot; &quot;ljoin&quot; &quot;lmitre&quot; &quot;lty&quot; ## [41] &quot;lwd&quot; &quot;mai&quot; &quot;mar&quot; &quot;mex&quot; &quot;mfcol&quot; ## [46] &quot;mfg&quot; &quot;mfrow&quot; &quot;mgp&quot; &quot;mkh&quot; &quot;new&quot; ## [51] &quot;oma&quot; &quot;omd&quot; &quot;omi&quot; &quot;page&quot; &quot;pch&quot; ## [56] &quot;pin&quot; &quot;plt&quot; &quot;ps&quot; &quot;pty&quot; &quot;smo&quot; ## [61] &quot;srt&quot; &quot;tck&quot; &quot;tcl&quot; &quot;usr&quot; &quot;xaxp&quot; ## [66] &quot;xaxs&quot; &quot;xaxt&quot; &quot;xpd&quot; &quot;yaxp&quot; &quot;yaxs&quot; ## [71] &quot;yaxt&quot; &quot;ylbias&quot; # Display the number of par() function list elements length(plot_pars) ## [1] 72 One of the important graphics parameters that can be set with the par() function is mfrow, which specifies the numbers of rows and columns in an array of plots. Valid values for this parameter are two-element numerical vectors, whose first element specifies the number of rows in the plot array and the second element specifies the number of rows. A more detailed discussion of using the mfrow parameter is given in Chapter 4 of this course. For now, note that to specify a plot array with three rows and one column, the command would be par(mfrow = c(3, 1)). The following exercise also introduces the type parameter for the plot() command, which specifies how the plot is drawn. The specific type values used here are: “p” for “points” “l” for “lines” “o” for “overlaid” (i.e., lines overlaid with points) “s” for “steps” # Set up a 2-by-2 plot array par(mfrow = c(2, 2)) # Plot the Animals2 brain weight data as points plot(Animals2$brain, type = &quot;p&quot;) # Add the title title(&quot;points&quot;) # Plot the brain weights with lines plot(Animals2$brain, type = &quot;l&quot;) # Add the title title(&quot;lines&quot;) # Plot the brain weights as lines overlaid with points plot(Animals2$brain, type = &quot;o&quot;) # Add the title title(&quot;overlaid&quot;) # Plot the brain weights as steps plot(Animals2$brain, type = &quot;s&quot;) # Add the title title(&quot;steps&quot;) The type = “n” option was discussed in the video and this exercise provides a simple example. This option is especially useful is when we are plotting data from multiple sources on a common set of axes. In such cases, we can compute ranges for the x- and y-axes so that all data points will appear on the plot, and then add the data with subsequent calls to points() or lines() as appropriate. This exercise asks you to generate a plot that compares mileage vs. horsepower data from two different sources: the mtcars data frame in the datasets package and the Cars93 data frame in the MASS package. To distinguish the different results from these data sources, the graphics parameter pch is used to specify point shapes. See ?points for a comprehensive list of some pch values and their corresponding point shapes. # Compute max_hp max_hp &lt;- max(Cars93$Horsepower, mtcars$hp) # Compute max_mpg max_mpg &lt;- max(Cars93$MPG.city, Cars93$MPG.highway, mtcars$mpg) # Create plot with type = &quot;n&quot; plot(max_mpg, max_hp, type = &quot;n&quot;, xlim = c(0, max_hp), ylim = c(0, max_mpg), xlab = &quot;Horsepower&quot;, ylab = &quot;Miles per gallon&quot;) # Add open circles to plot points(mtcars$hp,mtcars$mpg, pch = 1) # Add solid squares to plot points(Cars93$Horsepower, Cars93$MPG.city, pch = 15) # Add open triangles to plot points(Cars93$Horsepower, Cars93$MPG.highway, pch = 2) Numerical data is often assumed to conform approximately to a Gaussian probability distribution, characterized by the bell curve. One point of this exercise is to show what this bell curve looks like for exactly Gaussian data and the other is to show how the lines() function can be used to add lines to an existing plot. The curves you are asked to draw here have the same basic shape but differ in their details (specifically, the means and standard deviations of these Gaussian distributions are different). For this reason, it is useful to draw these curves with different line types to help us distinguish them. Note that line types are set by the lty argument, with the default value lty = 1 specifying solid lines, lty = 2 specifying dashed lines, and lty = 3 specifying dotted lines. Also note that the lwd argument specifies the relative width. # Create the numerical vector x x &lt;- seq(0, 10, length = 200) # Compute the Gaussian density for x with mean 2 and standard deviation 0.2 gauss1 &lt;- dnorm(x, mean = 2, sd = 0.2) # Compute the Gaussian density with mean 4 and standard deviation 0.5 gauss2 &lt;- dnorm(x, mean = 4, sd = 0.5) # Plot the first Gaussian density plot(x, gauss1, type = &quot;l&quot;, ylab = &quot;Gaussian probability density&quot;) # Add lines for the second Gaussian density lines(x, gauss2, lty = 2, lwd = 3) One advantage of specifying the pch argument locally is that, in a call to functions like plot() or points(), local specification allows us to make pch depend on a variable in our dataset. This provides a simple way of indicating different data subsets with different point shapes or symbols. The first plot specifies the point shapes using numerical values of the pch argument defined by the cyl variable in the mtcars data frame. The second plot illustrates the fact that pch can also be specified as a vector of single characters, causing each point to be drawn as the corresponding character. # Create an empty plot using type = &quot;n&quot; plot(mtcars$hp, mtcars$mpg, type = &quot;n&quot;, xlab = &quot;Horsepower&quot;, ylab = &quot;Gas mileage&quot;) # Add points with shapes determined by cylinder number points(mtcars$hp, mtcars$mpg, pch = mtcars$cyl) # Create a second empty plot plot(mtcars$hp, mtcars$mpg, type = &quot;n&quot;, xlab = &quot;Horsepower&quot;, ylab = &quot;Gas mileage&quot;) # Add points with shapes as cylinder characters points(mtcars$hp, mtcars$mpg, pch = as.character(mtcars$cyl)) The low-level plot function abline() adds a straight line to an existing plot. This line is specified by an intercept parameter a and a slope parameter b, and the simplest way to set these parameters is directly. For example, the command abline(a = 0, b = 1) adds an equality reference line with zero intercept and unit (i.e. 1) slope: points for which y = x fall on this reference line, while points with y &gt; x fall above it, and points with y &lt; x fall below it. An alternative way of specifying these parameters is through a linear regression model that determines them from data. One common application is to generate a scatterplot of y versus x, then fit a linear model that predicts y from x, and finally call abline() to add this best fit line to the plot. The standard R function that fits linear regression models is lm(), which supports the formula interface. Thus, to fit a linear model that predicts y from x in the data frame df, the call would be lm(y ~ x, data = df). This call returns a linear model object, which can then be passed as an argument to the abline() function to draw the desired line on our plot. # Build a linear regression model for the whiteside data linear_model &lt;- lm(Gas ~ Temp, data = whiteside) # Create a Gas vs. Temp scatterplot from the whiteside data plot(whiteside$Temp, whiteside$Gas) # Use abline() to add the linear regression line abline(linear_model, lty = 2) 7.4 Adding text One of the main uses of the text() function is to add informative labels to a data plot. The text() function takes three arguments: x, which specifies the value for the x variable, y, which specifies the value for the y variable, and label, which specifies the label for the x-y value pair. This exercise asks you to first create a scatterplot of city gas mileage versus horsepower from the Cars93 data, then identify an interesting subset of the data (i.e. the 3-cylinder cars) and label these points. You will find that assigning a vector to the x, y, and label arguments to text() will result in labeling multiple points at once. # Create MPG.city vs. Horsepower plot with solid squares plot(Cars93$Horsepower, Cars93$MPG.city, pch = 15) # Create index3, pointing to 3-cylinder cars index3 &lt;- which(Cars93$Cylinders == &quot;3&quot;) # Add text giving names of cars next to data points text(x = Cars93$Horsepower[index3], y = Cars93$MPG.city[index3], labels = Cars93$Make[index3], adj = 0) It was noted that the adj argument to the text() function determines the horizontal placement of the text and it can take any value between 0 and 1. In fact, this argument can take values outside this range. That is, making this value negative causes the text to start to the right of the specified x position. Similarly, making adj greater than 1 causes the text to end to the left of the x position. Another useful optional argument for the text() function is cex, which scales the default text size. As a specific example, setting cex = 1.5 increases the text size by 50 percent, relative to the default value. Similarly, specifying cex = 0.8 reduces the text size by 20 percent. Finally, the third optional parameter used here is font, which can be used to specify one of four text fonts: font = 1 is the default text font (neither italic nor bold), font = 2 specifies bold face text, font = 3 specifies italic text, and font = 4 specifies both bold and italic text. # Plot MPG.city vs. Horsepower as open circles plot(Cars93$Horsepower, Cars93$MPG.city, pch = 1) # Create index3, pointing to 3-cylinder cars index3 &lt;- which(Cars93$Cylinders == 3) # Highlight 3-cylinder cars as solid circles points(Cars93$Horsepower[index3], Cars93$MPG.city[index3], pch = 16) # Add car names, offset from points, with larger bold text text( Cars93$Horsepower[index3], Cars93$MPG.city[index3], Cars93$Make[index3], adj = -0.2, cex = 1.2, font = 4) In addition to the optional arguments used in the previous exercises, the text() function also supports a number of other optional arguments that can be used to enhance the text. This exercise uses the cex argument to reduce the text size and introduces two new arguments. The first is the col argument that specifies the color used to display the text, and the second is the srt argument that allows us to rotate the text. Color has been used in several of the previous exercises to specify point colors, and the effective use of color is discussed further in Chapter 5. One of the points of this exercise is to show that the specification of text color with the text() function is essentially the same as the specification of point color with the plot() function. As a specific example, setting col = “green” in the text() function causes the text to appear in green. If col is not specified, the text appears in the default color set by the par() function, which is typically black. The srt parameter allows us to rotate the text through an angle specified in degrees. The typical default value (set by the par() function) is 0, causing the text to appear horizontally, reading from left to right. Specifying srt = 90 causes the text to be rotated 90 degrees counter-clockwise so that it reads from bottom to top instead of left to right. # Plot Gas vs. Temp as solid triangles plot(whiteside$Temp, whiteside$Gas, pch=17) # Create indexB, pointing to &quot;Before&quot; data indexB &lt;- which(whiteside$Insul == &quot;Before&quot;) # Create indexA, pointing to &quot;After&quot; data indexA &lt;- which(whiteside$Insul == &quot;After&quot;) # Add &quot;Before&quot; text in blue, rotated 30 degrees, 80% size text(x = whiteside$Temp[indexB], y = whiteside$Gas[indexB], labels = &quot;Before&quot;, col = &#39;blue&#39;, srt = 30, cex = .8) # Add &quot;After&quot; text in red, rotated -20 degrees, 80% size text(x = whiteside$Temp[indexA], y = whiteside$Gas[indexA], labels = &quot;After&quot;, col = &#39;red&#39;, srt = -20, cex = .8) 7.4.1 Adding details to plots It is possible to add your own legend and custom axix, both in terms of scale and positioning i.e. at the top rather than at the bottom. This section gives some examples on how to acheive this. # Set up and label empty plot of Gas vs. Temp plot(whiteside$Temp, whiteside$Gas, type = &quot;n&quot;, xlab = &quot;Outside temperature&quot;, ylab = &quot;Heating gas consumption&quot;) # Create indexB, pointing to &quot;Before&quot; data indexB &lt;- which(whiteside$Insul == &quot;Before&quot;) # Create indexA, pointing to &quot;After&quot; data indexA &lt;- which(whiteside$Insul == &quot;After&quot;) # Add &quot;Before&quot; data as solid triangles points(whiteside$Temp[indexB], whiteside$Gas[indexB], pch = 17) # Add &quot;After&quot; data as open circles points(whiteside$Temp[indexA], whiteside$Gas[indexA], pch = 1) # Add legend that identifies points as &quot;Before&quot; and &quot;After&quot; legend(&quot;topright&quot;, pch = c(17, 1), legend = c(&quot;Before&quot;, &quot;After&quot;)) Typical base graphics functions like boxplot() provide x- and y-axes by default, with a label for the x-axis below the plot and one for the y-axis label to the left of the plot. These labels are generated automatically from the variable names used to generate the plot. Sometimes, we want to provide our own axes labels, and R makes this possible in two steps: first, we suppress the default axes when we create the plot by specifying axes = FALSE; then, we call the low-level graphics function axis() to create the axes we want. # Create a boxplot of sugars by shelf value, without axes boxplot(sugars ~ shelf, data = UScereal, axes = F) # Add a default y-axis to the left of the boxplot axis(side = 2) # Add an x-axis below the plot, labelled 1, 2, and 3 axis(side = 1, at = c(1,2,3)) # Add a second x-axis above the plot axis(side = 3, at = c(1,2,3), labels = c(&quot;floor&quot;,&quot;middle&quot;,&quot;top&quot;)) We may want to add a curved trend line that highlights this behavior of the data and the supsmu() function represents one way of doing this. To use this function, we need to specify values for the required arguments x and y, but it also has a number of optional arguments. Here, we consider the optional bass argument, which controls the degree of smoothness in the resulting trend curve. The default value is 0, but specifying larger values (up to a maximum of 10) results in a smoother curve. This exercise asks you to use the supsmu() function to add two trend lines to a scatterplot, one using the default parameters and the other with increased smoothness. # Create a scatterplot of MPG.city vs. Horsepower plot(Cars93$Horsepower, Cars93$MPG.city) # Call supsmu() to generate a smooth trend curve, with default bass trend1 &lt;- supsmu(Cars93$Horsepower, Cars93$MPG.city) # Add this trend curve to the plot lines(trend1) # Call supsmu() for a second trend curve, with bass = 10 trend2 &lt;- supsmu(Cars93$Horsepower, Cars93$MPG.city, bass = 10) # Add this trend curve as a heavy, dotted line lines(trend2, lty = 3, lwd = 2) 7.5 How much is too much The first example presented in this file applied the plot() function to a data frame, yielding an array of scatterplots with one for each pair of columns in the data frame. Thus, the number of plots in this array is equal to the square of the number of columns in the data frame. This means that if we apply the plot() function to a data frame with many columns, we will generate an enormous array of scatterplots, each of which will be too small to be useful. The purpose of this exercise is to provide a memorable example. # Compute the number of plots to be displayed ncol(Cars93)^2 ## [1] 729 # Plot the array of scatterplots plot(Cars93) The matplot() function can be used to easily generate a plot with several scatterplots on the same set of axes. By default, the points in these scatterplots are represented by the numbers 1 through n, where n is the total number of scatterplots included, but most of the options available with the plot() function are also possible by specifying the appropriate arguments. # Construct the vector keep_vars keep_vars &lt;- c(&quot;calories&quot;, &quot;protein&quot;, &quot;fat&quot;, &quot;fibre&quot;, &quot;carbo&quot;, &quot;sugars&quot;) # Use keep_vars to extract the desired subset of UScereal df &lt;- UScereal[, keep_vars] # Set up a two-by-two plot array par(mfrow = c(2, 2)) # Use matplot() to generate an array of two scatterplots matplot( UScereal$calories, UScereal[,c(&#39;protein&#39;, &#39;fat&#39;)], xlab = &quot;calories&quot;, ylab = &quot;&quot;) # Add a title title(&quot;Two scatterplots&quot;) # Use matplot() to generate an array of three scatterplots matplot( UScereal$calories, UScereal[,c(&#39;protein&#39;, &#39;fat&#39;, &#39;fibre&#39;)], xlab = &quot;calories&quot;, ylab = &quot;&quot;) # Add a title title(&quot;Three scatterplots&quot;) # Use matplot() to generate an array of four scatterplots matplot( UScereal$calories, UScereal[,c(&#39;protein&#39;, &#39;fat&#39;, &#39;fibre&#39;, &#39;carbo&#39;)], xlab = &quot;calories&quot;, ylab = &quot;&quot;) # Add a title title(&quot;Four scatterplots&quot;) # Use matplot() to generate an array of five scatterplots matplot( UScereal$calories, UScereal[,c(&#39;protein&#39;, &#39;fat&#39;, &#39;fibre&#39;, &#39;carbo&#39;,&#39;sugars&#39;)], xlab = &quot;calories&quot;, ylab = &quot;&quot;) # Add a title title(&quot;Five scatterplots&quot;) Any data visualization loses its utility if it becomes too complex. This exercise asks you to consider this problem with wordclouds, displays that present words in varying sizes depending on their frequency. That is, more frequent words appear larger in the display, while rarer words appear in a smaller font. In R, wordclouds are easy to generate with the wordcloud() function in the wordcloud package. This function is called with a character vector of words, and a second numerical vector giving the number of times each word appears in the collection used to generate the wordcloud. Two other useful arguments for this function are scale and min.freq. The scale argument is a two-component numerical vector giving the relative size of the largest word in the display and that of the smallest word. The wordcloud only includes those words that occur at least min.freq times in the collection and the default value for this argument is 3. library(wordcloud) ## Loading required package: methods ## Loading required package: RColorBrewer mfr_table &lt;- table(Cars93$Manufacturer) # Create the default wordcloud from this table wordcloud( words = names(mfr_table), freq = as.numeric(mfr_table), scale = c(2, 0.25)) # Change the minimum word frequency wordcloud( words = names(mfr_table), freq = as.numeric(mfr_table), scale = c(2, 0.25), min.freq = 1) # Create model_table of model frequencies model_table &lt;- table(Cars93$Model) # Create the wordcloud of all model names with smaller scaling wordcloud( words = names(model_table), freq = as.numeric(model_table), scale = c(.75, .25), min.freq = 1) 7.5.1 Mutiple Plots There are a couple of ways to set up multiple plot arrays, one is mfrow. This exercise and the next one are based on the Anscombe quartet, a collection of four datasets that appear to be essentially identical on the basis of simple summary statistics like means and standard deviations. For example, the mean x-values for these datasets are identical to three digits, while the mean y-values differ only in the third digit. In spite of these apparent similarities, the behavior of the four datasets is quite different and this becomes immediately apparent when we plot them. # Set up a two-by-two plot array par(mfrow = c(2,2)) # Plot y1 vs. x1 plot(anscombe$x1, anscombe$y1) # Plot y2 vs. x2 plot(anscombe$x2, anscombe$y2) # Plot y3 vs. x3 plot(anscombe$x3, anscombe$y3) # Plot y4 vs. x4 plot(anscombe$x4, anscombe$y4) The point of this exercise is to illustrate how much more clearly we can see the differences in these datasets if we plot all of them with the same x and y ranges. This exercise also illustrates the utility of improving the x- and y-axis labels and of adding informative plot titles. # Define common x and y limits for the four plots xmin &lt;- 4 xmax &lt;- 19 ymin &lt;- 3 ymax &lt;- 13 # Set up a two-by-two plot array par(mfrow = c(2,2)) # Plot y1 vs. x1 with common x and y limits, labels &amp; title plot(anscombe$x1, anscombe$y1, xlim = c(xmin, xmax), ylim = c(ymin, ymax), xlab = &quot;x value&quot;, ylab = &quot;y value&quot;, main = &quot;First dataset&quot;) # Do the same for the y2 vs. x2 plot plot(anscombe$x2, anscombe$y2, xlim = c(xmin, xmax), ylim = c(ymin, ymax), xlab = &quot;x value&quot;, ylab = &quot;y value&quot;, main = &quot;Second dataset&quot;) # Do the same for the y3 vs. x3 plot plot(anscombe$x3, anscombe$y3, xlim = c(xmin, xmax), ylim = c(ymin, ymax), xlab = &quot;x value&quot;, ylab = &quot;y value&quot;, main = &quot;Third dataset&quot;) # Do the same for the y4 vs. x4 plot plot(anscombe$x4, anscombe$y4, xlim = c(xmin, xmax), ylim = c(ymin, ymax), xlab = &quot;x value&quot;, ylab = &quot;y value&quot;, main = &quot;Fourth dataset&quot;) This exercise illustrates this idea, giving four views of the same dataset: a plot of the raw data values themselves, a histogram of these data values, a density plot, and a normal QQ-plot. # Set up a two-by-two plot array par(mfrow = c(2,2)) # Plot the raw duration data plot(geyser$duration, main = &quot;Raw data&quot;) # Plot the normalized histogram of the duration data truehist(geyser$duration, main = &quot;Histogram&quot;) # Plot the density of the duration data plot(density(geyser$duration), main = &quot;Density&quot;) # Construct the normal QQ-plot of the duration data qqPlot(geyser$duration, main = &quot;QQ-plot&quot;) As well as the par() function, you can also use the layout() function to setup a plot array. You can think of the layout matrix as the plot pane, where a 0 represents empty space and other numbers represent the plot number, which is determined by the sequence of visualization function calls. For example, a 1 in the layout matrix refers to the visualization that was first called, a 2 refers to the plot of the second visualization call, etc. This exercise asks you to create your own 3 x 2 layout matrix, using the c() function to concatenate numbers into vectors that will form the rows of the matrix. You will then use the matrix() function to convert these rows into a matrix and apply the layout() function to set up the desired plot array. The convenience function layout.show() can then be used to verify that the plot array has the shape you want. # Use the matrix function to create a matrix with three rows and two columns layoutMatrix &lt;- matrix( c( 0, 1, 2, 0, 0, 3 ), byrow = T, nrow = 3 ) # Call the layout() function to set up the plot array layout(layoutMatrix) # Show where the three plots will go layout.show(n = 3) # Set up the plot array layout(layoutMatrix) # Construct vectors indexB and indexA indexB &lt;- which(whiteside$Insul == &quot;Before&quot;) indexA &lt;- which(whiteside$Insul == &quot;After&quot;) # Create plot 1 and add title plot(whiteside$Temp[indexB], whiteside$Gas[indexB], ylim = c(0,8)) title(&quot;Before data only&quot;) # Create plot 2 and add title plot(whiteside$Temp, whiteside$Gas, ylim = c(0,8)) title(&quot;Complete dataset&quot;) # Create plot 3 and add title plot(whiteside$Temp[indexA], whiteside$Gas[indexA], ylim = c(0,8)) title(&quot;After data only&quot;) Besides creating non-rectangular arrays, the layout() function can be used to create plot arrays with different sized component plots – something else that is not possible by setting the par() function’s mfrow parameter. # Create row1, row2, and layoutVector row1 &lt;- c(1,0,0) row2 &lt;- c(0,2,2) layoutVector &lt;- c(row1,row2, row2) # Convert layoutVector into layoutMatrix layoutMatrix &lt;- matrix(layoutVector, byrow = T, nrow = 3) # Set up the plot array layout(layoutMatrix) # Plot scatterplot plot(Boston$rad, Boston$zn) # Plot sunflower plot sunflowerplot(Boston$rad, Boston$zn) 7.6 Advanced Plot Customisation Many functions, including barplot, will return invisible vectors of useful information. For instance, bar plot returns a hdden vector that the centre positions of each bar. These return values can be especially useful when we want to overlay text on the bars of a horizontal barplot. Then, we capture the return values and use them as the y parameter in a subsequent call to the text() function, allowing us to place the text at whatever x position we want but overlaid in the middle of each horizontal bar. This exercise asks you to construct a horizontal barplot that exploits these possibilities. # Create a table of Cylinders frequencies tbl &lt;- table(Cars93$Cylinders) # Generate a horizontal barplot of these frequencies mids &lt;- barplot(tbl, horiz = T, col = &quot;transparent&quot;, names.arg = &quot;&quot;) # Add names labels with text() text(20, mids, names(tbl)) # Add count labels with text() text(35, mids, as.numeric(tbl)) The scatterplot allows us to see how one numerical variable changes with the values of a second numerical variable. The symbols() function allows us to extend scatterplots to show the influence of other variables. This function is called with the variables x and y that define a scatterplot, along with another argument that specifies one of several possible shapes. Here, you are asked to use the circles argument to create a bubbleplot where each data point is represented by a circle whose radius depends on the third variable specified by the value of this argument. # Call symbols() to create the default bubbleplot symbols(Cars93$Horsepower,Cars93$MPG.city, circles = sqrt(Cars93$Price)) # Repeat, with the inches argument specified symbols(Cars93$Horsepower,Cars93$MPG.city, circles = sqrt(Cars93$Price), inches = 0.1) R provides support for saving graphical results in several different external file formats, including jpeg, png, tiff, or pdf files. In addition, we can incorporate graphical results into external documents, using tools like the Sweave() function or the knitr package or using R Markdown. The basis for saving outputs/plots is using the png() function, which specifies the name of the png file to be generated and sets up a special environment that captures all graphical output until we exit this environment with the dev.off() command. # Call png() with the name of the file we want to create png(&#39;bubbleplot.png&#39;) # Re-create the plot from the last exercise symbols(Cars93$Horsepower, Cars93$MPG.city, circles = sqrt(Cars93$Price), inches = 0.1) # Save our file and return to our interactive session dev.off() ## png ## 2 # Verify that we have created the file list.files(pattern = &quot;png&quot;) ## [1] &quot;bubbleplot.png&quot; 7.6.1 Using colour effectively Colour can be effective but limited - for instance, for those with colour blindness. Black and white documents or prints loose some of the colour dinstinction. Illinsky and Steele suggest ideally using 6 colours, not more than 12 and certaintly no more than 20. They provided a set of recommended 12 colors, in descending order of desirability from the top of the plot to the bottom. Also, the first six “more preferred” colors are displayed with longer bars to visually emphasize their preferred status over the other six. # Iliinsky and Steele color name vector IScolors &lt;- c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;blue&quot;, &quot;black&quot;, &quot;white&quot;, &quot;pink&quot;, &quot;cyan&quot;, &quot;gray&quot;, &quot;orange&quot;, &quot;brown&quot;, &quot;purple&quot;) # Create the data for the barplot barWidths &lt;- c(rep(2, 6), rep(1, 6)) # Recreate the horizontal barplot with colored bars barplot( rev(barWidths), horiz = T, col = rev(IScolors), axes = F, names.arg = rev(IScolors), las = 1) We can use colours to improve our plots, for instance by colouring bubbles in scatter plots. # Iliinsky and Steele color name vector IScolors &lt;- c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;blue&quot;, &quot;black&quot;, &quot;white&quot;, &quot;pink&quot;, &quot;cyan&quot;, &quot;gray&quot;, &quot;orange&quot;, &quot;brown&quot;, &quot;purple&quot;) # Create the `cylinderLevel` variable cylinderLevel &lt;- Cars93$Cylinders # Create the colored bubbleplot symbols( Cars93$Horsepower, Cars93$MPG.city, circles = as.numeric(Cars93$Cylinders), inches = 0.2, bg = IScolors[cylinderLevel]) Or in barcharts, liked this stacked bar. # Create a table of Cylinders by Origin tbl &lt;- table(Cars93$Cylinders, Cars93$Origin) # Create the default stacked barplot barplot(tbl) # Enhance this plot with color barplot(tbl, col = IScolors) 7.7 Other graphics systems Base R used in this chapter is flexible and relatively easy to learn. There are other graphic systems in R, but they typically have to be used instead of one or other - Base, grid lattice, ggplot - they are generally incompatible with each other. Grid - provides a lot more control than base graphics, but has a much steeper learning curver and requires a lot more code to produce even simple plots Latice - built on grid, again provides more detail than base allows ggplot2 - build by Hadley and again provides more graphics than base allows, we can build plots in stages, adding detail as we go Grid is often used with another package as a wrapper, to make working with grid easier and not direct. The example below uses tabplot package, which is built on top of grid. library(insuranceData) # Load the insuranceData package library(insuranceData) # Use the data() function to load the dataCar data frame data(dataCar) head(dataCar) ## veh_value exposure clm numclaims claimcst0 veh_body veh_age gender area ## 1 1.06 0.3039014 0 0 0 HBACK 3 F C ## 2 1.03 0.6488706 0 0 0 HBACK 2 F A ## 3 3.26 0.5694730 0 0 0 UTE 2 F E ## 4 4.14 0.3175907 0 0 0 STNWG 2 F D ## 5 0.72 0.6488706 0 0 0 HBACK 4 F C ## 6 2.01 0.8542094 0 0 0 HDTOP 3 M C ## agecat X_OBSTAT_ ## 1 2 01101 0 0 0 ## 2 4 01101 0 0 0 ## 3 2 01101 0 0 0 ## 4 2 01101 0 0 0 ## 5 2 01101 0 0 0 ## 6 4 01101 0 0 0 # Load the tabplot package suppressPackageStartupMessages(library(tabplot)) # Generate the default tableplot() display tableplot(dataCar) Here we used Lattice graphics package, lattice is a general-purpose graphics package that provides alternative implementations of many of the plotting functions available in base graphics. Specific examples include scatterplots with the xyplot() function, bar charts with the barchart() function, and boxplots with the bwplot() function. One important difference between lattice graphics and base graphics is that similar functions available in both graphics systems often produce very different results when applied to the same data. As a specific example, the bwplot() function creates horizontal boxplots, while the default result of the boxplot() is a vertical boxplot display. Another more important difference between lattice and base graphics is that lattice graphics supports conditional plots that show the separate relationships between variables within different groups. This capability is illustrated in this exercise, where you are asked to construct a plot showing the relationship between the variables calories and sugars from the UScereal data frame, conditional on the value of the shelf variable. # Load the lattice package library(lattice) # Use xyplot() to construct the conditional scatterplot xyplot(calories ~ sugars | shelf, data = UScereal) Finally we use ggplot2. Like the lattice package, the ggplot2 package is also based on grid graphics and it also represents a general purpose graphics package. The unique feature of the ggplot2 package is that it is based on the grammar of graphics, a systematic approach to building and modifying graphical displays, starting with a basic graphics element and refining it through the addition of successive modifiers. # Load the ggplot2 package library(ggplot2) # Create the basic plot (not displayed): basePlot basePlot &lt;- ggplot(Cars93, aes(x = Horsepower, y = MPG.city)) # Display the basic scatterplot basePlot + geom_point() # Color the points by Cylinders value basePlot + geom_point(colour = IScolors[Cars93$Cylinders]) # Make the point sizes also vary with Cylinders value basePlot + geom_point(colour = IScolors[Cars93$Cylinders], size = as.numeric(Cars93$Cylinders)) "],
["intermediate-ggplot2.html", "8 Intermediate ggplot2 8.1 Statistics 8.2 Coordinates Layer 8.3 Facets 8.4 Themes 8.5 Best Practice 8.6 Case Study - Descriptive statistics", " 8 Intermediate ggplot2 Notes taken during/inspired by the Datacamp course ‘Data Visualization with ggplot2 (Part 2)’ by Rick Scavetta. This course builds on the first course, which looked at how to build plots, aesthetics, layers and principles. This second part includes topics such as how to build meaningful plots, statistics and practical matters such as themes. The focus of this course is onfour layers (statistics, coordinates, facets and themes). Course slides: * Part 1 - Statistics * Part 2 - Coordinates and Facets * Part 3 - Themes * Part 4 - Best Practices * Part 5 - Case Study 8.1 Statistics There are two categories of statistical functions: Called from within a geom Called independently Note geom_FUN &lt;-&gt; stat_FUN Typically they are called using stat_ but ggplot will translate commands e.g. geom_histogram will call the histogram function. Often warning messages will appear as stat_FUN such as stat_bin when default bin widths are used. The following examples use the mtcars data for 32 cars from Motor Trends magazine from 1973. This dataset is small, intuitive, and contains a variety of continuous and categorical (both nominal and ordinal) variables. Firstly we will add some smoothing lines to the data. You can use either stat_smooth() or geom_smooth() to apply a linear model. library(ggplot2) # Explore the mtcars data frame with str() str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... # A scatter plot with LOESS smooth: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; # A scatter plot with an ordinary Least Squares linear model: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;) # The previous plot, without CI ribbon: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) # The previous plot, without points: ggplot(mtcars, aes(x = wt, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) We can also look sub-groups in our dataset. For this we’ll encounter the invisible group aesthetic. In the first plot, our smooth is calculated for each subgroup because there is an invisible aesthetic, group which inherits from col. In the second plot, we use multiple aesthetic layers, just like we can use multiple geom layers. Each aesthetic layer can be mapped onto a specific geom. # Define cyl as a factor variable ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) # Complete the following ggplot command as instructed ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) + stat_smooth(aes(group = 1), method = &quot;lm&quot;, se = F) Here we’ll consider the span for LOESS smoothing and we’ll take a look at a nice scenario of how to properly map our different models. LOESS smoothing is a non-parametric form of regression that uses a weighted, sliding-window, average to calculate a line of best fit. We can control the size of this window with the span argument. Plot 1: We can control the size of the loess window with the span argument. Plot 2: In this plot, we set a linear model for the entire dataset as well as each subgroup, defined by cyl. In the second stat_smooth(), Set method to “loess” Add span, set it to 0.7 Plot 3: Plot 2 presents a problem because there is a black line on our plot that is not included in the legend. To get this, we need to map something to col as an aesthetic, not just set col as an attribute. Add col to the aes() function in the second stat_smooth(), set it to “All”. This will name the line properly. Remove the col attribute in the second stat_smooth(). Otherwise, it will overwrite the col aesthetic. Plot 4: Now we should see our “All” model in the legend, but it’s not black anymore. Add a scale layer: scale_color_manual() with the first argument set to “Cylinders” and values set to the predfined myColors variable library(RColorBrewer) # Plot 1: change the LOESS span ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Add span below geom_smooth(se = F, span = 0.7) ## `geom_smooth()` using method = &#39;loess&#39; # Plot 2: Set the overall model to LOESS and use a span of 0.7 ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) + # Change method and add span below stat_smooth(method = &quot;loess&quot;, aes(group = 1), se = F, col = &quot;black&quot;, span = 0.7) # Plot 3: Set col to &quot;All&quot;, inside the aes layer of stat_smooth() ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) + stat_smooth(method = &quot;loess&quot;, # Add col inside aes() aes(group = 1, col = &quot;All&quot;), # Remove the col argument below se = F, span = 0.7) # Plot 4: Add scale_color_manual to change the colors myColors &lt;- c(brewer.pal(3, &quot;Dark2&quot;), &quot;black&quot;) ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F, span = 0.75) + stat_smooth(method = &quot;loess&quot;, aes(group = 1, col=&quot;All&quot;), se = F, span = 0.7) + # Add correct arguments to scale_color_manual scale_color_manual(&quot;Cylinders&quot;, values = myColors) The next example shows 4 plots Plot 1: builds a jittered plot of vocabulary against education of the Vocab data frame. Add a stat_smooth() layer with method set to “lm”. Make sure no CI ribbons are shown by setting se to FALSE. Plot 2: We’ll just focus on the linear models from now on. Copy the previous command, remove the geom_jitter() layer. Add the col aesthetic to the ggplot() command. Set it to factor(year). Plot 3: The default colors are pretty unintuitive. Since this can be considered an ordinal scale, it would be nice to use a sequential color palette. Copy the previous command, add scale_color_brewer() to use a default ColorBrewer. This should result in an error, since the default palette, “Blues”, only has 9 colors, but we have 16 years here. Plot 4: Overcome the error by using year as a numeric vector. You’ll have to specify the invisible group aesthetic which will be factor(year). You are given a scale layer which will fix your coloring, but you’ll need to make the following changes: Add group inside aes(), set it to factor(year). Inside stat_smooth(), set alpha equal to 0.6 and size equal to 2. When mapping onto color you can sometimes treat a continuous scale, like year, as an ordinal variable, but only if it is a regular series. The better alternative is to leave it as a continuous variable and use the group aesthetic as a factor to make sure your plot is drawn correctly. # load the data Vocab from the car package library(car) ## Warning: package &#39;car&#39; was built under R version 3.4.2 # load the RColorBrewer package for plot 4 library(RColorBrewer) # Plot 1: Jittered scatter plot, add a linear model (lm) smooth: ggplot(Vocab, aes(x = education, y = vocabulary)) + geom_jitter(alpha = 0.2) + stat_smooth(method = &quot;lm&quot;, se = FALSE) # Plot 2: Only lm, colored by year ggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) + stat_smooth(method = &quot;lm&quot;, se = FALSE) # Plot 3: Set a color brewer palette ggplot(Vocab, aes(x = education, y = vocabulary, col = factor(year))) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + scale_color_brewer() ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Blues is 9 ## Returning the palette you asked for with that many colors # Plot 4: Add the group, specify alpha and size ggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + stat_smooth(method = &quot;lm&quot;, se = F, alpha = 0.6, size = 2) + scale_color_gradientn(colors = brewer.pal(9,&quot;YlOrRd&quot;)) The previous example used the Vocab dataset and applied linear models describing vocabulary by education for different years. Here we’ll continue with that example by using stat_quantile() to apply a quantile regression (method rq). By default, the 1st, 2nd (i.e. median), and 3rd quartiles are modeled as a response to the predictor variable, in this case education. Specific quantiles can be specified with the quantiles argument. If you want to specify many quantile and color according to year, then things get too busy. The resulting plot will be a mess, because there are three quartiles drawn by default in the first plot. The second plot takes the code for the previous plot and sets the quantiles argument to 0.5 so that only the median is shown. # Use stat_quantile instead of stat_smooth: ggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + stat_quantile(alpha = 0.6, size = 2) + scale_color_gradientn(colors = brewer.pal(9,&quot;YlOrRd&quot;)) ## Loading required package: SparseM ## Loading required package: methods ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Warning in rq.fit.br(wx, wy, tau = tau, ...): Solution may be nonunique ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Warning in rq.fit.br(wx, wy, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(wx, wy, tau = tau, ...): Solution may be nonunique # Set quantile to 0.5: ggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) + stat_quantile(quantiles = 0.5, alpha = 0.6, size = 2) + scale_color_gradientn(colors = brewer.pal(9,&quot;YlOrRd&quot;)) ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x ## Warning in rq.fit.br(wx, wy, tau = tau, ...): Solution may be nonunique Another useful stat function is stat_sum() which calculates the count for each group. Typically we’d draw our models on top of the dots, but in this case we didn’t so that we could just keep recycling the p object. # Plot with linear and loess model p &lt;- ggplot(Vocab, aes(x = education, y = vocabulary)) + stat_smooth(method = &quot;loess&quot;, aes(col = &quot;red&quot;), se = F) + stat_smooth(method = &quot;lm&quot;, aes(col = &quot;blue&quot;), se = F) + scale_color_discrete(&quot;Model&quot;, labels = c(&quot;blue&quot; = &quot;lm&quot;, &quot;red&quot; = &quot;LOESS&quot;)) # Add stat_sum p + stat_sum() # Add stat_sum and set size range p + stat_sum() + scale_size(range = c(1, 10)) 8.1.1 Stats outside geoms We can use the fun.data = functio_name() which will feed the results in to our plot. In the Hmisc package, we can calculate a 95% CI using the command smean.cl.normal() or within ggplot2 using mean_cl_normal(). We can use any statistical method inside ggplot2, as long as the output format is the same as the required format of the geom that is called. So making sure the data is in the right format and that all the positions that we might use in our plots are defined is an important first step. The following example also sets up some base variables and base graphics. # Display structure of mtcars str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... # Convert cyl and am to factors: mtcars$cyl &lt;- as.factor(mtcars$cyl) mtcars$am &lt;- as.factor(mtcars$am) # Define positions: posn.d &lt;- position_dodge(width = 0.1) posn.jd &lt;- position_jitterdodge(jitter.width = 0.1, dodge.width = 0.2) posn.j &lt;- position_jitter(width = 0.2) # base layers: #wt.cyl.am &lt;- ggplot(mtcars, aex(x = cyl, y = wt, col = am, fill = am, group = am)) # more explicit naming wt.cyl.am &lt;- ggplot(mtcars, aes(cyl, wt, col = am, fill = am, group = am)) Plot 2: Add a stat_summary() layer to wt.cyl.am and calculate the mean and standard deviation as we did in the video: set fun.data to mean_sdl and specify fun.args to be list(mult = 1). Set the position argument to posn.d. Plot 3: Repeat the previous plot, but use the 95% confidence interval instead of the standard deviation. You can use mean_cl_normal instead of mean_sdl this time. There’s no need to specify fun.args in this case. Again, set position to posn.d. The above plots were simple because they implicitly used a default geom, which is geom_pointrange(). For Plot 4, fill in the blanks to calculate the mean and standard deviation separately with two stat_summary() functions: For the mean, use geom = “point” and set fun.y = mean. This time you should use fun.y because the point geom uses the y aesthetic behind the scenes. Add error bars with another stat_summary() function. Set geom = “errorbar” to get the real “T” tips. Set fun.data = mean_sdl. library(Hmisc) # required for the stat_summary() functions ## Warning: package &#39;Hmisc&#39; was built under R version 3.4.2 ## Loading required package: lattice ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:quantreg&#39;: ## ## untangle.specials ## Loading required package: Formula ## ## Attaching package: &#39;Hmisc&#39; ## The following object is masked from &#39;package:quantreg&#39;: ## ## latex ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, round.POSIXt, trunc.POSIXt, units # Plot 1: Jittered, dodged scatter plot with transparent points wt.cyl.am + geom_point(position = posn.jd, alpha = 0.6) # Plot 2: Mean and SD - the easy way wt.cyl.am + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn.d) # Plot 3: Mean and 95% CI - the easy way wt.cyl.am + stat_summary(fun.data = mean_cl_normal, position = posn.d) # Plot 4: Mean and SD - with T-tipped error bars - fill in ___ wt.cyl.am + stat_summary(geom = &quot;point&quot;, fun.y = mean, position = posn.d) + stat_summary(geom = &quot;errorbar&quot;, fun.data = mean_sdl, position = posn.d, fun.args = list(mult = 1), width = 0.1) The only difference between ggplot2::mean_sdl() and Hmisc::smean.sdl() is the naming convention. In order to use the results of a function directly in ggplot2 we need to ensure that the names of the variables match the aesthetics needed for our respective geoms. Here we’ll create two new functions. One function will measure the full range of the dataset and the other will measure the interquartile range. xx &lt;- 1:100 # our data # Function to save range for use in ggplot gg_range &lt;- function(x) { # Change x below to return the instructed values data.frame(ymin = min(x), # Min ymax = max(x)) # Max } gg_range(xx) ## ymin ymax ## 1 1 100 # Required output: # ymin ymax # 1 1 100 # Function to Custom function: med_IQR &lt;- function(x) { # Change x below to return the instructed values data.frame(y = median(x), # Median ymin = quantile(x)[2], # 1st quartile ymax = quantile(x)[4]) # 3rd quartile } med_IQR(xx) ## y ymin ymax ## 25% 50.5 25.75 75.25 # Required output: # y ymin ymax # 25% 50.5 25.75 75.25 We have now created functions that will allow us to plot the five-number summary (the minimum, 1st quartile, median, 3rd quartile, and the maximum). Here, we’ll implement that into a unique plot type. All the functions and objects from the previous exercise are available including the updated mtcars data frame, the position object posn.d, the base layers wt.cyl.am and the functions med_IQR() and gg_range(). # Add three stat_summary calls to wt.cyl.am wt.cyl.am + stat_summary(geom = &quot;linerange&quot;, fun.data = med_IQR, position = posn.d, size = 3) + stat_summary(geom = &quot;linerange&quot;, fun.data = gg_range, position = posn.d, size = 3, alpha = 0.4) + stat_summary(geom = &quot;point&quot;, fun.y = median, position = posn.d, size = 3, col = &quot;black&quot;, shape = &quot;X&quot;) 8.2 Coordinates Layer The coordinates layer controls the dimensions of the plot using the coord_ such as the coord_cartesian() which controls the x and y elements. You can use the scale_x_continous to effectivly show just a section of the plot if you so desire, but some data will be missed out as it itsn’t on the plot. Conversley the coord_cartesian will draw all the plot, but zoom in to just a section of it. We can also alter the aspect ratio, which can make certain trends in data easier to see, however there is a risk of deceiving the viewer, perhaps by mistake, if we are not careful. # Basic ggplot() command p &lt;- ggplot(mtcars, aes(x = wt, y = hp, col = am)) + geom_point() + geom_smooth() # Add scale_x_continuous p + scale_x_continuous(limits = c(3, 6), expand = c(0, 0)) ## `geom_smooth()` using method = &#39;loess&#39; ## Warning: Removed 12 rows containing non-finite values (stat_smooth). ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : span too small. fewer data values than degrees of freedom. ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : at 3.168 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : radius 4e-006 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : all data on boundary of neighborhood. make span bigger ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : pseudoinverse used at 3.168 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : neighborhood radius 0.002 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : reciprocal condition number 1 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : at 3.572 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : radius 4e-006 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : all data on boundary of neighborhood. make span bigger ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : There are other near singularities as well. 4e-006 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : zero-width neighborhood. make span bigger ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : zero-width neighborhood. make span bigger ## Warning: Computation failed in `stat_smooth()`: ## NA/NaN/Inf in foreign function call (arg 5) ## Warning: Removed 12 rows containing missing values (geom_point). # The proper way to zoom in - shows the data appropriatley p + coord_cartesian(xlim = c(3, 6)) ## `geom_smooth()` using method = &#39;loess&#39; We can set the aspect ratio of a plot with coord_fixed() or coord_equal(). Both use aspect = 1 as a default. A 1:1 aspect ratio is most appropriate when two continuous variables are on the same scale, as with the iris dataset. All variables are measured in centimeters, so it only makes sense that one unit on the plot should be the same physical distance on each axis. This gives a more truthful depiction of the relationship between the two variables since the aspect ratio can change the angle of our smoothing line. This would give an erroneous impression of the data. Of course the underlying linear models don’t change, but our perception can be influenced by the angle drawn. As a rule of thumb you’ll want to use a 1:1 aspect ratio when your axes show the same scales, but there are always exceptions. # Complete basic scatter plot function base.plot &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, se = F) # Plot base.plot: default aspect ratio base.plot # Fix aspect ratio (1:1) of base.plot base.plot + coord_equal() The coord_polar() function converts a planar x-y cartesian plot to polar coordinates. This can be useful if you are producing pie charts. We can imagine two forms for pie charts - the typical filled circle, or a colored ring. As an example, consider a stacked bar chart. Imagine that we just take the y axis on the left and bend it until it loops back on itself, while expanding the right side as we go along. We’d end up with a pie chart - it’s simply a bar chart transformed onto a polar coordinate system. Typical pie charts omit all of the non-data ink, which we’ll learn about in the next chapter. Pie charts are not really better than stacked bar charts, but we’ll come back to this point in the fourth chapter on best practices. This function is particularly useful if you are dealing with a cycle, like yearly data, that you would like to see represented as such. # Create stacked bar plot: thin.bar thin.bar &lt;- ggplot(mtcars, aes(x = 1, fill = cyl)) + geom_bar() # Convert thin.bar to pie chart thin.bar + coord_polar(theta = &quot;y&quot;) # Create stacked bar plot: wide.bar wide.bar &lt;- ggplot(mtcars, aes(x = 1, fill = cyl)) + geom_bar(width = 1) # Convert wide.bar to pie chart wide.bar + coord_polar(theta = &quot;y&quot;) 8.3 Facets These are small elements of information, as suggested by Edward Tufte in the Visualisation of Qunatative Information, 1983. The idea is we can split up a larger plot into smaller ones which use the exact same coordinate system. The most straightforward way of using facets is facet_grid(). Here we just need to specify the categorical variable to use on rows and columns using formula notation. Notice that we can also take advantage of ordinal variables by positioning them in the correct order as columns or rows, as is the case with the number of cylinders. # Basic scatter plot: p &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() # Separate rows according to transmission type, am p + facet_grid(. ~ am) # Separate columns according to cylinders, cyl p + facet_grid(cyl ~ .) # Separate by both columns and rows p + facet_grid(cyl ~ am) Facets are another way of presenting categorical variables. Sometimes it’s possible to overdo it. Here we’ll present a plot with 6 variables and see if we can add even more. Let’s begin by using a trick to map two variables onto two color scales - hue and lightness. We combine cyl and am into a single variable cyl_am. To accommodate this we also make a new color palette with alternating red and blue of increasing darkness. This is saved as myCol. The last plot created here is simply trying to plot too much information. # Code to create the cyl_am col and myCol vector mtcars$cyl_am &lt;- paste(mtcars$cyl, mtcars$am, sep = &quot;_&quot;) myCol &lt;- rbind(brewer.pal(9, &quot;Blues&quot;)[c(3,6,8)], brewer.pal(9, &quot;Reds&quot;)[c(3,6,8)]) # Basic scatter plot, add color scale: ggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) + geom_point() + scale_color_manual(values = myCol) # Facet according on rows and columns. ggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am)) + geom_point() + scale_color_manual(values = myCol) + facet_grid(vs ~ gear) # Add more variables. Note, with a denser plot we would need to set an alpha value to see all points ggplot(mtcars, aes(x = wt, y = mpg, col = cyl_am, size = disp)) + geom_point() + scale_color_manual(values = myCol) + facet_grid(vs ~ gear) When you have a categorical variable with many levels which are not all present in sub-groups of another variable, it may be desirable to drop the unused levels. As an example let’s return to the mammalian sleep dataset, mamsleep. It is available in your workspace. The variables of interest here are name, which contains the full popular name of each animal, and vore, the eating behavior. Each animal can only be classified under one eating habit, so if we facet according to vore, we don’t need to repeat the full list in each sub-plot. library(reshape2) # Setup the data mamsleep &lt;- msleep[,c(&quot;vore&quot;, &quot;name&quot;, &quot;sleep_total&quot;, &quot;sleep_rem&quot;)] mamsleep &lt;- mamsleep[complete.cases(mamsleep),] mamsleep &lt;- melt(mamsleep, id = c(&quot;vore&quot;,&quot;name&quot;), variable.name = &quot;sleep&quot;, value.name = &quot;time&quot;) levels(mamsleep$sleep) &lt;- sub(&quot;sleep_&quot;, &quot;&quot;, levels(mamsleep$sleep)) # Basic scatter plot p &lt;- ggplot(mamsleep, aes(x = time, y = name, col = sleep)) + geom_point() p # Facet rows accoding to vore p + facet_grid(vore ~ .) # Specify scale and space arguments to free up rows p + facet_grid(vore ~ ., scale = &#39;free_y&#39;, space = &#39;free_y&#39;) 8.4 Themes The Themes layer hold all the visual elements not part of the data, there are three types; text, lines and rectangles. Text includes things like the axis, legends, plot and axis titles. Headers for individual facets are named as strip text e.g. strip.text.x = Lines includes tick marks, axis lines and grid lines. Rectanges (rect) includes the plot area, legends and so on. Elements are in a hierachy so if you change some of the higher components, lower ones will be affected. Text is one instance of this. The diagram below shows the other nested elements. (#fig:Theme Inheritance)Inheritance in ggplot2 themes If we want certain items to not appear on the plot we can use the element_blank command e.g. line = element_blank(). # Define our pink for use myPink &lt;- &quot;#FEE0D2&quot; # Plot 1: change the plot background color to myPink: z + theme(plot.background = element_rect(fill = myPink)) # Plot 2: adjust the border to be a black line of size 3 z + theme(plot.background = element_rect(fill = myPink, colour = &quot;black&quot;, size = 3)) # Plot 3: set panel.background, legend.key, legend.background and strip.background to element_blank() uniform_panels &lt;- theme(panel.background = element_blank(), legend.key = element_blank(), legend.background=element_blank(), strip.background = element_blank()) z &lt;- z + theme(plot.background = element_rect(fill = myPink, color = &quot;black&quot;, size = 3)) + uniform_panels # Extend z with theme() function and three arguments to change the appearance of lines z + theme(panel.grid = element_blank(), axis.line = element_line(color = &quot;black&quot;), axis.ticks = element_line(color = &quot;black&quot;)) # Extend z with theme() function and four arguments to make the text better z + theme( strip.text = element_text(size = 16, color = myRed), axis.title.y = element_text(color = myRed, hjust = 0, face = &quot;italic&quot;), axis.title.x = element_text(color = myRed, hjust = 0, face = &quot;italic&quot;), axis.text = element_text(color = &quot;black&quot;) ) We can also use similar arguments to change the location and other settings of the legend # Move legend by position, now top right of third facet z + theme(legend.position = c(0.85, 0.85)) # Change direction of the legend entries z + theme(legend.direction = &quot;horizontal&quot;) # Change location by name z + theme(legend.position = &quot;bottom&quot;) # Remove legend entirely z + theme(legend.position = &quot;none&quot;) The different rectangles of your plot have spacing between them. There’s spacing between the facets, between the axis labels and the plot rectangle, between the plot rectangle and the entire panel background, etc. Suppose you want to have more spacing between the different facets. You can control this by specifying panel.spacing.x inside a theme() function you add to z. For the argument value, you should pass a unit object. To achieve this, load the grid package with library(). # Increase spacing between facets library(grid) z + theme(panel.spacing.x = unit(2, &quot;cm&quot;)) # Add code to remove any excess plot margin space z + theme(panel.spacing.x = unit(2, &quot;cm&quot;), plot.margin = unit(c(0, 0, 0, 0), &quot;cm&quot;)) 8.4.1 Recycling themes When using the same themes repeatedly, for instance in a publication or report, we wouldn’t want to repeate the theme instructions every time. To do this, we save our theme element as an R object. If a new chart type or chart element then appears in a later plot, we can reuse the previous theme but just extend with a little more code to account for this new chart or object type, without having to re-type all the theme code. If we want all our plots to use the same theme, we can use the theme_set(theme_name()) command and all plots will do this. To return to the default, you can speficy theme_grey(). There are many themes available by default in ggplot2: theme_bw(), theme_classic(), theme_gray(), etc. In the previous exercise, you saw that you can apply these themes to all following plots, with theme_set(): theme_set(theme_bw()) But you can also apply them on a particular plot, with: … + theme_bw() Next, it’s perfectly possible and super-easy to extend these themes with your own modifications. In this exercise, you will experiment with this and use some preset templates available from the ggthemes package. # Load ggthemes package library(ggthemes) # Apply theme_tufte z2 + theme_tufte() # Apply theme_tufte, modified: z2 + theme_tufte() + theme( legend.position = c(0.9, 0.9), legend.title = element_text(face = &quot;italic&quot;, size = 12), axis.title = element_text(face = &quot;bold&quot;, size = 14) ) 8.5 Best Practice Knowing how to create the plots is important, but choosing the right chart type is equally important. 8.5.1 Bar Plots Typically two types Absolute values e.g. counts Distribution plots Problems in interpretation can arise, for instance if a bar plot shows sleep patters, there will be a distribution around a mean, but no values of zero hours, but the plot axies is likely to go to zero. It is equally problematic to see extreme value, such as animals with many hours of sleep, since we typically show a mean and perhaps a standard deviation line. A better option would be to use a scatter plot of sleep times, with jitter and some transparency (alpha) to reduce any over plotting. The example below shows a bar plot with error bars, sometimes referred to as a dynamite plot. # Base layers m &lt;- ggplot(mtcars, aes(x = cyl, y = wt)) # Draw dynamite plot m + stat_summary(fun.y = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, width = 0.1) # Base layers, update m so that we split the bars according to transmission type, am m &lt;- ggplot(mtcars, aes(x = cyl,y = wt, col = am, fill = am)) # Plot 1: Draw dynamite plot - error bars overlap m + stat_summary(fun.y = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, width = 0.1) # Plot 2: Set position dodge in each stat function, dodge them, which does not work because the default dodging is different for the different stat_summary() functions m + stat_summary(fun.y = mean, geom = &quot;bar&quot;, position = &quot;dodge&quot;) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, width = 0.1, position = &quot;dodge&quot;) # Set your dodge posn manually posn.d &lt;- position_dodge(0.9) # Plot 3: Redraw dynamite plot m + stat_summary(fun.y = mean, geom = &quot;bar&quot;, position = posn.d) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, width = 0.1, position = posn.d) 8.5.2 Pie Charts Pie charts are not usually your first choice for plots. Angle, area and length elements are hard to compare, particularly if you are comparing more than one pie chart. Pie charts work based when comparing differences between of around 3 groups. A better alternative is to use bar charts - usually a horizontal bar where two bars can be plotted next to each other and compared. In this example we’re going to consider a typical use of pie charts - a categorical variable as the proportion of another categorical variable. For example, the proportion of each transmission type am, in each cylinder, cyl class. We use dummy aesthetic for the x. Changing the aes() function such that factor(1) maps onto x. # Convert bar chart to pie chart ggplot(mtcars, aes(x = factor(1), fill = am)) + geom_bar(position = &quot;fill&quot;, width = 1) + facet_grid(. ~ cyl) + coord_polar(theta = &quot;y&quot;) In the previous example, we looked at the proportions of one categorical variable (am) as a proportion of another (cyl). In this example, we’re interested in two, or possibly many, categorical variables independent of each other. The trick is to use a parallel coordinates plot. Each variable is plotted on its own axis and they are plotted as parallel axes. The individual observations are connected with lines, which can be colored according to another variable. This is a surprisingly useful visualization since we can combine many variables, even if they are on entirely different scales. A word of caution though: typically it is very taboo to draw lines in this way. It’s the reason why we don’t draw lines across levels of a nominal variable - the order, and thus the slope of the line, is meaningless. Parallel plots are a (very useful) exception to the rule! Parallel plots not only allow us to see many different variables at the same time, but because we can apply specific statistics to each axis or the order of the axes, we can derive even more information from these plots. # Parallel coordinates plot using GGally library(GGally) ## Warning: package &#39;GGally&#39; was built under R version 3.4.2 # All columns except am group_by_am &lt;- which(names(mtcars) == &#39;am&#39;) my_names_am &lt;- (1:11)[-group_by_am] # Basic parallel plot - each variable plotted as a z-score transformation ggparcoord(mtcars, my_names_am, groupColumn = group_by_am, alpha = 0.8) The parallel coordinate plot from the last exercise is an excellent example of an exploratory plot. It presents a massive amount of information and allows the specialist to explore many relationships all at once. Another great example is a plot matrix (a SPLOM, from scatter plot matrix). GGally::ggpairs(mtcars2) will produce the plot of a selection of the mtcars dataset, mtcars2. Depending on the nature of the dataset a specific plot type will be produced and if both variables are continuous the correlation (rho) will also be calculated. The relationship between the variables drat and mpg is shown in two areas. What is the correlation between these two variables? mtcars2 &lt;- mtcars[, c(1, 3, 5, 6, 7)] ggpairs(mtcars2) The answer is 0.681, this is in the first row, in the first column you can find the scatter plot. 8.5.3 Heat Maps Heat Maps can be difficult to read as the interpretation can change, dependent on the colours around each element. Plotting continous data on comon scales is always the better option. You may encounter a case in which you really do want to use one. Luckily, they’re fairly straightforward to produce in ggplot2. We begin by specifying two categorical variables for the x and y aesthetics. At the intersection of each category we’ll draw a box, except here we call it a tile, using the geom_tile() layer. Then we will fill each tile with a continuous variable. We’ll produce the heat map we saw in the video with the built-in barley dataset. The barley dataset is in the lattice package and has already been loaded for you. Begin by exploring the structure of the data in the console using str(). library(RColorBrewer) library(lattice) data(barley) # Create color palette myColors &lt;- brewer.pal(9, &quot;Reds&quot;) # Build the heat map from scratch ggplot(barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap( ~ site, ncol = 1) + scale_fill_gradientn(colors = myColors) There are several alternatives to heat maps. The best choice really depends on the data and the story you want to tell with this data. If there is a time component, the most obvious choice is a line plot like what we see in the viewer. # Line plots ggplot(barley, aes(x = year, y = variety, col = variety, group = variety)) + geom_line() + facet_wrap( ~ site, nrow = 1) You can use dodged error bars or you can use overlapping transparent ribbons (shown in the viewer). In this exercise we’ll try to recreate the second option, the transparent ribbons. ggplot(barley, aes(x = year, y = yield, colour = site, group = site, fill = site)) + stat_summary(fun.y = mean, geom = &quot;line&quot;) + stat_summary( fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;ribbon&quot;, col = NA, alpha = 0.1 ) 8.6 Case Study - Descriptive statistics Data from from the California Health Interview Survey (CHIS). The original dataset from 2009 has over 47,000 respondents across 536 variables, but we will use a slightly filtered version with around 44,000 responses over 10 variables. The data is filtered to show just the 4 largest ethnicity groups giving the over 44,000 responses. In this chapter we’re going to continuously build on our plotting functions and understanding to produce a mosaic plot (aka Merimeko plot). This is a visual representation of a contingency table, comparing two categorical variables. Essentially, our question is which groups are over or under represented in our dataset. To visualize this we’ll color groups according to their pearson residuals from a chi-squared test. At the end of it all we’ll wrap up our script into a flexible function so that we can look at other variables. We’ll familiarize ourselves with a small number of variables from the 2009 CHIS adult-response dataset (as opposed to children). We have selected the following variables to explore: RBMI: BMI Category description BMI_P: BMI value RACEHPR2: race SRSEX: sex SRAGE_P: age MARIT2: Marital status AB1: General Health Condition ASTCUR: Current Asthma Status AB51: Type I or Type II Diabetes POVLL: Poverty level We’ll filter our dataset to plot a more reliable subset (we’ll still retain over 95% of the data). Before we get into mosaic plots it’s worthwhile exploring the dataset using simple distribution plots - i.e. histograms. # load the data load(&quot;D:/CloudStation/Documents/2017/RData/CHIS2009_reduced_2.Rdata&quot;) # Explore the dataset with summary and str str(adult) ## &#39;data.frame&#39;: 44346 obs. of 10 variables: ## $ RBMI : num 3 3 3 2 3 4 3 2 3 3 ... ## $ BMI_P : num 28.9 26.1 25.1 25 25.1 ... ## $ RACEHPR2: num 6 6 6 6 6 6 6 6 6 6 ... ## $ SRSEX : num 1 2 1 1 1 2 1 2 1 2 ... ## $ SRAGE_P : num 32 80 71 39 75 53 42 33 67 52 ... ## $ MARIT2 : num 1 3 1 4 1 1 1 1 3 3 ... ## $ AB1 : num 1 1 2 1 2 3 2 2 1 5 ... ## $ ASTCUR : num 2 2 1 2 2 1 2 2 2 2 ... ## $ AB51 : num -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ... ## $ POVLL : num 4 4 4 4 4 4 4 3 4 4 ... summary(adult) ## RBMI BMI_P RACEHPR2 SRSEX ## Min. :1.000 Min. :12.65 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:22.77 1st Qu.:5.000 1st Qu.:1.000 ## Median :3.000 Median :25.72 Median :6.000 Median :2.000 ## Mean :2.748 Mean :26.64 Mean :5.088 Mean :1.591 ## 3rd Qu.:3.000 3rd Qu.:29.32 3rd Qu.:6.000 3rd Qu.:2.000 ## Max. :4.000 Max. :93.72 Max. :6.000 Max. :2.000 ## SRAGE_P MARIT2 AB1 ASTCUR ## Min. :18.00 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:44.00 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:2.000 ## Median :57.00 Median :1.000 Median :2.000 Median :2.000 ## Mean :56.14 Mean :2.043 Mean :2.525 Mean :1.915 ## 3rd Qu.:69.00 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:2.000 ## Max. :85.00 Max. :4.000 Max. :5.000 Max. :2.000 ## AB51 POVLL ## Min. :-1.0000 Min. :1.000 ## 1st Qu.:-1.0000 1st Qu.:2.000 ## Median :-1.0000 Median :4.000 ## Mean :-0.7108 Mean :3.196 ## 3rd Qu.:-1.0000 3rd Qu.:4.000 ## Max. : 3.0000 Max. :4.000 # Age histogram ggplot(adult, aes(x = SRAGE_P)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # BMI histogram ggplot(adult, aes(x = BMI_P)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Age colored by BMI, default binwidth ggplot(adult, aes(x = SRAGE_P, fill = factor(RBMI))) + geom_histogram(binwidth = 1) Next we want to tidy our dateset, we’ll remove some extreme values then provide some better labels to help with plotting. # Remove individual aboves 84, as it seems &gt;85 is grouped together adult &lt;- adult[adult$SRAGE_P &lt;= 84, ] # Remove individuals with a BMI below 16 and above or equal to 52 adult &lt;- adult[adult$BMI_P &gt;= 16 &amp; adult$BMI_P &lt; 52, ] # Relabel the race variable to make plotting easier: adult$RACEHPR2 &lt;- factor(adult$RACEHPR2, labels = c(&quot;Latino&quot;, &quot;Asian&quot;, &quot;African American&quot;, &quot;White&quot;)) # Relabel the BMI categories variable to make plotting easier: adult$RBMI &lt;- factor(adult$RBMI, labels = c(&quot;Under-weight&quot;, &quot;Normal-weight&quot;, &quot;Over-weight&quot;, &quot;Obese&quot;)) When we introduced histograms we focused on univariate data, which is exactly what we’ve been doing here. However, when we want to explore distributions further there is much more we can do. For example, there are density plots, which you’ll explore in the next course. For now, we’ll look deeper at frequency histograms and begin developing our mosaic plots. # The color scale used in the plot BMI_fill &lt;- scale_fill_brewer(&quot;BMI Category&quot;, palette = &quot;Reds&quot;) # Theme to fix category display in faceted plot fix_strips &lt;- theme(strip.text.y = element_text(angle = 0, hjust = 0, vjust = 0.1, size = 14), strip.background = element_blank(), legend.position = &quot;none&quot;) # Histogram, add BMI_fill and customizations ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(binwidth = 1) + fix_strips + BMI_fill + facet_grid(RBMI ~ .) + theme_classic() In the previous exercise we looked at different ways of showing the absolute count of multiple histograms. This is fine, but density would be more useful measure if we wanted to see how the frequency of one variable changes accross another. However, there are some difficulties here, so let’s take a closer look at different plots. # Plot 1 - Count histogram ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(binwidth = 1) + BMI_fill # Plot 2 - Density histogram - odd because we get the density within each BMI category, not within each age group ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(aes(y = ..density..),binwidth = 1) + BMI_fill # Plot 3 - Faceted count histogram - if we are interested in the frequency distribution within each BMI category ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(binwidth = 1) + BMI_fill + facet_grid(RBMI ~.) # Plot 4 - Faceted density histogram - similat to 3 but density rather than count ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(aes(y = ..density..),binwidth = 1) + BMI_fill + facet_grid(RBMI ~.) # Plot 5 - Density histogram with position = &quot;fill&quot; - not an accurate representation, as density calculates the proportion across category, and not across bin ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(aes(y = ..density..),binwidth = 1, position = &quot;fill&quot;) + BMI_fill # Plot 6 - The accurate histogram ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(aes(y = ..count../sum(..count..)),binwidth = 1, position = &quot;fill&quot;) + BMI_fill In the previous exercise we looked at how to produce a frequency histogram when we have many sub-categories. The problem here is that this can’t be facetted because the calculations occur on the fly inside ggplot2. To overcome this we’re going to calculate the proportions outside ggplot2. This is the beginning of our flexible script for a mosaic plot. # An attempt to facet the accurate frequency histogram from before (failed) ggplot(adult, aes (x = SRAGE_P, fill= factor(RBMI))) + geom_histogram(aes(y = ..count../sum(..count..)), binwidth = 1, position = &quot;fill&quot;) + BMI_fill + facet_grid(RBMI ~ .) # Create DF with table() DF &lt;- table(adult$RBMI, adult$SRAGE_P) # Use apply on DF to get frequency of each group - the second argument is 2 because we want to do calculations on each column DF_freq &lt;- apply(DF, 2, function(x) x/sum(x)) # Load reshape2 and use melt on DF to create DF_melted library(reshape2) DF_melted &lt;- melt(DF_freq) # Change names of DF_melted names(DF_melted) &lt;- c(&quot;FILL&quot;, &quot;X&quot;, &quot;value&quot;) # Add code to make this a faceted plot - note we are now using geom_bar ggplot(DF_melted, aes(x = X, y = value, fill = FILL)) + geom_bar(stat = &quot;identity&quot;, position = &quot;stack&quot;) + BMI_fill + facet_grid(FILL ~ .) 8.6.1 Mosaic Plots Mosaic plots are similar to the last plot we created, however they show the proportion that the item makes up of the whole dataset, not just a particular year. In effect, we have a large contigency table derived from two categorical variables - BMI category and age (we treat this as an ordinal variable for the mosaic plot). We want to know if the proportions of one variable, such as BMI, within groups of another variable such as age, stray from the null model of equal proportions. This is in effect a visual representation of a chi-squared plot, as developed by Michael Friendly. We can now think about colouring the bars for each group based on it’s residual value. Mosaic plots are a bit more involved, because the aim is not to draw bars, but rather rectangles, for which we can control the widths. Whilst bars are simply rectangles, but we don’t have easy access to the xmin and xmax aesthetics, but in geom_rect() we do. Likewise, we also have access to ymin and ymax. So we’re going to draw a box for every one of our 268 distinct groups of BMI category and age. In this first section we need to create our data frame, based on our contigency table, but with some more calculations. # The initial contingency table DF &lt;- as.data.frame.matrix(table(adult$SRAGE_P, adult$RBMI)) # Add the columns groupsSum, xmax and xmin. Remove groupSum again. DF$groupSum &lt;- rowSums(DF) DF$xmax &lt;- cumsum(DF$groupSum) DF$xmin &lt;- DF$xmax - DF$groupSum # The groupSum column needs to be removed DF$groupSum &lt;- NULL # Copy row names to variable X as the names of the x axis groups are stored in the row name DF$X &lt;- row.names(DF) # Melt the dataset library(reshape2) DF_melted &lt;- melt(DF, id.vars = c(&quot;X&quot;, &quot;xmin&quot;, &quot;xmax&quot;), variable.name = &quot;FILL&quot;) # dplyr call to calculate ymin and ymax - first groups by X and then calculates cumulative proportions library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:GGally&#39;: ## ## nasa ## The following objects are masked from &#39;package:Hmisc&#39;: ## ## combine, src, summarize ## The following object is masked from &#39;package:car&#39;: ## ## recode ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union DF_melted &lt;- DF_melted %&gt;% group_by(X) %&gt;% mutate(ymax = cumsum(value/sum(value)), ymin = ymax - value/sum(value)) # Plot rectangles - don&#39;t change. library(ggthemes) ## Warning: package &#39;ggthemes&#39; was built under R version 3.4.2 ggplot(DF_melted, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = FILL)) + geom_rect(colour = &quot;white&quot;) + scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) + BMI_fill + theme_tufte() Previously we generated a plot where each individual bar was plotted separately using rectangles . This means we have access to each piece and we can apply different fill parameters. So let’s make some new parameters. To get the Pearson residuals, we’ll use the chisq.test(). # Perform chi.sq test (RBMI and SRAGE_P) results &lt;- chisq.test(table(adult$RBMI, adult$SRAGE_P)) # Melt results$residuals and store as resid resid &lt;- melt(results$residuals) # Change names of resid - so that we have a consistent naming convention names(resid) &lt;- c(&quot;FILL&quot;, &quot;X&quot;, &quot;residual&quot;) # merge the two datasets: DF_all &lt;- merge(DF_melted, resid) # Update plot command library(ggthemes) ggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + geom_rect() + scale_fill_gradient2() + scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) + theme_tufte() Now that we are not coloring according to BMI category, we have to add the group labels manually. Also, we neglected to label the x-axis properly. Here we’ll use the label aesthetic inside geom_text(). The actual labels will be the FILL and X columns in the DF data frame. Since we have axes on the left and bottom of our plot, we’ll add information to the top and right inner edges of the plot. We could have also added margin text, but that is a more advanced topic. This will be a suitable solution for the moment. To position our labels correctly, we need to calculate the midpoint between each xmax and xmin value. To get this, calculate the half difference between each pair of xmax and xmin then add this value to xmin. For the y label positions, we only want to work with the values at the maximum xmax, i.e. at the very end. Now that xtext and ytext are available, we can add the labels to our plot. In the two geom_text() functions, separate aesthetics are defined that control the x and y positioning of the labels. For the age groups, set the x position with xtext. The y position is fixed since our y axis is always going to end at 1. For the labeling of the y axis, the second geom_text() has three aesthetics since the position on the right will depend on the size of our dataset in the future. Some additional attributes have been set inside geom_text(), outside the aes() function. This is just some fine tweaking to get the positioning and angle correct. It’s not perfect, but since this is an exploratory plot, it does a pretty good job. # Position for labels on x axis DF_all$xtext &lt;- DF_all$xmin + (DF_all$xmax - DF_all$xmin)/2 # Position for labels on y axis (don&#39;t change) index &lt;- DF_all$xmax == max(DF_all$xmax) DF_all$ytext &lt;- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2 # Plot ggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + geom_rect(col = &quot;white&quot;) + # geom_text for ages (i.e. the x axis) geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) + # geom_text for BMI (i.e. the fill axis) geom_text(aes(x = max(xmax), y = ytext, label = FILL), size = 3, hjust = 1, show.legend = FALSE) + scale_fill_gradient2() + theme_tufte() + theme(legend.position = &quot;bottom&quot;) We can now wrap all the steps into a single function that we can use to examine any two variables of interest in our data frame (or in any other data frame for that matter). Notice that the function takes multiple arguments, such as the data frame of interest and the variables that you want to create the mosaic plot for. None of the arguments have default values, so you’ll have to specify all three if you want the mosaicGG() function to work. # Load all packages library(ggplot2) library(reshape2) library(dplyr) library(ggthemes) # Script generalized into a function mosaicGG &lt;- function(data, X, FILL) { # Proportions in raw data DF &lt;- as.data.frame.matrix(table(data[[X]], data[[FILL]])) DF$groupSum &lt;- rowSums(DF) DF$xmax &lt;- cumsum(DF$groupSum) DF$xmin &lt;- DF$xmax - DF$groupSum DF$X &lt;- row.names(DF) DF$groupSum &lt;- NULL DF_melted &lt;- melt(DF, id = c(&quot;X&quot;, &quot;xmin&quot;, &quot;xmax&quot;), variable.name = &quot;FILL&quot;) library(dplyr) DF_melted &lt;- DF_melted %&gt;% group_by(X) %&gt;% mutate(ymax = cumsum(value/sum(value)), ymin = ymax - value/sum(value)) # Chi-sq test results &lt;- chisq.test(table(data[[FILL]], data[[X]])) # fill and then x resid &lt;- melt(results$residuals) names(resid) &lt;- c(&quot;FILL&quot;, &quot;X&quot;, &quot;residual&quot;) # Merge data DF_all &lt;- merge(DF_melted, resid) # Positions for labels DF_all$xtext &lt;- DF_all$xmin + (DF_all$xmax - DF_all$xmin)/2 index &lt;- DF_all$xmax == max(DF_all$xmax) DF_all$ytext &lt;- DF_all$ymin[index] + (DF_all$ymax[index] - DF_all$ymin[index])/2 # plot: g &lt;- ggplot(DF_all, aes(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, fill = residual)) + geom_rect(col = &quot;white&quot;) + geom_text(aes(x = xtext, label = X), y = 1, size = 3, angle = 90, hjust = 1, show.legend = FALSE) + geom_text(aes(x = max(xmax), y = ytext, label = FILL), size = 3, hjust = 1, show.legend = FALSE) + scale_fill_gradient2(&quot;Residuals&quot;) + scale_x_continuous(&quot;Individuals&quot;, expand = c(0,0)) + scale_y_continuous(&quot;Proportion&quot;, expand = c(0,0)) + theme_tufte() + theme(legend.position = &quot;bottom&quot;) print(g) } # BMI described by age mosaicGG(adult, &quot;SRAGE_P&quot;,&quot;RBMI&quot;) # Poverty described by age mosaicGG(adult, &quot;SRAGE_P&quot;,&quot;POVLL&quot;) # mtcars: am described by cyl mosaicGG(mtcars, &quot;cyl&quot;, &quot;am&quot;) ## Warning in chisq.test(table(data[[FILL]], data[[X]])): Chi-squared ## approximation may be incorrect # Vocab: vocabulary described by education library(car) mosaicGG(Vocab, &quot;education&quot;, &quot;vocabulary&quot;) ## Warning in chisq.test(table(data[[FILL]], data[[X]])): Chi-squared ## approximation may be incorrect "],
["advanced-ggplot2.html", "9 Advanced ggplot2 9.1 Refresher 9.2 Statistical plots 9.3 Multiple groups or variables 9.4 Plots for Specific Data 1", " 9 Advanced ggplot2 Notes taken during/inspired by the Datacamp course ‘Data Visualization with ggplot2 (Part 3)’ by Rick Scavetta. This course builds on the first course and second courses, which looked at how to build plots, aesthetics, statistics and practical matters such as themes. The focus of this course is on more specific plot types, including looking at handling large data plots. We also look at maps and video frames aka animations. The fourth chapter looks at the internals of GGPlot2 including the grid extra package. Finally we will look at a case study, which includes looking at how we can use the extensions function to build our own plots from scratch. Course slides: * Part 1 - Statistical Plots * Part 2 - Plots for Specific Data Part 1 * Part 3 - Plots for Specific Data Part 2 * Part 4 - GGPlot2 Internals * Part 5 - Case Study 9.1 Refresher As a refresher to statistical plots, let’s build a scatter plot with an additional statistic layer. A dataset called movies_small is coded in your workspace. It is a random sample of 1000 observations from the larger movies dataset, that’s inside the ggplot2movies package. The dataset contains information on movies from IMDB. The variable votes is the number of IMDB users who have rated a movie and the rating (converted into a categorical variable) is the average rating for the movie. library(ggplot2) #load the data movies_small &lt;- readRDS(&quot;D:/CloudStation/Documents/2017/RData/ch1_movies_small.RDS&quot;) # Explore movies_small with str() str(movies_small) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 10000 obs. of 24 variables: ## $ title : chr &quot;Fair and Worm-er&quot; &quot;Shelf Life&quot; &quot;House: After Five Years of Living&quot; &quot;Three Long Years&quot; ... ## $ year : int 1946 2000 1955 2003 1963 1992 1999 1972 1994 1985 ... ## $ length : int 7 4 11 76 103 107 87 84 127 94 ... ## $ budget : int NA NA NA NA NA NA NA NA NA NA ... ## $ rating : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 7 7 6 8 8 5 4 8 5 5 ... ## $ votes : int 16 11 15 11 103 28 105 9 37 28 ... ## $ r1 : num 0 0 14.5 4.5 4.5 4.5 14.5 0 4.5 4.5 ... ## $ r2 : num 0 0 0 0 4.5 0 4.5 0 4.5 0 ... ## $ r3 : num 0 0 4.5 4.5 0 4.5 4.5 0 14.5 4.5 ... ## $ r4 : num 0 0 4.5 0 4.5 4.5 4.5 0 4.5 14.5 ... ## $ r5 : num 4.5 4.5 0 0 4.5 0 4.5 14.5 24.5 4.5 ... ## $ r6 : num 4.5 24.5 34.5 4.5 4.5 0 14.5 0 4.5 14.5 ... ## $ r7 : num 64.5 4.5 24.5 0 14.5 4.5 14.5 14.5 14.5 14.5 ... ## $ r8 : num 14.5 24.5 4.5 4.5 14.5 24.5 14.5 24.5 14.5 14.5 ... ## $ r9 : num 0 0 0 14.5 14.5 24.5 14.5 14.5 4.5 4.5 ... ## $ r10 : num 14.5 24.5 14.5 44.5 44.5 24.5 14.5 44.5 4.5 24.5 ... ## $ mpaa : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Action : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Animation : int 1 0 0 0 0 0 0 0 0 0 ... ## $ Comedy : int 1 0 0 1 0 1 1 1 0 0 ... ## $ Drama : int 0 0 0 0 1 0 0 0 1 1 ... ## $ Documentary: int 0 0 1 0 0 0 0 0 0 0 ... ## $ Romance : int 0 0 0 0 0 0 1 0 0 0 ... ## $ Short : int 1 1 1 0 0 0 0 0 0 0 ... # Build a scatter plot with mean and 95% CI ggplot(movies_small, aes(x = rating, y = votes)) + geom_point() + stat_summary(fun.data = &quot;mean_cl_normal&quot;, geom = &quot;crossbar&quot;, width = 0.2, col = &quot;red&quot;) + scale_y_log10() Next we are going to look at the diamons dataset. Recall that there are a variety of scale_ functions. Here, data are transformed or filtered first, after which the plot and associated statistics are computed. For example, scale_y_continuous(limits = c(100, 1000) will remove values outside that range. Contrast this to coord_cartesian(), which computes the statistics before plotting. That means that the plot and summary statistics are performed on the raw data. That’s why we say that coord_cartesian(c(100, 1000)) “zooms in” a plot. This was discussed in the chapter on coordinates in course 2. Here we’re going to expand on this and introduce scale_x_log10() and scale_y_log10() which perform log10 transformations, and coord_equal(), which sets an aspect ratio of 1 (coord_fixed() is also an option). str(diamonds) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 53940 obs. of 10 variables: ## $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... # Produce the plot - To get nice formatting we&#39;re using the expression() function for the labels ggplot(diamonds, aes(x = carat, y = price, col = color)) + geom_point(alpha = 0.5, size = 0.5, shape = 16) + scale_x_log10(expression(log[10](Carat)), limits = c(0.1,10)) + scale_y_log10(expression(log[10](Price)), limits = c(100,100000)) + scale_color_brewer(palette = &quot;YlOrRd&quot;) + coord_equal() + # sets the aspect ratio to 1 theme_classic() Or we can present the data as a smooth/linear model # Add smooth layer and facet the plot ggplot(diamonds, aes(x = carat, y = price, col = color)) + stat_smooth(method = &quot;lm&quot;) + scale_x_log10(expression(log[10](Carat)), limits = c(0.1,10)) + scale_y_log10(expression(log[10](Price)), limits = c(100,100000)) + scale_color_brewer(palette = &quot;YlOrRd&quot;) + coord_equal() + theme_classic() 9.2 Statistical plots Whilst all the previous plots could be considered as statistical plots, we now concentrate on those more suited to a statistical or academic audienc - two examples are box plots and density plots. In this exercise you’ll return to the first plotting exercise and see how box plots compare to dot plots for representing high-density data. 9.2.1 Box plots Box plots are very useful, but they don’t solve all your problems all the time, for example, when your data are heavily skewed, you will still need to transform it. You’ll see that here, using the movies_small dataset, a subset of 10,000 observations of ggplot2movies::movies. # Add a boxplot geom d &lt;- ggplot(movies_small, aes(x = rating, y = votes)) + geom_point() + geom_boxplot() + stat_summary(fun.data = &quot;mean_cl_normal&quot;, geom = &quot;crossbar&quot;, width = 0.2, col = &quot;red&quot;) # Untransformed plot d # Transform the scale d + scale_y_log10() # Transform the coordinates d + coord_trans(y = &quot;log10&quot;) Notice how different the normal distribution estimation (red boxes) and boxplots (less prone to outliers) are. If you only have continuous variables, you can convert them into ordinal variables using any of the following functions: cut_interval(x, n) makes n groups from vector x with equal range. cut_number(x, n) makes n groups from vector x with (approximately) equal numbers of observations. cut_width(x, width) makes groups of width width from vector x. This is useful when you want to summarize a complex scatter plot. By applying these functions to the carat variable and mapping that onto the group aesthetic, you can convert the scatter plot in the viewer into a series of box plots on the fly. Going from a continuous to a categorical variable reduces the amount of information, but sometimes that helps us understand the data. # Plot object p p &lt;- ggplot(diamonds, aes(x = carat, y = price)) # Use cut_interval p + geom_boxplot(aes(group = cut_interval(carat, n = 10))) # Use cut_number p + geom_boxplot(aes(group = cut_number(carat, n = 10))) # Use cut_width p + geom_boxplot(aes(group = cut_width(carat, width = 0.25))) Be aware that there are many ways to calculate the IQR, short for inter-quartile range (that is Q3−Q1Q3−Q1). These are defined in the help pages for the quantile() function: ?quantile Generally, the IQR becomes more consistent across methods as the sample size increases, you are likely to encounter spurious artefacts when drawing box plots with small sample sizes. 9.2.2 Density Plots Density plots are similar to histograms but less well used. They are used to display the distribution of univariate data, such as probabilty density functions (PDFs). One aspect you can set is the bandwidth, which helps to determine how ‘lumpy’ or how high the seperation is between each individual peak in a dataset. To make a straightforward density plot, add a geom_density() layer. Before plotting, you will calculate the emperical density function, similar to how you can use the density() function in the stats package, available by default when you start R. The following default parameters are used (you can specify these arguments both in density() as well as geom_density()): bw = “nrd0”, telling R which rule to use to choose an appropriate bandwidth. kernel = “gaussian”, telling R to use the Gaussian kernel. There is some test data, containing three columns: norm, bimodal and uniform. Each column represents 200 samples from a normal, bimodal and uniform distribution. # Load the test data load(&quot;D:/CloudStation/Documents/2017/RData/test_datasets.RData&quot;) test_data &lt;- ch1_test_data # Calculating density: d d &lt;- density(test_data$norm) # Use which.max() to calculate mode mode &lt;- d$x[which.max(d$y)] # Finish the ggplot call ggplot(test_data, aes(x = norm)) + geom_rug() + geom_density() + geom_vline(xintercept = mode, col = &quot;red&quot;) Sometimes it is useful to compare a histogram with a density plot. However, the histogram’s y-scale must first be converted to frequency instead of absolute count. After doing so, you can add an empirical PDF using geom_density() or a theoretical PDF using stat_function(). # Arguments you&#39;ll need later on fun_args &lt;- list(mean = mean(test_data$norm), sd = sd(test_data$norm)) # Finish the ggplot ggplot(test_data, aes(x = norm)) + geom_histogram(aes(y = ..density..)) + geom_density(col = &quot;red&quot;) + stat_function(fun = dnorm, args = fun_args, col = &quot;blue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. There are three parameters that you may be tempted to adjust in a density plot: bw - the smoothing bandwidth to be used, see ?density for details adjust - adjustment of the bandwidth, see density for details kernel - kernel used for density estimation, defined as “g” = gaussian “r” = rectangular “t” = triangular “e” = epanechnikov “b” = biweight “c” = cosine “o” = optcosine In this exercise you’ll use a dataset containing only four points, small_data, so that you can see how these three arguments affect the shape of the density plot. The vector get_bw contains the bandwidth that is used by default in geom_density(). p is a basic plotting object that you can start from. x &lt;- c(-3.5, 0.0, 0.5, 6.0) small_data &lt;- data.frame(x) # Get the bandwith get_bw &lt;- density(small_data$x)$bw # Basic plotting object p &lt;- ggplot(small_data, aes(x = x)) + geom_rug() + coord_cartesian(ylim = c(0,0.5)) # Create three plots p + geom_density() p + geom_density(adjust = 0.25) p + geom_density(bw = 0.25 * get_bw) # Create two plots p + geom_density(kernel = &quot;r&quot;) p + geom_density(kernel = &quot;e&quot;) Notice how the curve contained more features and their individual heights were increased as the bandwidth decreased. 9.3 Multiple groups or variables Groups = levels of a factor variable. A drawback of showing a box plot per group, is that you don’t have any indication of the sample size, n, in each group, that went into making the plot. One way of dealing with this is to use a variable width for the box, which reflects differences in n. # Diamond box plot sized according to the number of observations ggplot(diamonds, aes(x = cut, y = price, col = color)) + geom_boxplot(varwidth = TRUE) + facet_grid(. ~ color) This helps us see the differences in group size, but unfortunately there is no legend, so it’s not a complete solution. The next section of code combines multiple density plots. Here, you’ll combine just two distributions, a normal and a bimodal. The first thing to remember is that you can consider values as two separate variables, like in the test_data data frame, or as a single continuous variable with their ID as a separate categorical variable, like in the test_data2 data frame. test_data2 is more convenient for combining and comparing multiple distributions. A small number of overlapping density plots are a fantastic way of comparing distinct distributions, for example, when descriptive statistics only (mean and sd) don’t represent the data well enough. # Load the data test_data &lt;- ch1_test_data test_data2 &lt;- ch1_test_data2 # check the structure str(test_data) ## &#39;data.frame&#39;: 200 obs. of 3 variables: ## $ norm : num -0.5605 -0.2302 1.5587 0.0705 0.1293 ... ## $ bimodal: num 0.199 -0.688 -2.265 -1.457 -2.414 ... ## $ uniform: num -0.117 -0.537 -1.515 -1.812 -0.949 ... str(test_data2) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ dist : Factor w/ 2 levels &quot;norm&quot;,&quot;bimodal&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ value: num -0.5605 -0.2302 1.5587 0.0705 0.1293 ... # Plot with test_data ggplot(test_data, aes(x = norm)) + geom_rug() + geom_density() # Plot two distributions with test_data2 ggplot(test_data2, aes(x = value, fill = dist, col = dist)) + geom_rug(alpha = 0.6) + geom_density(alpha = 0.6) When you looked at multiple box plots, you compared the total sleep time of various mammals, sorted according to their eating habits. One thing you noted is that for insectivores, box plots didn’t really make sense, since there were only 5 observations to begin with. You decided that you could nonetheless use the width of a box plot to show the difference in sample size between the groups. Here, you’ll see a similar thing with density plots. A cleaned up version of the mammalian dataset is first loaded as mammals. mammals &lt;- readRDS(&quot;D:/CloudStation/Documents/2017/RData/mammals.RDS&quot;) # Individual densities ggplot(mammals[mammals$vore == &quot;Insectivore&quot;, ], aes(x = sleep_total, fill = vore)) + geom_density(col = NA, alpha = 0.35) + scale_x_continuous(limits = c(0, 24)) + coord_cartesian(ylim = c(0, 0.3)) # With faceting ggplot(mammals, aes(x = sleep_total, fill = vore)) + geom_density(col = NA, alpha = 0.35) + scale_x_continuous(limits = c(0, 24)) + coord_cartesian(ylim = c(0, 0.3)) + facet_wrap( ~ vore, nrow = 2) # Note that by default, the x ranges fill the scale ggplot(mammals, aes(x = sleep_total, fill = vore)) + geom_density(col = NA, alpha = 0.35) + scale_x_continuous(limits = c(0, 24)) + coord_cartesian(ylim = c(0, 0.3)) # Trim each density plot individually ggplot(mammals, aes(x = sleep_total, fill = vore)) + geom_density(col = NA, alpha = 0.35, trim = TRUE) + scale_x_continuous(limits=c(0,24)) + coord_cartesian(ylim = c(0, 0.3)) When plotting a single variable, the density plots (and their bandwidths) are calculated separate for each variable (see the first plot). However, when you compare several variables (such as eating habits) it’s useful to see the density of each subset in relation to the whole data set. This holds true for multiple density plots as well as for violin plots. For this, we need to weight the density plots so that they’re relative to each other. Each density plot is adjusted according to what proportion of the total data set each sub-group represents. We calculated this using the dplyr commands in the third section. After executing the commnads, it will have the variable n, which we’ll use for weighting. To generate the weighted density plot, use aes() to map n onto the weight aesthetic inside geom_density(). The results will be more detailed and accurate. # Unweighted density plot from before ggplot(mammals, aes(x = sleep_total, fill = vore)) + geom_density(col = NA, alpha = 0.35) + scale_x_continuous(limits = c(0, 24)) + coord_cartesian(ylim = c(0, 0.3)) # Unweighted violin plot ggplot(mammals, aes(x = vore, y = sleep_total, fill = vore)) + geom_violin() # Calculate weighting measure library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union mammals2 &lt;- mammals %&gt;% group_by(vore) %&gt;% mutate(n = n() / nrow(mammals)) -&gt; mammals # Weighted density plot ggplot(mammals2, aes(x = sleep_total, fill = vore)) + geom_density(aes(weight = n), col = NA, alpha = 0.35) + scale_x_continuous(limits = c(0, 24)) + coord_cartesian(ylim = c(0, 0.3)) ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density # Weighted violin plot ggplot(mammals2, aes(x = vore, y = sleep_total, fill = vore)) + geom_violin(aes(weight = n), col = NA) ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density ## Warning in density.default(x, weights = w, bw = bw, adjust = adjust, kernel ## = kernel, : sum(weights) != 1 -- will not get true density We can also create 2D density plots. You can consider two orthogonal density plots in the form of a 2D density plot. Just like with a 1D density plot, you can adjust the bandwidth of both axes independently. The data is stored in the faithful data frame, available in the datasets package. The object p contains the base definitions of a plot. Think about the message in your scatter plots, sometimes clusters of high-density are more intersting than linear models. # Base layers p &lt;- ggplot(faithful, aes(x = waiting, y = eruptions)) + scale_y_continuous(limits = c(1, 5.5), expand = c(0, 0)) + scale_x_continuous(limits = c(40, 100), expand = c(0, 0)) + coord_fixed(60 / 4.5) # 1 - Use geom_density_2d() p + geom_density_2d() # 2 - Use stat_density_2d() with arguments p + stat_density_2d(aes(col = ..level..), h = c(5, 0.5)) Next we use the viridis package. This package contains multi-hue color palettes suitable for continuous variables. The advantage of these scales is that instead of providing an even color gradient for a continuous scale, they highlight the highest values by using an uneven color gradient on purpose. The high values are lighter colors (yellow versus blue), so they stand out more. # Load in the viridis package library(viridis) ## Loading required package: viridisLite # Add viridis color scale ggplot(faithful, aes(x = waiting, y = eruptions)) + scale_y_continuous(limits = c(1, 5.5), expand = c(0,0)) + scale_x_continuous(limits = c(40, 100), expand = c(0,0)) + coord_fixed(60/4.5) + stat_density_2d(geom = &quot;tile&quot;, aes(fill = ..density..), h=c(5,.5), contour = FALSE) + scale_fill_viridis() 9.4 Plots for Specific Data 1 In this and the next section, we cover a lot of different plot types which are meant for specific use cases. They give an overview of the different plots so you can remember them when you have appropriate data, even if that isn’t very often. 9.4.1 Big data This could be in the form of many observations, or it could be many variables (multidimensional data) or some combination thereof. In the case of many observations (big n) there are some techniques we can use: Reducing overplotting Reducing the amount of information that is plotted Aggregating data In the case of multidimensional or hign data (big p) there are other techniques we can use: Data Reduction methods (e.g. PCA) Use facets in plots Use a SPLOM - Scatter PLOt Matrix, a nice example is the chart.Correlation fun in PerformanceAnalytics package Use a parallel coordinate plot, which can be used for continous and discreet data, inluding those on different scales First we will look at SPLOMs. Base R features two useful quick-and-dirty pairs plots functions. They both only take continuous variables. There are two datasets - iris dataset and mtcars_fact, a version of mtcars where categorical variables have been converted into actual factor columns. Scatter PLOt Matrices. # Convert nums to factors where needed for mtcars mtcars_fact &lt;- mtcars mtcars_fact[c(2, 8:11)] &lt;- lapply(mtcars_fact[c(2, 8:11)], as.factor) # pairs pairs(iris[1:4]) # chart.Correlation library(PerformanceAnalytics) chart.Correlation(iris[1:4]) # ggpairs library(GGally) ggpairs(mtcars_fact[1:3]) Instead of using an off-the-shelf correlation matrix function, you can of course create your own plot. For starters, a correlation matrix can be calculated using, for example, cor(dataframe) (if all variables are numerical). Before you can use your data frame to create your own correlation matrix plot, you’ll need to get it in the right format. There is a definition of cor_list(), a function that re-formats the data frame x. Here, L is used to add the points to the lower triangle of the matrix, and M is used to add the numerical values as text to the upper triangle of the matrix. With reshape2::melt(), the correlation matrices L and M are each converted into a three-column data frame: the x and y axes of the correlation matrix make up the first two columns and the corresponding correlation coefficient makes up the third column. These become the new variables “points” and “labels”, which can be mapped onto the size aesthetic for the points in the lower triangle and onto the label aesthetic for the text in the upper triangle, respectively. Their values will be the same, but their positions on the plot will be symmetrical about the diagonal! Merging L and M, you have everything you need. We use reshape2 instead of tidyr is that reshape2::melt() can handle a matrix, whereas tidyr::gather() requires a data frame. At this point you just need to understand how to use the output from cor_list(). First use dplyr to execute this function on the continuous variables in the iris data frame (the first four columns), but separately for each species. Next, you’ll actually plot the resulting data frame with ggplot2 functions. library(reshape2) cor_list &lt;- function(x) { L &lt;- M &lt;- cor(x) M[lower.tri(M, diag = TRUE)] &lt;- NA M &lt;- melt(M) names(M)[3] &lt;- &quot;points&quot; L[upper.tri(L, diag = TRUE)] &lt;- NA L &lt;- melt(L) names(L)[3] &lt;- &quot;labels&quot; merge(M, L) } # Calculate xx with cor_list library(dplyr) xx &lt;- iris %&gt;% group_by(Species) %&gt;% do(cor_list(.[1:4])) # Finish the plot ggplot(xx, aes(x = Var1, y = Var2)) + geom_point( aes(col = points, size = abs(points)), shape = 16 ) + geom_text( aes(col = labels, size = abs(labels), label = round(labels, 2)) ) + scale_size(range = c(0, 6)) + scale_color_gradient2(&quot;r&quot;, limits = c(-1, 1)) + scale_y_discrete(&quot;&quot;, limits = rev(levels(xx$Var1))) + scale_x_discrete(&quot;&quot;) + guides(size = FALSE) + geom_abline(slope = -1, intercept = nlevels(xx$Var1) + 1) + coord_fixed() + facet_grid(. ~ Species) + theme(axis.text.y = element_text(angle = 45, hjust = 1), axis.text.x = element_text(angle = 45, hjust = 1), strip.background = element_blank()) ## Warning: Removed 30 rows containing missing values (geom_point). ## Warning: Removed 30 rows containing missing values (geom_text). "],
["introduction-to-data.html", "10 Introduction to Data 10.1 Language of Data 10.2 Observational Studies and Experiments 10.3 Sampling strategies and experimental design", " 10 Introduction to Data Notes taken during/inspired by the Datacamp course ‘Introduction to Data’ by Mine Cetinkaya-Rundel. The supporting textbook is Diez, Barr, and Cetinkaya-Rundel (2015). 10.1 Language of Data The course makes use of the openintro package, accompanying the textbook. Let’s load the package and our first dataset, email50. # Load packages library(&quot;openintro&quot;) library(&quot;dplyr&quot;) # Load data data(email50) # View its structure str(email50) ## &#39;data.frame&#39;: 50 obs. of 21 variables: ## $ spam : num 0 0 1 0 0 0 0 0 0 0 ... ## $ to_multiple : num 0 0 0 0 0 0 0 0 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 4 0 0 0 0 0 1 0 ... ## $ sent_email : num 1 0 0 0 0 0 0 1 1 0 ... ## $ time : POSIXct, format: &quot;2012-01-04 13:19:16&quot; &quot;2012-02-16 20:10:06&quot; ... ## $ image : num 0 0 0 0 0 0 0 0 0 0 ... ## $ attach : num 0 0 2 0 0 0 0 0 0 0 ... ## $ dollar : num 0 0 0 0 9 0 0 0 0 23 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 0 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 1 0 0 0 0 0 ... ## $ num_char : num 21.705 7.011 0.631 2.454 41.623 ... ## $ line_breaks : int 551 183 28 61 1088 5 17 88 242 578 ... ## $ format : num 1 1 0 0 1 0 0 1 1 1 ... ## $ re_subj : num 1 0 0 0 0 0 0 1 1 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 1 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 8 1 2 1 43 0 0 2 22 3 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 2 3 1 2 2 2 2 2 2 2 ... #glimpse the first few items using dplyr glimpse(email50) ## Observations: 50 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0... ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0... ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ cc &lt;int&gt; 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ sent_email &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ time &lt;dttm&gt; 2012-01-04 13:19:16, 2012-02-16 20:10:06, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ attach &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0... ## $ dollar &lt;dbl&gt; 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, ... ## $ winner &lt;fctr&gt; no, no, no, no, no, no, no, no, no, no, no, no, ... ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ password &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0... ## $ num_char &lt;dbl&gt; 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809... ## $ line_breaks &lt;int&gt; 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167... ## $ format &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1... ## $ re_subj &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ exclaim_mess &lt;dbl&gt; 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, ... ## $ number &lt;fctr&gt; small, big, none, small, small, small, small, sm... When using certain functions, such as filters on categorical variables, the way R handles the filtered out variables is to leave the items in as place holders (empty containers), even though the place holder is empty. This can have undesirable effects, particularly if using the filtered object for modelling. We then end up with zero values which are actually filtered out factors. # Subset of emails with big numbers: email50_big email50_big &lt;- email50 %&gt;% filter(number == &quot;big&quot;) # Glimpse the subset glimpse(email50_big) ## Observations: 7 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0 ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ cc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0 ## $ sent_email &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0 ## $ time &lt;dttm&gt; 2012-02-16 20:10:06, 2012-02-04 23:26:09, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ attach &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ dollar &lt;dbl&gt; 0, 0, 3, 2, 0, 0, 0 ## $ winner &lt;fctr&gt; no, no, yes, no, no, no, no ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ password &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 8 ## $ num_char &lt;dbl&gt; 7.011, 10.368, 42.793, 26.520, 6.563, 11.223, 10.613 ## $ line_breaks &lt;int&gt; 183, 198, 712, 692, 140, 512, 225 ## $ format &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ re_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0 ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_mess &lt;dbl&gt; 1, 1, 2, 7, 2, 9, 9 ## $ number &lt;fctr&gt; big, big, big, big, big, big, big # Table of number variable - now we have just 7 values table(email50_big$number) ## ## none small big ## 0 0 7 # Drop levels email50_big$number &lt;- droplevels(email50_big$number) # Another table of number variable table(email50_big$number) ## ## big ## 7 In some instance you want to create a discreet function from a numeric value. That is to say we want to create a categorical value based on some groups of numbers. This can be achived as shown below. Note that when calculating a function R will typically either: Assign a value e.g. med_num_char &lt;- median(email50$num_char) Print a result e.g. median(email50$num_char) But we can do both by adding brackets (med_num_char &lt;- median(email50$num_char)) # Calculate median number of characters: med_num_char (med_num_char &lt;- median(email50$num_char)) ## [1] 6.8895 # Create num_char_cat variable in email50 email50 &lt;- email50 %&gt;% mutate(num_char_cat = ifelse(num_char &lt; med_num_char, &quot;below median&quot;, &quot;at or above median&quot;)) # Count emails in each category table(email50$num_char_cat) ## ## at or above median below median ## 25 25 We can also use the mutate function from dplyr to create a new variable from categorical variables # Create number_yn column in email50 email50 &lt;- email50 %&gt;% mutate(number_yn, ifelse(number == &quot;none&quot;, &quot;no&quot;, &quot;yes&quot;)) # Visualize number_yn ggplot(email50, aes(x = number_yn)) + geom_bar() We often want to compare two or three variables, which is most easily done using the ggplot package # Load ggplot2 library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:openintro&#39;: ## ## diamonds # Scatterplot of exclaim_mess vs. num_char ggplot(email50, aes(x = num_char, y = exclaim_mess, color = factor(spam))) + geom_point() 10.2 Observational Studies and Experiments Typically there are two types of study, if we are interested in whether variable Y is caused by some factors (X) we could have two types of studies. Observational Study: We are observing, rather than specifically interfere or direct how the data is collected - only correlation can be inferred. In this case, we might survey people and look for patterns in their characteristics (X) and the outcome variable (Y) Experimental Study: We randomly assign subjects to various treatments - causation can be inferred. In this case, we would get a group of individuals together then randomly assign them to a group of interest (X), removing the decision from the subjects of the study, we often have a control group also. Another differentiation to be aware of is between Random sampling: We select our subjects at random in order that we can make inferences from our sample, to the wider population Random assignment: Subjects are randomly assigned to various treatments and helps us to make causal conclusions We can therefore combine random sampling with random assignment, to allow causal and generalisable conclusions, however in practice we typically have one or the other - random sampling only (not causal but generalisable), or random assignment (causal but not generalisable) - the negation of both leads to results that are neither causal nor generalisable, but may highlight a need for further research. Sometimes when there are looking for associations between variables, it is possible to omit variables of interest, which may be confounding variables. For instance, we may have two variables (x) that appear to show a relationship with another (y) but the inclusion of a third variable (x’) causes the apparent relationship to breakdown. If we fail to consider other associated variables, we may fall in to a Simpsons Paradox in which a trend appears in different groups, but disappears when the groups are combined together. Simpsons paradox is a form of Ecological Fallacy. One of the best known examples of Simpsons Paradox comes from admissions data for University of California, Berkeley. library(tidyr) data(&quot;UCBAdmissions&quot;) ucb_admit &lt;- as.data.frame(UCBAdmissions) # Restrucutre data - this is to follow the example provided, it takes the aggregated data from the original data frame and disaggregates # it using indexing by repeating the row indices Freq times for each row - see https://stackoverflow.com/questions/45445919/convert-wide-to-long-with-frequency-column ucb_admit_disagg = ucb_admit[rep(1:nrow(ucb_admit), ucb_admit$Freq), -grep(&quot;Freq&quot;, names(ucb_admit))] # Count number of male and female applicants admitted ucb_counts &lt;- ucb_admit_disagg %&gt;% count(Gender, Admit) # View result ucb_counts ## # A tibble: 4 x 3 ## Gender Admit n ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 Male Admitted 1198 ## 2 Male Rejected 1493 ## 3 Female Admitted 557 ## 4 Female Rejected 1278 # Spread the output across columns and calculate percentages ucb_counts %&gt;% spread(Admit, n) %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 2 x 4 ## Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Male 1198 1493 0.4451877 ## 2 Female 557 1278 0.3035422 So far, it seems that the results suggest females are less likely to be admitted, but what if we look at the results by department? # Table of counts of admission status and gender for each department admit_by_dept &lt;- ucb_admit_disagg %&gt;% count(Dept, Gender, Admit) %&gt;% spread(Admit, n) # View result admit_by_dept ## # A tibble: 12 x 4 ## Dept Gender Admitted Rejected ## * &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 A Male 512 313 ## 2 A Female 89 19 ## 3 B Male 353 207 ## 4 B Female 17 8 ## 5 C Male 120 205 ## 6 C Female 202 391 ## 7 D Male 138 279 ## 8 D Female 131 244 ## 9 E Male 53 138 ## 10 E Female 94 299 ## 11 F Male 22 351 ## 12 F Female 24 317 # Percentage of those admitted to each department admit_by_dept %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 12 x 5 ## Dept Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A Male 512 313 0.62060606 ## 2 A Female 89 19 0.82407407 ## 3 B Male 353 207 0.63035714 ## 4 B Female 17 8 0.68000000 ## 5 C Male 120 205 0.36923077 ## 6 C Female 202 391 0.34064081 ## 7 D Male 138 279 0.33093525 ## 8 D Female 131 244 0.34933333 ## 9 E Male 53 138 0.27748691 ## 10 E Female 94 299 0.23918575 ## 11 F Male 22 351 0.05898123 ## 12 F Female 24 317 0.07038123 Now we begin to see that for some departments, there is a higher proportion of females being accpeted than males. Equally for some departments, the rejection rate is very high for both males and females e.g. Dept F. In 4 of the 6 departments, females have a higher proportion of applications being admitted than males. Males tended to apply to less competitive departments than females, the less competitive departments had higher admission rates. 10.3 Sampling strategies and experimental design We use sampling when we do not want, for whatever reason, to conduct a full Census. A simple random sample is usually the most basic method. We can also use stratified sampling to ensure representation from certain groups. Or we use cluster sampling usually for economic reasons. Or some combination in multistage sampling. data(county) # Simple random sample: county_srs county_srs &lt;- county %&gt;% sample_n(size = 20) # Count counties by state county_srs %&gt;% group_by(state) %&gt;% count() ## # A tibble: 18 x 2 ## # Groups: state [18] ## state n ## &lt;fctr&gt; &lt;int&gt; ## 1 Arkansas 1 ## 2 Colorado 1 ## 3 Georgia 1 ## 4 Illinois 2 ## 5 Indiana 1 ## 6 Kansas 1 ## 7 Maryland 1 ## 8 Minnesota 1 ## 9 Mississippi 1 ## 10 Nebraska 1 ## 11 North Dakota 1 ## 12 Pennsylvania 1 ## 13 South Dakota 1 ## 14 Texas 1 ## 15 Utah 1 ## 16 Virginia 2 ## 17 West Virginia 1 ## 18 Wisconsin 1 For a stratified sample we would do something similar. # Stratified sample states_str &lt;- us_regions %&gt;% group_by(region) %&gt;% sample_n(size = 2) # Count states by region states_str %&gt;% group_by(region) %&gt;% count() The principles of experimental design include 4 key components: Control: compare treatment of interest to a control group Randomise: randomly assign subjects to treatments Replicate: collect a sufficiently large sample within a study, or replicate the entire study Block: account for the potential effect of confounding variables We group subjects into blocks based on these confounding variables, then randomise within each block to treatment groups. So for instance, if we were testing whether an online or classroom R course was more effective using an experiment, one possible confounding variable would be previous programming experience. Therefore we would seperate out - block - those with and those without previous programming experience, ensuring we have an equal number in each treatment group (online vs classroom) of those with and without previous experience. In random sampling, you use stratifying to control for a variable. In random assignment, you use blocking to achieve the same goal. References "],
["references-1.html", "References", " References "],
["foundations-of-inference.html", "11 Foundations of Inference 11.1 Introduction to Inference 11.2 Home Ownership by Gender 11.3 Density Plots 11.4 Gender Discrimination (p-values) 11.5 Opportunity Cost 11.6 Type I and Type II errors 11.7 Bootstrapping", " 11 Foundations of Inference Notes taken during/inspired by the Datacamp course ‘Foundations of Inference’ by Jo Hardin, collaborators; Nick Carchedi and Tom Jeon. 11.1 Introduction to Inference Classical statistical inference is the process of making claims about a population based on a sample of information. We are making an inference from a small group (sample) to a much larger one (population). We typically have: Null Hypothesis \\(H_{0}\\): What we are researching has no effect Alternate Hypothesis \\(H_{A}\\): What we are researching does have an effect Under the null hypothesis, chance alone is responsible for the results. Under the alternate hypothesis, we reject the null hypothesis, by using statistical techniques that indicate that chance is not responsible for our findings. Hypothesis or statistical testing goes back over 300 years, with the first recorded use by John Arbuthnot: Table 4.1: Statistical Testing Applications Year Person Context 1710 Arbuthnot Sex ratio at birth 1767 Michelle Distribution of stars 1823 Laplace Moon phase and barometric changes 1900 K. Pearson Goodness of fit 1908 Gosset A single mean Source: (Huberty 1993, pg 318) Contemporary statistical testing is a usually that of either Fisher or Neyman-Pearson approaches. Fisher tends to use a single hypothesis test and a p-value strength of evidence test, where as the Neyman-Pearson test will set a critical alpha value and compare the null hypothesis against an alternative hypothesis, rejecting the null if the test statistic is high enough (Huberty 1993, pg 318). The course goes on to say that idea behind statistical inference is to understand samples from a hypothetical population, where the null hypothesis is true - there is no difference between two groups. We can do this by calculating one statistic - for instance the proportion (mean) of a test group who show a positive response when testing a new drug, compared to a placebo control group - for each repeated sample from a population, then work out the difference between these two groups means. With each sample, the mean will change, resulting in a changing difference for each sample. We can then generate a distribution (histogram) of differences, assuming the null hypothesis - that there is no link between drug effectiveness between a test group and a control group - is true. “Generating a distribution of the statistic from the null population gives information about whether the observed data are inconsistent with the null hypothesis”. That is to say, by taking repeated samples and creating a distribution, we can then say whether our observed difference is consistent (within an acceptable value range due to chance) to the null hypothesis. The null samples consist of randomly shuffled drug effectiveness variables (permuted samples from the population), so that the samples don’t have any dependency between the two groups and effectiveness. 11.2 Home Ownership by Gender Data used in the exercises are from NHANES 2009-2012 With Adjusted Weighting. This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. # Load packages library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) library(&quot;NHANES&quot;) library(&quot;oilabs&quot;) # Create bar plot for Home Ownership by Gender ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Relative frequencies&quot;) # Density for SleepHrsNight coloured by SleepTrouble, faceted by HealthGen ggplot(NHANES, aes(x = SleepHrsNight, col = SleepTrouble)) + geom_density(adjust = 2) + facet_wrap(~ HealthGen) Next we want to create a selection for just our variables of interest - rent and owner occupation. # Subset the data: homes homes &lt;- NHANES %&gt;% select(Gender, HomeOwn) %&gt;% filter(HomeOwn %in% c(&quot;Own&quot;, &quot;Rent&quot;)) We build a distribution of differences assuming the null hypothesis - that there is no link between gender and home ownership - is true. In this first step, we just do a single iteration, or permutation from the true values. The null (permuted) version here will create a randomly shuffled home ownership variable, so that the permuted version does not have any dependency between gender and homeownership. We effectively have the same gender split variables as per the original, with the same owned and rented proportions, but disassociated from the gender variable - just randomly shuffled. # Perform one permutation homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 0.0008207949 It is easier to see what is going on by breaking the results down iteratively. Our selected and filtered homes dataset looks like. head(homes) ## # A tibble: 6 x 2 ## Gender HomeOwn ## &lt;fctr&gt; &lt;fctr&gt; ## 1 male Own ## 2 male Own ## 3 male Own ## 4 male Own ## 5 female Rent ## 6 male Rent Next we shuffle this data, let’s call it homes 2. we can then check the total number of owns and rents are the same using the summary function, which confirms the data is just randomly shuffled. homes2 &lt;- homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) tail(homes2) ## # A tibble: 6 x 3 ## # Groups: Gender [2] ## Gender HomeOwn HomeOwn_perm ## &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; ## 1 male Rent Rent ## 2 male Rent Rent ## 3 female Own Rent ## 4 male Own Rent ## 5 male Own Own ## 6 male Own Own summary(homes2) ## Gender HomeOwn HomeOwn_perm ## female:4890 Own :6425 Own :6425 ## male :4822 Rent :3287 Rent :3287 ## Other: 0 Other: 0 Then we calculate the mean value of home ownership (Own) across our original and shuffled (permutated) data homes3 &lt;- homes2 %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) homes3 ## # A tibble: 2 x 3 ## Gender prop_own_perm prop_own ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 0.6640082 0.6654397 ## 2 male 0.6590626 0.6576109 FFinally we calculate the differences in ownership - note that the difference for the permuted value here may be different from the full code above, as it a new random permutation and we have used the set.seed() function which would create an identical permutation. homes4 &lt;- homes3 %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) homes4 ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 -0.00494555 11.3 Density Plots Next we can make multiple permutations using the rep_sample_n from the oilabs package. We specify the data (tbl), the sample size, the number of samples to take (reps), and whether sampling should be done with or without replacement (replace). The output includes a new column, replicate, which indicates the sample number. We can create 100 permutations and create a dot plot of the results. # Perform 100 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 100) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Dotplot of 100 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_dotplot(binwidth = .001) We can go further and run 1000 permutations and create a density chart. set.seed(666) # Perform 1000 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 1000) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Density plot of 1000 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() Now we have our density plot of the null hypothesis - randomly permuted samples - we can see where our actual observed difference lies, plus how many randomly permuted differences were less than the observed difference. # Plot permuted differences ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences homeown_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 21.5 So in this instance, when we set the seed of 666 we end up with 20.5% of randomly shuffled (permuted) differences being greater than the observed difference, so the observed difference is consistent with the null hypothesis. That it to say it is within the range we may expect by chance alone, were we to repeat the exercise, although we should specify a distribtion we are comparing against, in this which is inferred as being the normal distribution in this instance. We can therefore say that there is no statistically significant difference between gender and home ownership. Or put more formally We fail to reject the null hypothesis: There is no evidence that our data are inconsistent with the null hypothesis 11.4 Gender Discrimination (p-values) In this section we use data from Rosen and Jerdee (1974), where 48 male bank supervisors were given personnel files and asked if they should be promoted to Branch Manager. All files were identical, but half (24) were named as female, and the other half (24) were named male. The results showed 21 males were promoted and 14 females, meaning 35 of the total 48 were promoted. In Rosen and Jerdee (1974) sex was given along with an indication of the difficulty - routine or complex - here we only look at the routine promotion candidates. Do we know if gender is a statistically significant factor? Null Hypothesis \\(H_{0}\\): Gender and promotion are unrelated variables Alternate Hypothesis \\(H_{A}\\): Men are more likely to be promoted First, we create the data frame disc disc &lt;- data.frame( promote = c(rep(&quot;promoted&quot;, 35), rep(&quot;not_promoted&quot;, 13)), sex = c(rep(&quot;male&quot;, 21), rep(&quot;female&quot;, 14), rep(&quot;male&quot;, 3), rep(&quot;female&quot;, 10)) ) Then let’s see the resulting table and proportion who were promoted table(disc) ## sex ## promote female male ## not_promoted 10 3 ## promoted 14 21 disc %&gt;% group_by(sex) %&gt;% summarise(promoted_prop = mean(promote == &quot;promoted&quot;)) ## # A tibble: 2 x 2 ## sex promoted_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 female 0.5833333 ## 2 male 0.8750000 So there difference in promotions by gender is around 0.3 or around 30%, but could this be due to chance? We can create 1000 permutations and compare our observed diffrence to the distribution, plus how many randomly permuted differences were less than the observed difference. # Create a data frame of differences in promotion rates set.seed(42) disc_perm &lt;- disc %&gt;% rep_sample_n(size = nrow(disc), reps = 1000) %&gt;% mutate(prom_perm = sample(promote)) %&gt;% group_by(replicate, sex) %&gt;% summarize(prop_prom_perm = mean(prom_perm == &quot;promoted&quot;), prop_prom = mean(promote == &quot;promoted&quot;)) %&gt;% summarize(diff_perm = diff(prop_prom_perm), diff_orig = diff(prop_prom)) # male - female # Histogram of permuted differences ggplot(disc_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences disc_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 99.3 So here, just 0.5% of the randomly permuted/shuffled results are greater than our observed promotion differences, or 99.5% are lower, so our results are definitely quite extreme. We typically use a 5% cut off, which the course mentions is arbitrary and historic, being attributed to Fisher. So we can say at 0.5% our value is within this critical region, meaning the results are statistically significant - we should not ignore them. We can calculate quantiles of the null statistic using our randomly generated shuffles. disc_perm %&gt;% summarize(q.90 = quantile(diff_perm, p = 0.90), q.95 = quantile(diff_perm, p = 0.95), q.99 = quantile(diff_perm, p = 0.99)) ## # A tibble: 1 x 3 ## q.90 q.95 q.99 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2083333 0.2083333 0.2916667 So here, 95% of our null differences are 0.208 or lower, indeed 99% are 0.292 or lower, so our observed difference of 0.3 is quite extreme - it is in the critical region of the distribution. We can go one step further by calculating the p-value. The p-value is: the probability of observing data as or more extreme than what we actually got given that the null hypothesis is true. disc_perm %&gt;% summarize(mean(diff_orig &lt;= diff_perm)) ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.025 So the p-value here is 0.028 (less than 3 %). If repeat the exercise with smaller and larger number of shuffles we would get different p-values. ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.01 ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.0227 With 100 shuffles our p-value is 0.03, and with 10,000 shuffles our p-value is 0.0235. If we had a two-tailed test - for instance if we said the original research hypothesis had focused on any difference in promotion rates between men and women instead of focusing on whether men are more likely to be promoted than women - we could simple double the p-value. In both cases, the p-value is below or close to the 0.05 (5%) critical value, meaning we can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. However, as both values are close to the critical value, we should indicate that more work should be done. Indeed since the Rosen and Jerdee (1974) study, many further studies have been undertaken and found a similar pattern of discrimination. 11.5 Opportunity Cost In Frederick et al. (2009) their study showed that when potential purchasers were reminded that if they did not buy a particular DVD they could instead save the money, when compared to a control group who were just told they could not buy the DVD, those being reminded of the saving appeared to be more inclined not to make the purchase - 34 in the treatment group did not buy compared to 19 in the control. So our test is setup as: Null Hypothesis \\(H_{0}\\): Reminding students will have no impact on their spending decisions Alternate Hypothesis \\(H_{A}\\): Reminding students will reduce the chance they continue with a purchase We can create a data frame containing the results and find the initial proportions. #create the data frame opportunity &lt;- data.frame( decision = c(rep(&quot;buyDVD&quot;, 97), rep(&quot;nobuyDVD&quot;, 53)), group = c(rep(&quot;control&quot;, 56), rep(&quot;treatment&quot;, 41), rep(&quot;control&quot;, 19), rep(&quot;treatment&quot;, 34)) ) # Tabulate the data opportunity %&gt;% select(decision, group) %&gt;% table() ## group ## decision control treatment ## buyDVD 56 41 ## nobuyDVD 19 34 # Find the proportion who bought the DVD in each group opportunity %&gt;% group_by(group) %&gt;% summarize(buy_prop = mean(decision == &quot;buyDVD&quot;)) ## # A tibble: 2 x 2 ## group buy_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 control 0.7466667 ## 2 treatment 0.5466667 So around 55% of the treatment group - those who were reminded they could save the money - bought the DVD, comapred to 75% of the control group. We can represent this with a bar plot. As before, we can calculate 1000 random shuffles and then compare our difference in proportions, to the distribution of those 1000 samples. And finally, we can calculate the p-value ## # A tibble: 1 x 1 ## `mean(diff_perm &lt;= diff_orig)` ## &lt;dbl&gt; ## 1 0.012 In this instance, of p-value is substantially less than the usual critical value - 0.8% versus the usual value of 5% - so we can can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. Our results would only occur 8 times in 1000 by chance. We can therefore accept the alternative hypothesis (\\(H_{A}\\)) that reminding students does cause them to be less likely to buy a DVD, as they were randomly assigned to the treatment and control groups, therefore any difference is due to the reminder to save. Who can we therefore make the inference to? Our sample was drawn from the student population for the Frederick et al. (2009) study, so we would be able to generalise to that student population however defined, but not to another wider population. 11.6 Type I and Type II errors In our research and conslusions there is a risk that we will be incorrect, we will make an error. The two errors are: Type I error : The null hypothesis (\\(H_{0}\\)) is true, but is rejected. On the basis of the evidence, we have decided to erroneously accept the alternative hypothesis (\\(H_{A}\\)) when in fact the null hypothesis is correct. It is sometimes called a false positive. Type II error : the null hypothesis is false, but erroneously fails to be rejected. On the basis of the evidence, we have failed to accept the alternative hypothesis despite it being correct - an effect that exists in the population. It is sometimes called a false negative. If we return to our previous example, our associated errors would be Type I: There is not a difference in proportions, but the observed difference is big enough to indicate that the proportions are different. Type II: There is a difference in proportions, but the observed difference is not large enough to indicate that the proportions are different. 11.7 Bootstrapping Sometimes we are not neccessarily interested in testing a hypothesis, we are instead interested in making a claim about how our sample can be inferred to a large population. To do so we use confidece intervals. When calculating confidence intervals there is no null hypothesis like in hypothesis testing. We need to understand how samples from our population vary around the parameter of interest. In an ideal world we would take many samples from the population or know what the true value is in the population, but realistically this is not possible, so we use booststrapping. Bootstrapping is the process of taking repeated samples from the same sample, to estimate the variability. As our population parameters are not known, we can use our sample to estimate a simulated population parameter (\\(\\hat{p}*\\)) by repeated sampling. We can then estimate other parameters such as the standard deviation, s.e. and the confidence interval. Instead of taking repeated samples from our population, we take repeated samples from our data, with replacement, each bootstrap sample is the same size as the original sample. Figure 11.1: Illustration of the bootstrap approach on a small sample containing n = 3 observations (James et al. 2013, pg 190) Firstly we setup our single poll, where 70% (21/30) are intended to vote for a particular candidate # Setup our single poll example one_poll &lt;- sample(rep(c(0, 1), times = c(9,21))) one_poll &lt;- tbl_df(one_poll) colnames(one_poll) &lt;- &quot;vote&quot; Next we can create 1000 bootstrap samples from this original poll, then calculate the variability set.seed(42) # Generate 1000 resamples of one_poll: one_poll_boot_30 one_poll_boot_30 &lt;- one_poll %&gt;% rep_sample_n(size = 30, replace = TRUE, reps = 1000) # Compute p-hat* for each resampled poll ex1_props &lt;- one_poll_boot_30 %&gt;% summarize(prop_yes = mean(vote)) %&gt;% summarize(sd(prop_yes)) #compute variability p-hat* ex1_props ## # A tibble: 1 x 1 ## `sd(prop_yes)` ## &lt;dbl&gt; ## 1 0.08624387 So the variability - the standard error or SE - of \\(\\hat{p}*\\) is 0.0841. We can now use this SE to calculate a confidence interval, since 95% of samples will be within +/- 1.96 standard errors of the centre of the distribution assuming a normal distribution \\(N(\\mu, \\sigma ^2)\\). We also use the bootstrap to calculate our bootstrap confidence interval, to give a range of possible values. # Compute p-hat for one poll p_hat &lt;- mean(one_poll$vote) set.seed(42) # Bootstrap to find the SE of p-hat: one_poll_boot one_poll_boot &lt;- one_poll %&gt;% rep_sample_n(30, replace = TRUE, reps = 1000) %&gt;% summarize(prop_yes_boot = mean(vote)) # Create an interval of possible values one_poll_boot %&gt;% summarize(lower = p_hat - 1.96 * sd(prop_yes_boot), upper = p_hat + 1.96 * sd(prop_yes_boot)) ## # A tibble: 1 x 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.530962 0.869038 So our possible range of values, using the bootstrap at 95%, is between 53.2% and 86.8%. Going back to our original statement, we had a single poll where 70% of those polled intended to vote for a particular candidate. We can now say, using the bootstrap t-confidence interval, we are 95% confident that the true proportion planning to vote for the candidate is between 53% and 87%. We are assuming that the distribution is normally distributed \\(N(\\mu, \\sigma ^2)\\). References "],
["references-2.html", "References", " References "],
["exploratory-data-analysis.html", "12 Exploratory Data Analysis 12.1 Categorical Data 12.2 Numerical Data 12.3 Numerical Summaries 12.4 Email Case Study", " 12 Exploratory Data Analysis Notes taken during/inspired by the Datacamp course “Exploratory Data Analysis” by Andrew Bray. 12.1 Categorical Data Common functions when looking at categorical, aka factors variables, are levels(df\\(var) and to get a contigency or xtab table the table(df\\)var1, df$var2). We can also create bar charts to visually represent the data using ggplot. # Read in our dataset thanks to fivethirtyeight https://github.com/fivethirtyeight/data/tree/master/comic-characters comics &lt;- read.csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/comic-characters/dc-wikia-data.csv&quot;, stringsAsFactors = TRUE) comics$name &lt;- as.character(comics$name) # Check levels of align levels(comics$ALIGN) ## [1] &quot;&quot; &quot;Bad Characters&quot; &quot;Good Characters&quot; ## [4] &quot;Neutral Characters&quot; &quot;Reformed Criminals&quot; # Check the levels of gender levels(comics$SEX) ## [1] &quot;&quot; &quot;Female Characters&quot; ## [3] &quot;Genderless Characters&quot; &quot;Male Characters&quot; ## [5] &quot;Transgender Characters&quot; # Create a 2-way contingency table table(comics$ALIGN, comics$SEX) ## ## Female Characters Genderless Characters ## 25 220 0 ## Bad Characters 63 597 11 ## Good Characters 30 953 6 ## Neutral Characters 7 196 3 ## Reformed Criminals 0 1 0 ## ## Male Characters Transgender Characters ## 356 0 ## Bad Characters 2223 1 ## Good Characters 1843 0 ## Neutral Characters 359 0 ## Reformed Criminals 2 0 To simplify an analysis, it often helps to drop levels with small amounts of data. In R, this requires two steps: first filtering out any rows with the levels that have very low counts, then removing these levels from the factor variable with droplevels(). This is because the droplevels() function would keep levels that have just 1 or 2 counts; it only drops levels that don“t exist in a dataset. # Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Remove align level comics &lt;- comics %&gt;% filter(ALIGN != &quot;Reformed Criminals&quot;) %&gt;% droplevels() While a contingency table represents the counts numerically, it“s often more useful to represent them graphically. Here you“ll construct two side-by-side barcharts of the comics data. This shows that there can often be two or more options for presenting the same data. Passing the argument position =”dodge&quot; to geom_bar() says that you want a side-by-side (i.e. not stacked) barchart. # Load ggplot2 library(ggplot2) # Create side-by-side barchart of gender by alignment ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;dodge&quot;) # Create side-by-side barchart of alignment by gender ggplot(comics, aes(x = SEX, fill = ALIGN)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 90)) When creatign tables, it is often easier to look at proportions for patterns rather than counts. We can do this using conditional proportions, by using the prop.table(df_counts, n) where n is the number we want to condition our frequency/count table by, 1 = rows and 2 = columns. tab &lt;- table(comics$ALIGN, comics$SEX) options(scipen = 999, digits = 2) # Print fewer digits prop.table(tab) # Joint proportions (totals in the entire table) ## ## Female Characters Genderless Characters ## 0.00363 0.03192 0.00000 ## Bad Characters 0.00914 0.08661 0.00160 ## Good Characters 0.00435 0.13826 0.00087 ## Neutral Characters 0.00102 0.02843 0.00044 ## ## Male Characters Transgender Characters ## 0.05165 0.00000 ## Bad Characters 0.32250 0.00015 ## Good Characters 0.26737 0.00000 ## Neutral Characters 0.05208 0.00000 prop.table(tab, 2) # Conditional on columns (column totals) ## ## Female Characters Genderless Characters ## 0.200 0.112 0.000 ## Bad Characters 0.504 0.304 0.550 ## Good Characters 0.240 0.485 0.300 ## Neutral Characters 0.056 0.100 0.150 ## ## Male Characters Transgender Characters ## 0.074 0.000 ## Bad Characters 0.465 1.000 ## Good Characters 0.385 0.000 ## Neutral Characters 0.075 0.000 Here we see that approx. 49% of female characters are good, compared to 39% for males. Bar charts can tell dramatically different stories depending on whether they represent counts or proportions and, if proportions, what the proportions are conditioned on. To demonstrate this difference, you“ll construct two barcharts in this exercise: one of counts and one of proportions. # Plot of gender by align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar() # Plot proportion of gender, conditional on align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;fill&quot;) Conditional barchart Now, if you want to break down the distribution of alignment based on gender, you“re looking for conditional distributions. You could make these by creating multiple filtered datasets (one for each gender) or by faceting the plot of alignment based on gender. As a point of comparison, we“ve provided your plot of the marginal distribution of alignment from the last exercise. # Plot of alignment broken down by gender ggplot(comics, aes(x = ALIGN)) + geom_bar() + facet_wrap(~ SEX) 12.2 Numerical Data # Data courtesy of http://www.idvbook.com/teaching-aid/data-sets/ with some variable name modifictions to match those in the exercise library(readxl) cars &lt;- read_excel(&quot;04cars data.xls&quot;, sheet = 1) cars &lt;- cars[-2] # remove variable 2 # Rename vars names(cars) &lt;- c(&quot;name&quot;, &quot;sports_car&quot;, &quot;suv&quot;, &quot;wagon&quot;, &quot;minivan&quot;, &quot;pickup&quot;, &quot;all_wheel&quot;, &quot;rear_wheel&quot;, &quot;msrp&quot;, &quot;dealer_cost&quot;, &quot;eng_size&quot;, &quot;ncyl&quot;, &quot;horsepwr&quot;, &quot;city_mpg&quot;,&quot;hwy_mpg&quot;, &quot;weight&quot;, &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;) # Change data tpyes as needed cars[2:7] &lt;- sapply(cars[2:7],as.logical) cars[c(8:10,12:19)] &lt;- sapply(cars[c(8:10,12:19)],as.integer) ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion # Learn data structure str(cars) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 428 obs. of 19 variables: ## $ name : chr &quot;Acura 3.5 RL 4dr&quot; &quot;Acura 3.5 RL w/Navigation 4dr&quot; &quot;Acura MDX&quot; &quot;Acura NSX coupe 2dr manual S&quot; ... ## $ sports_car : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ suv : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ wagon : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ rear_wheel : int 0 0 0 1 0 0 0 0 0 0 ... ## $ msrp : int 43755 46100 36945 89765 23820 33195 26990 25940 31840 42490 ... ## $ dealer_cost: int 39014 41100 33337 79978 21761 30299 24647 23508 28846 38325 ... ## $ eng_size : num 3.5 3.5 3.5 3.2 2 3.2 2.4 1.8 3 3 ... ## $ ncyl : int 6 6 6 6 4 6 4 4 6 6 ... ## $ horsepwr : int 225 225 265 290 200 270 200 170 220 220 ... ## $ city_mpg : int 18 18 17 17 24 20 22 22 20 20 ... ## $ hwy_mpg : int 24 24 23 24 31 28 29 31 28 27 ... ## $ weight : int 3880 3893 4451 3153 2778 3575 3230 3252 3462 3814 ... ## $ wheel_base : int 115 115 106 100 101 108 105 104 104 105 ... ## $ length : int 197 197 189 174 172 186 183 179 179 180 ... ## $ width : int 72 72 77 71 68 72 69 70 70 70 ... # Create faceted histogram ggplot(cars, aes(x = city_mpg)) + geom_histogram() + facet_wrap(~ suv) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). The mileage of a car tends to be associated with the size of its engine (as measured by the number of cylinders). To explore the relationship between these two variables, you could stick to using histograms, but in this exercise you“ll try your hand at two alternatives: the box plot and the density plot. # Filter cars with 4, 6, 8 cylinders common_cyl &lt;- filter(cars, ncyl %in% c(4,6,8)) # Create box plots of city mpg by ncyl ggplot(common_cyl, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 11 rows containing non-finite values (stat_boxplot). # Create overlaid density plots for same data ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) + geom_density(alpha = .3) ## Warning: Removed 11 rows containing non-finite values (stat_density). Now, turn your attention to a new variable: horsepwr. The goal is to get a sense of the marginal distribution of this variable and then compare it to the distribution of horsepower conditional on the price of the car being less than $25,000. You“ll be making two plots using the”data pipeline&quot; paradigm, where you start with the raw data and end with the plot. In addition to indicating the center and spread of a distribution, a box plot provides a graphical means to detect outliers. You can apply this method to the msrp column (manufacturer“s suggested retail price) to detect if there are unusually expensive or cheap cars. # Create hist cars %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + ggtitle(&quot;ALL Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Create hist of horsepwr for affordable cars cars %&gt;% filter(msrp &lt; 25000) %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ggtitle(&quot;Affordable Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 1 rows containing missing values (geom_bar). # Construct box plot of msrp cars %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() # Exclude outliers from data cars_no_out &lt;- cars %&gt;% filter(msrp &lt; 100000) # Construct box plot of msrp using the reduced dataset cars_no_out %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() Consider two other columns in the cars dataset: city_mpg and width. Which is the most appropriate plot for displaying the important features of their distributions? Remember, both density plots and box plots display the central tendency and spread of the data, but the box plot is more robust to outliers. # Create plot of city_mpg cars %&gt;% ggplot(aes(x = width)) + geom_density() ## Warning: Removed 28 rows containing non-finite values (stat_density). # Create plot of width cars %&gt;% ggplot(aes(x = 1, y = city_mpg)) + geom_boxplot() ## Warning: Removed 14 rows containing non-finite values (stat_boxplot). Faceting is a valuable technique for looking at several conditional distributions at the same time. If the faceted distributions are laid out in a grid, you can consider the association between a variable and two others, one on the rows of the grid and the other on the columns. # Facet hists using hwy mileage and ncyl common_cyl %&gt;% ggplot(aes(x = hwy_mpg)) + geom_histogram() + facet_grid(ncyl ~ suv) + ggtitle(&quot;Faceted heavy mpg histograms by No. of Cyl and Suv&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 11 rows containing non-finite values (stat_bin). 12.3 Numerical Summaries Throughout this chapter, you will use data from gapminder, which tracks demographic data in countries of the world over time. To learn more about it, you can bring up the help file with ?gapminder. For this exercise, focus on how the life expectancy differs from continent to continent. This requires that you conduct your analysis not at the country level, but aggregated up to the continent level. This is made possible by the one-two punch of group_by() and summarize(), a very powerful syntax for carrying out the same analysis on different subsets of the full dataset. library(gapminder) # Create dataset of 2007 data gap2007 &lt;- filter(gapminder, year == 2007) # Compute groupwise mean and median lifeExp gap2007 %&gt;% group_by(continent) %&gt;% summarize(mean(lifeExp), median(lifeExp)) ## # A tibble: 5 x 3 ## continent `mean(lifeExp)` `median(lifeExp)` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 55 53 ## 2 Americas 74 73 ## 3 Asia 71 72 ## 4 Europe 78 79 ## 5 Oceania 81 81 # Generate box plots of lifeExp for each continent gap2007 %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() Let“s extend the powerful group_by() and summarize() syntax to measures of spread. If you”re unsure whether you“re working with symmetric or skewed distributions, it”s a good idea to consider a robust measure like IQR in addition to the usual measures of variance or standard deviation. # Compute groupwise measures of spread gap2007 %&gt;% group_by(continent) %&gt;% summarize(sd(lifeExp), IQR(lifeExp), n()) ## # A tibble: 5 x 4 ## continent `sd(lifeExp)` `IQR(lifeExp)` `n()` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 9.63 11.61 52 ## 2 Americas 4.44 4.63 25 ## 3 Asia 7.96 10.15 33 ## 4 Europe 2.98 4.78 30 ## 5 Oceania 0.73 0.52 2 # Generate overlaid density plots gap2007 %&gt;% ggplot(aes(x = lifeExp, fill = continent)) + geom_density(alpha = 0.3) # Compute stats for lifeExp in Americas gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% summarize(mean(lifeExp), sd(lifeExp)) ## # A tibble: 1 x 2 ## `mean(lifeExp)` `sd(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 74 4.4 # Compute stats for population gap2007 %&gt;% summarize(median(pop), IQR(pop)) ## # A tibble: 1 x 2 ## `median(pop)` `IQR(pop)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10517531 26702008 12.3.1 Transformations In some data, there are different ‘humps’ in the distribution, which are calls the modes or the modality of the dataset Unimode - single mean, this is a normal distribtion Bimodal - two common distributions Multimodal - three modes or more We also should consider whether the distribution is skewed. Right skewed data has a long tail to the right, with the majority of the distribution to the left - we often see this with income distributions. Left skewed has a small number of observations to the left and the majoirty of the distribution to the right. A normal distribution is typically smyterical. Highly skewed distributions can make it very difficult to learn anything from a visualization. Transformations can be helpful in revealing the more subtle structure. Here you’ll focus on the population variable, which exhibits strong right skew, and transform it with the natural logarithm function (log() in R). # Create density plot of old variable gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() # Transform the skewed pop variable gap2007 &lt;- gap2007 %&gt;% mutate(log_pop = log(pop)) # Create density plot of new variable gap2007 %&gt;% ggplot(aes(x = log_pop)) + geom_density() 12.3.2 Outliers It is often useful within a dataset to identify, using a column, whether the data is an outlier, this can be done by using the mutate function e.g. df &lt;- df %&gt;% mutate(is_outlier &gt; value), then filtering and arranging the resulting table e.g. df %&gt;% filter(is_outlier) %&gt;% arrange(desc(value)). We can also use this outlier column to remove the values from a plot e.g. df %&gt;% filter(!is_outlier) %&gt;% ggplot … The determination of the outlier value might be arbitary, or you could use a percentile value (say top or bottom 2%). # Filter for Asia, add column indicating outliers gap_asia &lt;- gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% mutate(is_outlier = lifeExp &lt;50) # Remove outliers, create box plot of lifeExp gap_asia %&gt;% filter(!is_outlier) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() 12.4 Email Case Study The example EDA comes from manually classified 3,900+ emails from the openintro package. Is there an association between spam and the length of an email? You could imagine a story either way: Spam is more likely to be a short message tempting me to click on a link, or *My normal email is likely shorter since I exchange brief emails with my friends all the time. Here, you’ll use the email dataset to settle that question. Begin by bringing up the help file and learning about all the variables with ?email. As you explore the association between spam and the length of an email, use this opportunity to try out linking a dplyr chain with the layers in a ggplot2 object. library(openintro) ## Please visit openintro.org for free statistics materials ## ## Attaching package: &#39;openintro&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## cars ## The following object is masked from &#39;package:ggplot2&#39;: ## ## diamonds ## The following objects are masked from &#39;package:datasets&#39;: ## ## cars, trees # Compute summary statistics email %&gt;% group_by(spam) %&gt;% summarise(median(num_char), IQR(num_char)) ## # A tibble: 2 x 3 ## spam `median(num_char)` `IQR(num_char)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.8 13.6 ## 2 1 1.0 2.8 # Create plot email %&gt;% mutate(log_num_char = log(num_char)) %&gt;% ggplot(aes(x = factor(spam), y = log_num_char)) + geom_boxplot() Let’s look at a more obvious indicator of spam: exclamation marks. exclaim_mess contains the number of exclamation marks in each message. Using summary statistics and visualization, see if there is a relationship between this variable and whether or not a message is spam. Note: when computing the log(0) is -Inf in R, which isn’t a very useful value! You can get around this by adding a small number (like .01) to the quantity inside the log() function. This way, your value is never zero. This small shift to the right won’t affect your results. # Compute center and spread for exclaim_mess by spam email %&gt;% group_by(spam) %&gt;% summarise(mean(exclaim_mess), sd(exclaim_mess)) ## # A tibble: 2 x 3 ## spam `mean(exclaim_mess)` `sd(exclaim_mess)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.5 48 ## 2 1 7.3 80 # Create plot for spam and exclaim_mess email %&gt;% ggplot(aes(log(exclaim_mess)+0.1)) + geom_histogram() + facet_wrap( ~ spam) + ggtitle(&quot;Number of exclamation marks by not-spam vs spam&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1435 rows containing non-finite values (stat_bin). If it was difficult to work with the heavy skew of exclaim_mess, the number of images attached to each email (image) poses even more of a challenge. table(email$image) ## ## 0 1 2 3 4 5 9 20 ## 3811 76 17 11 2 2 1 1 Recall that this tabulates the number of cases in each category (so there were 3811 emails with 0 images, for example). Given the very low counts at the higher number of images, let’s collapse image into a categorical variable that indicates whether or not the email had at least one image. In this exercise, you’ll create this new variable and explore its association with spam. ** Here we deal with zero inflation** by converting the many zero values and the non zeros in to a categorical variable. There are other strategies, such as doing analysis on these two groups seperatley. # Create plot of proportion of spam by image email %&gt;% mutate(has_image = image &gt; 0) %&gt;% ggplot(aes(x = has_image, fill = factor(spam))) + geom_bar(position = &quot;fill&quot;) Sometimes it is neccessary to check if our understanding of the data and how it has been created is correct and if the values we expect are in fact true. In this instance, we check first if the number of charecters in the email is greater than zero (which it should be), then secondly whether images count as attachments using a boolean operator. If image is never greater than attach, we can infer that images are counted as attachments. # Verify that all emails have non-negative values for num_char sum(email$num_char &lt; 0) ## [1] 0 # Test if images count as attachments sum(email$images &gt;= email$attach) ## [1] 0 When you have a specific question about a dataset, you can find your way to an answer by carefully constructing the appropriate chain of R code. For example, consider the following question: “Within non-spam emails, is the typical length of emails shorter for those that were sent to multiple people?” This can be answered with the following chain: email %&gt;% filter(spam == &quot;not-spam&quot;) %&gt;% group_by(to_multiple) %&gt;% summarize(median(num_char)) ## # A tibble: 0 x 2 ## # ... with 2 variables: to_multiple &lt;dbl&gt;, median(num_char) &lt;lgl&gt; The code makes it clear that you are using num_char to measure the length of an email and median() as the measure of what is typical. If you run this code, you’ll learn that the answer to the question is “yes”: the typical length of non-spam sent to multiple people is a bit lower than those sent to only one person. This chain concluded with summary statistics, but others might end in a plot; it all depends on the question that you’re trying to answer. For emails containing the word “dollar”, does the typical spam email contain a greater number of occurrences of the word than the typical non-spam email? Create a summary statistic that answers this question. If you encounter an email with greater than 10 occurrences of the word “dollar”, is it more likely to be spam or not-spam? Create a barchart that answers this question. # Question 1 email %&gt;% filter(dollar &gt; 0) %&gt;% group_by(spam) %&gt;% summarize(median(dollar)) ## # A tibble: 2 x 2 ## spam `median(dollar)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 4 ## 2 1 2 # Question 2 email %&gt;% filter(dollar &gt; 10) %&gt;% ggplot(aes(x = spam)) + geom_bar() Turn your attention to the variable called number. To explore the association between this variable and spam, select and construct an informative plot. For illustrating relationships between categorical variables, you’ve seen # Reorder levels email$number &lt;- factor(email$number, levels = c(&quot;none&quot;, &quot;small&quot;, &quot;big&quot;)) # Construct plot of number ggplot(email, aes(x = number)) + geom_bar() + facet_wrap(~ spam) "],
["correlation-and-regression.html", "13 Correlation and Regression 13.1 Visualizing two variables 13.2 Correlation 13.3 Linear Regression 13.4 Model fit", " 13 Correlation and Regression Notes taken during/inspired by the Datacamp course ‘Correlation and Regression’ by Ben Baumer. 13.1 Visualizing two variables Some common terminology of data includes Response variable a.k.a. y, dependent (usually on the vertical axis if using a scatter plot) Explanatory variable, something you think might be related to the response a.k.a. x, independent, predictor (usually on the horizontal axis) library(openintro) library(ggplot2) library(dplyr) library(tidyr) # load the data data(ncbirths) # Scatterplot of weight vs. weeks ggplot(ncbirths, aes(weeks, weight)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). If it is helpful, you can think of boxplots as scatterplots for which the variable on the x-axis has been discretized. The cut() function takes two arguments: the continuous variable you want to discretize and the number of breaks that you want to make in that continuous variable in order to discretize it. # Boxplot of weight vs. weeks ggplot(data = ncbirths, aes(x = cut(weeks, breaks = 5), y = weight)) + geom_boxplot() 13.1.1 Transformations Here the relationship is hard to see. data(mammals) # Mammals scatterplot ggplot(mammals, aes(BodyWt, BrainWt)) + geom_point() The relationship between two variables may not be linear. In these cases we can sometimes see strange and even inscrutable patterns in a scatterplot of the data. Sometimes there really is no meaningful relationship between the two variables. Other times, a careful transformation of one or both of the variables can reveal a clear relationship. ggplot2 provides several different mechanisms for viewing transformed relationships. The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes. # Scatterplot with coord_trans() ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + geom_point() + coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) # Scatterplot with scale_x_log10() and scale_y_log10() ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + geom_point() + scale_x_log10() + scale_y_log10() 13.1.2 Identifying Outliers It is clear here, using the Using the mlbBat10 dataset, a scatterplot illustrates how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP). data(&quot;mlbBat10&quot;) # Baseball player scatterplot ggplot(mlbBat10, aes(OBP, SLG)) + geom_point() Most of the points are clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty is caused by a few outlying players whose on-base percentages (OBPs) were exceptionally high. These values are present in our dataset only because these players had very few batting opportunities. Both OBP and SLG are known as rate statistics, since they measure the frequency of certain events (as opposed to their count). In order to compare these rates sensibly, it makes sense to include only players with a reasonable number of opportunities, so that these observed rates have the chance to approach their long-run frequencies. In Major League Baseball, batters qualify for the batting title only if they have 3.1 plate appearances per game. This translates into roughly 502 plate appearances in a 162-game season. The mlbBat10 dataset does not include plate appearances as a variable, but we can use at-bats (AB) – which constitute a subset of plate appearances – as a proxy. # Scatterplot of SLG vs. OBP mlbBat10 %&gt;% filter(AB &gt;= 200) %&gt;% ggplot(aes(x = OBP, y = SLG)) + geom_point() # Identify the outlying player mlbBat10 %&gt;% filter(AB &gt;= 200, OBP &lt; 0.2) ## name team position G AB R H 2B 3B HR RBI TB BB SO SB CS OBP ## 1 B Wood LAA 3B 81 226 20 33 2 0 4 14 47 6 71 1 0 0.174 ## SLG AVG ## 1 0.208 0.146 13.2 Correlation We typically calculate the Pearsons aka Pearson product-moment correlation. The cor(x, y) function will compute the Pearson product-moment correlation between variables, x and y. Since this quantity is symmetric with respect to x and y, it doesn’t matter in which order you put the variables. At the same time, the cor() function is very conservative when it encounters missing data (e.g. NAs). The use argument allows you to override the default behavior of returning NA whenever any of the values encountered is NA. Setting the use argument to “pairwise.complete.obs” allows cor() to compute the correlation coefficient for those observations where the values of x and y are both not missing. data(ncbirths) # Compute correlation between the birthweight and mother&#39;s age ncbirths %&gt;% summarize(N = n(), r = cor(mage, weight)) ## N r ## 1 1000 0.05506589 # Compute correlation for all non-missing pairs ncbirths %&gt;% summarize(N = n(), r = cor(weight, weeks, use = &quot;pairwise.complete.obs&quot;)) ## N r ## 1 1000 0.6701013 13.2.1 Anscombe Dataset In 1973, Francis Anscombe famously created four synthetic datasets with remarkably similar numerical properties, but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable, set, that distinguishes the quartet. data(&quot;anscombe&quot;) # Tidy the data for plotting Anscombe &lt;- anscombe %&gt;% mutate(id = seq_len(n())) %&gt;% gather(key, value, -id) %&gt;% separate(key, c(&quot;variable&quot;, &quot;set&quot;), 1, convert = TRUE) %&gt;% mutate(set = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)[set]) %&gt;% spread(variable, value) # Plot the four variants ggplot(data = Anscombe, aes(x = x, y = y)) + geom_point() + facet_wrap(~ set) # Compute statistics for the sets Anscombe %&gt;% group_by(set) %&gt;% summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x,y)) ## # A tibble: 4 x 7 ## set N `mean(x)` `sd(x)` `mean(y)` `sd(y)` `cor(x, y)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11 9 3.316625 7.500909 2.031568 0.8164205 ## 2 2 11 9 3.316625 7.500909 2.031657 0.8162365 ## 3 3 11 9 3.316625 7.500000 2.030424 0.8162867 ## 4 4 11 9 3.316625 7.500909 2.030579 0.8165214 13.3 Linear Regression The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a “best fit” line that cuts through the data in a way that minimizes the distance between the line and the data points. We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we’ll set this argument to the value “lm”. data(bdims) # Scatterplot with regression line ggplot(data = bdims, aes(x = hgt, y = wgt)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) Sometimes it is better to think of the model error as ‘noise’ which we might try to better incorporate by creating a better model. Two facts enable you to compute the slope b1 and intercept b0 of a simple linear regression model from some basic summary statistics. First, the slope can be defined as: b1=rX,Y⋅sYsX where rX,Y represents the correlation (cor()) of X and Y and sX and sY represent the standard deviation (sd()) of X and Y, respectively. Second, the point (x¯,y¯)is always on the least squares regression line, where x¯and y¯denote the average of x and y, respectively. The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (Y) as a function of height (X). N &lt;- c(1507) r &lt;- c(0.7173011) mean_hgt &lt;- c(171.1438) sd_hgt &lt;- c(9.407205) mean_wgt &lt;- c(69.14753) sd_wgt &lt;- c(13.34576) bdims_summary &lt;- data.frame(N, r, mean_hgt, sd_hgt, mean_wgt, sd_wgt) # Print bdims_summary bdims_summary ## N r mean_hgt sd_hgt mean_wgt sd_wgt ## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 # Add slope and intercept bdims_summary %&gt;% mutate(slope = r * sd_wgt / sd_hgt, intercept = mean_wgt - slope * mean_hgt) ## N r mean_hgt sd_hgt mean_wgt sd_wgt slope intercept ## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 1.017617 -105.0112 13.3.1 Regression to the Mean Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While “regression to the mean” and “linear regression” are not the same thing, we will examine them together in this exercise. One way to see the effects of regression to the mean is to compare the heights of parents to their children’s heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches. # Galton data from http://www.math.uah.edu/stat/data/Galton.html Galton &lt;- read.csv(&quot;Galton.csv&quot;) # Height of children vs. height of father Galton %&gt;% filter(Gender == &quot;M&quot;) %&gt;% ggplot(aes(x = Father, y = Height)) + geom_point() + geom_abline(slope = 1, intercept = 0) + geom_smooth(method = &quot;lm&quot;, se = FALSE) # Height of children vs. height of mother Galton %&gt;% filter(Gender == &quot;F&quot;) %&gt;% ggplot(aes(x = Mother, y = Height)) + geom_point() + geom_abline(slope = 1, intercept = 0) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 13.3.2 Fitting linear models While the geom_smooth(method = “lm”) function is useful for drawing linear models on a scatterplot, it doesn’t actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is lm(). This function generally takes two arguments: A formula that specifies the model A data argument for the data frame that contains the data you want to use to fit the model The lm() function return a model object having class “lm”. This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc. # Linear model for weight as a function of height lm(wgt ~ hgt, data = bdims) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Coefficients: ## (Intercept) hgt ## -105.011 1.018 # Linear model for SLG as a function of OBP lm(SLG ~ OBP, data = mlbBat10) ## ## Call: ## lm(formula = SLG ~ OBP, data = mlbBat10) ## ## Coefficients: ## (Intercept) OBP ## 0.009407 1.110323 # Log-linear model for body weight as a function of brain weight lm(log(BodyWt) ~ log(BrainWt), data = mammals) ## ## Call: ## lm(formula = log(BodyWt) ~ log(BrainWt), data = mammals) ## ## Coefficients: ## (Intercept) log(BrainWt) ## -2.509 1.225 An “lm” object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information. The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2R2, adjusted R2R2, and the residual standard error. The summary of an “lm” object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.). Once you have fit a regression model, you are often interested in the fitted values (y^i) and the residuals (ei), where i indexes the observations. The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable. In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the fitted.values() and residuals() functions mod &lt;- lm(wgt ~ hgt, data = bdims) # Show the coefficients coef(mod) ## (Intercept) hgt ## -105.011254 1.017617 # Show the full output summary(mod) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## hgt 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 # Mean of weights equal to mean of fitted values? mean(bdims$wgt) == mean(fitted.values(mod)) ## [1] TRUE # Mean of the residuals mean(residuals(mod)) ## [1] -1.266971e-15 As you fit a regression model, there are some quantities (e.g. R2) that apply to the model as a whole, while others apply to each observation (e.g. y^i). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables. The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals. # Load broom library(broom) # Create bdims_tidy bdims_tidy &lt;- augment(mod) ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead # Glimpse the resulting data frame glimpse(bdims_tidy) ## Observations: 507 ## Variables: 9 ## $ wgt &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62.... ## $ hgt &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 18... ## $ .fitted &lt;dbl&gt; 72.05406, 73.37697, 91.89759, 84.77427, 85.48661, 7... ## $ .se.fit &lt;dbl&gt; 0.4320546, 0.4520060, 1.0667332, 0.7919264, 0.81834... ## $ .resid &lt;dbl&gt; -6.4540648, -1.5769666, -11.1975919, -12.1742745, -... ## $ .hat &lt;dbl&gt; 0.002154570, 0.002358152, 0.013133942, 0.007238576,... ## $ .sigma &lt;dbl&gt; 9.312824, 9.317005, 9.303732, 9.301360, 9.312471, 9... ## $ .cooksd &lt;dbl&gt; 5.201807e-04, 3.400330e-05, 9.758463e-03, 6.282074e... ## $ .std.resid &lt;dbl&gt; -0.69413418, -0.16961994, -1.21098084, -1.31269063,... 13.4 Model fit One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are. However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or root mean squared error (RMSERMSE). R calls this quantity the residual standard error. To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. # View summary of model summary(mod) ## ## Call: ## lm(formula = wgt ~ hgt, data = bdims) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.743 -6.402 -1.231 5.059 41.103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -105.01125 7.53941 -13.93 &lt;2e-16 *** ## hgt 1.01762 0.04399 23.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.308 on 505 degrees of freedom ## Multiple R-squared: 0.5145, Adjusted R-squared: 0.5136 ## F-statistic: 535.2 on 1 and 505 DF, p-value: &lt; 2.2e-16 # Compute the mean of the residuals mean(residuals(mod)) ## [1] -1.266971e-15 # Compute RMSE sqrt(sum(residuals(mod)^2) / df.residual(mod)) ## [1] 9.30804 Another measure we can use is R squared, whihc is the he coefficient of determination. This gives us the interpretation of R2 as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model. In the example above, our model has an r-squared value of 51.5%. We can also calculate the R-squared value manually if desired. # Compute R-squared bdims_tidy %&gt;% summarize(var_y = var(wgt), var_e = var(.resid)) %&gt;% mutate(R_squared = 1 - var_e / var_y) ## var_y var_e R_squared ## 1 178.1094 86.46839 0.5145208 13.4.1 Unusual points As the model tries to fit the data on average, some extreme values can overly influence the model. We can quantify how much influence a particular point has by using the leverage, which is a measure for each observation as a function of the value of the explanatory variable and the mean of the explanatory variable. Therefore points to the centre line have a low leverage score, whilst points far from the line have a higher leverage. The explanatory variable y does not come in to effect. This can be calculated as the .hat value using augment() from broom. It is possible to have a value with a high leverage but a low overall impact on the model, if the point lies close to the line of the model. In this case, the residual is small for the point. Conversely, a point with a high leverage score and a high residual - a point laying a distance a way from other meaures and not predicted well by the model - does have an impact.We say such a point is influential. Numerically we can use cooks distance (.cooksd)to quantify this influence, which can also be calculated using the augment() function from broom. # Rank points of high leverage mod %&gt;% augment() %&gt;% arrange(desc(.hat)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## wgt hgt .fitted .se.fit .resid .hat .sigma .cooksd ## 1 85.5 198.1 96.57863 1.255712 -11.078629 0.01819968 9.303950 0.0133734319 ## 2 90.9 197.1 95.56101 1.214264 -4.661012 0.01701803 9.314916 0.0022081690 ## 3 49.8 147.2 44.78194 1.131432 5.018065 0.01477545 9.314548 0.0022120570 ## 4 80.7 193.5 91.89759 1.066733 -11.197592 0.01313394 9.303732 0.0097584634 ## 5 95.9 193.0 91.38878 1.046493 4.511216 0.01264027 9.315075 0.0015228117 ## 6 44.8 149.5 47.12245 1.037916 -2.322454 0.01243391 9.316688 0.0003968468 ## .std.resid ## 1 -1.2012024 ## 2 -0.5050673 ## 3 0.5431383 ## 4 -1.2109808 ## 5 0.4877505 ## 6 -0.2510763 # Rank influential points mod %&gt;% augment() %&gt;% arrange(desc(.cooksd)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## wgt hgt .fitted .se.fit .resid .hat .sigma .cooksd ## 1 73.2 151.1 48.75064 0.9737632 24.44936 0.010944356 9.252694 0.03859555 ## 2 116.4 177.8 75.92101 0.5065670 40.47899 0.002961811 9.140611 0.02817388 ## 3 104.1 165.1 62.99728 0.4914889 41.10272 0.002788117 9.135102 0.02733574 ## 4 108.6 190.5 88.84474 0.9464667 19.75526 0.010339372 9.275186 0.02377609 ## 5 67.3 152.4 50.07354 0.9223084 17.22646 0.009818289 9.285305 0.01714950 ## 6 76.8 157.5 55.26339 0.7287405 21.53661 0.006129560 9.267446 0.01661032 ## .std.resid ## 1 2.641185 ## 2 4.355274 ## 3 4.421999 ## 4 2.133444 ## 5 1.859860 ## 6 2.320888 When you have such outlying variables, you need to decide what to do. The main thing is to remove the variables from the model, but you need to consider the implications. There are other statistical techniques (see the EDA Chapter) for removing outliers. Think about whether the scope of the inference changes if you remove those values. Observations can be outliers for a number of different reasons. Statisticians must always be careful—and more importantly, transparent—when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher R2R2 is not a good enough reason! In the mlbBat10 data, the outlier with an OBP of 0.550 is Bobby Scales, an infielder who had four hits in 13 at-bats for the Chicago Cubs. Scales also walked seven times, resulting in his unusually high OBP. The justification for removing Scales here is weak. While his performance was unusual, there is nothing to suggest that it is not a valid data point, nor is there a good reason to think that somehow we will learn more about Major League Baseball players by excluding him. Nevertheless, we can demonstrate how removing him will affect our model. # Create nontrivial_players nontrivial_players &lt;- mlbBat10 %&gt;% filter(AB &gt;= 10 &amp; OBP &lt; 0.5) # Fit model to new data mod_cleaner &lt;- lm(SLG ~ OBP, data = nontrivial_players) # View model summary summary(mod_cleaner) ## ## Call: ## lm(formula = SLG ~ OBP, data = nontrivial_players) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.31383 -0.04165 -0.00261 0.03992 0.35819 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.043326 0.009823 -4.411 1.18e-05 *** ## OBP 1.345816 0.033012 40.768 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07011 on 734 degrees of freedom ## Multiple R-squared: 0.6937, Adjusted R-squared: 0.6932 ## F-statistic: 1662 on 1 and 734 DF, p-value: &lt; 2.2e-16 # Visualize new model ggplot(data = nontrivial_players, aes(x = OBP, y = SLG)) + geom_point() + geom_smooth(method = &quot;lm&quot;) 13.4.2 High leverage Points Not all points of high leverage are influential. While the high leverage observation corresponding to Bobby Scales in the previous exercise is influential, the three observations for players with OBP and SLG values of 0 are not influential. This is because they happen to lie right near the regression anyway. Thus, while their extremely low OBP gives them the power to exert influence over the slope of the regression line, their low SLG prevents them from using it. mod &lt;- lm(formula = SLG ~ OBP, data = filter(mlbBat10, AB &gt;= 10)) # Rank high leverage points mod %&gt;% augment() %&gt;% arrange(desc(.hat),desc(.cooksd)) %&gt;% head() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## SLG OBP .fitted .se.fit .resid .hat .sigma ## 1 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 2 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 3 0.000 0.000 -0.03744579 0.009956861 0.03744579 0.01939493 0.07153050 ## 4 0.308 0.550 0.69049108 0.009158810 -0.38249108 0.01641049 0.07011360 ## 5 0.000 0.037 0.01152451 0.008770891 -0.01152451 0.01504981 0.07154283 ## 6 0.038 0.038 0.01284803 0.008739031 0.02515197 0.01494067 0.07153800 ## .cooksd .std.resid ## 1 0.0027664282 0.5289049 ## 2 0.0027664282 0.5289049 ## 3 0.0027664282 0.5289049 ## 4 0.2427446800 -5.3943121 ## 5 0.0002015398 -0.1624191 ## 6 0.0009528017 0.3544561 "],
["supervised-learning.html", "14 Supervised Learning 14.1 Tree Based Models 14.2 Gradient Boosting Machines", " 14 Supervised Learning Notes taken during/inspired by the Datacamp course ‘Supervised Learning in R: Regression’ by Nina Zumel and John Mount. 14.1 Tree Based Models Tree based models can be used for both regression and classification models. Decision Trees say ‘if a AND b AND c THEN y’. We can therefore model non-linear models and multiplicative relationships - what is the affect of this AND that when combined together. We can use RMSE as a measure of accuracy of the model. The challenge with tree models is that they are interested in the model space as a whole, splitting this in to regions. Linear models can be better for linear relationships. We can adjust the tree depth, but there is a risk of overfitting (too deep/complex) or underfitting (to shallow/coarse). An ensemble model can be built combining different trees or indeed different models together, which will usually have the outcome of being better than a sinlge tree and less prone to overfitting, but at the loss of interpretability. 14.1.1 Random Forests One example of an ensemble approach is a random forest, building multiple trees from the training data. We can average the results of multiple models together to reduce the degree of overfitting. To build a random forest we perform the following Draw bootstrapped sample from training data For each sample grow a tree At each node, pick best variable to split on (from a random subset of all variables) Continue until tree is grown To score a datum, evaluate it with all the trees and average the results. We can use the ranger package to fit random forests. If the outcome is numeric, ranger will automatically do regression rather than classification. The default is for 500 trees, a minimum approach is 200. The value respect.unordered.factors will handle categorical values, set it to “order” if using cateogrical values, which will convert the values to numeric values. The measures of accuracy are R squared and OOB (Out of Bag or out of sample performance). You should still evaluate the model further using test data. In this exercise you will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July. You will use the ranger package to fit the random forest model. For this exercise, the key arguments to the ranger() call are: formula data num.trees: the number of trees in the forest. respect.unordered.factors : Specifies how to treat unordered factor variables. We recommend setting this to “order” for regression. seed: because this is a random algorithm, you will set the seed to get reproducible results Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in the variables outcome and vars, and use paste() to assemble a string representing the model formula. bikes &lt;- load(url(&quot;https://assets.datacamp.com/production/course_3851/datasets/Bikes.RData&quot;)) # bikesJuly is in the workspace str(bikesJuly) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 ... ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 ... ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ... ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... # Random seed to reproduce results seed &lt;- 423563 # The outcome column (outcome &lt;- &quot;cnt&quot;) ## [1] &quot;cnt&quot; # The input variables (vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;)) ## [1] &quot;hr&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot; ## [6] &quot;atemp&quot; &quot;hum&quot; &quot;windspeed&quot; # Create the formula string for bikes rented as a function of the inputs (fmla &lt;- paste(&quot;cnt&quot;, &quot;~&quot;, paste(vars, collapse = &quot; + &quot;))) ## [1] &quot;cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed&quot; # Load the package ranger library(ranger) # Fit and print the random forest model (bike_model_rf &lt;- ranger(fmla, # formula bikesJuly, # data num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed)) ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 In this exercise you will use the model that you fit in the previous exercise to predict bike rentals for the month of August. The predict() function for a ranger model produces a list. One of the elements of this list is predictions, a vector of predicted values. You can access predictions with the $ notation for accessing named elements of a list: predict(model, data)$predictions library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) # bikesAugust is in the workspace str(bikesAugust) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... ## $ cnt : int 47 33 13 7 4 49 185 487 681 350 ... ## $ instant : int 13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ... ## $ mnth : int 8 8 8 8 8 8 8 8 8 8 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... # bike_model_rf is in the workspace bike_model_rf ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## OOB prediction error (MSE): 8230.568 ## R squared (OOB): 0.8205434 # Make predictions on the August data bikesAugust$pred &lt;- predict(bike_model_rf, bikesAugust)$predictions # Calculate the RMSE of the predictions bikesAugust %&gt;% mutate(residual = cnt - pred) %&gt;% # calculate the residual summarize(rmse = sqrt(mean(residual^2))) # calculate rmse ## rmse ## 1 97.18347 # Plot actual outcome vs predictions (predictions on x-axis) ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() In the previous exercise, you saw that the random forest bike model did better on the August data than the quasiposson model, in terms of RMSE. In this exercise you will visualize the random forest model’s August predictions as a function of time. The corresponding plot from the quasipoisson model that you built in a previous exercise is in the workspace for you to compare. Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares. library(tidyr) # Plot predictions and cnt by date/time randomforest_plot &lt;- bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% # set start to 0, convert unit to days gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Random Forest plot&quot;) randomforest_plot The random forest model captured the day-to-day variations in peak demand better than the quasipoisson model, but it still underestmates peak demand, and also overestimates minimum demand. So there is still room for improvement. 14.1.2 One-Hot-Encoding Categorical Variables For modelling purposes, we need to convert categorical variables to indicator variables. Some R packages do this automatically, but some non-native R packages, such as the xgboost package does not. So, these categorical variables need to be converted to numeric ones. We can use the vtreat package. DesignTreatmentsZ() to design a treatment plan from the training data, then prepare() to created “clean” data all numerical no missing values use prepare() with treatment plan for all future data In this exercise you will use vtreat to one-hot-encode a categorical variable on a small example. vtreat creates a treatment plan to transform categorical variables into indicator variables (coded “lev”), and to clean bad values out of numerical variables (coded “clean”). To design a treatment plan use the function designTreatmentsZ() treatplan &lt;- designTreatmentsZ(data, varlist) data: the original training data frame varlist: a vector of input variables to be treated (as strings). designTreatmentsZ() returns a list with an element scoreFrame: a data frame that includes the names and types of the new variables: scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code) varName: the name of the new treated variable origName: the name of the original variable that the treated variable comes from code: the type of the new variable. “clean”: a numerical variable with no NAs or NaNs “lev”: an indicator variable for a specific level of the original categorical variable. (magrittr::use_series() is an alias for $ that you can use in pipes.) For these exercises, we want varName where code is either “clean” or “lev”: (newvarlist &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;) %&gt;% magrittr::use_series(varName)) To transform the data set into all numerical and one-hot-encoded variables, use prepare(): data.treat &lt;- prepare(treatplan, data, varRestrictions = newvarlist) treatplan: the treatment plan data: the data frame to be treated varRestrictions: the variables desired in the treated data # Create the dataframe for cleaning color &lt;- c(&quot;b&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;, &quot;b&quot;, &quot;r&quot;, &quot;g&quot;, &quot;b&quot;, &quot;b&quot;) size &lt;- c(13, 11, 15, 14, 13, 11, 9, 12, 7, 12) popularity &lt;- c(1.0785088, 1.3956245, 0.9217988, 1.2025453, 1.0838662, 0.8043527, 1.1035440, 0.8746332, 0.6947058, 0.8832502) dframe &lt;- cbind(color, size, popularity) dframe &lt;- as.data.frame((dframe)) # dframe is in the workspace dframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 # Create and print a vector of variable names (vars &lt;- c(&quot;color&quot;, &quot;size&quot;)) ## [1] &quot;color&quot; &quot;size&quot; # Load the package vtreat library(vtreat) # Create the treatment plan treatplan &lt;- designTreatmentsZ(dframe, vars) ## [1] &quot;desigining treatments Fri Nov 10 05:07:33 2017&quot; ## [1] &quot;designing treatments Fri Nov 10 05:07:33 2017&quot; ## [1] &quot; have level statistics Fri Nov 10 05:07:33 2017&quot; ## [1] &quot;design var color Fri Nov 10 05:07:33 2017&quot; ## [1] &quot;design var size Fri Nov 10 05:07:34 2017&quot; ## [1] &quot; scoring treatments Fri Nov 10 05:07:34 2017&quot; ## [1] &quot;have treatment plan Fri Nov 10 05:07:34 2017&quot; # Examine the scoreFrame (scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code)) ## varName origName code ## 1 color_lev_x.b color lev ## 2 color_lev_x.g color lev ## 3 color_lev_x.r color lev ## 4 color_catP color catP ## 5 size_lev_x.11 size lev ## 6 size_lev_x.12 size lev ## 7 size_lev_x.13 size lev ## 8 size_lev_x.14 size lev ## 9 size_lev_x.15 size lev ## 10 size_lev_x.7 size lev ## 11 size_lev_x.9 size lev ## 12 size_catP size catP # We only want the rows with codes &quot;clean&quot; or &quot;lev&quot; (newvars &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% magrittr::use_series(varName)) ## [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot; ## [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot; ## [9] &quot;size_lev_x.7&quot; &quot;size_lev_x.9&quot; # Create the treated training data (dframe.treat &lt;- prepare(treatplan, dframe, varRestriction = newvars)) ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 The new indicator variables have ‘lev’ in their names, and the new cleaned continuous variables have ’_clean’ in their names. The treated data is all numerical, with no missing values, and is suitable for use with xgboost and other R modeling functions. When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such novel levels appear, using model.matrix or caret::dummyVars to one-hot-encode will not work correctly. vtreat is a “safer” alternative to model.matrix for one-hot-encoding, because it can manage novel levels safely. vtreat also manages missing values in the data (both categorical and continuous). # Create the testframe for testing new vars color &lt;- c(&quot;g&quot;, &quot;g&quot;, &quot;y&quot;, &quot;g&quot;, &quot;g&quot;, &quot;y&quot;, &quot;b&quot;, &quot;g&quot;, &quot;g&quot;, &quot;r&quot;) size &lt;- c(7, 8, 10, 12, 6, 8, 12, 12, 12, 8) popularity &lt;- c(0.9733920, 0.9122529, 1.4217153, 1.1905828, 0.9866464, 1.3697515, 1.0959387, 0.9161547, 1.0000460, 1.3137360) testframe &lt;- cbind(color, size, popularity) testframe &lt;- as.data.frame((dframe)) # treatplan is in the workspace summary(treatplan) ## Length Class Mode ## treatments 4 -none- list ## scoreFrame 8 data.frame list ## outcomename 1 -none- character ## vtreatVersion 1 package_version list ## outcomeType 1 -none- character ## outcomeTarget 1 -none- character ## meanY 1 -none- logical ## splitmethod 1 -none- character # newvars is in the workspace newvars ## [1] &quot;color_lev_x.b&quot; &quot;color_lev_x.g&quot; &quot;color_lev_x.r&quot; &quot;size_lev_x.11&quot; ## [5] &quot;size_lev_x.12&quot; &quot;size_lev_x.13&quot; &quot;size_lev_x.14&quot; &quot;size_lev_x.15&quot; ## [9] &quot;size_lev_x.7&quot; &quot;size_lev_x.9&quot; # Print dframe and testframe dframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 testframe ## color size popularity ## 1 b 13 1.0785088 ## 2 r 11 1.3956245 ## 3 r 15 0.9217988 ## 4 r 14 1.2025453 ## 5 r 13 1.0838662 ## 6 b 11 0.8043527 ## 7 r 9 1.103544 ## 8 g 12 0.8746332 ## 9 b 7 0.6947058 ## 10 b 12 0.8832502 # Use prepare() to one-hot-encode testframe (testframe.treat &lt;- prepare(treatplan, testframe, varRestriction = newvars)) ## color_lev_x.b color_lev_x.g color_lev_x.r size_lev_x.11 size_lev_x.12 ## 1 1 0 0 0 0 ## 2 0 0 1 1 0 ## 3 0 0 1 0 0 ## 4 0 0 1 0 0 ## 5 0 0 1 0 0 ## 6 1 0 0 1 0 ## 7 0 0 1 0 0 ## 8 0 1 0 0 1 ## 9 1 0 0 0 0 ## 10 1 0 0 0 1 ## size_lev_x.13 size_lev_x.14 size_lev_x.15 size_lev_x.7 size_lev_x.9 ## 1 1 0 0 0 0 ## 2 0 0 0 0 0 ## 3 0 0 1 0 0 ## 4 0 1 0 0 0 ## 5 1 0 0 0 0 ## 6 0 0 0 0 0 ## 7 0 0 0 0 1 ## 8 0 0 0 0 0 ## 9 0 0 0 1 0 ## 10 0 0 0 0 0 vtreat encodes novel colors like yellow that were not present in the data as all zeros: ‘none of the known colors’. This allows downstream models to accept these novel values without crashing. In this exercise you will create one-hot-encoded data frames of the July/August bike data, for use with xgboost later on. vars defines the variable vars with the list of variable columns for the model. # The outcome column (outcome &lt;- &quot;cnt&quot;) ## [1] &quot;cnt&quot; # The input columns (vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;)) ## [1] &quot;hr&quot; &quot;holiday&quot; &quot;workingday&quot; &quot;weathersit&quot; &quot;temp&quot; ## [6] &quot;atemp&quot; &quot;hum&quot; &quot;windspeed&quot; # Load the package vtreat library(vtreat) # Create the treatment plan from bikesJuly (the training data) treatplan &lt;- designTreatmentsZ(bikesJuly, vars, verbose = FALSE) # Get the &quot;clean&quot; and &quot;lev&quot; variables from the scoreFrame (newvars &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% # get the rows you care about magrittr::use_series(varName)) # get the varName column ## [1] &quot;hr_lev_x.0&quot; ## [2] &quot;hr_lev_x.1&quot; ## [3] &quot;hr_lev_x.10&quot; ## [4] &quot;hr_lev_x.11&quot; ## [5] &quot;hr_lev_x.12&quot; ## [6] &quot;hr_lev_x.13&quot; ## [7] &quot;hr_lev_x.14&quot; ## [8] &quot;hr_lev_x.15&quot; ## [9] &quot;hr_lev_x.16&quot; ## [10] &quot;hr_lev_x.17&quot; ## [11] &quot;hr_lev_x.18&quot; ## [12] &quot;hr_lev_x.19&quot; ## [13] &quot;hr_lev_x.2&quot; ## [14] &quot;hr_lev_x.20&quot; ## [15] &quot;hr_lev_x.21&quot; ## [16] &quot;hr_lev_x.22&quot; ## [17] &quot;hr_lev_x.23&quot; ## [18] &quot;hr_lev_x.3&quot; ## [19] &quot;hr_lev_x.4&quot; ## [20] &quot;hr_lev_x.5&quot; ## [21] &quot;hr_lev_x.6&quot; ## [22] &quot;hr_lev_x.7&quot; ## [23] &quot;hr_lev_x.8&quot; ## [24] &quot;hr_lev_x.9&quot; ## [25] &quot;holiday_clean&quot; ## [26] &quot;workingday_clean&quot; ## [27] &quot;weathersit_lev_x.Clear.to.partly.cloudy&quot; ## [28] &quot;weathersit_lev_x.Light.Precipitation&quot; ## [29] &quot;weathersit_lev_x.Misty&quot; ## [30] &quot;temp_clean&quot; ## [31] &quot;atemp_clean&quot; ## [32] &quot;hum_clean&quot; ## [33] &quot;windspeed_clean&quot; # Prepare the training data bikesJuly.treat &lt;- prepare(treatplan, bikesJuly, varRestriction = newvars) # Prepare the test data bikesAugust.treat &lt;- prepare(treatplan, bikesAugust, varRestriction = newvars) # Call str() on the treated data str(bikesAugust.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday_clean : num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 0 0 1 0 0 0 ... ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Misty : num 0 0 0 0 1 1 0 1 1 1 ... ## $ temp_clean : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp_clean : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum_clean : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed_clean : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... str(bikesJuly.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ hr_lev_x.0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x.20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x.3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x.4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x.5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x.6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x.7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x.8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x.9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ holiday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday_clean : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Clear.to.partly.cloudy: num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x.Light.Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x.Misty : num 0 0 0 0 0 0 0 0 0 0 ... ## $ temp_clean : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp_clean : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum_clean : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed_clean : num 0 0.1343 0.0896 0.1343 0.194 ... 14.2 Gradient Boosting Machines Gradient boosting is an interative ensemble method, by improving the model each time. We start the model with a usually shallow tree. Next, we fit another model to the residuals ofd the model, then find the weighted sum of the second and first models that give the best fit. We can regualrise the learning by the factor eta, eta = 1 gives fast learning but with overfitting risk, smaller eta reduces speed of learning but reduces the risk of overfitting. We then repeat this process until the stopping condition is met. Gradient boosting works on the training data, so it can be easy to overfit. The best approach then is to use OOB and cross validation (CV) for each model, then determine how many trees to use. xgb.cv() is the function we use and has a number of diagnostic measures. One such measure is the xgb.cv()$evaluation_log: records estimated RMSE for each round - find the number that minimises the RMSE Inputs to xgb.cv() and xgboost() are: data: input data as matrix ; label: outcome label: vector of outcomes (also numeric) objective: for regression - “reg:linear” nrounds: maximum number of trees to fit eta: learning rate max_depth: depth of trees early_stopping_rounds: after this many rounds without improvement, stop nfold (xgb.cv() only): number of folds for cross validation. 5 is a good number verbose: 0 to stay silent. Then we use elog &lt;- as.data.frame(cv\\(evaluation_log) nrounds &lt;- which.min(elog\\)test_rmse_mean) With the resulting number being the best number of trees. We then use xbgoost with this number (nrounds &lt;- n) to get the final model. In this exercise you will get ready to build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July. The July data is loaded into your workspace. Remember that bikesJuly.treat no longer has the outcome column, so you must get it from the untreated data: bikesJuly$cnt. You will use the xgboost package to fit the random forest model. The function xgb.cv() uses cross-validation to estimate the out-of-sample learning error as each new tree is added to the model. The appropriate number of trees to use in the final model is the number that minimizes the holdout RMSE. # The July data is in the workspace ls() ## [1] &quot;bike_model_rf&quot; &quot;bikes&quot; &quot;bikesAugust&quot; ## [4] &quot;bikesAugust.treat&quot; &quot;bikesJuly&quot; &quot;bikesJuly.treat&quot; ## [7] &quot;color&quot; &quot;dframe&quot; &quot;dframe.treat&quot; ## [10] &quot;fmla&quot; &quot;newvars&quot; &quot;outcome&quot; ## [13] &quot;popularity&quot; &quot;randomforest_plot&quot; &quot;scoreFrame&quot; ## [16] &quot;seed&quot; &quot;size&quot; &quot;testframe&quot; ## [19] &quot;testframe.treat&quot; &quot;treatplan&quot; &quot;vars&quot; # Load the package xgboost library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice # Run xgb.cv cv &lt;- xgb.cv(data = as.matrix(bikesJuly.treat), label = bikesJuly$cnt, nrounds = 100, nfold = 5, objective = &quot;reg:linear&quot;, eta = 0.3, max_depth = 6, early_stopping_rounds = 10, verbose = 0 # silent ) # Get the evaluation log elog &lt;- as.data.frame(cv$evaluation_log) # Determine and print how many trees minimize training and test error elog %&gt;% summarize(ntrees.train = which.min(elog$train_rmse_mean), # find the index of min(train_rmse_mean) ntrees.test = which.min(elog$test_rmse_mean)) # find the index of min(test_rmse_mean) ## ntrees.train ntrees.test ## 1 88 78 In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test error starts to increase. It’s important to use cross-validation to find the right number of trees (as determined by ntrees.test) and avoid an overfit model. # The number of trees to use, as determined by xgb.cv ntrees &lt;- 84 # Run xgboost bike_model_xgb &lt;- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix label = bikesJuly$cnt, # column of outcomes nrounds = ntrees, # number of trees to build objective = &quot;reg:linear&quot;, # objective eta = 0.3, depth = 6, verbose = 0 # silent ) # Make predictions bikesAugust$pred &lt;- predict(bike_model_xgb, as.matrix(bikesAugust.treat)) # Plot predictions (on x axis) vs actual bike rental count ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() Overall, the scatterplot looked pretty good, but did you notice that the model made some negative predictions? In the next exercise, you’ll compare this model’s RMSE to the previous bike models that you’ve built. Finally we can calculate the RMSE # Calculate RMSE bikesAugust %&gt;% mutate(residuals = cnt - pred) %&gt;% summarize(rmse = sqrt(mean(residuals ^ 2))) ## rmse ## 1 76.36407 Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous model. Perhaps rounding negative predictions up to zero is a reasonable tradeoff. Finally we can compare the results graphically. randomforest_plot # Plot predictions and actual bike rentals as a function of time (days) bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% # set start to 0, convert unit to days gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Gradient Boosting model&quot;) We can also plot the importance of the top factors names &lt;- dimnames(data.matrix(bikesJuly.treat[,-1]))[[2]] importance_matrix &lt;- xgb.importance(names, model = bike_model_xgb) xgb.plot.importance(importance_matrix[1:10,]) Looking at the results indicates that the temperature and clear/partly cloudy and the two most important factors, followed by the windspeed. The other factors relate to the time of day - higher at commuting times (9-10 am and 6-7 pm) and lower at night (2 and 4 am). "],
["dimensional-modelling.html", "15 Dimensional Modelling 15.1 Introduction to Dimensional Data 15.2 Architecture considerations 15.3 Graphical Representations 15.4 Kimball Approach 15.5 Four-Step Dimensional Design Process 15.6 Tips", " 15 Dimensional Modelling 15.1 Introduction to Dimensional Data Dimensional modelling helps to build the ability for users to query the information, for instance analysing results by a geographic region. Multi-dimensional modelling is an extension to allowing multiple ways to analsye the information, by geographic region but also over time, by product or service, by store or office and so on. It provides a way for a system user, manager or analyst to navigate what information - the ‘information space’ - is available in a database or data warehouse, but at a more intuitive level (see Meridith 2017, lecture 4). The goal is to help understanding, exploration and to make better decisions. A dimension is simply a direction, usually query or analytically based, in which you can move. Dimensional modelling is different from Entity Relationship diagrams which are more typically used for database design, however they do share some similarities and are sometimes used for dimensional modelling particuarly by those from a database or IT background. The dimensions used therefore become the ways in which the end user wants to query the information. Typical terms used in the BI arena for helping to navigate this ‘information space’ include; ‘slice and dice’ meaning to make a large data space in to a smaller one (you are making a selection or subset of all the available data), ‘drill down’ meaning to go in to a lower level of a hierachy (moving from a geographic region to a particular store), ‘drill up’ meaning to go in to a higher level (sometimes called rolling-up) and ‘drill across’ meaning adding more data (or facts) about something, typically from another source (a different fact table). There are two slightly different interpretations of a dimensional model (Meridith 2017, lecture 4): OLAP: A dimension is a structural attribute of a data cube. A dimension acts as an index for identifying values in a multi-dimensional array Kimball: A dimension table are where the textual descriptions which relate to aspects of the business are stored In both instances however, they provide ways to interact and understand our information. There are two things we are typically trying to map: Facts: Data itself, values, sales and so on e.g. a sales transaction number and the products sold Dimensions: Different ways of presenting or quering the information, this is often in the form of attributes about the fact e.g. product specific and store details 15.1.1 Data Modelling levels There are three aspects of information with a Business Intelligence system - conceptual, logical and physical - which exist on a spectrum. Conceptual: The business needs are usually the high level conceptual solution, what things we want to include at a more abstract level Logical: We start thinking about what data to include in the model and what data is available, it starts giving something which can be implemented in to a warehouse Physical: The final solution which is usually then what is implemented in the data warehouse. It is the more technical/IT solution and may include normalisation (3NF or higher) and perhaps other database optimisations to improve performance of the system. In some instances, the conceptual and logical can become one and the same thing. Table 4.1: The three levels of data modelling Feature Conceptual Logical Logical Entity Names Y Y Entity Relationships Y Y Attributes Y Primary Keys Y Y Foreign Keys Y Y Table Names Y Column Names Y Column Data Types 15.2 Architecture considerations There are a number of different approaches to implementing a data warehouse, or Enterprise Data Warehouse (EDW) from the IT or technical perspective. However, all approaches use the dimensional data modelling technique. A full detailed explanation of all possible architecture approaches, including hybrid approaches, is not included here. Instead we discuss at a high level the three main approaches - Kimball Inmon and Data Valut - and touch on a couple of others. Where they differ in terms of data modelling in part depends on the location of the dimensional model. Kimball - as the last part of the Extract Transform and Load (ETL) process the data is structured and loaded in to the desired dimensional model(s). There is no EDW in the Kimball approach, instead the presentation area is where data is organized, stored, and made available for direct querying by users, report writers, and other analytical BI applications. Data is stored in the multi-dimensional views as different data marts, which are typically subsets of all the data originally extracted, perhaps for different business users or services Inmon - suggests that the data should be relationally designed. The data is stored in an EDW in third normal form (3NF). The dimensional model then transates the data from the EDW in to something for an end user, visualisation tool or other such BI tool, potentially including data marts. A Hub and Spoke system is often used to describe the approach, with the EDW being the hub and the spokes being the depdendent data marts. This helps to ensure a ‘single verison of the truth’ Data Vault - Centralised approach - similar to Inmon but without the dependent data marts (spokes). Users directly target the EDW and there may be many different dimensional data models Hybrid - there are various different ways this could be setup, however one way would be that data is still stored in the EDW, but the dimensional model is used to help structure the data in the EDW. Therefore the extra translation required from the EDW to a BI tool is reduced. In the Kimball approach when attributes in separate dimension tables have the same column names and domain contents. After validating the data for conformance with the defined one-to-one and many-to-one business rules [as part of the ETL processs], it may be pointless to take the final step of building a 3NF physical database, just before transforming the data once again into denormalized structures for the BI presentation area. (Kimball and Ross 2013, pg 20) For the kimball approach to work, so called ‘conformed dimensions’ must be developed which are said to conform when attributes in separate dimension tables have the same column names and domain contents (Kimball and Ross 2013, pg 51). Inmon sees that the dimensional modelling technique can cause problems when teams need different star schemas - dimensional models - which then lead to a need to combine the different joins together, or lead to issues of duplication and inconsistencies. simply doing dimensional modeling as a basis for data warehouse design leads down a dark path when multiple star joins are considered. It is never apparent that there is a problem with star joins when you are looking at just one star join. But when you look at multiple star joins, the limitations of dimensional modeling become apparent. (Inmon 2000) Inmon concludes that dimensional modelling is only really suitable for data marts (ibid). 15.3 Graphical Representations Figure 15.1: High Level Overview of a Data Warehouse (Schnider, Martino, and Eschermann 2014, pg 3) Figure 15.2: Star schema versus OLAP cube (Kimball and Ross 2013, pg 9) Figure 15.3: Star schema example (Kimball and Ross 2013, pg 16) Figure 15.4: Star and Snowflake Schemas (Sharda, Delan, and Turban 2014, pg 139) Figure 15.5: Example slices from a OLAP data cube (Sharda, Delan, and Turban 2014, pg 141) Figure 15.6: Star schema reporting (Kimball and Ross 2013, pg 17) 15.4 Kimball Approach Before work begins of the data modelling, it is neccessary to understand the needs of the business and the underlying data (Kimball and Ross 2013, pg 37). The business needs arise out of meetings with manangers, decision makers and other representatives of the business. Kimball also recommends meetings with ‘source system experts and doing high-level data profiling to assess data feasibilities’ (Kimball and Ross 2013, pg 38). Whilst the data modeller is ‘in charge’ the actual model should unfold via a series of interactive workshops with those business representatives. Data governance reps should also be involved to obtain buy-in. In this sense, the Kimball approach covers both the conceptual and physical, it may also include some considerations of physical level at initiation. 15.5 Four-Step Dimensional Design Process Kimball outlines four key decisions that are to be made during the design of a dimensional model include: Select the business process - the operational activities done by the business, these activities create the facts Declare the grain - what a single row represents. The atomic grain is the lowest data captured by the business, which is the ideal and can be aggregared (rolled-up) to other levels. Different grains must not be mixed in the same fact table Identify the dimensions - the descriptive attributes about the facts, to be used for analysis. Provide the “who, what, where, when, why, and how” (6W) context Identify the facts - the measurements (how many) from the business process, it should relate to a physical observable event, rather than reporting needs Typically the output of this process is a star schema, with a fact table at the centre supported by the associated dimension tables, with primary/forenigh key relationships. This is often then structured into a online analytical processing (OLAP) cube, which contains the facts and dimensions appropriate to the analysis, but allows for more detailed analytical capabilities than SQL. Sometimes aggregated fact tables are built to speed up query performance, as are aggregated OLAP cubes which are typically designed for users. A key advantage of the dimensional model approach is that new dimensions can be added to an existing fact table by adding a new foreign key column. Discussion then of * Thomsen diagrams OLAP Solutions (2nd ed) 2002, an abstract, but can be a little simple * ADAPT Diagrams, White Paper Bulos and Foresman - included in some Microsoft products such as Visio and SQL Server, a bit too technical but good for communicating to IT * BEAM/Agile approach 8 mins 15.6 Tips Think about the types of analysis or questions that the user or manager may want to ask. This will help structure the data and help to ensure nothing is missing At the same time, just because something exists in the organisation or in a data source does not mean it has to be included. You need to think about that to include and what to exclude Equally, there may be instances where there is a desire to add something in to the model but it does not currently exist. This should be flagged and discussed with those intending to use the BI tool / output What are the end uses of the system or systems? If there are potentially multiple systems, multiple teams and multiple views on the data, it may make sense to store the data in its original state (3NF) in the EDW or similar store, then do the dimensional mapping in the BI tool, so it can be customised to the audience (Meridith 2017, lecture 4). This can lead to some duplication, however an option might be to share the dimensional models in some central repository, allowing users to customise for their use, whilst still being able to share the same source data and the benefits this brings. Evidently this lends itself to an Inmon or other such approach and less so the Kimball approach References "],
["references-3.html", "References", " References "]
]
