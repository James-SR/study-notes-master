[
["index.html", "Study notes Preface", " Study notes James Solomon-Rounce Last updated:2017-08-15 Preface The following notes were taken by me for educational, non-commercial, purposes. If you find the information useful, buy the material/take the course. Thank you to the original content providers. Additional ramblings are my own. "],
["importing-data-part-1.html", "1 Importing data - Part 1 1.1 Introduction 1.2 Reading CSV files 1.3 Reading tab deliminated files or other table formats 1.4 Readr and data.table 1.5 Reading Excel files 1.6 XLConnect - read and write to excel", " 1 Importing data - Part 1 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 1)’ by Filip Schouwenaars. 1.1 Introduction Data often comes from many different sources and formats, including Flat files - simple tables e.g. csv Excel Databases - MySQL, Postgres Websites - APIs, JSON, scraping Other statistical software - SPSS, STATA, SAS 1.2 Reading CSV files Reading csv files can be achived with simple code like read.csv(&quot;file.csv&quot;, stringsAsFactors = FALSE) We may want to import strings as categorical variables, in which case we would set stringsAsFactors = TRUE which is also the default option, if not stated. When working across different machines or operating systems, problems can arise due to different ways of addressing file locations and differing file locations. Therefore, it can be easier to set a relative path to the users home directory, which would be achieved with the following code. path &lt;- file.path(&quot;~&quot;, &quot;datasets&quot;, &quot;file.csv&quot;) path ## [1] &quot;~/datasets/file.csv&quot; Then use the file path as before, assigning to a dataframe. df &lt;- read.csv(path, stringsAsFactors = FALSE) 1.3 Reading tab deliminated files or other table formats In a similar way to before, we add the path to the file and if we want strings as strings, for instance read.delim(&quot;file.csv&quot;, stringsAsFactors = FALSE) However, if the file comes in another format perhaps due to the system encoding or setup, it is still possible to try and read the file as a tabular formatting converting it to a data frame. To do so, we use the read.table() command which has a lot of arguments that can be customised. You can specify column names and types for instance. If for instance we have a file format where the objects are separated by a / rather than a comma or tab as before, we could use read.table(&quot;file.txt&quot;, header = TRUE, sep = &quot;/&quot;, stringsAsFactors = FALSE) Or, if you have a file which has no column/variable names and tabs as spaces, you would read the file as: # Path to the file.txt file: path path &lt;- file.path(&quot;data&quot;, &quot;file.txt&quot;) # Import the file.txt file: hotdogs file &lt;- read.table(path, sep = &quot;\\t&quot;, # specify seperator - tab in this instance col.names = c(&quot;VarName1&quot;, &quot;VarName2&quot;, &quot;VarName3&quot;), # specifiy variable names colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # specify the column/variable classes Both read.csv and read.delim are wrapper functions of read.table(), both use read.table but have different default options depending on the file type. There are two further wrapper functions - read.csv2 and read.delim2 - which deal with regional differences in formatting, notably that some areas use full stops as decimal places, whereas other areas use commas for decimal places. 1.4 Readr and data.table These two packages are other ways of reading in files. Readr uses the tibble, so will be compatible with other tidyverse packages such as dplyr. It is faster than utils, the r default and also prints out the column classes, depending on what other packages are loaded. It is not necessary to specify stringsAsFactors = FALSE. library(readr) read_csv(&quot;file.csv&quot;) #read comma seperated read_tsv(&quot;file2.txt&quot;) #read tab seperated files #If there are no row heads, you can create a vector then read it in using the col_names argument #specify the vector for column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) #read in the vector df &lt;- read_tsv(&quot;file3.txt&quot;, col_names = properties) Like the utils package, these are wrapper functions, with the base function being read_delim(). Unlike the utils package, read_delim() expects the first row to contain headers, so this doesn’t need to be explicit. As mentioned previously, it is also not necessary to specify the we don’t want strings as factors. You can specify col_names using a vector as before, or we can read them directly at the time. If we also want to explicitly state the column types, perahps because the automatically assigned variable is not correct, we can do so with col_type using abbreviations: c = character d = double i = integer n = number l = logical D = date T = date time t = time ? = guess _ = skip column (underscore) Finally, we can use skip and n_max to specify how many rows to skip at the beginning of a file, perhaps due to a large header, and the maximum now of rows to read, perhaps due to a very large file with many rows. read_delim(&quot;file4.txt&quot;, delim = &quot;/&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;)) read_delim(&quot;file5.txt&quot;, delim = &quot;/&quot;, col_types = &quot;ccid&quot;) read_delim(&quot;file6.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;), skip = 12, n_max = 50000) Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column. Look at the collector documentation for more details. Two examples are shown below, one for columns to be interpreted as integers and one for a column with factors. # The collectors needed for importing fac &lt;- col_factor(levels = c(&quot;Beef&quot;, &quot;Meat&quot;, &quot;Poultry&quot;)) int &lt;- col_integer() # Edit the col_types argument with the specified collectors hotdogs_factor &lt;- read_tsv(&quot;hotdogs.txt&quot;, col_names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), col_types = list(fac, int, int)) 1.4.1 data.table fread data.table is a tool for doing fast data analysis, particularly on large datasets. It also has a function to read data using the fread() command. It can automatically infer column names, types and separators. You can also drop or select columns at read time. df &lt;- fread(&quot;file7.csv&quot;, select = c(&quot;colname1&quot;, &quot;colname2&quot;)) The readr package fill create different dataframe types or object classes - ‘tbl_df’, ‘tbl’ and ‘data.frame’ - which can be useful for different purposes, such as for use in dplyr. Fread creates a data.table object class. 1.5 Reading Excel files There are many packages for reading Excel files, one package is the readxl package by Hadley Wickham. There are to main functions excel_sheets(): lists the sheets within an excel file or workbook read_excel(): import the data, unless specified the first sheet is read, this can either be done with sheet = 7, or sheet = “name”. So to read an urbanpop.xlsx file containing three sheets of urban populations, for different time frames, our code would look similar to that below. library(readxl) #list the sheerts in the file excel_sheets(&quot;urbanpop.xlsx&quot;) # Read the sheets, one by one pop_1 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 1) pop_2 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2) pop_3 &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 3) # Put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) # IF we want to read all the files, a more efficient way to read all the files in the file uses lapply pop_list &lt;- lapply(excel_sheets(&quot;urbanpop.xlsx&quot;), read_excel, path = &quot;urbanpop.xlsx&quot;) There are other arguments that can be used with the read_excel() function: col_names: If true, the first row is read, if false R will assign it’s own names or you specify a charecter vector manually col_types: If NULL, R gueses the data types of the columns. Alternatively, they can be specified e.g. text, numeric, date, blank (which ignores the col) skip: Speficies the number of rows to ignore # Some examples # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = FALSE) # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(&quot;urbanpop_nonames.xlsx&quot;, col_names = cols) # Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel urbanpop_sel &lt;- read_excel(&quot;urbanpop.xlsx&quot;, sheet = 2, col_names = FALSE, skip = 21) # Print out the first observation from urbanpop_sel urbanpop_sel[1,] 1.5.1 Alternatives for importing Excel files One alternative is the gdata package, which is a suite of tools for data. There is a read.xls() function which only, currently, supports XLS files although xlsx could be supported with a driver. The data is interpreted by the read.xls file using perl into a csv file, which is then read using the read.csv function - itself a offshoot of read.table, in to an R data frame. Hadley’s readxl package is faster, but is quite early in it’s development so some of the functions may change. For gdata, as it is an offshoot of read.table(), all of the same arguments can be used by read.xls(). 1.6 XLConnect - read and write to excel Most of the Excel tools can become accessible but inside R, using XLConnect. It is possible to use XLS and XLSX and it will create a ‘workbook’ object in R, but it does require Java to work. library(XLConnect) #create a connect to a file and list the sheets book &lt;- loadWorkbook(&quot;file.xlsx&quot;) getSheets(book) #read in the specific sheet but only the columns we are interested in wardData &lt;- readWorksheet(book, sheet = &quot;sheet_1&quot;, startCol = 3, endCol = 5) # read in the names column, previoulsy excluded wardNames &lt;- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1) #cbind the data and names together selection &lt;- cbind(wardNames, wardData) XLConnect has more features than simply reading sheets. It is possible to write data back to the Excel file also. We can add sheets, write or add data to sheets, rename and remove sheets. # Add a worksheet to my_book, named &quot;summary&quot; createSheet(my_book, &quot;summary&quot;) # Add data in summ to &quot;data_summary&quot; sheet writeWorksheet(my_book, summ, &quot;summary&quot;) # Save workbook as summary.xlsx saveWorkbook(my_book, &quot;summary.xlsx&quot;) # Rename &quot;summary&quot; sheet to &quot;data_summary&quot; renameSheet(my_book, sheet = 4, &quot;data_summary&quot;) # Remove the third sheet removeSheet(my_book, sheet = 3) "],
["importing-data-part-2.html", "2 Importing data - Part 2 2.1 Importing from Databases - 1 2.2 SQL Queries Inside R 2.3 Web Data 2.4 JSON and APIs 2.5 Importing from other statistical software", " 2 Importing data - Part 2 Notes taken during/inspired by the Datacamp course ‘Importing Data in R (Part 2)’ by Filip Schouwenaars. 2.1 Importing from Databases - 1 In a professional or commercial setting, you often deal with more complicated file structures and source systems that simple flat files. Often the data is stored in a DBMS or Database Management System and SQL is the usual way of quering the DBMS. As there can be slight differences, you are likely to need different packages, some include: MySQL: Use the RMySQL package PostgresSQL: Use the RPostgresSQL package Oracle: Use the ROracle (etc…) Conventions are specified in the DBI - another R package, DBI is the interface and the other packages are the implentation. Some of the packages will automaticlaly install the DBI package as well. To connect to a database we would so something like the following. # Load the DBI package library(DBI) ## Loading required package: methods # Edit dbConnect() call - the first part specifies how connections are map to the database con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Build a vector of table names: tables tables &lt;- dbListTables(con) # Display structure of tables str(tables) ## chr [1:3] &quot;comments&quot; &quot;tweats&quot; &quot;users&quot; # Import the users table from tweater: users users &lt;- dbReadTable(con, &quot;users&quot;) # Print users users ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja # Or we can import all tables using lapply tables &lt;- lapply(tables, dbReadTable, conn = con) # Print out tables tables ## [[1]] ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! ## 3 1011 49 5 love it ## 4 1012 87 1 awesome! thanks! ## 5 1010 88 6 yuck! ## 6 1026 77 4 not my thing! ## 7 1004 49 1 this is fabulous! ## 8 1030 75 6 so easy! ## 9 1025 88 2 oh yes ## 10 1007 49 3 serious? ## 11 1020 77 1 couldn&#39;t be better ## 12 1014 77 1 saved my day ## ## [[2]] ## id user_id ## 1 75 3 ## 2 88 4 ## 3 77 6 ## 4 87 5 ## 5 49 1 ## 6 24 7 ## post ## 1 break egg. bake egg. eat egg. ## 2 wash strawberries. add ice. blend. enjoy. ## 3 2 slices of bread. add cheese. grill. heaven. ## 4 open and crush avocado. add shrimps. perfect starter. ## 5 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 6 just eat an apple. simply and healthy. ## date ## 1 2015-09-05 ## 2 2015-09-14 ## 3 2015-09-21 ## 4 2015-09-22 ## 5 2015-09-22 ## 6 2015-09-24 ## ## [[3]] ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja 2.2 SQL Queries Inside R OFten you don’t want an entire tabel from a database, but a selection from the table. You can use SQL queries from inside R to extract only what you are interested in. You can alternatively use subset on the imported table, but often it is easier to extract only what you need first, particularly when working with large databases. The SQL goes inside e.g. dbGetQuery(con, “SQL QUERY”). # Connect to the database library(DBI) con &lt;- dbConnect(RMySQL::MySQL(), dbname = &quot;tweater&quot;, host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, port = 3306, user = &quot;student&quot;, password = &quot;datacamp&quot;) # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth &lt;- dbGetQuery(con, &quot;SELECT tweat_id FROM comments WHERE user_id = 1&quot;) # Print elisabeth elisabeth ## tweat_id ## 1 87 ## 2 49 ## 3 77 ## 4 77 # Import post column of tweats where date is higher than &#39;2015-09-21&#39;: latest latest &lt;- dbGetQuery(con, &quot;SELECT post FROM tweats WHERE date &gt; &#39;2015-09-21&#39;&quot;) # Print latest latest ## post ## 1 open and crush avocado. add shrimps. perfect starter. ## 2 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 3 just eat an apple. simply and healthy. # Create data frame specific using boolean specific &lt;- dbGetQuery(con, &quot;SELECT message FROM comments WHERE tweat_id = 77 AND user_id &gt; 4&quot;) # Print specific specific ## message ## 1 great! # Create data frame short selecting two columns short &lt;- dbGetQuery(con, &quot;SELECT id, name FROM users WHERE CHAR_LENGTH(name) &lt; 5&quot;) # Print short short ## id name ## 1 2 mike ## 2 3 thea ## 3 6 kate # We can also join elements from different tables using the same id/key dbGetQuery(con, &quot;SELECT post, message FROM tweats INNER JOIN comments on tweats.id = tweat_id WHERE tweat_id = 77&quot;) ## post message ## 1 2 slices of bread. add cheese. grill. heaven. great! ## 2 2 slices of bread. add cheese. grill. heaven. not my thing! ## 3 2 slices of bread. add cheese. grill. heaven. couldn&#39;t be better ## 4 2 slices of bread. add cheese. grill. heaven. saved my day You’ve used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed: Sending the specified query with dbSendQuery(); Fetching the result of executing the query on the database with dbFetch(); Clearing the result with dbClearResult(). Let’s not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query’s result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch(). It is important to close the connection to the database once complete using the dbDisconnect() function # Send query to the database res &lt;- dbSendQuery(con, &quot;SELECT * FROM comments WHERE user_id &gt; 4&quot;) # Use dbFetch() twice dbFetch(res, n = 2) ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! dbFetch(res) # imports all ## id tweat_id user_id message ## 1 1011 49 5 love it ## 2 1010 88 6 yuck! ## 3 1030 75 6 so easy! # Clear res dbClearResult(res) ## [1] TRUE # Create the data frame long_tweats long_tweats &lt;- dbGetQuery(con, &quot;SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) &gt; 40&quot;) # Print long_tweats print(long_tweats) ## post ## 1 wash strawberries. add ice. blend. enjoy. ## 2 2 slices of bread. add cheese. grill. heaven. ## 3 open and crush avocado. add shrimps. perfect starter. ## 4 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## date ## 1 2015-09-14 ## 2 2015-09-21 ## 3 2015-09-22 ## 4 2015-09-22 # Disconnect from the database dbDisconnect(con) ## [1] TRUE 2.3 Web Data HyperText Transfer Protocol (HTTP) is the ‘language of the web’ and consists of a set of rules about data exchange between computers. If the file is a csv file, we can use functions like read.csv() and add in the url in quotations marks, read.csv will recognise this is a URL and will issue a HTTP GET command to download the file. This will also work on https sites on newer versions of R. We can also use the readr package and other packages. # Load the readr package library(readr) # Import the csv file: pools url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; pools &lt;- read_csv(url_csv) ## Parsed with column specification: ## cols( ## Name = col_character(), ## Address = col_character(), ## Latitude = col_double(), ## Longitude = col_double() ## ) # Import the txt file: potatoes url_delim &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt&quot; potatoes &lt;- read_tsv(url_delim) ## Parsed with column specification: ## cols( ## area = col_integer(), ## temp = col_integer(), ## size = col_integer(), ## storage = col_integer(), ## method = col_integer(), ## texture = col_double(), ## flavor = col_double(), ## moistness = col_double() ## ) # Print pools and potatoes pools ## # A tibble: 20 x 4 ## Name ## &lt;chr&gt; ## 1 Acacia Ridge Leisure Centre ## 2 Bellbowrie Pool ## 3 Carole Park ## 4 Centenary Pool (inner City) ## 5 Chermside Pool ## 6 Colmslie Pool (Morningside) ## 7 Spring Hill Baths (inner City) ## 8 Dunlop Park Pool (Corinda) ## 9 Fortitude Valley Pool ## 10 Hibiscus Sports Complex (upper MtGravatt) ## 11 Ithaca Pool ( Paddington) ## 12 Jindalee Pool ## 13 Manly Pool ## 14 Mt Gravatt East Aquatic Centre ## 15 Musgrave Park Pool (South Brisbane) ## 16 Newmarket Pool ## 17 Runcorn Pool ## 18 Sandgate Pool ## 19 Langlands Parks Pool (Stones Corner) ## 20 Yeronga Park Pool ## # ... with 3 more variables: Address &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt; potatoes ## # A tibble: 160 x 8 ## area temp size storage method texture flavor moistness ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3.0 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3.0 1.7 ## 7 1 1 1 2 2 2.6 3.1 2.4 ## 8 1 1 1 2 3 3.0 3.0 2.9 ## 9 1 1 1 2 4 2.2 3.2 2.5 ## 10 1 1 1 2 5 2.0 2.8 1.9 ## # ... with 150 more rows # https URL to the swimming_pools csv file. url_csv &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; # Import the file using read.csv(): pools1 pools1 &lt;- read.csv(url_csv) str(pools1) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : Factor w/ 20 levels &quot;Acacia Ridge Leisure Centre&quot;,..: 1 2 3 4 5 6 19 7 8 9 ... ## $ Address : Factor w/ 20 levels &quot;1 Fairlead Crescent, Manly&quot;,..: 5 20 18 10 9 11 6 15 12 17 ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... Some packages, like the readxl package, do not currently recognise urls. However, we can use the donwload.file() or other command to download the file and then read it in locally. This process can be much quicker that browsing the internet then downloading the file. library(readxl) # Specification of url: url_xls url_xls &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls&quot; # Download file behind URL, name it local_latitude.xls download.file(url_xls, destfile = &quot;local_latitude.xls&quot;) # Import the local .xls file with readxl: excel_readxl excel_readxl &lt;- read_excel(&quot;local_latitude.xls&quot;) # https URL to the wine RData file. url_rdata &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData&quot; # Download the wine file to your working directory download.file(url_rdata, destfile = &quot;wine_local.RData&quot;) # Load the wine data into your workspace using load() load(&quot;wine_local.RData&quot;) # Print out the summary of the wine data summary(wine) ## Alcohol Malic acid Ash Alcalinity of ash ## Min. :11.03 Min. :0.74 Min. :1.360 Min. :10.60 ## 1st Qu.:12.36 1st Qu.:1.60 1st Qu.:2.210 1st Qu.:17.20 ## Median :13.05 Median :1.87 Median :2.360 Median :19.50 ## Mean :12.99 Mean :2.34 Mean :2.366 Mean :19.52 ## 3rd Qu.:13.67 3rd Qu.:3.10 3rd Qu.:2.560 3rd Qu.:21.50 ## Max. :14.83 Max. :5.80 Max. :3.230 Max. :30.00 ## Magnesium Total phenols Flavanoids Nonflavanoid phenols ## Min. : 70.00 Min. :0.980 Min. :0.340 Min. :0.1300 ## 1st Qu.: 88.00 1st Qu.:1.740 1st Qu.:1.200 1st Qu.:0.2700 ## Median : 98.00 Median :2.350 Median :2.130 Median :0.3400 ## Mean : 99.59 Mean :2.292 Mean :2.023 Mean :0.3623 ## 3rd Qu.:107.00 3rd Qu.:2.800 3rd Qu.:2.860 3rd Qu.:0.4400 ## Max. :162.00 Max. :3.880 Max. :5.080 Max. :0.6600 ## Proanthocyanins Color intensity Hue Proline ## Min. :0.410 Min. : 1.280 Min. :1.270 Min. : 278.0 ## 1st Qu.:1.250 1st Qu.: 3.210 1st Qu.:1.930 1st Qu.: 500.0 ## Median :1.550 Median : 4.680 Median :2.780 Median : 672.0 ## Mean :1.587 Mean : 5.055 Mean :2.604 Mean : 745.1 ## 3rd Qu.:1.950 3rd Qu.: 6.200 3rd Qu.:3.170 3rd Qu.: 985.0 ## Max. :3.580 Max. :13.000 Max. :4.000 Max. :1680.0 We can also read http content using the httr package. This includes JSON formatted text, which httr will convert to a named list. # Load the httr package library(httr) # Get the url, save response to resp url &lt;- &quot;http://www.example.com/&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.example.com/] ## Date: 2017-08-15 04:02 ## Status: 200 ## Content-Type: text/html ## Size: 1.27 kB ## &lt;!doctype html&gt; ## &lt;html&gt; ## &lt;head&gt; ## &lt;title&gt;Example Domain&lt;/title&gt; ## ## &lt;meta charset=&quot;utf-8&quot; /&gt; ## &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; ## &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt; ## &lt;style type=&quot;text/css&quot;&gt; ## body { ## ... # Get the raw content of resp: raw_content raw_content &lt;- content(resp, as = &quot;raw&quot;) # Print the head of raw_content head(raw_content) ## [1] 3c 21 64 6f 63 74 # JSON formatted # Get the url url &lt;- &quot;http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.omdbapi.com/?apikey=ff21610b&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json] ## Date: 2017-08-15 04:02 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 902 B # Print content of resp as text content(resp, as = &quot;text&quot;) ## [1] &quot;{\\&quot;Title\\&quot;:\\&quot;Annie Hall\\&quot;,\\&quot;Year\\&quot;:\\&quot;1977\\&quot;,\\&quot;Rated\\&quot;:\\&quot;PG\\&quot;,\\&quot;Released\\&quot;:\\&quot;20 Apr 1977\\&quot;,\\&quot;Runtime\\&quot;:\\&quot;93 min\\&quot;,\\&quot;Genre\\&quot;:\\&quot;Comedy, Romance\\&quot;,\\&quot;Director\\&quot;:\\&quot;Woody Allen\\&quot;,\\&quot;Writer\\&quot;:\\&quot;Woody Allen, Marshall Brickman\\&quot;,\\&quot;Actors\\&quot;:\\&quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane\\&quot;,\\&quot;Plot\\&quot;:\\&quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.\\&quot;,\\&quot;Language\\&quot;:\\&quot;English, German\\&quot;,\\&quot;Country\\&quot;:\\&quot;USA\\&quot;,\\&quot;Awards\\&quot;:\\&quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.\\&quot;,\\&quot;Poster\\&quot;:\\&quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg\\&quot;,\\&quot;Ratings\\&quot;:[{\\&quot;Source\\&quot;:\\&quot;Internet Movie Database\\&quot;,\\&quot;Value\\&quot;:\\&quot;8.1/10\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Rotten Tomatoes\\&quot;,\\&quot;Value\\&quot;:\\&quot;97%\\&quot;}],\\&quot;Metascore\\&quot;:\\&quot;N/A\\&quot;,\\&quot;imdbRating\\&quot;:\\&quot;8.1\\&quot;,\\&quot;imdbVotes\\&quot;:\\&quot;210,152\\&quot;,\\&quot;imdbID\\&quot;:\\&quot;tt0075686\\&quot;,\\&quot;Type\\&quot;:\\&quot;movie\\&quot;,\\&quot;DVD\\&quot;:\\&quot;28 Apr 1998\\&quot;,\\&quot;BoxOffice\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Production\\&quot;:\\&quot;United Artists\\&quot;,\\&quot;Website\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Response\\&quot;:\\&quot;True\\&quot;}&quot; # Print content of resp content(resp) ## $Title ## [1] &quot;Annie Hall&quot; ## ## $Year ## [1] &quot;1977&quot; ## ## $Rated ## [1] &quot;PG&quot; ## ## $Released ## [1] &quot;20 Apr 1977&quot; ## ## $Runtime ## [1] &quot;93 min&quot; ## ## $Genre ## [1] &quot;Comedy, Romance&quot; ## ## $Director ## [1] &quot;Woody Allen&quot; ## ## $Writer ## [1] &quot;Woody Allen, Marshall Brickman&quot; ## ## $Actors ## [1] &quot;Woody Allen, Diane Keaton, Tony Roberts, Carol Kane&quot; ## ## $Plot ## [1] &quot;Neurotic New York comedian Alvy Singer falls in love with the ditzy Annie Hall.&quot; ## ## $Language ## [1] &quot;English, German&quot; ## ## $Country ## [1] &quot;USA&quot; ## ## $Awards ## [1] &quot;Won 4 Oscars. Another 26 wins &amp; 8 nominations.&quot; ## ## $Poster ## [1] &quot;https://images-na.ssl-images-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg&quot; ## ## $Ratings ## $Ratings[[1]] ## $Ratings[[1]]$Source ## [1] &quot;Internet Movie Database&quot; ## ## $Ratings[[1]]$Value ## [1] &quot;8.1/10&quot; ## ## ## $Ratings[[2]] ## $Ratings[[2]]$Source ## [1] &quot;Rotten Tomatoes&quot; ## ## $Ratings[[2]]$Value ## [1] &quot;97%&quot; ## ## ## ## $Metascore ## [1] &quot;N/A&quot; ## ## $imdbRating ## [1] &quot;8.1&quot; ## ## $imdbVotes ## [1] &quot;210,152&quot; ## ## $imdbID ## [1] &quot;tt0075686&quot; ## ## $Type ## [1] &quot;movie&quot; ## ## $DVD ## [1] &quot;28 Apr 1998&quot; ## ## $BoxOffice ## [1] &quot;N/A&quot; ## ## $Production ## [1] &quot;United Artists&quot; ## ## $Website ## [1] &quot;N/A&quot; ## ## $Response ## [1] &quot;True&quot; 2.4 JSON and APIs JSON is both easy for machines to parse and generate and is human readable. APIs are programtical ways of getting data, consisting of a set of protocols to interact with some other system or database. JSON can be useful since it is often well structured and can save time over, say, parsing a html page. So for instance, you can use the OMDb API to return JSON formatted text about a movie, rather than parse an IMDB html page entry. One package for handling JSON in R is jsonlite. library(jsonlite) # wine_json is a JSON wine_json &lt;- &#39;{&quot;name&quot;:&quot;Chateau Migraine&quot;, &quot;year&quot;:1997, &quot;alcohol_pct&quot;:12.4, &quot;color&quot;:&quot;red&quot;, &quot;awarded&quot;:false}&#39; # Convert wine_json into a list: wine wine &lt;- fromJSON(wine_json) # Print structure of wine str(wine) ## List of 5 ## $ name : chr &quot;Chateau Migraine&quot; ## $ year : int 1997 ## $ alcohol_pct: num 12.4 ## $ color : chr &quot;red&quot; ## $ awarded : logi FALSE # Definition of quandl_url quandl_url &lt;- &quot;http://www.quandl.com/api/v1/datasets/IWS/INTERNET_INDIA.json?auth_token=i83asDsiWUUyfoypkgMz&quot; # Import Quandl data: quandl_data quandl_data &lt;- fromJSON(quandl_url) # Print structure of quandl_data str(quandl_data) ## List of 17 ## $ errors : Named list() ## $ id : int 2351831 ## $ source_name : chr &quot;Internet World Stats&quot; ## $ source_code : chr &quot;IWS&quot; ## $ code : chr &quot;INTERNET_INDIA&quot; ## $ name : chr &quot;India Internet Usage&quot; ## $ urlize_name : chr &quot;India-Internet-Usage&quot; ## $ display_url : chr &quot;http://www.internetworldstats.com/asia/in.htm&quot; ## $ description : chr &quot;Internet Usage and Population Statistics&quot; ## $ updated_at : chr &quot;2016-01-01T04:23:55.235Z&quot; ## $ frequency : chr &quot;annual&quot; ## $ from_date : chr &quot;1998-12-31&quot; ## $ to_date : chr &quot;2012-12-31&quot; ## $ column_names: chr [1:4] &quot;YEAR&quot; &quot;Users&quot; &quot;Population&quot; &quot;% Pen.&quot; ## $ premium : logi FALSE ## $ data : chr [1:13, 1:4] &quot;2012-12-31&quot; &quot;2010-12-31&quot; &quot;2009-12-31&quot; &quot;2007-12-31&quot; ... ## $ type : chr &quot;Time Series&quot; There are two types of JSON structures JSON objects - has key value pairs e.g. name:James, age:21 etc JSON arrays - a sequence of values, numbers, nulls e.g. 4, “a”, 10, false, null etc You can also nest JSON objects or arrays within each other. Some examples are below. YOu can also use the minify and prettify functions to convert a JSON string to a more compact of easier to read version. Similar functions can also be used inside the toJSON() function e.g. toJSON(x, pretty = TRUE) # Challenge 1 json1 &lt;- &#39;[1, 2, 3, 4, 5, 6]&#39; fromJSON(json1) ## [1] 1 2 3 4 5 6 # Challenge 2 json2 &lt;- &#39;{&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]}&#39; fromJSON(json2) ## $a ## [1] 1 2 3 ## ## $b ## [1] 4 5 6 # You can also convert data to JSON from other formats. Here we take a csv and format it into a JSON array # URL pointing to the .csv file url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv&quot; # Import the .csv file located at url_csv water &lt;- read.csv(url_csv, stringsAsFactors = FALSE) # Convert the data file according to the requirements water_json &lt;- toJSON(water) # Print out water_json water_json ## [{&quot;water&quot;:&quot;Algeria&quot;,&quot;X1992&quot;:0.064,&quot;X2002&quot;:0.017},{&quot;water&quot;:&quot;American Samoa&quot;},{&quot;water&quot;:&quot;Angola&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Antigua and Barbuda&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Argentina&quot;,&quot;X1992&quot;:0.0007,&quot;X1997&quot;:0.0007,&quot;X2002&quot;:0.0007},{&quot;water&quot;:&quot;Australia&quot;,&quot;X1992&quot;:0.0298,&quot;X2002&quot;:0.0298},{&quot;water&quot;:&quot;Austria&quot;,&quot;X1992&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Bahamas&quot;,&quot;X1992&quot;:0.0013,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Bahrain&quot;,&quot;X1992&quot;:0.0441,&quot;X2002&quot;:0.0441,&quot;X2007&quot;:0.1024},{&quot;water&quot;:&quot;Barbados&quot;,&quot;X2007&quot;:0.0146},{&quot;water&quot;:&quot;British Virgin Islands&quot;,&quot;X2007&quot;:0.0042},{&quot;water&quot;:&quot;Canada&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cape Verde&quot;,&quot;X1992&quot;:0.002,&quot;X1997&quot;:0.0017},{&quot;water&quot;:&quot;Cayman Islands&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Central African Rep.&quot;},{&quot;water&quot;:&quot;Chile&quot;,&quot;X1992&quot;:0.0048,&quot;X2002&quot;:0.0048},{&quot;water&quot;:&quot;Colombia&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cuba&quot;,&quot;X1992&quot;:0.0069,&quot;X1997&quot;:0.0069,&quot;X2002&quot;:0.0069},{&quot;water&quot;:&quot;Cyprus&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.0335},{&quot;water&quot;:&quot;Czech Rep.&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Denmark&quot;,&quot;X1992&quot;:0.015,&quot;X2002&quot;:0.015},{&quot;water&quot;:&quot;Djibouti&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Ecuador&quot;,&quot;X1992&quot;:0.0022,&quot;X1997&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Egypt&quot;,&quot;X1992&quot;:0.025,&quot;X1997&quot;:0.025,&quot;X2002&quot;:0.1},{&quot;water&quot;:&quot;El Salvador&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Finland&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;France&quot;,&quot;X1992&quot;:0.0117,&quot;X2002&quot;:0.0117},{&quot;water&quot;:&quot;Gibraltar&quot;,&quot;X1992&quot;:0.0077},{&quot;water&quot;:&quot;Greece&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01},{&quot;water&quot;:&quot;Honduras&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Hungary&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;India&quot;,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Indonesia&quot;,&quot;X1992&quot;:0.0187,&quot;X2002&quot;:0.0187},{&quot;water&quot;:&quot;Iran&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.003,&quot;X2007&quot;:0.2},{&quot;water&quot;:&quot;Iraq&quot;,&quot;X1997&quot;:0.0074,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Ireland&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Israel&quot;,&quot;X1992&quot;:0.0256,&quot;X2002&quot;:0.0256,&quot;X2007&quot;:0.14},{&quot;water&quot;:&quot;Italy&quot;,&quot;X1992&quot;:0.0973,&quot;X2002&quot;:0.0973},{&quot;water&quot;:&quot;Jamaica&quot;,&quot;X1992&quot;:0.0005,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Japan&quot;,&quot;X1997&quot;:0.04,&quot;X2002&quot;:0.04},{&quot;water&quot;:&quot;Jordan&quot;,&quot;X1997&quot;:0.002,&quot;X2007&quot;:0.0098},{&quot;water&quot;:&quot;Kazakhstan&quot;,&quot;X1997&quot;:1.328,&quot;X2002&quot;:1.328},{&quot;water&quot;:&quot;Kuwait&quot;,&quot;X1992&quot;:0.507,&quot;X1997&quot;:0.231,&quot;X2002&quot;:0.4202},{&quot;water&quot;:&quot;Lebanon&quot;,&quot;X2007&quot;:0.0473},{&quot;water&quot;:&quot;Libya&quot;,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Malaysia&quot;,&quot;X1992&quot;:0.0043,&quot;X2002&quot;:0.0043},{&quot;water&quot;:&quot;Maldives&quot;,&quot;X1992&quot;:0.0004},{&quot;water&quot;:&quot;Malta&quot;,&quot;X1992&quot;:0.024,&quot;X1997&quot;:0.031,&quot;X2002&quot;:0.031},{&quot;water&quot;:&quot;Marshall Islands&quot;,&quot;X1992&quot;:0.0007},{&quot;water&quot;:&quot;Mauritania&quot;,&quot;X1992&quot;:0.002,&quot;X2002&quot;:0.002},{&quot;water&quot;:&quot;Mexico&quot;,&quot;X1992&quot;:0.0307,&quot;X2002&quot;:0.0307},{&quot;water&quot;:&quot;Morocco&quot;,&quot;X1992&quot;:0.0034,&quot;X1997&quot;:0.0034,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Namibia&quot;,&quot;X1992&quot;:0.0003,&quot;X2002&quot;:0.0003},{&quot;water&quot;:&quot;Netherlands Antilles&quot;,&quot;X1992&quot;:0.063},{&quot;water&quot;:&quot;Nicaragua&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Nigeria&quot;,&quot;X1992&quot;:0.003,&quot;X2002&quot;:0.003},{&quot;water&quot;:&quot;Norway&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Oman&quot;,&quot;X1997&quot;:0.034,&quot;X2002&quot;:0.034,&quot;X2007&quot;:0.109},{&quot;water&quot;:&quot;Peru&quot;,&quot;X1992&quot;:0.0054,&quot;X2002&quot;:0.0054},{&quot;water&quot;:&quot;Poland&quot;,&quot;X1992&quot;:0.007,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Portugal&quot;,&quot;X1992&quot;:0.0016,&quot;X2002&quot;:0.0016},{&quot;water&quot;:&quot;Qatar&quot;,&quot;X1992&quot;:0.065,&quot;X1997&quot;:0.099,&quot;X2002&quot;:0.099,&quot;X2007&quot;:0.18},{&quot;water&quot;:&quot;Saudi Arabia&quot;,&quot;X1992&quot;:0.683,&quot;X1997&quot;:0.727,&quot;X2002&quot;:0.863,&quot;X2007&quot;:1.033},{&quot;water&quot;:&quot;Senegal&quot;,&quot;X1992&quot;:0,&quot;X2002&quot;:0},{&quot;water&quot;:&quot;Somalia&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;South Africa&quot;,&quot;X1992&quot;:0.018,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Spain&quot;,&quot;X1992&quot;:0.1002,&quot;X2002&quot;:0.1002},{&quot;water&quot;:&quot;Sudan&quot;,&quot;X1992&quot;:0.0004,&quot;X1997&quot;:0.0004,&quot;X2002&quot;:0.0004},{&quot;water&quot;:&quot;Sweden&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Trinidad and Tobago&quot;,&quot;X2007&quot;:0.036},{&quot;water&quot;:&quot;Tunisia&quot;,&quot;X1992&quot;:0.008,&quot;X2002&quot;:0.013},{&quot;water&quot;:&quot;Turkey&quot;,&quot;X1992&quot;:0.0005,&quot;X2002&quot;:0.0005,&quot;X2007&quot;:0.0005},{&quot;water&quot;:&quot;United Arab Emirates&quot;,&quot;X1992&quot;:0.163,&quot;X1997&quot;:0.385,&quot;X2007&quot;:0.95},{&quot;water&quot;:&quot;United Kingdom&quot;,&quot;X1992&quot;:0.0333,&quot;X2002&quot;:0.0333},{&quot;water&quot;:&quot;United States&quot;,&quot;X1992&quot;:0.58,&quot;X2002&quot;:0.58},{&quot;water&quot;:&quot;Venezuela&quot;,&quot;X1992&quot;:0.0052,&quot;X2002&quot;:0.0052},{&quot;water&quot;:&quot;Yemen, Rep.&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01}] # Convert mtcars to a pretty JSON: pretty_json pretty_json &lt;- toJSON(mtcars, pretty = TRUE) # Print pretty_json pretty_json ## [ ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.62, ## &quot;qsec&quot;: 16.46, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4&quot; ## }, ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.875, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Mazda RX4 Wag&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 108, ## &quot;hp&quot;: 93, ## &quot;drat&quot;: 3.85, ## &quot;wt&quot;: 2.32, ## &quot;qsec&quot;: 18.61, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Datsun 710&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 258, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.215, ## &quot;qsec&quot;: 19.44, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Hornet 4 Drive&quot; ## }, ## { ## &quot;mpg&quot;: 18.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Hornet Sportabout&quot; ## }, ## { ## &quot;mpg&quot;: 18.1, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 225, ## &quot;hp&quot;: 105, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.46, ## &quot;qsec&quot;: 20.22, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Valiant&quot; ## }, ## { ## &quot;mpg&quot;: 14.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.21, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 15.84, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Duster 360&quot; ## }, ## { ## &quot;mpg&quot;: 24.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 146.7, ## &quot;hp&quot;: 62, ## &quot;drat&quot;: 3.69, ## &quot;wt&quot;: 3.19, ## &quot;qsec&quot;: 20, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 240D&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 140.8, ## &quot;hp&quot;: 95, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.15, ## &quot;qsec&quot;: 22.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Merc 230&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.3, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280&quot; ## }, ## { ## &quot;mpg&quot;: 17.8, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Merc 280C&quot; ## }, ## { ## &quot;mpg&quot;: 16.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 4.07, ## &quot;qsec&quot;: 17.4, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SE&quot; ## }, ## { ## &quot;mpg&quot;: 17.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.73, ## &quot;qsec&quot;: 17.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SL&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.78, ## &quot;qsec&quot;: 18, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;_row&quot;: &quot;Merc 450SLC&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 472, ## &quot;hp&quot;: 205, ## &quot;drat&quot;: 2.93, ## &quot;wt&quot;: 5.25, ## &quot;qsec&quot;: 17.98, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Cadillac Fleetwood&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 460, ## &quot;hp&quot;: 215, ## &quot;drat&quot;: 3, ## &quot;wt&quot;: 5.424, ## &quot;qsec&quot;: 17.82, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Lincoln Continental&quot; ## }, ## { ## &quot;mpg&quot;: 14.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 440, ## &quot;hp&quot;: 230, ## &quot;drat&quot;: 3.23, ## &quot;wt&quot;: 5.345, ## &quot;qsec&quot;: 17.42, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Chrysler Imperial&quot; ## }, ## { ## &quot;mpg&quot;: 32.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 78.7, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 2.2, ## &quot;qsec&quot;: 19.47, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat 128&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 75.7, ## &quot;hp&quot;: 52, ## &quot;drat&quot;: 4.93, ## &quot;wt&quot;: 1.615, ## &quot;qsec&quot;: 18.52, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Honda Civic&quot; ## }, ## { ## &quot;mpg&quot;: 33.9, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 71.1, ## &quot;hp&quot;: 65, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 1.835, ## &quot;qsec&quot;: 19.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corolla&quot; ## }, ## { ## &quot;mpg&quot;: 21.5, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.1, ## &quot;hp&quot;: 97, ## &quot;drat&quot;: 3.7, ## &quot;wt&quot;: 2.465, ## &quot;qsec&quot;: 20.01, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Toyota Corona&quot; ## }, ## { ## &quot;mpg&quot;: 15.5, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 318, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.52, ## &quot;qsec&quot;: 16.87, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Dodge Challenger&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 304, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.435, ## &quot;qsec&quot;: 17.3, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;AMC Javelin&quot; ## }, ## { ## &quot;mpg&quot;: 13.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 350, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.73, ## &quot;wt&quot;: 3.84, ## &quot;qsec&quot;: 15.41, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Camaro Z28&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 400, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.845, ## &quot;qsec&quot;: 17.05, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Pontiac Firebird&quot; ## }, ## { ## &quot;mpg&quot;: 27.3, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 79, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 1.935, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;_row&quot;: &quot;Fiat X1-9&quot; ## }, ## { ## &quot;mpg&quot;: 26, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.3, ## &quot;hp&quot;: 91, ## &quot;drat&quot;: 4.43, ## &quot;wt&quot;: 2.14, ## &quot;qsec&quot;: 16.7, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Porsche 914-2&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 95.1, ## &quot;hp&quot;: 113, ## &quot;drat&quot;: 3.77, ## &quot;wt&quot;: 1.513, ## &quot;qsec&quot;: 16.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Lotus Europa&quot; ## }, ## { ## &quot;mpg&quot;: 15.8, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 351, ## &quot;hp&quot;: 264, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 3.17, ## &quot;qsec&quot;: 14.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 4, ## &quot;_row&quot;: &quot;Ford Pantera L&quot; ## }, ## { ## &quot;mpg&quot;: 19.7, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 145, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.62, ## &quot;wt&quot;: 2.77, ## &quot;qsec&quot;: 15.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 6, ## &quot;_row&quot;: &quot;Ferrari Dino&quot; ## }, ## { ## &quot;mpg&quot;: 15, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 301, ## &quot;hp&quot;: 335, ## &quot;drat&quot;: 3.54, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 14.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 8, ## &quot;_row&quot;: &quot;Maserati Bora&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 121, ## &quot;hp&quot;: 109, ## &quot;drat&quot;: 4.11, ## &quot;wt&quot;: 2.78, ## &quot;qsec&quot;: 18.6, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;_row&quot;: &quot;Volvo 142E&quot; ## } ## ] # Minify pretty_json: mini_json mini_json &lt;- minify(pretty_json) # Print mini_json mini_json ## [{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.62,&quot;qsec&quot;:16.46,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4&quot;},{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.875,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Mazda RX4 Wag&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:108,&quot;hp&quot;:93,&quot;drat&quot;:3.85,&quot;wt&quot;:2.32,&quot;qsec&quot;:18.61,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Datsun 710&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:6,&quot;disp&quot;:258,&quot;hp&quot;:110,&quot;drat&quot;:3.08,&quot;wt&quot;:3.215,&quot;qsec&quot;:19.44,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Hornet 4 Drive&quot;},{&quot;mpg&quot;:18.7,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:175,&quot;drat&quot;:3.15,&quot;wt&quot;:3.44,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Hornet Sportabout&quot;},{&quot;mpg&quot;:18.1,&quot;cyl&quot;:6,&quot;disp&quot;:225,&quot;hp&quot;:105,&quot;drat&quot;:2.76,&quot;wt&quot;:3.46,&quot;qsec&quot;:20.22,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Valiant&quot;},{&quot;mpg&quot;:14.3,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:245,&quot;drat&quot;:3.21,&quot;wt&quot;:3.57,&quot;qsec&quot;:15.84,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Duster 360&quot;},{&quot;mpg&quot;:24.4,&quot;cyl&quot;:4,&quot;disp&quot;:146.7,&quot;hp&quot;:62,&quot;drat&quot;:3.69,&quot;wt&quot;:3.19,&quot;qsec&quot;:20,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 240D&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:140.8,&quot;hp&quot;:95,&quot;drat&quot;:3.92,&quot;wt&quot;:3.15,&quot;qsec&quot;:22.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Merc 230&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.3,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280&quot;},{&quot;mpg&quot;:17.8,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Merc 280C&quot;},{&quot;mpg&quot;:16.4,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:4.07,&quot;qsec&quot;:17.4,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SE&quot;},{&quot;mpg&quot;:17.3,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.73,&quot;qsec&quot;:17.6,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SL&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.78,&quot;qsec&quot;:18,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;_row&quot;:&quot;Merc 450SLC&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:472,&quot;hp&quot;:205,&quot;drat&quot;:2.93,&quot;wt&quot;:5.25,&quot;qsec&quot;:17.98,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Cadillac Fleetwood&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:460,&quot;hp&quot;:215,&quot;drat&quot;:3,&quot;wt&quot;:5.424,&quot;qsec&quot;:17.82,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Lincoln Continental&quot;},{&quot;mpg&quot;:14.7,&quot;cyl&quot;:8,&quot;disp&quot;:440,&quot;hp&quot;:230,&quot;drat&quot;:3.23,&quot;wt&quot;:5.345,&quot;qsec&quot;:17.42,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Chrysler Imperial&quot;},{&quot;mpg&quot;:32.4,&quot;cyl&quot;:4,&quot;disp&quot;:78.7,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:2.2,&quot;qsec&quot;:19.47,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat 128&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:75.7,&quot;hp&quot;:52,&quot;drat&quot;:4.93,&quot;wt&quot;:1.615,&quot;qsec&quot;:18.52,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Honda Civic&quot;},{&quot;mpg&quot;:33.9,&quot;cyl&quot;:4,&quot;disp&quot;:71.1,&quot;hp&quot;:65,&quot;drat&quot;:4.22,&quot;wt&quot;:1.835,&quot;qsec&quot;:19.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corolla&quot;},{&quot;mpg&quot;:21.5,&quot;cyl&quot;:4,&quot;disp&quot;:120.1,&quot;hp&quot;:97,&quot;drat&quot;:3.7,&quot;wt&quot;:2.465,&quot;qsec&quot;:20.01,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Toyota Corona&quot;},{&quot;mpg&quot;:15.5,&quot;cyl&quot;:8,&quot;disp&quot;:318,&quot;hp&quot;:150,&quot;drat&quot;:2.76,&quot;wt&quot;:3.52,&quot;qsec&quot;:16.87,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Dodge Challenger&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:304,&quot;hp&quot;:150,&quot;drat&quot;:3.15,&quot;wt&quot;:3.435,&quot;qsec&quot;:17.3,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;AMC Javelin&quot;},{&quot;mpg&quot;:13.3,&quot;cyl&quot;:8,&quot;disp&quot;:350,&quot;hp&quot;:245,&quot;drat&quot;:3.73,&quot;wt&quot;:3.84,&quot;qsec&quot;:15.41,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Camaro Z28&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:8,&quot;disp&quot;:400,&quot;hp&quot;:175,&quot;drat&quot;:3.08,&quot;wt&quot;:3.845,&quot;qsec&quot;:17.05,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Pontiac Firebird&quot;},{&quot;mpg&quot;:27.3,&quot;cyl&quot;:4,&quot;disp&quot;:79,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:1.935,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;_row&quot;:&quot;Fiat X1-9&quot;},{&quot;mpg&quot;:26,&quot;cyl&quot;:4,&quot;disp&quot;:120.3,&quot;hp&quot;:91,&quot;drat&quot;:4.43,&quot;wt&quot;:2.14,&quot;qsec&quot;:16.7,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Porsche 914-2&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:95.1,&quot;hp&quot;:113,&quot;drat&quot;:3.77,&quot;wt&quot;:1.513,&quot;qsec&quot;:16.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Lotus Europa&quot;},{&quot;mpg&quot;:15.8,&quot;cyl&quot;:8,&quot;disp&quot;:351,&quot;hp&quot;:264,&quot;drat&quot;:4.22,&quot;wt&quot;:3.17,&quot;qsec&quot;:14.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:4,&quot;_row&quot;:&quot;Ford Pantera L&quot;},{&quot;mpg&quot;:19.7,&quot;cyl&quot;:6,&quot;disp&quot;:145,&quot;hp&quot;:175,&quot;drat&quot;:3.62,&quot;wt&quot;:2.77,&quot;qsec&quot;:15.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:6,&quot;_row&quot;:&quot;Ferrari Dino&quot;},{&quot;mpg&quot;:15,&quot;cyl&quot;:8,&quot;disp&quot;:301,&quot;hp&quot;:335,&quot;drat&quot;:3.54,&quot;wt&quot;:3.57,&quot;qsec&quot;:14.6,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:8,&quot;_row&quot;:&quot;Maserati Bora&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:4,&quot;disp&quot;:121,&quot;hp&quot;:109,&quot;drat&quot;:4.11,&quot;wt&quot;:2.78,&quot;qsec&quot;:18.6,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;_row&quot;:&quot;Volvo 142E&quot;}] 2.5 Importing from other statistical software Common software packages include SAS, STATA and SPSS. Two packages useful for importing data from these packages are: haven: by Hadley Wickham and is under active development. It aims to be more consistent, easier and faster than foreign. It can read SAS, Stata and SPSS and will read in the file as an D dataframe. foreign: is an older package by the R Core Team. Foreign support more data formats than haven including Weka and Systat # Load the haven package library(haven) # Import sales.sas7bdat: sales sales &lt;- read_sas(&quot;sales.sas7bdat&quot;) # Display the structure of sales str(sales) # Import the data from the URL: sugar sugar &lt;- read_dta(&quot;http://assets.datacamp.com/production/course_1478/datasets/trade.dta&quot;) # Structure of sugar str(sugar) # Convert values in Date column to dates sugar$Date &lt;- as.Date(as_factor(sugar$Date)) # Structure of sugar again str(sugar) # Import person.sav: traits traits &lt;- read_sav(&quot;person.sav&quot;) # Summarize traits summary(traits) # Print out a subset subset(traits, Extroversion &gt; 40 &amp; Agreeableness &gt; 40) When using SPSS files, it is often the case that the variable labels are also imported, it is best to change these in to standard R factors. # Import SPSS data from the URL: work work &lt;- read_sav(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav&quot;) # Display summary of work$GENDER summary(work$GENDER) # Convert work$GENDER to a factor work$GENDER &lt;- as_factor(work$GENDER) # Display summary of work$GENDER again summary(work$GENDER) Foreign cannot use single SAS datafiles like haven, it works with SAS library files .xport. Foreign tends to use dots in the function names rather than underscores in haven e.g. read.dta() vs read_dta(). Foreign does not provide consistency with it’s functions i.e. read.dta() has different arguments than read.spss(), however foreign provides more control over the data importing, such as dealing with multiple types of missing data which are often present in survey data, more comprehensively than haven. Although haven is still being developed. # Load the foreign package library(foreign) # Specify the file path using file.path(): path path &lt;- file.path(&quot;worldbank&quot;, &quot;edequality.dta&quot;) # Create and print structure of edu_equal_1 edu_equal_1 &lt;- read.dta(path) str(edu_equal_1) # Create and print structure of edu_equal_2 edu_equal_2 &lt;- read.dta(path, convert.factors = FALSE) str(edu_equal_2) # Create and print structure of edu_equal_3 edu_equal_3 &lt;- read.dta(path, convert.underscore = TRUE) str(edu_equal_3) # Import international.sav as a data frame: demo demo &lt;- read.spss(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/international.sav&quot;, to.data.frame = TRUE) # Create boxplot of gdp variable of demo boxplot(demo$gdp) "],
["references.html", "References", " References "],
["introduction-to-data.html", "3 Introduction to Data 3.1 Language of Data 3.2 Observational Studies and Experiments 3.3 Sampling strategies and experimental design", " 3 Introduction to Data Notes taken during/inspired by the Datacamp course ‘Introduction to Data’ by Mine Cetinkaya-Rundel. The supporting textbook is Diez, Barr, and Cetinkaya-Rundel (2015). 3.1 Language of Data The course makes use of the openintro package, accompanying the textbook. Let’s load the package and our first dataset, email50. # Load packages library(&quot;openintro&quot;) library(&quot;dplyr&quot;) # Load data data(email50) # View its structure str(email50) ## &#39;data.frame&#39;: 50 obs. of 21 variables: ## $ spam : num 0 0 1 0 0 0 0 0 0 0 ... ## $ to_multiple : num 0 0 0 0 0 0 0 0 0 0 ... ## $ from : num 1 1 1 1 1 1 1 1 1 1 ... ## $ cc : int 0 0 4 0 0 0 0 0 1 0 ... ## $ sent_email : num 1 0 0 0 0 0 0 1 1 0 ... ## $ time : POSIXct, format: &quot;2012-01-04 13:19:16&quot; &quot;2012-02-16 20:10:06&quot; ... ## $ image : num 0 0 0 0 0 0 0 0 0 0 ... ## $ attach : num 0 0 2 0 0 0 0 0 0 0 ... ## $ dollar : num 0 0 0 0 9 0 0 0 0 23 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num 0 0 0 0 0 0 0 0 0 0 ... ## $ viagra : num 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num 0 0 0 0 1 0 0 0 0 0 ... ## $ num_char : num 21.705 7.011 0.631 2.454 41.623 ... ## $ line_breaks : int 551 183 28 61 1088 5 17 88 242 578 ... ## $ format : num 1 1 0 0 1 0 0 1 1 1 ... ## $ re_subj : num 1 0 0 0 0 0 0 1 1 0 ... ## $ exclaim_subj: num 0 0 0 0 0 0 0 0 1 0 ... ## $ urgent_subj : num 0 0 0 0 0 0 0 0 0 0 ... ## $ exclaim_mess: num 8 1 2 1 43 0 0 2 22 3 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 2 3 1 2 2 2 2 2 2 2 ... #glimpse the first few items using dplyr glimpse(email50) ## Observations: 50 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0... ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0... ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ cc &lt;int&gt; 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ sent_email &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ time &lt;dttm&gt; 2012-01-04 13:19:16, 2012-02-16 20:10:06, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ attach &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0... ## $ dollar &lt;dbl&gt; 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, ... ## $ winner &lt;fctr&gt; no, no, no, no, no, no, no, no, no, no, no, no, ... ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ password &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0... ## $ num_char &lt;dbl&gt; 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809... ## $ line_breaks &lt;int&gt; 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167... ## $ format &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1... ## $ re_subj &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1... ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0... ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ exclaim_mess &lt;dbl&gt; 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, ... ## $ number &lt;fctr&gt; small, big, none, small, small, small, small, sm... When using certain functions, such as filters on categorical variables, the way R handles the filtered out variables is to leave the items in as place holders (empty containers), even though the place holder is empty. This can have undesirable effects, particularly if using the filtered object for modelling. We then end up with zero values which are actually filtered out factors. # Subset of emails with big numbers: email50_big email50_big &lt;- email50 %&gt;% filter(number == &quot;big&quot;) # Glimpse the subset glimpse(email50_big) ## Observations: 7 ## Variables: 21 ## $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0 ## $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ cc &lt;int&gt; 0, 0, 0, 0, 0, 0, 0 ## $ sent_email &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0 ## $ time &lt;dttm&gt; 2012-02-16 20:10:06, 2012-02-04 23:26:09, 2012-0... ## $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ attach &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ dollar &lt;dbl&gt; 0, 0, 3, 2, 0, 0, 0 ## $ winner &lt;fctr&gt; no, no, yes, no, no, no, no ## $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ password &lt;dbl&gt; 0, 2, 0, 0, 0, 0, 8 ## $ num_char &lt;dbl&gt; 7.011, 10.368, 42.793, 26.520, 6.563, 11.223, 10.613 ## $ line_breaks &lt;int&gt; 183, 198, 712, 692, 140, 512, 225 ## $ format &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1 ## $ re_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0 ## $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0 ## $ exclaim_mess &lt;dbl&gt; 1, 1, 2, 7, 2, 9, 9 ## $ number &lt;fctr&gt; big, big, big, big, big, big, big # Table of number variable - now we have just 7 values table(email50_big$number) ## ## none small big ## 0 0 7 # Drop levels email50_big$number &lt;- droplevels(email50_big$number) # Another table of number variable table(email50_big$number) ## ## big ## 7 In some instance you want to create a discreet function from a numeric value. That is to say we want to create a categorical value based on some groups of numbers. This can be achived as shown below. Note that when calculating a function R will typically either: Assign a value e.g. med_num_char &lt;- median(email50$num_char) Print a result e.g. median(email50$num_char) But we can do both by adding brackets (med_num_char &lt;- median(email50$num_char)) # Calculate median number of characters: med_num_char (med_num_char &lt;- median(email50$num_char)) ## [1] 6.8895 # Create num_char_cat variable in email50 email50 &lt;- email50 %&gt;% mutate(num_char_cat = ifelse(num_char &lt; med_num_char, &quot;below median&quot;, &quot;at or above median&quot;)) # Count emails in each category table(email50$num_char_cat) ## ## at or above median below median ## 25 25 We can also use the mutate function from dplyr to create a new variable from categorical variables # Create number_yn column in email50 email50 &lt;- email50 %&gt;% mutate(number_yn, ifelse(number == &quot;none&quot;, &quot;no&quot;, &quot;yes&quot;)) # Visualize number_yn ggplot(email50, aes(x = number_yn)) + geom_bar() We often want to compare two or three variables, which is most easily done using the ggplot package # Load ggplot2 library(ggplot2) # Scatterplot of exclaim_mess vs. num_char ggplot(email50, aes(x = num_char, y = exclaim_mess, color = factor(spam))) + geom_point() 3.2 Observational Studies and Experiments Typically there are two types of study, if we are interested in whether variable Y is caused by some factors (X) we could have two types of studies. Observational Study: We are observing, rather than specifically interfere or direct how the data is collected - only correlation can be inferred. In this case, we might survey people and look for patterns in their characteristics (X) and the outcome variable (Y) Experimental Study: We randomly assign subjects to various treatments - causation can be inferred. In this case, we would get a group of individuals together then randomly assign them to a group of interest (X), removing the decision from the subjects of the study, we often have a control group also. Another differentiation to be aware of is between Random sampling: We select our subjects at random in order that we can make inferences from our sample, to the wider population Random assignment: Subjects are randomly assigned to various treatments and helps us to make causal conclusions We can therefore combine random sampling with random assignment, to allow causal and generalisable conclusions, however in practice we typically have one or the other - random sampling only (not causal but generalisable), or random assignment (causal but not generalisable) - the negation of both leads to results that are neither causal nor generalisable, but may highlight a need for further research. Sometimes when there are looking for associations between variables, it is possible to omit variables of interest, which may be confounding variables. For instance, we may have two variables (x) that appear to show a relationship with another (y) but the inclusion of a third variable (x’) causes the apparent relationship to breakdown. If we fail to consider other associated variables, we may fall in to a Simpsons Paradox in which a trend appears in different groups, but disappears when the groups are combined together. Simpsons paradox is a form of Ecological Fallacy. One of the best known examples of Simpsons Paradox comes from admissions data for University of California, Berkeley. library(tidyr) data(&quot;UCBAdmissions&quot;) ucb_admit &lt;- as.data.frame(UCBAdmissions) # Restrucutre data - this is to follow the example provided, it takes the aggregated data from the original data frame and disaggregates # it using indexing by repeating the row indices Freq times for each row - see https://stackoverflow.com/questions/45445919/convert-wide-to-long-with-frequency-column ucb_admit_disagg = ucb_admit[rep(1:nrow(ucb_admit), ucb_admit$Freq), -grep(&quot;Freq&quot;, names(ucb_admit))] # Count number of male and female applicants admitted ucb_counts &lt;- ucb_admit_disagg %&gt;% count(Gender, Admit) # View result ucb_counts ## # A tibble: 4 x 3 ## Gender Admit n ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; ## 1 Male Admitted 1198 ## 2 Male Rejected 1493 ## 3 Female Admitted 557 ## 4 Female Rejected 1278 # Spread the output across columns and calculate percentages ucb_counts %&gt;% spread(Admit, n) %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 2 x 4 ## Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Male 1198 1493 0.4451877 ## 2 Female 557 1278 0.3035422 So far, it seems that the results suggest females are less likely to be admitted, but what if we look at the results by department? # Table of counts of admission status and gender for each department admit_by_dept &lt;- ucb_admit_disagg %&gt;% count(Dept, Gender, Admit) %&gt;% spread(Admit, n) # View result admit_by_dept ## # A tibble: 12 x 4 ## Dept Gender Admitted Rejected ## * &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 A Male 512 313 ## 2 A Female 89 19 ## 3 B Male 353 207 ## 4 B Female 17 8 ## 5 C Male 120 205 ## 6 C Female 202 391 ## 7 D Male 138 279 ## 8 D Female 131 244 ## 9 E Male 53 138 ## 10 E Female 94 299 ## 11 F Male 22 351 ## 12 F Female 24 317 # Percentage of those admitted to each department admit_by_dept %&gt;% mutate(Perc_Admit = Admitted / (Admitted + Rejected)) ## # A tibble: 12 x 5 ## Dept Gender Admitted Rejected Perc_Admit ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A Male 512 313 0.62060606 ## 2 A Female 89 19 0.82407407 ## 3 B Male 353 207 0.63035714 ## 4 B Female 17 8 0.68000000 ## 5 C Male 120 205 0.36923077 ## 6 C Female 202 391 0.34064081 ## 7 D Male 138 279 0.33093525 ## 8 D Female 131 244 0.34933333 ## 9 E Male 53 138 0.27748691 ## 10 E Female 94 299 0.23918575 ## 11 F Male 22 351 0.05898123 ## 12 F Female 24 317 0.07038123 Now we begin to see that for some departments, there is a higher proportion of females being accpeted than males. Equally for some departments, the rejection rate is very high for both males and females e.g. Dept F. In 4 of the 6 departments, females have a higher proportion of applications being admitted than males. Males tended to apply to less competitive departments than females, the less competitive departments had higher admission rates. 3.3 Sampling strategies and experimental design We use sampling when we do not want, for whatever reason, to conduct a full Census. A simple random sample is usually the most basic method. We can also use stratified sampling to ensure representation from certain groups. Or we use cluster sampling usually for economic reasons. Or some combination in multistage sampling. data(county) # Simple random sample: county_srs county_srs &lt;- county %&gt;% sample_n(size = 20) # Count counties by state county_srs %&gt;% group_by(state) %&gt;% count() ## # A tibble: 15 x 2 ## # Groups: state [15] ## state n ## &lt;fctr&gt; &lt;int&gt; ## 1 Alabama 1 ## 2 Arkansas 1 ## 3 Colorado 1 ## 4 Georgia 3 ## 5 Illinois 1 ## 6 Iowa 1 ## 7 Louisiana 2 ## 8 Massachusetts 1 ## 9 Michigan 1 ## 10 Mississippi 2 ## 11 New Mexico 1 ## 12 Oklahoma 1 ## 13 South Carolina 2 ## 14 South Dakota 1 ## 15 Texas 1 For a stratified sample we would do something similar. # Stratified sample states_str &lt;- us_regions %&gt;% group_by(region) %&gt;% sample_n(size = 2) # Count states by region states_str %&gt;% group_by(region) %&gt;% count() The principles of experimental design include 4 key components: Control: compare treatment of interest to a control group Randomise: randomly assign subjects to treatments Replicate: collect a sufficiently large sample within a study, or replicate the entire study Block: account for the potential effect of confounding variables We group subjects into blocks based on these confounding variables, then randomise within each block to treatment groups. So for instance, if we were testing whether an online or classroom R course was more effective using an experiment, one possible confounding variable would be previous programming experience. Therefore we would seperate out - block - those with and those without previous programming experience, ensuring we have an equal number in each treatment group (online vs classroom) of those with and without previous experience. In random sampling, you use stratifying to control for a variable. In random assignment, you use blocking to achieve the same goal. References "],
["references-1.html", "References", " References "],
["foundations-of-inference.html", "4 Foundations of Inference 4.1 Introduction to Inference 4.2 Home Ownership by Gender 4.3 Density Plots 4.4 Gender Discrimination (p-values) 4.5 Opportunity Cost 4.6 Type I and Type II errors 4.7 Bootstrapping", " 4 Foundations of Inference Notes taken during/inspired by the Datacamp course ‘Foundations of Inference’ by Jo Hardin, collaborators; Nick Carchedi and Tom Jeon. 4.1 Introduction to Inference Classical statistical inference is the process of making claims about a population based on a sample of information. We are making an inference from a small group (sample) to a much larger one (population). We typically have: Null Hypothesis \\(H_{0}\\): What we are researching has no effect Alternate Hypothesis \\(H_{A}\\): What we are researching does have an effect Under the null hypothesis, chance alone is responsible for the results. Under the alternate hypothesis, we reject the null hypothesis, by using statistical techniques that indicate that chance is not responsible for our findings. Hypothesis or statistical testing goes back over 300 years, with the first recorded use by John Arbuthnot: Table 4.1: Statistical Testing Applications Year Person Context 1710 Arbuthnot Sex ratio at birth 1767 Michelle Distribution of stars 1823 Laplace Moon phase and barometric changes 1900 K. Pearson Goodness of fit 1908 Gosset A single mean Source: (Huberty 1993, pg 318) Contemporary statistical testing is a usually that of either Fisher or Neyman-Pearson approaches. Fisher tends to use a single hypothesis test and a p-value strength of evidence test, where as the Neyman-Pearson test will set a critical alpha value and compare the null hypothesis against an alternative hypothesis, rejecting the null if the test statistic is high enough (Huberty 1993, pg 318). The course goes on to say that idea behind statistical inference is to understand samples from a hypothetical population, where the null hypothesis is true - there is no difference between two groups. We can do this by calculating one statistic - for instance the proportion (mean) of a test group who show a positive response when testing a new drug, compared to a placebo control group - for each repeated sample from a population, then work out the difference between these two groups means. With each sample, the mean will change, resulting in a changing difference for each sample. We can then generate a distribution (histogram) of differences, assuming the null hypothesis - that there is no link between drug effectiveness between a test group and a control group - is true. “Generating a distribution of the statistic from the null population gives information about whether the observed data are inconsistent with the null hypothesis”. That is to say, by taking repeated samples and creating a distribution, we can then say whether our observed difference is consistent (within an acceptable value range due to chance) to the null hypothesis. The null samples consist of randomly shuffled drug effectiveness variables (permuted samples from the population), so that the samples don’t have any dependency between the two groups and effectiveness. 4.2 Home Ownership by Gender Data used in the exercises are from NHANES 2009-2012 With Adjusted Weighting. This is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC). The NHANES target population is “the non-institutionalized civilian resident population of the United States”. NHANES, (American National Health and Nutrition Examination surveys), use complex survey designs (see http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf) that oversample certain subpopulations like racial minorities. # Load packages library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) library(&quot;NHANES&quot;) library(&quot;oilabs&quot;) # Create bar plot for Home Ownership by Gender ggplot(NHANES, aes(x = Gender, fill = HomeOwn)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;Relative frequencies&quot;) # Density for SleepHrsNight coloured by SleepTrouble, faceted by HealthGen ggplot(NHANES, aes(x = SleepHrsNight, col = SleepTrouble)) + geom_density(adjust = 2) + facet_wrap(~ HealthGen) Next we want to create a selection for just our variables of interest - rent and owner occupation. # Subset the data: homes homes &lt;- NHANES %&gt;% select(Gender, HomeOwn) %&gt;% filter(HomeOwn %in% c(&quot;Own&quot;, &quot;Rent&quot;)) We build a distribution of differences assuming the null hypothesis - that there is no link between gender and home ownership - is true. In this first step, we just do a single iteration, or permutation from the true values. The null (permuted) version here will create a randomly shuffled home ownership variable, so that the permuted version does not have any dependency between gender and homeownership. We effectively have the same gender split variables as per the original, with the same owned and rented proportions, but disassociated from the gender variable - just randomly shuffled. # Perform one permutation homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 -2.96867e-06 It is easier to see what is going on by breaking the results down iteratively. Our selected and filtered homes dataset looks like. head(homes) ## # A tibble: 6 x 2 ## Gender HomeOwn ## &lt;fctr&gt; &lt;fctr&gt; ## 1 male Own ## 2 male Own ## 3 male Own ## 4 male Own ## 5 female Rent ## 6 male Rent Next we shuffle this data, let’s call it homes 2. we can then check the total number of owns and rents are the same using the summary function, which confirms the data is just randomly shuffled. homes2 &lt;- homes %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(Gender) tail(homes2) ## # A tibble: 6 x 3 ## # Groups: Gender [2] ## Gender HomeOwn HomeOwn_perm ## &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; ## 1 male Rent Rent ## 2 male Rent Rent ## 3 female Own Rent ## 4 male Own Own ## 5 male Own Rent ## 6 male Own Rent summary(homes2) ## Gender HomeOwn HomeOwn_perm ## female:4890 Own :6425 Own :6425 ## male :4822 Rent :3287 Rent :3287 ## Other: 0 Other: 0 Then we calculate the mean value of home ownership (Own) across our original and shuffled (permutated) data homes3 &lt;- homes2 %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) homes3 ## # A tibble: 2 x 3 ## Gender prop_own_perm prop_own ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 0.6701431 0.6654397 ## 2 male 0.6528411 0.6576109 FFinally we calculate the differences in ownership - note that the difference for the permuted value here may be different from the full code above, as it a new random permutation and we have used the set.seed() function which would create an identical permutation. homes4 &lt;- homes3 %&gt;% summarize(diff_perm = diff(prop_own), diff_orig = diff(prop_own_perm)) homes4 ## # A tibble: 1 x 2 ## diff_perm diff_orig ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.007828723 -0.017302 4.3 Density Plots Next we can make multiple permutations using the rep_sample_n from the oilabs package. We specify the data (tbl), the sample size, the number of samples to take (reps), and whether sampling should be done with or without replacement (replace). The output includes a new column, replicate, which indicates the sample number. We can create 100 permutations and create a dot plot of the results. # Perform 100 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 100) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Dotplot of 100 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_dotplot(binwidth = .001) We can go further and run 1000 permutations and create a density chart. set.seed(666) # Perform 1000 permutations homeown_perm &lt;- homes %&gt;% rep_sample_n(size = nrow(homes), reps = 1000) %&gt;% mutate(HomeOwn_perm = sample(HomeOwn)) %&gt;% group_by(replicate, Gender) %&gt;% summarize(prop_own_perm = mean(HomeOwn_perm == &quot;Own&quot;), prop_own = mean(HomeOwn == &quot;Own&quot;)) %&gt;% summarize(diff_perm = diff(prop_own_perm), diff_orig = diff(prop_own)) # male - female # Density plot of 1000 permuted differences in proportions ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() Now we have our density plot of the null hypothesis - randomly permuted samples - we can see where our actual observed difference lies, plus how many randomly permuted differences were less than the observed difference. # Plot permuted differences ggplot(homeown_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences homeown_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 21.5 So in this instance, when we set the seed of 666 we end up with 20.5% of randomly shuffled (permuted) differences being greater than the observed difference, so the observed difference is consistent with the null hypothesis. That it to say it is within the range we may expect by chance alone, were we to repeat the exercise, although we should specify a distribtion we are comparing against, in this which is inferred as being the normal distribution in this instance. We can therefore say that there is no statistically significant difference between gender and home ownership. Or put more formally We fail to reject the null hypothesis: There is no evidence that our data are inconsistent with the null hypothesis 4.4 Gender Discrimination (p-values) In this section we use data from Rosen and Jerdee (1974), where 48 male bank supervisors were given personnel files and asked if they should be promoted to Branch Manager. All files were identical, but half (24) were named as female, and the other half (24) were named male. The results showed 21 males were promoted and 14 females, meaning 35 of the total 48 were promoted. In Rosen and Jerdee (1974) sex was given along with an indication of the difficulty - routine or complex - here we only look at the routine promotion candidates. Do we know if gender is a statistically significant factor? Null Hypothesis \\(H_{0}\\): Gender and promotion are unrelated variables Alternate Hypothesis \\(H_{A}\\): Men are more likely to be promoted First, we create the data frame disc disc &lt;- data.frame( promote = c(rep(&quot;promoted&quot;, 35), rep(&quot;not_promoted&quot;, 13)), sex = c(rep(&quot;male&quot;, 21), rep(&quot;female&quot;, 14), rep(&quot;male&quot;, 3), rep(&quot;female&quot;, 10)) ) Then let’s see the resulting table and proportion who were promoted table(disc) ## sex ## promote female male ## not_promoted 10 3 ## promoted 14 21 disc %&gt;% group_by(sex) %&gt;% summarise(promoted_prop = mean(promote == &quot;promoted&quot;)) ## # A tibble: 2 x 2 ## sex promoted_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 female 0.5833333 ## 2 male 0.8750000 So there difference in promotions by gender is around 0.3 or around 30%, but could this be due to chance? We can create 1000 permutations and compare our observed diffrence to the distribution, plus how many randomly permuted differences were less than the observed difference. # Create a data frame of differences in promotion rates set.seed(42) disc_perm &lt;- disc %&gt;% rep_sample_n(size = nrow(disc), reps = 1000) %&gt;% mutate(prom_perm = sample(promote)) %&gt;% group_by(replicate, sex) %&gt;% summarize(prop_prom_perm = mean(prom_perm == &quot;promoted&quot;), prop_prom = mean(promote == &quot;promoted&quot;)) %&gt;% summarize(diff_perm = diff(prop_prom_perm), diff_orig = diff(prop_prom)) # male - female # Histogram of permuted differences ggplot(disc_perm, aes(x = diff_perm)) + geom_density() + geom_vline(aes(xintercept = diff_orig), col = &quot;red&quot;) # Compare permuted differences to observed difference and calculate the percent of differences disc_perm %&gt;% summarize(sum(diff_orig &gt;= diff_perm)) /1000 * 100 ## sum(diff_orig &gt;= diff_perm) ## 1 99.3 So here, just 0.5% of the randomly permuted/shuffled results are greater than our observed promotion differences, or 99.5% are lower, so our results are definitely quite extreme. We typically use a 5% cut off, which the course mentions is arbitrary and historic, being attributed to Fisher. So we can say at 0.5% our value is within this critical region, meaning the results are statistically significant - we should not ignore them. We can calculate quantiles of the null statistic using our randomly generated shuffles. disc_perm %&gt;% summarize(q.90 = quantile(diff_perm, p = 0.90), q.95 = quantile(diff_perm, p = 0.95), q.99 = quantile(diff_perm, p = 0.99)) ## # A tibble: 1 x 3 ## q.90 q.95 q.99 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2083333 0.2083333 0.2916667 So here, 95% of our null differences are 0.208 or lower, indeed 99% are 0.292 or lower, so our observed difference of 0.3 is quite extreme - it is in the critical region of the distribution. We can go one step further by calculating the p-value. The p-value is: the probability of observing data as or more extreme than what we actually got given that the null hypothesis is true. disc_perm %&gt;% summarize(mean(diff_orig &lt;= diff_perm)) ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.025 So the p-value here is 0.028 (less than 3 %). If repeat the exercise with smaller and larger number of shuffles we would get different p-values. ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.01 ## # A tibble: 1 x 1 ## `mean(diff_orig &lt;= diff_perm)` ## &lt;dbl&gt; ## 1 0.0227 With 100 shuffles our p-value is 0.03, and with 10,000 shuffles our p-value is 0.0235. If we had a two-tailed test - for instance if we said the original research hypothesis had focused on any difference in promotion rates between men and women instead of focusing on whether men are more likely to be promoted than women - we could simple double the p-value. In both cases, the p-value is below or close to the 0.05 (5%) critical value, meaning we can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. However, as both values are close to the critical value, we should indicate that more work should be done. Indeed since the Rosen and Jerdee (1974) study, many further studies have been undertaken and found a similar pattern of discrimination. 4.5 Opportunity Cost In Frederick et al. (2009) their study showed that when potential purchasers were reminded that if they did not buy a particular DVD they could instead save the money, when compared to a control group who were just told they could not buy the DVD, those being reminded of the saving appeared to be more inclined not to make the purchase - 34 in the treatment group did not buy compared to 19 in the control. So our test is setup as: Null Hypothesis \\(H_{0}\\): Reminding students will have no impact on their spending decisions Alternate Hypothesis \\(H_{A}\\): Reminding students will reduce the chance they continue with a purchase We can create a data frame containing the results and find the initial proportions. #create the data frame opportunity &lt;- data.frame( decision = c(rep(&quot;buyDVD&quot;, 97), rep(&quot;nobuyDVD&quot;, 53)), group = c(rep(&quot;control&quot;, 56), rep(&quot;treatment&quot;, 41), rep(&quot;control&quot;, 19), rep(&quot;treatment&quot;, 34)) ) # Tabulate the data opportunity %&gt;% select(decision, group) %&gt;% table() ## group ## decision control treatment ## buyDVD 56 41 ## nobuyDVD 19 34 # Find the proportion who bought the DVD in each group opportunity %&gt;% group_by(group) %&gt;% summarize(buy_prop = mean(decision == &quot;buyDVD&quot;)) ## # A tibble: 2 x 2 ## group buy_prop ## &lt;fctr&gt; &lt;dbl&gt; ## 1 control 0.7466667 ## 2 treatment 0.5466667 So around 55% of the treatment group - those who were reminded they could save the money - bought the DVD, comapred to 75% of the control group. We can represent this with a bar plot. As before, we can calculate 1000 random shuffles and then compare our difference in proportions, to the distribution of those 1000 samples. And finally, we can calculate the p-value ## # A tibble: 1 x 1 ## `mean(diff_perm &lt;= diff_orig)` ## &lt;dbl&gt; ## 1 0.012 In this instance, of p-value is substantially less than the usual critical value - 0.8% versus the usual value of 5% - so we can can reject the null hypthesis as there is evidence that our data are inconsistent with the null hypothesis. Our results would only occur 8 times in 1000 by chance. We can therefore accept the alternative hypothesis (\\(H_{A}\\)) that reminding students does cause them to be less likely to buy a DVD, as they were randomly assigned to the treatment and control groups, therefore any difference is due to the reminder to save. Who can we therefore make the inference to? Our sample was drawn from the student population for the Frederick et al. (2009) study, so we would be able to generalise to that student population however defined, but not to another wider population. 4.6 Type I and Type II errors In our research and conslusions there is a risk that we will be incorrect, we will make an error. The two errors are: Type I error : The null hypothesis (\\(H_{0}\\)) is true, but is rejected. On the basis of the evidence, we have decided to erroneously accept the alternative hypothesis (\\(H_{A}\\)) when in fact the null hypothesis is correct. It is sometimes called a false positive. Type II error : the null hypothesis is false, but erroneously fails to be rejected. On the basis of the evidence, we have failed to accept the alternative hypothesis despite it being correct - an effect that exists in the population. It is sometimes called a false negative. If we return to our previous example, our associated errors would be Type I: There is not a difference in proportions, but the observed difference is big enough to indicate that the proportions are different. Type II: There is a difference in proportions, but the observed difference is not large enough to indicate that the proportions are different. 4.7 Bootstrapping Sometimes we are not neccessarily interested in testing a hypothesis, we are instead interested in making a claim about how our sample can be inferred to a large population. To do so we use confidece intervals. When calculating confidence intervals there is no null hypothesis like in hypothesis testing. We need to understand how samples from our population vary around the parameter of interest. In an ideal world we would take many samples from the population or know what the true value is in the population, but realistically this is not possible, so we use booststrapping. Bootstrapping is the process of taking repeated samples from the same sample, to estimate the variability. As our population parameters are not known, we can use our sample to estimate a simulated population parameter (\\(\\hat{p}*\\)) by repeated sampling. We can then estimate other parameters such as the standard deviation, s.e. and the confidence interval. Instead of taking repeated samples from our population, we take repeated samples from our data, with replacement, each bootstrap sample is the same size as the original sample. Figure 4.1: Illustration of the bootstrap approach on a small sample containing n = 3 observations (James et al. 2013, pg 190) Firstly we setup our single poll, where 70% (21/30) are intended to vote for a particular candidate # Setup our single poll example one_poll &lt;- sample(rep(c(0, 1), times = c(9,21))) one_poll &lt;- tbl_df(one_poll) colnames(one_poll) &lt;- &quot;vote&quot; Next we can create 1000 bootstrap samples from this original poll, then calculate the variability set.seed(42) # Generate 1000 resamples of one_poll: one_poll_boot_30 one_poll_boot_30 &lt;- one_poll %&gt;% rep_sample_n(size = 30, replace = TRUE, reps = 1000) # Compute p-hat* for each resampled poll ex1_props &lt;- one_poll_boot_30 %&gt;% summarize(prop_yes = mean(vote)) %&gt;% summarize(sd(prop_yes)) #compute variability p-hat* ex1_props ## # A tibble: 1 x 1 ## `sd(prop_yes)` ## &lt;dbl&gt; ## 1 0.08624387 So the variability - the standard error or SE - of \\(\\hat{p}*\\) is 0.0841. We can now use this SE to calculate a confidence interval, since 95% of samples will be within +/- 1.96 standard errors of the centre of the distribution assuming a normal distribution \\(N(\\mu, \\sigma ^2)\\). We also use the bootstrap to calculate our bootstrap confidence interval, to give a range of possible values. # Compute p-hat for one poll p_hat &lt;- mean(one_poll$vote) set.seed(42) # Bootstrap to find the SE of p-hat: one_poll_boot one_poll_boot &lt;- one_poll %&gt;% rep_sample_n(30, replace = TRUE, reps = 1000) %&gt;% summarize(prop_yes_boot = mean(vote)) # Create an interval of possible values one_poll_boot %&gt;% summarize(lower = p_hat - 1.96 * sd(prop_yes_boot), upper = p_hat + 1.96 * sd(prop_yes_boot)) ## # A tibble: 1 x 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.530962 0.869038 So our possible range of values, using the bootstrap at 95%, is between 53.2% and 86.8%. Going back to our original statement, we had a single poll where 70% of those polled intended to vote for a particular candidate. We can now say, using the bootstrap t-confidence interval, we are 95% confident that the true proportion planning to vote for the candidate is between 53% and 87%. We are assuming that the distribution is normally distributed \\(N(\\mu, \\sigma ^2)\\). References "],
["references-2.html", "References", " References "],
["exploratory-data-analysis.html", "5 Exploratory Data Analysis 5.1 Categorical Data 5.2 Numerical Data 5.3 Numerical Summaries 5.4 Email Case Study", " 5 Exploratory Data Analysis Notes taken during/inspired by the Datacamp course “Exploratory Data Analysis” by Andrew Bray. 5.1 Categorical Data Common functions when looking at categorical, aka factors variables, are levels(df\\(var) and to get a contigency or xtab table the table(df\\)var1, df$var2). We can also create bar charts to visually represent the data using ggplot. # Read in our dataset thanks to fivethirtyeight https://github.com/fivethirtyeight/data/tree/master/comic-characters comics &lt;- read.csv(&quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/comic-characters/dc-wikia-data.csv&quot;, stringsAsFactors = TRUE) comics$name &lt;- as.character(comics$name) # Check levels of align levels(comics$ALIGN) ## [1] &quot;&quot; &quot;Bad Characters&quot; &quot;Good Characters&quot; ## [4] &quot;Neutral Characters&quot; &quot;Reformed Criminals&quot; # Check the levels of gender levels(comics$SEX) ## [1] &quot;&quot; &quot;Female Characters&quot; ## [3] &quot;Genderless Characters&quot; &quot;Male Characters&quot; ## [5] &quot;Transgender Characters&quot; # Create a 2-way contingency table table(comics$ALIGN, comics$SEX) ## ## Female Characters Genderless Characters ## 25 220 0 ## Bad Characters 63 597 11 ## Good Characters 30 953 6 ## Neutral Characters 7 196 3 ## Reformed Criminals 0 1 0 ## ## Male Characters Transgender Characters ## 356 0 ## Bad Characters 2223 1 ## Good Characters 1843 0 ## Neutral Characters 359 0 ## Reformed Criminals 2 0 To simplify an analysis, it often helps to drop levels with small amounts of data. In R, this requires two steps: first filtering out any rows with the levels that have very low counts, then removing these levels from the factor variable with droplevels(). This is because the droplevels() function would keep levels that have just 1 or 2 counts; it only drops levels that don“t exist in a dataset. # Load dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Remove align level comics &lt;- comics %&gt;% filter(ALIGN != &quot;Reformed Criminals&quot;) %&gt;% droplevels() While a contingency table represents the counts numerically, it“s often more useful to represent them graphically. Here you“ll construct two side-by-side barcharts of the comics data. This shows that there can often be two or more options for presenting the same data. Passing the argument position =”dodge&quot; to geom_bar() says that you want a side-by-side (i.e. not stacked) barchart. # Load ggplot2 library(ggplot2) # Create side-by-side barchart of gender by alignment ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;dodge&quot;) # Create side-by-side barchart of alignment by gender ggplot(comics, aes(x = SEX, fill = ALIGN)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 90)) When creatign tables, it is often easier to look at proportions for patterns rather than counts. We can do this using conditional proportions, by using the prop.table(df_counts, n) where n is the number we want to condition our frequency/count table by, 1 = rows and 2 = columns. tab &lt;- table(comics$ALIGN, comics$SEX) options(scipen = 999, digits = 2) # Print fewer digits prop.table(tab) # Joint proportions (totals in the entire table) ## ## Female Characters Genderless Characters ## 0.00363 0.03192 0.00000 ## Bad Characters 0.00914 0.08661 0.00160 ## Good Characters 0.00435 0.13826 0.00087 ## Neutral Characters 0.00102 0.02843 0.00044 ## ## Male Characters Transgender Characters ## 0.05165 0.00000 ## Bad Characters 0.32250 0.00015 ## Good Characters 0.26737 0.00000 ## Neutral Characters 0.05208 0.00000 prop.table(tab, 2) # Conditional on columns (column totals) ## ## Female Characters Genderless Characters ## 0.200 0.112 0.000 ## Bad Characters 0.504 0.304 0.550 ## Good Characters 0.240 0.485 0.300 ## Neutral Characters 0.056 0.100 0.150 ## ## Male Characters Transgender Characters ## 0.074 0.000 ## Bad Characters 0.465 1.000 ## Good Characters 0.385 0.000 ## Neutral Characters 0.075 0.000 Here we see that approx. 49% of female characters are good, compared to 39% for males. Bar charts can tell dramatically different stories depending on whether they represent counts or proportions and, if proportions, what the proportions are conditioned on. To demonstrate this difference, you“ll construct two barcharts in this exercise: one of counts and one of proportions. # Plot of gender by align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar() # Plot proportion of gender, conditional on align ggplot(comics, aes(x = ALIGN, fill = SEX)) + geom_bar(position = &quot;fill&quot;) Conditional barchart Now, if you want to break down the distribution of alignment based on gender, you“re looking for conditional distributions. You could make these by creating multiple filtered datasets (one for each gender) or by faceting the plot of alignment based on gender. As a point of comparison, we“ve provided your plot of the marginal distribution of alignment from the last exercise. # Plot of alignment broken down by gender ggplot(comics, aes(x = ALIGN)) + geom_bar() + facet_wrap(~ SEX) 5.2 Numerical Data # Data courtesy of http://www.idvbook.com/teaching-aid/data-sets/ with some variable name modifictions to match those in the exercise library(readxl) cars &lt;- read_excel(&quot;04cars data.xls&quot;, sheet = 1) cars &lt;- cars[-2] # remove variable 2 # Rename vars names(cars) &lt;- c(&quot;name&quot;, &quot;sports_car&quot;, &quot;suv&quot;, &quot;wagon&quot;, &quot;minivan&quot;, &quot;pickup&quot;, &quot;all_wheel&quot;, &quot;rear_wheel&quot;, &quot;msrp&quot;, &quot;dealer_cost&quot;, &quot;eng_size&quot;, &quot;ncyl&quot;, &quot;horsepwr&quot;, &quot;city_mpg&quot;,&quot;hwy_mpg&quot;, &quot;weight&quot;, &quot;wheel_base&quot;, &quot;length&quot;, &quot;width&quot;) # Change data tpyes as needed cars[2:7] &lt;- sapply(cars[2:7],as.logical) cars[c(8:10,12:19)] &lt;- sapply(cars[c(8:10,12:19)],as.integer) ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion ## Warning in lapply(X = X, FUN = FUN, ...): NAs introduced by coercion # Learn data structure str(cars) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 428 obs. of 19 variables: ## $ name : chr &quot;Acura 3.5 RL 4dr&quot; &quot;Acura 3.5 RL w/Navigation 4dr&quot; &quot;Acura MDX&quot; &quot;Acura NSX coupe 2dr manual S&quot; ... ## $ sports_car : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ suv : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ wagon : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ rear_wheel : int 0 0 0 1 0 0 0 0 0 0 ... ## $ msrp : int 43755 46100 36945 89765 23820 33195 26990 25940 31840 42490 ... ## $ dealer_cost: int 39014 41100 33337 79978 21761 30299 24647 23508 28846 38325 ... ## $ eng_size : num 3.5 3.5 3.5 3.2 2 3.2 2.4 1.8 3 3 ... ## $ ncyl : int 6 6 6 6 4 6 4 4 6 6 ... ## $ horsepwr : int 225 225 265 290 200 270 200 170 220 220 ... ## $ city_mpg : int 18 18 17 17 24 20 22 22 20 20 ... ## $ hwy_mpg : int 24 24 23 24 31 28 29 31 28 27 ... ## $ weight : int 3880 3893 4451 3153 2778 3575 3230 3252 3462 3814 ... ## $ wheel_base : int 115 115 106 100 101 108 105 104 104 105 ... ## $ length : int 197 197 189 174 172 186 183 179 179 180 ... ## $ width : int 72 72 77 71 68 72 69 70 70 70 ... # Create faceted histogram ggplot(cars, aes(x = city_mpg)) + geom_histogram() + facet_wrap(~ suv) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). The mileage of a car tends to be associated with the size of its engine (as measured by the number of cylinders). To explore the relationship between these two variables, you could stick to using histograms, but in this exercise you“ll try your hand at two alternatives: the box plot and the density plot. # Filter cars with 4, 6, 8 cylinders common_cyl &lt;- filter(cars, ncyl %in% c(4,6,8)) # Create box plots of city mpg by ncyl ggplot(common_cyl, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 11 rows containing non-finite values (stat_boxplot). # Create overlaid density plots for same data ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) + geom_density(alpha = .3) ## Warning: Removed 11 rows containing non-finite values (stat_density). Now, turn your attention to a new variable: horsepwr. The goal is to get a sense of the marginal distribution of this variable and then compare it to the distribution of horsepower conditional on the price of the car being less than $25,000. You“ll be making two plots using the”data pipeline&quot; paradigm, where you start with the raw data and end with the plot. In addition to indicating the center and spread of a distribution, a box plot provides a graphical means to detect outliers. You can apply this method to the msrp column (manufacturer“s suggested retail price) to detect if there are unusually expensive or cheap cars. # Create hist cars %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + ggtitle(&quot;ALL Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Create hist of horsepwr for affordable cars cars %&gt;% filter(msrp &lt; 25000) %&gt;% ggplot(aes(horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ggtitle(&quot;Affordable Cars&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). ## Warning: Removed 1 rows containing missing values (geom_bar). # Construct box plot of msrp cars %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() # Exclude outliers from data cars_no_out &lt;- cars %&gt;% filter(msrp &lt; 100000) # Construct box plot of msrp using the reduced dataset cars_no_out %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() Consider two other columns in the cars dataset: city_mpg and width. Which is the most appropriate plot for displaying the important features of their distributions? Remember, both density plots and box plots display the central tendency and spread of the data, but the box plot is more robust to outliers. # Create plot of city_mpg cars %&gt;% ggplot(aes(x = width)) + geom_density() ## Warning: Removed 28 rows containing non-finite values (stat_density). # Create plot of width cars %&gt;% ggplot(aes(x = 1, y = city_mpg)) + geom_boxplot() ## Warning: Removed 14 rows containing non-finite values (stat_boxplot). Faceting is a valuable technique for looking at several conditional distributions at the same time. If the faceted distributions are laid out in a grid, you can consider the association between a variable and two others, one on the rows of the grid and the other on the columns. # Facet hists using hwy mileage and ncyl common_cyl %&gt;% ggplot(aes(x = hwy_mpg)) + geom_histogram() + facet_grid(ncyl ~ suv) + ggtitle(&quot;Faceted heavy mpg histograms by No. of Cyl and Suv&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 11 rows containing non-finite values (stat_bin). 5.3 Numerical Summaries Throughout this chapter, you will use data from gapminder, which tracks demographic data in countries of the world over time. To learn more about it, you can bring up the help file with ?gapminder. For this exercise, focus on how the life expectancy differs from continent to continent. This requires that you conduct your analysis not at the country level, but aggregated up to the continent level. This is made possible by the one-two punch of group_by() and summarize(), a very powerful syntax for carrying out the same analysis on different subsets of the full dataset. library(gapminder) # Create dataset of 2007 data gap2007 &lt;- filter(gapminder, year == 2007) # Compute groupwise mean and median lifeExp gap2007 %&gt;% group_by(continent) %&gt;% summarize(mean(lifeExp), median(lifeExp)) ## # A tibble: 5 x 3 ## continent `mean(lifeExp)` `median(lifeExp)` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 55 53 ## 2 Americas 74 73 ## 3 Asia 71 72 ## 4 Europe 78 79 ## 5 Oceania 81 81 # Generate box plots of lifeExp for each continent gap2007 %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() Let“s extend the powerful group_by() and summarize() syntax to measures of spread. If you”re unsure whether you“re working with symmetric or skewed distributions, it”s a good idea to consider a robust measure like IQR in addition to the usual measures of variance or standard deviation. # Compute groupwise measures of spread gap2007 %&gt;% group_by(continent) %&gt;% summarize(sd(lifeExp), IQR(lifeExp), n()) ## # A tibble: 5 x 4 ## continent `sd(lifeExp)` `IQR(lifeExp)` `n()` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 9.63 11.61 52 ## 2 Americas 4.44 4.63 25 ## 3 Asia 7.96 10.15 33 ## 4 Europe 2.98 4.78 30 ## 5 Oceania 0.73 0.52 2 # Generate overlaid density plots gap2007 %&gt;% ggplot(aes(x = lifeExp, fill = continent)) + geom_density(alpha = 0.3) # Compute stats for lifeExp in Americas gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% summarize(mean(lifeExp), sd(lifeExp)) ## # A tibble: 1 x 2 ## `mean(lifeExp)` `sd(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 74 4.4 # Compute stats for population gap2007 %&gt;% summarize(median(pop), IQR(pop)) ## # A tibble: 1 x 2 ## `median(pop)` `IQR(pop)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10517531 26702008 5.3.1 Transformations In some data, there are different ‘humps’ in the distribution, which are calls the modes or the modality of the dataset Unimode - single mean, this is a normal distribtion Bimodal - two common distributions Multimodal - three modes or more We also should consider whether the distribution is skewed. Right skewed data has a long tail to the right, with the majority of the distribution to the left - we often see this with income distributions. Left skewed has a small number of observations to the left and the majoirty of the distribution to the right. A normal distribution is typically smyterical. Highly skewed distributions can make it very difficult to learn anything from a visualization. Transformations can be helpful in revealing the more subtle structure. Here you’ll focus on the population variable, which exhibits strong right skew, and transform it with the natural logarithm function (log() in R). # Create density plot of old variable gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() # Transform the skewed pop variable gap2007 &lt;- gap2007 %&gt;% mutate(log_pop = log(pop)) # Create density plot of new variable gap2007 %&gt;% ggplot(aes(x = log_pop)) + geom_density() 5.3.2 Outliers It is often useful within a dataset to identify, using a column, whether the data is an outlier, this can be done by using the mutate function e.g. df &lt;- df %&gt;% mutate(is_outlier &gt; value), then filtering and arranging the resulting table e.g. df %&gt;% filter(is_outlier) %&gt;% arrange(desc(value)). We can also use this outlier column to remove the values from a plot e.g. df %&gt;% filter(!is_outlier) %&gt;% ggplot … The determination of the outlier value might be arbitary, or you could use a percentile value (say top or bottom 2%). # Filter for Asia, add column indicating outliers gap_asia &lt;- gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% mutate(is_outlier = lifeExp &lt;50) # Remove outliers, create box plot of lifeExp gap_asia %&gt;% filter(!is_outlier) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() 5.4 Email Case Study The example EDA comes from manually classified 3,900+ emails from the openintro package. Is there an association between spam and the length of an email? You could imagine a story either way: Spam is more likely to be a short message tempting me to click on a link, or *My normal email is likely shorter since I exchange brief emails with my friends all the time. Here, you’ll use the email dataset to settle that question. Begin by bringing up the help file and learning about all the variables with ?email. As you explore the association between spam and the length of an email, use this opportunity to try out linking a dplyr chain with the layers in a ggplot2 object. library(openintro) ## Please visit openintro.org for free statistics materials ## ## Attaching package: &#39;openintro&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## cars ## The following object is masked from &#39;package:datasets&#39;: ## ## cars # Compute summary statistics email %&gt;% group_by(spam) %&gt;% summarise(median(num_char), IQR(num_char)) ## # A tibble: 2 x 3 ## spam `median(num_char)` `IQR(num_char)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.8 13.6 ## 2 1 1.0 2.8 # Create plot email %&gt;% mutate(log_num_char = log(num_char)) %&gt;% ggplot(aes(x = factor(spam), y = log_num_char)) + geom_boxplot() Let’s look at a more obvious indicator of spam: exclamation marks. exclaim_mess contains the number of exclamation marks in each message. Using summary statistics and visualization, see if there is a relationship between this variable and whether or not a message is spam. Note: when computing the log(0) is -Inf in R, which isn’t a very useful value! You can get around this by adding a small number (like .01) to the quantity inside the log() function. This way, your value is never zero. This small shift to the right won’t affect your results. # Compute center and spread for exclaim_mess by spam email %&gt;% group_by(spam) %&gt;% summarise(mean(exclaim_mess), sd(exclaim_mess)) ## # A tibble: 2 x 3 ## spam `mean(exclaim_mess)` `sd(exclaim_mess)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.5 48 ## 2 1 7.3 80 # Create plot for spam and exclaim_mess email %&gt;% ggplot(aes(log(exclaim_mess)+0.1)) + geom_histogram() + facet_wrap( ~ spam) + ggtitle(&quot;Number of exclamation marks by not-spam vs spam&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1435 rows containing non-finite values (stat_bin). If it was difficult to work with the heavy skew of exclaim_mess, the number of images attached to each email (image) poses even more of a challenge. table(email$image) ## ## 0 1 2 3 4 5 9 20 ## 3811 76 17 11 2 2 1 1 Recall that this tabulates the number of cases in each category (so there were 3811 emails with 0 images, for example). Given the very low counts at the higher number of images, let’s collapse image into a categorical variable that indicates whether or not the email had at least one image. In this exercise, you’ll create this new variable and explore its association with spam. ** Here we deal with zero inflation** by converting the many zero values and the non zeros in to a categorical variable. There are other strategies, such as doing analysis on these two groups seperatley. # Create plot of proportion of spam by image email %&gt;% mutate(has_image = image &gt; 0) %&gt;% ggplot(aes(x = has_image, fill = factor(spam))) + geom_bar(position = &quot;fill&quot;) Sometimes it is neccessary to check if our understanding of the data and how it has been created is correct and if the values we expect are in fact true. In this instance, we check first if the number of charecters in the email is greater than zero (which it should be), then secondly whether images count as attachments using a boolean operator. If image is never greater than attach, we can infer that images are counted as attachments. # Verify that all emails have non-negative values for num_char sum(email$num_char &lt; 0) ## [1] 0 # Test if images count as attachments sum(email$images &gt;= email$attach) ## [1] 0 When you have a specific question about a dataset, you can find your way to an answer by carefully constructing the appropriate chain of R code. For example, consider the following question: “Within non-spam emails, is the typical length of emails shorter for those that were sent to multiple people?” This can be answered with the following chain: email %&gt;% filter(spam == &quot;not-spam&quot;) %&gt;% group_by(to_multiple) %&gt;% summarize(median(num_char)) ## # A tibble: 0 x 2 ## # ... with 2 variables: to_multiple &lt;dbl&gt;, median(num_char) &lt;lgl&gt; The code makes it clear that you are using num_char to measure the length of an email and median() as the measure of what is typical. If you run this code, you’ll learn that the answer to the question is “yes”: the typical length of non-spam sent to multiple people is a bit lower than those sent to only one person. This chain concluded with summary statistics, but others might end in a plot; it all depends on the question that you’re trying to answer. For emails containing the word “dollar”, does the typical spam email contain a greater number of occurrences of the word than the typical non-spam email? Create a summary statistic that answers this question. If you encounter an email with greater than 10 occurrences of the word “dollar”, is it more likely to be spam or not-spam? Create a barchart that answers this question. # Question 1 email %&gt;% filter(dollar &gt; 0) %&gt;% group_by(spam) %&gt;% summarize(median(dollar)) ## # A tibble: 2 x 2 ## spam `median(dollar)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 4 ## 2 1 2 # Question 2 email %&gt;% filter(dollar &gt; 10) %&gt;% ggplot(aes(x = spam)) + geom_bar() Turn your attention to the variable called number. To explore the association between this variable and spam, select and construct an informative plot. For illustrating relationships between categorical variables, you’ve seen # Reorder levels email$number &lt;- factor(email$number, levels = c(&quot;none&quot;, &quot;small&quot;, &quot;big&quot;)) # Construct plot of number ggplot(email, aes(x = number)) + geom_bar() + facet_wrap(~ spam) "],
["dimensional-modelling.html", "6 Dimensional Modelling 6.1 Introduction to Dimensional Data 6.2 Architecture considerations 6.3 Graphical Representations 6.4 Kimball Approach 6.5 Four-Step Dimensional Design Process 6.6 Tips", " 6 Dimensional Modelling 6.1 Introduction to Dimensional Data Dimensional modelling helps to build the ability for users to query the information, for instance analysing results by a geographic region. Multi-dimensional modelling is an extension to allowing multiple ways to analsye the information, by geographic region but also over time, by product or service, by store or office and so on. It provides a way for a system user, manager or analyst to navigate what information - the ‘information space’ - is available in a database or data warehouse, but at a more intuitive level (see Meridith 2017, lecture 4). The goal is to help understanding, exploration and to make better decisions. A dimension is simply a direction, usually query or analytically based, in which you can move. Dimensional modelling is different from Entity Relationship diagrams which are more typically used for database design, however they do share some similarities and are sometimes used for dimensional modelling particuarly by those from a database or IT background. The dimensions used therefore become the ways in which the end user wants to query the information. Typical terms used in the BI arena for helping to navigate this ‘information space’ include; ‘slice and dice’ meaning to make a large data space in to a smaller one (you are making a selection or subset of all the available data), ‘drill down’ meaning to go in to a lower level of a hierachy (moving from a geographic region to a particular store), ‘drill up’ meaning to go in to a higher level (sometimes called rolling-up) and ‘drill across’ meaning adding more data (or facts) about something, typically from another source (a different fact table). There are two slightly different interpretations of a dimensional model (Meridith 2017, lecture 4): OLAP: A dimension is a structural attribute of a data cube. A dimension acts as an index for identifying values in a multi-dimensional array Kimball: A dimension table are where the textual descriptions which relate to aspects of the business are stored In both instances however, they provide ways to interact and understand our information. There are two things we are typically trying to map: Facts: Data itself, values, sales and so on e.g. a sales transaction number and the products sold Dimensions: Different ways of presenting or quering the information, this is often in the form of attributes about the fact e.g. product specific and store details 6.1.1 Data Modelling levels There are three aspects of information with a Business Intelligence system - conceptual, logical and physical - which exist on a spectrum. Conceptual: The business needs are usually the high level conceptual solution, what things we want to include at a more abstract level Logical: We start thinking about what data to include in the model and what data is available, it starts giving something which can be implemented in to a warehouse Physical: The final solution which is usually then what is implemented in the data warehouse. It is the more technical/IT solution and may include normalisation (3NF or higher) and perhaps other database optimisations to improve performance of the system. In some instances, the conceptual and logical can become one and the same thing. Table 4.1: The three levels of data modelling Feature Conceptual Logical Logical Entity Names Y Y Entity Relationships Y Y Attributes Y Primary Keys Y Y Foreign Keys Y Y Table Names Y Column Names Y Column Data Types 6.2 Architecture considerations There are a number of different approaches to implementing a data warehouse, or Enterprise Data Warehouse (EDW) from the IT or technical perspective. However, all approaches use the dimensional data modelling technique. A full detailed explanation of all possible architecture approaches, including hybrid approaches, is not included here. Instead we discuss at a high level the three main approaches - Kimball Inmon and Data Valut - and touch on a couple of others. Where they differ in terms of data modelling in part depends on the location of the dimensional model. Kimball - as the last part of the Extract Transform and Load (ETL) process the data is structured and loaded in to the desired dimensional model(s). There is no EDW in the Kimball approach, instead the presentation area is where data is organized, stored, and made available for direct querying by users, report writers, and other analytical BI applications. Data is stored in the multi-dimensional views as different data marts, which are typically subsets of all the data originally extracted, perhaps for different business users or services Inmon - suggests that the data should be relationally designed. The data is stored in an EDW in third normal form (3NF). The dimensional model then transates the data from the EDW in to something for an end user, visualisation tool or other such BI tool, potentially including data marts. A Hub and Spoke system is often used to describe the approach, with the EDW being the hub and the spokes being the depdendent data marts. This helps to ensure a ‘single verison of the truth’ Data Vault - Centralised approach - similar to Inmon but without the dependent data marts (spokes). Users directly target the EDW and there may be many different dimensional data models Hybrid - there are various different ways this could be setup, however one way would be that data is still stored in the EDW, but the dimensional model is used to help structure the data in the EDW. Therefore the extra translation required from the EDW to a BI tool is reduced. In the Kimball approach when attributes in separate dimension tables have the same column names and domain contents. After validating the data for conformance with the defined one-to-one and many-to-one business rules [as part of the ETL processs], it may be pointless to take the final step of building a 3NF physical database, just before transforming the data once again into denormalized structures for the BI presentation area. (Kimball and Ross 2013, pg 20) For the kimball approach to work, so called ‘conformed dimensions’ must be developed which are said to conform when attributes in separate dimension tables have the same column names and domain contents (Kimball and Ross 2013, pg 51). Inmon sees that the dimensional modelling technique can cause problems when teams need different star schemas - dimensional models - which then lead to a need to combine the different joins together, or lead to issues of duplication and inconsistencies. simply doing dimensional modeling as a basis for data warehouse design leads down a dark path when multiple star joins are considered. It is never apparent that there is a problem with star joins when you are looking at just one star join. But when you look at multiple star joins, the limitations of dimensional modeling become apparent. (Inmon 2000) Inmon concludes that dimensional modelling is only really suitable for data marts (ibid). 6.3 Graphical Representations Figure 6.1: High Level Overview of a Data Warehouse (Schnider, Martino, and Eschermann 2014, pg 3) Figure 6.2: Star schema versus OLAP cube (Kimball and Ross 2013, pg 9) Figure 6.3: Star schema example (Kimball and Ross 2013, pg 16) Figure 6.4: Star and Snowflake Schemas (Sharda, Delan, and Turban 2014, pg 139) Figure 6.5: Example slices from a OLAP data cube (Sharda, Delan, and Turban 2014, pg 141) Figure 6.6: Star schema reporting (Kimball and Ross 2013, pg 17) 6.4 Kimball Approach Before work begins of the data modelling, it is neccessary to understand the needs of the business and the underlying data (Kimball and Ross 2013, pg 37). The business needs arise out of meetings with manangers, decision makers and other representatives of the business. Kimball also recommends meetings with ‘source system experts and doing high-level data profiling to assess data feasibilities’ (Kimball and Ross 2013, pg 38). Whilst the data modeller is ‘in charge’ the actual model should unfold via a series of interactive workshops with those business representatives. Data governance reps should also be involved to obtain buy-in. In this sense, the Kimball approach covers both the conceptual and physical, it may also include some considerations of physical level at initiation. 6.5 Four-Step Dimensional Design Process Kimball outlines four key decisions that are to be made during the design of a dimensional model include: Select the business process - the operational activities done by the business, these activities create the facts Declare the grain - what a single row represents. The atomic grain is the lowest data captured by the business, which is the ideal and can be aggregared (rolled-up) to other levels. Different grains must not be mixed in the same fact table Identify the dimensions - the descriptive attributes about the facts, to be used for analysis. Provide the “who, what, where, when, why, and how” (6W) context Identify the facts - the measurements (how many) from the business process, it should relate to a physical observable event, rather than reporting needs Typically the output of this process is a star schema, with a fact table at the centre supported by the associated dimension tables, with primary/forenigh key relationships. This is often then structured into a online analytical processing (OLAP) cube, which contains the facts and dimensions appropriate to the analysis, but allows for more detailed analytical capabilities than SQL. Sometimes aggregated fact tables are built to speed up query performance, as are aggregated OLAP cubes which are typically designed for users. A key advantage of the dimensional model approach is that new dimensions can be added to an existing fact table by adding a new foreign key column. Discussion then of * Thomsen diagrams OLAP Solutions (2nd ed) 2002, an abstract, but can be a little simple * ADAPT Diagrams, White Paper Bulos and Foresman - included in some Microsoft products such as Visio and SQL Server, a bit too technical but good for communicating to IT * BEAM/Agile approach 8 mins 6.6 Tips Think about the types of analysis or questions that the user or manager may want to ask. This will help structure the data and help to ensure nothing is missing At the same time, just because something exists in the organisation or in a data source does not mean it has to be included. You need to think about that to include and what to exclude Equally, there may be instances where there is a desire to add something in to the model but it does not currently exist. This should be flagged and discussed with those intending to use the BI tool / output What are the end uses of the system or systems? If there are potentially multiple systems, multiple teams and multiple views on the data, it may make sense to store the data in its original state (3NF) in the EDW or similar store, then do the dimensional mapping in the BI tool, so it can be customised to the audience (Meridith 2017, lecture 4). This can lead to some duplication, however an option might be to share the dimensional models in some central repository, allowing users to customise for their use, whilst still being able to share the same source data and the benefits this brings. Evidently this lends itself to an Inmon or other such approach and less so the Kimball approach References "],
["references-3.html", "References", " References "]
]
