<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Study notes</title>
  <meta name="description" content="Study notes taken from courses and self learning.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Study notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/James-SR/study-notes-master" />
  
  <meta property="og:description" content="Study notes taken from courses and self learning." />
  <meta name="github-repo" content="James-SR/study-notes-master" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Study notes" />
  
  <meta name="twitter:description" content="Study notes taken from courses and self learning." />
  

<meta name="author" content="James Solomon-Rounce">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="exploratory-data-analysis.html">
<link rel="next" href="supervised-learning.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Study Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html"><i class="fa fa-check"></i><b>1</b> Importing data - Part 1</a><ul>
<li class="chapter" data-level="1.1" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#reading-csv-files"><i class="fa fa-check"></i><b>1.2</b> Reading CSV files</a></li>
<li class="chapter" data-level="1.3" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#reading-tab-deliminated-files-or-other-table-formats"><i class="fa fa-check"></i><b>1.3</b> Reading tab deliminated files or other table formats</a></li>
<li class="chapter" data-level="1.4" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#readr-and-data.table"><i class="fa fa-check"></i><b>1.4</b> Readr and data.table</a><ul>
<li class="chapter" data-level="1.4.1" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#data.table-fread"><i class="fa fa-check"></i><b>1.4.1</b> data.table fread</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#reading-excel-files"><i class="fa fa-check"></i><b>1.5</b> Reading Excel files</a><ul>
<li class="chapter" data-level="1.5.1" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#alternatives-for-importing-excel-files"><i class="fa fa-check"></i><b>1.5.1</b> Alternatives for importing Excel files</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="importing-data-part-1.html"><a href="importing-data-part-1.html#xlconnect---read-and-write-to-excel"><i class="fa fa-check"></i><b>1.6</b> XLConnect - read and write to excel</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="importing-data-part-2.html"><a href="importing-data-part-2.html"><i class="fa fa-check"></i><b>2</b> Importing data - Part 2</a><ul>
<li class="chapter" data-level="2.1" data-path="importing-data-part-2.html"><a href="importing-data-part-2.html#importing-from-databases---1"><i class="fa fa-check"></i><b>2.1</b> Importing from Databases - 1</a></li>
<li class="chapter" data-level="2.2" data-path="importing-data-part-2.html"><a href="importing-data-part-2.html#sql-queries-inside-r"><i class="fa fa-check"></i><b>2.2</b> SQL Queries Inside R</a></li>
<li class="chapter" data-level="2.3" data-path="importing-data-part-2.html"><a href="importing-data-part-2.html#web-data"><i class="fa fa-check"></i><b>2.3</b> Web Data</a></li>
<li class="chapter" data-level="2.4" data-path="importing-data-part-2.html"><a href="importing-data-part-2.html#json-and-apis"><i class="fa fa-check"></i><b>2.4</b> JSON and APIs</a></li>
<li class="chapter" data-level="2.5" data-path="importing-data-part-2.html"><a href="importing-data-part-2.html#importing-from-other-statistical-software"><i class="fa fa-check"></i><b>2.5</b> Importing from other statistical software</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="3" data-path="cleaning-data.html"><a href="cleaning-data.html"><i class="fa fa-check"></i><b>3</b> Cleaning Data</a><ul>
<li class="chapter" data-level="3.1" data-path="cleaning-data.html"><a href="cleaning-data.html#tidying-data"><i class="fa fa-check"></i><b>3.1</b> Tidying data</a></li>
<li class="chapter" data-level="3.2" data-path="cleaning-data.html"><a href="cleaning-data.html#preparing-data-for-analysis"><i class="fa fa-check"></i><b>3.2</b> Preparing data for analysis</a></li>
<li class="chapter" data-level="3.3" data-path="cleaning-data.html"><a href="cleaning-data.html#string-manipulation"><i class="fa fa-check"></i><b>3.3</b> String manipulation</a></li>
<li class="chapter" data-level="3.4" data-path="cleaning-data.html"><a href="cleaning-data.html#missing-specials-and-outliers"><i class="fa fa-check"></i><b>3.4</b> Missing, Specials and Outliers</a></li>
<li class="chapter" data-level="3.5" data-path="cleaning-data.html"><a href="cleaning-data.html#examples"><i class="fa fa-check"></i><b>3.5</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html"><i class="fa fa-check"></i><b>4</b> Importing &amp; Cleaning Data in R: Case Studies</a><ul>
<li class="chapter" data-level="4.1" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html#ticket-sales-data"><i class="fa fa-check"></i><b>4.1</b> Ticket Sales Data</a><ul>
<li class="chapter" data-level="4.1.1" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html#removing-redundant-info"><i class="fa fa-check"></i><b>4.1.1</b> Removing redundant info</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html#working-with-dates"><i class="fa fa-check"></i><b>4.2</b> Working with dates</a></li>
<li class="chapter" data-level="4.3" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html#mbta-ridership-data"><i class="fa fa-check"></i><b>4.3</b> MBTA Ridership Data</a></li>
<li class="chapter" data-level="4.4" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html#world-food-facts"><i class="fa fa-check"></i><b>4.4</b> World Food Facts</a></li>
<li class="chapter" data-level="4.5" data-path="importing-cleaning-data-in-r-case-studies.html"><a href="importing-cleaning-data-in-r-case-studies.html#school-attendance-data"><i class="fa fa-check"></i><b>4.5</b> School Attendance Data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-data.html"><a href="introduction-to-data.html"><i class="fa fa-check"></i><b>5</b> Introduction to Data</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-data.html"><a href="introduction-to-data.html#language-of-data"><i class="fa fa-check"></i><b>5.1</b> Language of Data</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-data.html"><a href="introduction-to-data.html#observational-studies-and-experiments"><i class="fa fa-check"></i><b>5.2</b> Observational Studies and Experiments</a></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-data.html"><a href="introduction-to-data.html#sampling-strategies-and-experimental-design"><i class="fa fa-check"></i><b>5.3</b> Sampling strategies and experimental design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="6" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html"><i class="fa fa-check"></i><b>6</b> Foundations of Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#introduction-to-inference"><i class="fa fa-check"></i><b>6.1</b> Introduction to Inference</a></li>
<li class="chapter" data-level="6.2" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#home-ownership-by-gender"><i class="fa fa-check"></i><b>6.2</b> Home Ownership by Gender</a></li>
<li class="chapter" data-level="6.3" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#density-plots"><i class="fa fa-check"></i><b>6.3</b> Density Plots</a></li>
<li class="chapter" data-level="6.4" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#gender-discrimination-p-values"><i class="fa fa-check"></i><b>6.4</b> Gender Discrimination (p-values)</a></li>
<li class="chapter" data-level="6.5" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#opportunity-cost"><i class="fa fa-check"></i><b>6.5</b> Opportunity Cost</a></li>
<li class="chapter" data-level="6.6" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>6.6</b> Type I and Type II errors</a></li>
<li class="chapter" data-level="6.7" data-path="foundations-of-inference.html"><a href="foundations-of-inference.html#bootstrapping"><i class="fa fa-check"></i><b>6.7</b> Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#categorical-data"><i class="fa fa-check"></i><b>7.1</b> Categorical Data</a></li>
<li class="chapter" data-level="7.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#numerical-data"><i class="fa fa-check"></i><b>7.2</b> Numerical Data</a></li>
<li class="chapter" data-level="7.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#numerical-summaries"><i class="fa fa-check"></i><b>7.3</b> Numerical Summaries</a><ul>
<li class="chapter" data-level="7.3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#transformations"><i class="fa fa-check"></i><b>7.3.1</b> Transformations</a></li>
<li class="chapter" data-level="7.3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#outliers"><i class="fa fa-check"></i><b>7.3.2</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#email-case-study"><i class="fa fa-check"></i><b>7.4</b> Email Case Study</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html"><i class="fa fa-check"></i><b>8</b> Correlation and Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#visualizing-two-variables"><i class="fa fa-check"></i><b>8.1</b> Visualizing two variables</a><ul>
<li class="chapter" data-level="8.1.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#transformations-1"><i class="fa fa-check"></i><b>8.1.1</b> Transformations</a></li>
<li class="chapter" data-level="8.1.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#identifying-outliers"><i class="fa fa-check"></i><b>8.1.2</b> Identifying Outliers</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#correlation"><i class="fa fa-check"></i><b>8.2</b> Correlation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#anscombe-dataset"><i class="fa fa-check"></i><b>8.2.1</b> Anscombe Dataset</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#linear-regression"><i class="fa fa-check"></i><b>8.3</b> Linear Regression</a><ul>
<li class="chapter" data-level="8.3.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#regression-to-the-mean"><i class="fa fa-check"></i><b>8.3.1</b> Regression to the Mean</a></li>
<li class="chapter" data-level="8.3.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#fitting-linear-models"><i class="fa fa-check"></i><b>8.3.2</b> Fitting linear models</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#model-fit"><i class="fa fa-check"></i><b>8.4</b> Model fit</a><ul>
<li class="chapter" data-level="8.4.1" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#unusual-points"><i class="fa fa-check"></i><b>8.4.1</b> Unusual points</a></li>
<li class="chapter" data-level="8.4.2" data-path="correlation-and-regression.html"><a href="correlation-and-regression.html#high-leverage-points"><i class="fa fa-check"></i><b>8.4.2</b> High leverage Points</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>9</b> Supervised Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="supervised-learning.html"><a href="supervised-learning.html#tree-based-models"><i class="fa fa-check"></i><b>9.1</b> Tree Based Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests"><i class="fa fa-check"></i><b>9.1.1</b> Random Forests</a></li>
<li class="chapter" data-level="9.1.2" data-path="supervised-learning.html"><a href="supervised-learning.html#one-hot-encoding-categorical-variables"><i class="fa fa-check"></i><b>9.1.2</b> One-Hot-Encoding Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="supervised-learning.html"><a href="supervised-learning.html#gradient-boosting-machines"><i class="fa fa-check"></i><b>9.2</b> Gradient Boosting Machines</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html"><i class="fa fa-check"></i><b>10</b> Dimensional Modelling</a><ul>
<li class="chapter" data-level="10.1" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#introduction-to-dimensional-data"><i class="fa fa-check"></i><b>10.1</b> Introduction to Dimensional Data</a><ul>
<li class="chapter" data-level="10.1.1" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#data-modelling-levels"><i class="fa fa-check"></i><b>10.1.1</b> Data Modelling levels</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#architecture-considerations"><i class="fa fa-check"></i><b>10.2</b> Architecture considerations</a></li>
<li class="chapter" data-level="10.3" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#graphical-representations"><i class="fa fa-check"></i><b>10.3</b> Graphical Representations</a></li>
<li class="chapter" data-level="10.4" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#kimball-approach"><i class="fa fa-check"></i><b>10.4</b> Kimball Approach</a></li>
<li class="chapter" data-level="10.5" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#four-step-dimensional-design-process"><i class="fa fa-check"></i><b>10.5</b> Four-Step Dimensional Design Process</a></li>
<li class="chapter" data-level="10.6" data-path="dimensional-modelling.html"><a href="dimensional-modelling.html#tips"><i class="fa fa-check"></i><b>10.6</b> Tips</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-3.html"><a href="references-3.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Study notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="correlation-and-regression" class="section level1">
<h1><span class="header-section-number">8</span> Correlation and Regression</h1>
<hr />
<p>Notes taken during/inspired by the Datacamp course ‘Correlation and Regression’ by Ben Baumer.</p>
<div id="visualizing-two-variables" class="section level2">
<h2><span class="header-section-number">8.1</span> Visualizing two variables</h2>
<p>Some common terminology of data includes</p>
<ul>
<li>Response variable a.k.a. y, dependent (usually on the vertical axis if using a scatter plot)</li>
<li>Explanatory variable, something you think might be related to the response a.k.a. x, independent, predictor (usually on the horizontal axis)</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(openintro)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)

<span class="co"># load the data</span>
<span class="kw">data</span>(ncbirths)

<span class="co"># Scatterplot of weight vs. weeks</span>
<span class="kw">ggplot</span>(ncbirths, <span class="kw">aes</span>(weeks, weight)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_point).</code></pre>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>If it is helpful, you can think of boxplots as scatterplots for which the variable on the x-axis has been discretized.</p>
<p>The cut() function takes two arguments: the continuous variable you want to discretize and the number of breaks that you want to make in that continuous variable in order to discretize it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Boxplot of weight vs. weeks</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> ncbirths, 
       <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">cut</span>(weeks, <span class="dt">breaks =</span> <span class="dv">5</span>), <span class="dt">y =</span> weight)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>()</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div id="transformations-1" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Transformations</h3>
<p>Here the relationship is hard to see.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(mammals)
<span class="co"># Mammals scatterplot</span>
<span class="kw">ggplot</span>(mammals, <span class="kw">aes</span>(BodyWt, BrainWt)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The relationship between two variables may not be linear. In these cases we can sometimes see strange and even inscrutable patterns in a scatterplot of the data. Sometimes there really is no meaningful relationship between the two variables. Other times, a careful transformation of one or both of the variables can reveal a clear relationship.</p>
<p>ggplot2 provides several different mechanisms for viewing transformed relationships. The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Scatterplot with coord_trans()</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> mammals, <span class="kw">aes</span>(<span class="dt">x =</span> BodyWt, <span class="dt">y =</span> BrainWt)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">coord_trans</span>(<span class="dt">x =</span> <span class="st">&quot;log10&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;log10&quot;</span>)</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Scatterplot with scale_x_log10() and scale_y_log10()</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> mammals, <span class="kw">aes</span>(<span class="dt">x =</span> BodyWt, <span class="dt">y =</span> BrainWt)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">scale_x_log10</span>() +<span class="st"> </span><span class="kw">scale_y_log10</span>()</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
</div>
<div id="identifying-outliers" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Identifying Outliers</h3>
<p>It is clear here, using the Using the mlbBat10 dataset, a scatterplot illustrates how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;mlbBat10&quot;</span>)
<span class="co"># Baseball player scatterplot</span>
<span class="kw">ggplot</span>(mlbBat10, <span class="kw">aes</span>(OBP, SLG)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Most of the points are clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty is caused by a few outlying players whose on-base percentages (OBPs) were exceptionally high. These values are present in our dataset only because these players had very few batting opportunities.</p>
<p>Both OBP and SLG are known as rate statistics, since they measure the frequency of certain events (as opposed to their count). In order to compare these rates sensibly, it makes sense to include only players with a reasonable number of opportunities, so that these observed rates have the chance to approach their long-run frequencies.</p>
<p>In Major League Baseball, batters qualify for the batting title only if they have 3.1 plate appearances per game. This translates into roughly 502 plate appearances in a 162-game season. The mlbBat10 dataset does not include plate appearances as a variable, but we can use at-bats (AB) – which constitute a subset of plate appearances – as a proxy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Scatterplot of SLG vs. OBP</span>
mlbBat10 %&gt;%
<span class="st">  </span><span class="kw">filter</span>(AB &gt;=<span class="st"> </span><span class="dv">200</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> OBP, <span class="dt">y =</span> SLG)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Identify the outlying player</span>
mlbBat10 %&gt;%
<span class="st">  </span><span class="kw">filter</span>(AB &gt;=<span class="st"> </span><span class="dv">200</span>, OBP &lt;<span class="st"> </span><span class="fl">0.2</span>)</code></pre></div>
<pre><code>##     name team position  G  AB  R  H 2B 3B HR RBI TB BB SO SB CS   OBP
## 1 B Wood  LAA       3B 81 226 20 33  2  0  4  14 47  6 71  1  0 0.174
##     SLG   AVG
## 1 0.208 0.146</code></pre>
</div>
</div>
<div id="correlation" class="section level2">
<h2><span class="header-section-number">8.2</span> Correlation</h2>
<p>We typically calculate the Pearsons aka Pearson product-moment correlation. The cor(x, y) function will compute the Pearson product-moment correlation between variables, x and y. Since this quantity is symmetric with respect to x and y, it doesn’t matter in which order you put the variables.</p>
<p>At the same time, the cor() function is very conservative when it encounters missing data (e.g. NAs). The use argument allows you to override the default behavior of returning NA whenever any of the values encountered is NA. Setting the use argument to “pairwise.complete.obs” allows cor() to compute the correlation coefficient for those observations where the values of x and y are both not missing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(ncbirths)
<span class="co"># Compute correlation between the birthweight and mother&#39;s age</span>
ncbirths %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">N =</span> <span class="kw">n</span>(), <span class="dt">r =</span> <span class="kw">cor</span>(mage, weight))</code></pre></div>
<pre><code>##      N          r
## 1 1000 0.05506589</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute correlation for all non-missing pairs</span>
ncbirths %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">N =</span> <span class="kw">n</span>(), <span class="dt">r =</span> <span class="kw">cor</span>(weight, weeks, <span class="dt">use =</span> <span class="st">&quot;pairwise.complete.obs&quot;</span>))</code></pre></div>
<pre><code>##      N         r
## 1 1000 0.6701013</code></pre>
<div id="anscombe-dataset" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Anscombe Dataset</h3>
<p>In 1973, Francis Anscombe famously created four synthetic datasets with remarkably similar numerical properties, but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable, set, that distinguishes the quartet.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;anscombe&quot;</span>)

<span class="co"># Tidy the data for plotting</span>
Anscombe &lt;-<span class="st"> </span>anscombe %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">id =</span> <span class="kw">seq_len</span>(<span class="kw">n</span>())) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(key, value, -id) %&gt;%
<span class="st">  </span><span class="kw">separate</span>(key, <span class="kw">c</span>(<span class="st">&quot;variable&quot;</span>, <span class="st">&quot;set&quot;</span>), <span class="dv">1</span>, <span class="dt">convert =</span> <span class="ot">TRUE</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">set =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;3&quot;</span>, <span class="st">&quot;4&quot;</span>)[set]) %&gt;%
<span class="st">  </span><span class="kw">spread</span>(variable, value)

<span class="co"># Plot the four variants</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> Anscombe, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>set)</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute statistics for the sets</span>
Anscombe %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(set) %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">N =</span> <span class="kw">n</span>(), <span class="kw">mean</span>(x), <span class="kw">sd</span>(x), <span class="kw">mean</span>(y), <span class="kw">sd</span>(y), <span class="kw">cor</span>(x,y))</code></pre></div>
<pre><code>## # A tibble: 4 x 7
##     set     N `mean(x)`  `sd(x)` `mean(y)`  `sd(y)` `cor(x, y)`
##   &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
## 1     1    11         9 3.316625  7.500909 2.031568   0.8164205
## 2     2    11         9 3.316625  7.500909 2.031657   0.8162365
## 3     3    11         9 3.316625  7.500000 2.030424   0.8162867
## 4     4    11         9 3.316625  7.500909 2.030579   0.8165214</code></pre>
</div>
</div>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">8.3</span> Linear Regression</h2>
<p>The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a “best fit” line that cuts through the data in a way that minimizes the distance between the line and the data points.</p>
<p>We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we’ll set this argument to the value “lm”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(bdims)

<span class="co"># Scatterplot with regression line</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> bdims, <span class="kw">aes</span>(<span class="dt">x =</span> hgt, <span class="dt">y =</span> wgt)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Sometimes it is better to think of the model error as ‘noise’ which we might try to better incorporate by creating a better model.</p>
<p>Two facts enable you to compute the slope b1 and intercept b0 of a simple linear regression model from some basic summary statistics.</p>
<p>First, the slope can be defined as:</p>
<p>b1=rX,Y⋅sYsX</p>
<p>where rX,Y represents the correlation (cor()) of X and Y and sX and sY represent the standard deviation (sd()) of X and Y, respectively.</p>
<p>Second, the point (x¯,y¯)is always on the least squares regression line, where x¯and y¯denote the average of x and y, respectively.</p>
<p>The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (Y) as a function of height (X).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1507</span>)
r &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.7173011</span>)
mean_hgt &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">171.1438</span>)
sd_hgt &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">9.407205</span>)
mean_wgt &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">69.14753</span>)
sd_wgt &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">13.34576</span>)
bdims_summary &lt;-<span class="st"> </span><span class="kw">data.frame</span>(N, r, mean_hgt, sd_hgt, mean_wgt, sd_wgt)

<span class="co"># Print bdims_summary</span>
bdims_summary</code></pre></div>
<pre><code>##      N         r mean_hgt   sd_hgt mean_wgt   sd_wgt
## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Add slope and intercept</span>
bdims_summary %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">slope =</span> r *<span class="st"> </span>sd_wgt /<span class="st"> </span>sd_hgt, 
         <span class="dt">intercept =</span> mean_wgt -<span class="st"> </span>slope *<span class="st"> </span>mean_hgt)</code></pre></div>
<pre><code>##      N         r mean_hgt   sd_hgt mean_wgt   sd_wgt    slope intercept
## 1 1507 0.7173011 171.1438 9.407205 69.14753 13.34576 1.017617 -105.0112</code></pre>
<div id="regression-to-the-mean" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Regression to the Mean</h3>
<p>Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While “regression to the mean” and “linear regression” are not the same thing, we will examine them together in this exercise.</p>
<p>One way to see the effects of regression to the mean is to compare the heights of parents to their children’s heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Galton data from http://www.math.uah.edu/stat/data/Galton.html</span>
Galton &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;Galton.csv&quot;</span>)

<span class="co"># Height of children vs. height of father</span>
Galton %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Gender ==<span class="st"> &quot;M&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Father, <span class="dt">y =</span> Height)) +
<span class="st">    </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Height of children vs. height of mother</span>
Galton %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Gender ==<span class="st"> &quot;F&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Mother, <span class="dt">y =</span> Height)) +
<span class="st">    </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="fitting-linear-models" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Fitting linear models</h3>
<p>While the geom_smooth(method = “lm”) function is useful for drawing linear models on a scatterplot, it doesn’t actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is lm(). This function generally takes two arguments:</p>
<p>A formula that specifies the model A data argument for the data frame that contains the data you want to use to fit the model The lm() function return a model object having class “lm”. This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Linear model for weight as a function of height</span>
<span class="kw">lm</span>(wgt ~<span class="st"> </span>hgt, <span class="dt">data =</span> bdims)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wgt ~ hgt, data = bdims)
## 
## Coefficients:
## (Intercept)          hgt  
##    -105.011        1.018</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Linear model for SLG as a function of OBP</span>
<span class="kw">lm</span>(SLG ~<span class="st"> </span>OBP, <span class="dt">data =</span> mlbBat10)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = SLG ~ OBP, data = mlbBat10)
## 
## Coefficients:
## (Intercept)          OBP  
##    0.009407     1.110323</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Log-linear model for body weight as a function of brain weight</span>
<span class="kw">lm</span>(<span class="kw">log</span>(BodyWt) ~<span class="st"> </span><span class="kw">log</span>(BrainWt), <span class="dt">data =</span> mammals)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(BodyWt) ~ log(BrainWt), data = mammals)
## 
## Coefficients:
##  (Intercept)  log(BrainWt)  
##       -2.509         1.225</code></pre>
<p>An “lm” object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.</p>
<p>The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2R2, adjusted R2R2, and the residual standard error. The summary of an “lm” object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.).</p>
<p>Once you have fit a regression model, you are often interested in the fitted values (y^i) and the residuals (ei), where i indexes the observations.</p>
<p>The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable.</p>
<p>In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the fitted.values() and residuals() functions</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod &lt;-<span class="st"> </span><span class="kw">lm</span>(wgt ~<span class="st"> </span>hgt, <span class="dt">data =</span> bdims)

<span class="co"># Show the coefficients</span>
<span class="kw">coef</span>(mod)</code></pre></div>
<pre><code>## (Intercept)         hgt 
## -105.011254    1.017617</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Show the full output</span>
<span class="kw">summary</span>(mod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wgt ~ hgt, data = bdims)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -18.743  -6.402  -1.231   5.059  41.103 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -105.01125    7.53941  -13.93   &lt;2e-16 ***
## hgt            1.01762    0.04399   23.14   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.308 on 505 degrees of freedom
## Multiple R-squared:  0.5145, Adjusted R-squared:  0.5136 
## F-statistic: 535.2 on 1 and 505 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mean of weights equal to mean of fitted values?</span>
<span class="kw">mean</span>(bdims$wgt) ==<span class="st"> </span><span class="kw">mean</span>(<span class="kw">fitted.values</span>(mod))</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mean of the residuals</span>
<span class="kw">mean</span>(<span class="kw">residuals</span>(mod))</code></pre></div>
<pre><code>## [1] -1.266971e-15</code></pre>
<p>As you fit a regression model, there are some quantities (e.g. R2) that apply to the model as a whole, while others apply to each observation (e.g. y^i). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.</p>
<p>The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load broom</span>
<span class="kw">library</span>(broom)

<span class="co"># Create bdims_tidy</span>
bdims_tidy &lt;-<span class="st"> </span><span class="kw">augment</span>(mod)</code></pre></div>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Glimpse the resulting data frame</span>
<span class="kw">glimpse</span>(bdims_tidy)</code></pre></div>
<pre><code>## Observations: 507
## Variables: 9
## $ wgt        &lt;dbl&gt; 65.6, 71.8, 80.7, 72.6, 78.8, 74.8, 86.4, 78.4, 62....
## $ hgt        &lt;dbl&gt; 174.0, 175.3, 193.5, 186.5, 187.2, 181.5, 184.0, 18...
## $ .fitted    &lt;dbl&gt; 72.05406, 73.37697, 91.89759, 84.77427, 85.48661, 7...
## $ .se.fit    &lt;dbl&gt; 0.4320546, 0.4520060, 1.0667332, 0.7919264, 0.81834...
## $ .resid     &lt;dbl&gt; -6.4540648, -1.5769666, -11.1975919, -12.1742745, -...
## $ .hat       &lt;dbl&gt; 0.002154570, 0.002358152, 0.013133942, 0.007238576,...
## $ .sigma     &lt;dbl&gt; 9.312824, 9.317005, 9.303732, 9.301360, 9.312471, 9...
## $ .cooksd    &lt;dbl&gt; 5.201807e-04, 3.400330e-05, 9.758463e-03, 6.282074e...
## $ .std.resid &lt;dbl&gt; -0.69413418, -0.16961994, -1.21098084, -1.31269063,...</code></pre>
</div>
</div>
<div id="model-fit" class="section level2">
<h2><span class="header-section-number">8.4</span> Model fit</h2>
<p>One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are.</p>
<p>However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or root mean squared error (RMSERMSE). R calls this quantity the residual standard error.</p>
<p>To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># View summary of model</span>
<span class="kw">summary</span>(mod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wgt ~ hgt, data = bdims)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -18.743  -6.402  -1.231   5.059  41.103 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -105.01125    7.53941  -13.93   &lt;2e-16 ***
## hgt            1.01762    0.04399   23.14   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.308 on 505 degrees of freedom
## Multiple R-squared:  0.5145, Adjusted R-squared:  0.5136 
## F-statistic: 535.2 on 1 and 505 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute the mean of the residuals</span>
<span class="kw">mean</span>(<span class="kw">residuals</span>(mod))</code></pre></div>
<pre><code>## [1] -1.266971e-15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute RMSE</span>
<span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="kw">residuals</span>(mod)^<span class="dv">2</span>) /<span class="st"> </span><span class="kw">df.residual</span>(mod))</code></pre></div>
<pre><code>## [1] 9.30804</code></pre>
<p>Another measure we can use is R squared, whihc is the he coefficient of determination. This gives us the interpretation of R2 as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model. In the example above, our model has an r-squared value of 51.5%. We can also calculate the R-squared value manually if desired.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute R-squared</span>
bdims_tidy %&gt;%
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">var_y =</span> <span class="kw">var</span>(wgt), <span class="dt">var_e =</span> <span class="kw">var</span>(.resid)) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">R_squared =</span> <span class="dv">1</span> -<span class="st"> </span>var_e /<span class="st"> </span>var_y)</code></pre></div>
<pre><code>##      var_y    var_e R_squared
## 1 178.1094 86.46839 0.5145208</code></pre>
<div id="unusual-points" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Unusual points</h3>
<p>As the model tries to fit the data on average, some extreme values can overly influence the model. We can quantify how much influence a particular point has by using the leverage, which is a measure for each observation as a function of the value of the explanatory variable and the mean of the explanatory variable. Therefore points to the centre line have a low leverage score, whilst points far from the line have a higher leverage. The explanatory variable y does not come in to effect. This can be calculated as the .hat value using augment() from broom.</p>
<p>It is possible to have a value with a high leverage but a low overall impact on the model, if the point lies close to the line of the model. In this case, the residual is small for the point. Conversely, a point with a high leverage score and a high residual - a point laying a distance a way from other meaures and not predicted well by the model - does have an impact.We say such a point is influential. Numerically we can use cooks distance (.cooksd)to quantify this influence, which can also be calculated using the augment() function from broom.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Rank points of high leverage</span>
mod %&gt;%
<span class="st">  </span><span class="kw">augment</span>() %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(.hat)) %&gt;%
<span class="st">  </span><span class="kw">head</span>()</code></pre></div>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>##    wgt   hgt  .fitted  .se.fit     .resid       .hat   .sigma      .cooksd
## 1 85.5 198.1 96.57863 1.255712 -11.078629 0.01819968 9.303950 0.0133734319
## 2 90.9 197.1 95.56101 1.214264  -4.661012 0.01701803 9.314916 0.0022081690
## 3 49.8 147.2 44.78194 1.131432   5.018065 0.01477545 9.314548 0.0022120570
## 4 80.7 193.5 91.89759 1.066733 -11.197592 0.01313394 9.303732 0.0097584634
## 5 95.9 193.0 91.38878 1.046493   4.511216 0.01264027 9.315075 0.0015228117
## 6 44.8 149.5 47.12245 1.037916  -2.322454 0.01243391 9.316688 0.0003968468
##   .std.resid
## 1 -1.2012024
## 2 -0.5050673
## 3  0.5431383
## 4 -1.2109808
## 5  0.4877505
## 6 -0.2510763</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Rank influential points</span>
mod %&gt;%
<span class="st">  </span><span class="kw">augment</span>() %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(.cooksd)) %&gt;%
<span class="st">  </span><span class="kw">head</span>()</code></pre></div>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>##     wgt   hgt  .fitted   .se.fit   .resid        .hat   .sigma    .cooksd
## 1  73.2 151.1 48.75064 0.9737632 24.44936 0.010944356 9.252694 0.03859555
## 2 116.4 177.8 75.92101 0.5065670 40.47899 0.002961811 9.140611 0.02817388
## 3 104.1 165.1 62.99728 0.4914889 41.10272 0.002788117 9.135102 0.02733574
## 4 108.6 190.5 88.84474 0.9464667 19.75526 0.010339372 9.275186 0.02377609
## 5  67.3 152.4 50.07354 0.9223084 17.22646 0.009818289 9.285305 0.01714950
## 6  76.8 157.5 55.26339 0.7287405 21.53661 0.006129560 9.267446 0.01661032
##   .std.resid
## 1   2.641185
## 2   4.355274
## 3   4.421999
## 4   2.133444
## 5   1.859860
## 6   2.320888</code></pre>
<p>When you have such outlying variables, you need to decide what to do. The main thing is to remove the variables from the model, but you need to consider the implications. There are other statistical techniques (see the EDA Chapter) for removing outliers. Think about whether the scope of the inference changes if you remove those values.</p>
<p>Observations can be outliers for a number of different reasons. Statisticians must always be careful—and more importantly, transparent—when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher R2R2 is not a good enough reason!</p>
<p>In the mlbBat10 data, the outlier with an OBP of 0.550 is Bobby Scales, an infielder who had four hits in 13 at-bats for the Chicago Cubs. Scales also walked seven times, resulting in his unusually high OBP. The justification for removing Scales here is weak. While his performance was unusual, there is nothing to suggest that it is not a valid data point, nor is there a good reason to think that somehow we will learn more about Major League Baseball players by excluding him.</p>
<p>Nevertheless, we can demonstrate how removing him will affect our model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create nontrivial_players</span>
nontrivial_players &lt;-<span class="st"> </span>mlbBat10 %&gt;%
<span class="st">  </span><span class="kw">filter</span>(AB &gt;=<span class="st"> </span><span class="dv">10</span> &amp;<span class="st"> </span>OBP &lt;<span class="st"> </span><span class="fl">0.5</span>)


<span class="co"># Fit model to new data</span>
mod_cleaner &lt;-<span class="st"> </span><span class="kw">lm</span>(SLG ~<span class="st"> </span>OBP, <span class="dt">data =</span> nontrivial_players)

<span class="co"># View model summary</span>
<span class="kw">summary</span>(mod_cleaner)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = SLG ~ OBP, data = nontrivial_players)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.31383 -0.04165 -0.00261  0.03992  0.35819 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.043326   0.009823  -4.411 1.18e-05 ***
## OBP          1.345816   0.033012  40.768  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.07011 on 734 degrees of freedom
## Multiple R-squared:  0.6937, Adjusted R-squared:  0.6932 
## F-statistic:  1662 on 1 and 734 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Visualize new model</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> nontrivial_players, <span class="kw">aes</span>(<span class="dt">x =</span> OBP, <span class="dt">y =</span> SLG)) +
<span class="st"> </span><span class="kw">geom_point</span>() +
<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p><img src="CorrelationAndRegression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="high-leverage-points" class="section level3">
<h3><span class="header-section-number">8.4.2</span> High leverage Points</h3>
<p>Not all points of high leverage are influential. While the high leverage observation corresponding to Bobby Scales in the previous exercise is influential, the three observations for players with OBP and SLG values of 0 are not influential.</p>
<p>This is because they happen to lie right near the regression anyway. Thus, while their extremely low OBP gives them the power to exert influence over the slope of the regression line, their low SLG prevents them from using it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> SLG ~<span class="st"> </span>OBP, <span class="dt">data =</span> <span class="kw">filter</span>(mlbBat10, AB &gt;=<span class="st"> </span><span class="dv">10</span>))

<span class="co"># Rank high leverage points</span>
mod %&gt;%
<span class="st">  </span><span class="kw">augment</span>() %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(.hat),<span class="kw">desc</span>(.cooksd)) %&gt;%
<span class="st">  </span><span class="kw">head</span>()</code></pre></div>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>## Warning: Deprecated: please use `purrr::possibly()` instead</code></pre>
<pre><code>##     SLG   OBP     .fitted     .se.fit      .resid       .hat     .sigma
## 1 0.000 0.000 -0.03744579 0.009956861  0.03744579 0.01939493 0.07153050
## 2 0.000 0.000 -0.03744579 0.009956861  0.03744579 0.01939493 0.07153050
## 3 0.000 0.000 -0.03744579 0.009956861  0.03744579 0.01939493 0.07153050
## 4 0.308 0.550  0.69049108 0.009158810 -0.38249108 0.01641049 0.07011360
## 5 0.000 0.037  0.01152451 0.008770891 -0.01152451 0.01504981 0.07154283
## 6 0.038 0.038  0.01284803 0.008739031  0.02515197 0.01494067 0.07153800
##        .cooksd .std.resid
## 1 0.0027664282  0.5289049
## 2 0.0027664282  0.5289049
## 3 0.0027664282  0.5289049
## 4 0.2427446800 -5.3943121
## 5 0.0002015398 -0.1624191
## 6 0.0009528017  0.3544561</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploratory-data-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Study-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
