# Importing data - Part 1
***
Notes taken during/inspired by the Datacamp course 'Importing Data in R (Part 1)' by Filip Schouwenaars.

## Introduction 

Data often comes from many different sources and formats, including

* Flat files - simple tables e.g. csv
* Excel
* Databases - MySQL, Postgres
* Websites - APIs, JSON, scraping
* Other statistical software - SPSS, STATA, SAS

## Reading CSV files

Reading csv files can be achived with simple code like

```{r eval=FALSE}
read.csv("file.csv", stringsAsFactors = FALSE)
```

We may want to import strings as categorical variables, in which case we would set stringsAsFactors = TRUE which is also the default option, if not stated.

When working across different machines or operating systems, problems can arise due to different ways of addressing file locations and differing file locations.  Therefore, it can be easier to set a relative path to the users home directory, which would be achieved with the following code.

```{r}
path <- file.path("~", "datasets", "file.csv")
path
```

Then use the file path as before, assigning to a dataframe.

```{r eval=FALSE}
df <- read.csv(path, stringsAsFactors = FALSE)
```

## Reading tab deliminated files or other table formats

In a similar way to before, we add the path to the file and if we want strings as strings, for instance

```{r eval=FALSE}
read.delim("file.csv", stringsAsFactors = FALSE)
```

However, if the file comes in another format perhaps due to the system encoding or setup, it is still possible to try and read the file as a tabular formating convering it to a data frame.  To do so, we use the read.table() command which has a lot of arguments that can be customised.  You can specifiy column names and types for instance.  If for instance we have a file format where the objects are seperated by a / rather than a comma or tab as before, we could use

```{r eval = FALSE}
read.table("file.txt",
           header = TRUE,
           sep = "/",
           stringsAsFactors = FALSE)

```

Or, if you have a file which has no column/variable names and tabs as spaces, you would read the file as:

```{r eval = FALSE}
# Path to the file.txt file: path
path <- file.path("data", "file.txt")

# Import the file.txt file: hotdogs
file <- read.table(path, 
                      sep = "\t",                                        # specify seperator - tab in this instance
                      col.names = c("VarName1", "VarName2", "VarName3"), # specifiy variable names
                      colClasses = c("factor", "NULL", "numeric"))       # specify the column/variable classes
```

Both read.csv and read.delim are wrapper functions of read.table(), both use read.table but have different default options depending on the file type.  There are two further wrapper functions - read.csv2 and read.delim2 - which deal with regional differences in formatting, notably that some areas use full stops as decimal places, whereas other areas use commas for decimal places.

## Readr and data.table

These two packages are other ways of reading in files.  Readr uses the tibble, so will be compatable with other tidyverse packages such as dplyr.  It is faster than utils, the r default and also prints out the column classes, depending on what other packages are loaded. It is not neccessary to specifiy stringsAsFactors = FALSE.  Like the utils package, these are wrapper functions, with the base function being read_delim().

```{r eval = FALSE}
library(readr)
read_csv("file.csv")    #read comma seperated
read_tsv("file2.txt")   #read tab seperated files

#If there are no row heards, you can create a vector then read it in using the col_names argument

#specify the vector for column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")
#read in the vector
potatoes <- read_tsv("potatoes.txt", col_names = properties)

```

