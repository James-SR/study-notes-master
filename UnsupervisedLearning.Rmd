Unsupervised Learning in R
***
Notes taken during/inspired by the Datacamp course 'Unsupervised Learning in R' by Hank Roark. 

Course slides:
* [Part 1 - Unsupervised learning in R](https://s3.amazonaws.com/assets.datacamp.com/production/course_1815/slides/ch1.pdf)
* [Part 2 - Hierarchical clustering](https://s3.amazonaws.com/assets.datacamp.com/production/course_1815/slides/ch2.pdf)
* [Part 3 - Dimensionality reduction with PCA](https://s3.amazonaws.com/assets.datacamp.com/production/course_1815/slides/ch3.pdf)
* [Part 4 - Putting it all together with a case study](https://s3.amazonaws.com/assets.datacamp.com/production/course_1815/slides/ch4.pdf)
* [Additional - 10 Algorithms in Plan English](https://hackerbits.com/data/top-10-data-mining-algorithms-in-plain-english/)

## Unsupervised learning

Often we are presented with trying to find 'patterns in the data' where there is perhaps no single goal.  This requires more cereativity but this can also be where unsupervised learning helps.  

We start with k means, which starts by randomly assigning points to a cluster, then is run multiple times with the besxt solution ebing selected from multiple runs.  The k means algorithm needs a measure of quality to determine the 'best' of these multiple runs.  This measure is the total sum of squares within that measurement.  This is done by measuring the distance from each observation to the centre of each cluster, determining the squared distance from the observation to the centre (squared eclidean distance).  Running the model mulitple times helps to find the global minium within cluster sum of squares.  

If the number of clusters or groups within your data is not known, the best approach is to run the number of clusters from 1 to n, then record the total within cluster some of squares (TSS) for each number of clusters as a scree plot, we are trying to find an 'elbow' or sometimes referred to as a 'dog leg' in the data so that for the additional cluster the reduction in TSS is very small.  We then use this elbow point to approximate the number of clusters.  

```{r}
# Create the data - x

set.seed(42)
x <- matrix(rnorm(600), ncol=2)
l1 <- 100
l2 <- 250
x[1:l1,1] <- x[1:l1,1]+2
x[1:l1,2] <- x[1:l1,2]+2
x[(l1+1):l2,1] <- x[(l1+1):l2,1]-5
x[(l1+1):l2,2] <- x[(l1+1):l2,2]+2
x[(l2+1):nrow(x),1] <- x[(l2+1):nrow(x),1]+1
x[(l2+1):nrow(x),2] <- x[(l2+1):nrow(x),2]+0

# Set up 2 x 3 plotting grid
par(mfrow = c(2, 3))

# Set seed
set.seed(1)

for(i in 1:6) {
  # Run kmeans() on x with three clusters and one start
  km.out <- kmeans(x, centers = 3, nstart = 1)
  
  # Plot clusters
  plot(x, col = km.out$cluster, 
       main = km.out$tot.withinss, 
       xlab = "", ylab = "")
}
```

The k-means algorithm assumes the number of clusters as part of the input. If you know the number of clusters in advance (e.g. due to certain business constraints) this makes setting the number of clusters easy. However, as you saw in the video, if you do not know the number of clusters and need to determine it, you will need to run the algorithm multiple times, each time with a different number of clusters. From this, you can observe how a measure of model quality changes with the number of clusters.

In this exercise, you will run kmeans() multiple times to see how model quality changes as the number of clusters changes. Plots displaying this information help to determine the number of clusters and are often referred to as scree plots.

The ideal plot will have an elbow where the quality measure improves more slowly as the number of clusters increases. This indicates that the quality of the model is no longer improving substantially as the model complexity (i.e. number of clusters) increases. In other words, the elbow indicates the number of clusters inherent in the data.

```{r}
# Initialize total within sum of squares error: wss
wss <- 0

# For 1 to 15 cluster centers
for (i in 1:15) {
  km.out <- kmeans(x, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

# Plot total within sum of squares vs. number of clusters
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Set k equal to the number of clusters corresponding to the elbow location
k <- 2
```

### Pokemon Data

The Pokemon data is unlabelled since we don't have a specific outcome or result we are predicting.  It appears to be a bit like Top Trumps.  The data is hosted on [Kaggle](https://www.kaggle.com/abcsds/pokemon).  We will try and find homogenous groups of Pokemon, the number of clusters is not known before hand so we will have to determine the number ourselves.

Here we use the iter.max argument to kmeans(). As kmeans() is an iterative algorithm, repeating over and over until some stopping criterion is reached. The default number of iterations for kmeans() is 10, which is not enough for the algorithm to converge and reach its stopping criterion, so we'll set the number of iterations to 50 to overcome this issue. To see what happens when kmeans() does not converge, try running the example with a lower number of iterations (e.g. 3). This is another example of what might happen when you encounter real data and use real cases.

```{r}
# Load the data
poke <- read.csv("D:/CloudStation/Documents/2017/RData/pokemon.csv")
pokemon <- poke[, 6:11]

# Initialize total within sum of squares error: wss
wss <- 0

# Look over 1 to 15 possible clusters
for (i in 1:15) {
  # Fit the model: km.out
  km.out <- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50)
  # Save the within cluster sum of squares
  wss[i] <- km.out$tot.withinss
}

# Produce a scree plot
plot(1:15, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")

# Select number of clusters
k <- 3

# Build model with k clusters: km.out
km.pokemon <- kmeans(pokemon, centers = k, nstart = 20, iter.max = 50)


# View the resulting model
km.pokemon

# Plot of Defense vs. Speed by cluster membership
plot(pokemon[, c("Defense", "Speed")],
     col = km.pokemon$cluster,
     main = paste("k-means clustering of Pokemon with", k, "clusters"),
     xlab = "Defense", ylab = "Speed")
```

## Heirachical Clustering

This is useful when the number of clusters is now known ahead of time, whereas k means requies you to specificy a number of clusters.  We could do bottom up ot top down, here we look at bottom up.  In bottom up, we assign each point to their own cluster initially, then find the closest two clusters and join these together in to a single cluster.  We then continue by combining the next two closest clusters together and repeating until there is only a single cluster with all points.  

To create a hierarchical clustering model of x, the first step to hierarchical clustering is determining the similarity between observations, which you will do with the dist() function.

```{r}
# Create hierarchical clustering model: hclust.out
hclust.out <- hclust(dist(x))

# Inspect the result
summary(hclust.out)
```

The easiest way to interpret the results is to use the dendogram function in R which also allows us to visually see the results.  We can the add a line, which is specifying that you want clusters together that are no further apart than that height/line.  Distance could be calculated by diferent measures, but we just look at euclidean distance.  

The cutree() is the R function that cuts a hierarchical model. The h and k arguments to cutree() allow you to cut the tree based on a certain height h or a certain number of clusters k.

Here we use cutree() to cut the hierarchical model you created earlier based on each of these two criteria. The output of each cutree() call represents the cluster assignments for each observation in the original dataset. 

```{r}
# Cut by height
cutree(hclust.out, h = 7)

# Cut by number of clusters
cutree(hclust.out, k = 3)
```

Four methods to determine which cluster should be linked:
* Complete: pairwise similarity between all observations in cluster 1 and cluster 2, and uses largest of similarities
* Single: same as above but uses smallest of similarities
* Average: same as above but uses average of similarities
* Centroid: finds centroid of cluster 1 and centroid of cluster 2, and uses similarity between two centroids

Generally, complete and average tend to produce more balanced trees and are the most commonly used.  Single tends to produce unbalanced trees, centroids can created inverted trees so can be undesirable.  We specifiy the cluster linkage method in the hclust function using the method = <linkage method>.   

Whether you want balanced or unbalanced trees for your hierarchical clustering model depends on the context of the problem you're trying to solve. Balanced trees are essential if you want an even number of observations assigned to each cluster. On the other hand, if you want to detect outliers, for example, an unbalanced tree is more desirable because pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters.

```{r}
# Cluster using complete linkage: hclust.complete
hclust.complete <- hclust(dist(x), method = "complete")

# Cluster using average linkage: hclust.average
hclust.average <- hclust(dist(x), method = "average")

# Cluster using single linkage: hclust.single
hclust.single <- hclust(dist(x), method = "single")

# Plot dendrogram of hclust.complete
plot(hclust.complete, main = "Complete")

# Plot dendrogram of hclust.average
plot(hclust.average, main = "Average")

# Plot dendrogram of hclust.single
plot(hclust.single, main = "Single")
```

### Pokemon Data

Clustering real data may require scaling the features if they have different distributions. So far we have been working with synthetic data that did not need scaling.

Here you will go back to working with "real" data, the pokemon dataset introduced in the first chapter. You will observe the distribution (mean and standard deviation) of each feature, scale the data accordingly, then produce a hierarchical clustering model using the complete linkage method.

You first check to see if the column means and standard deviations vary. If they do, you scale the data, convert the scaled data to a similarity matrix and passed it into the hclust() function.

```{r}
# View column means
colMeans(pokemon)

# View column standard deviations
apply(pokemon, 2, sd)

# Scale the data
pokemon.scaled <- scale(pokemon)

# Create hierarchical clustering model: hclust.pokemon
hclust.pokemon <- hclust(dist(pokemon.scaled), method = "complete")
```

Comparing k-means and hierarchical clustering, you'll see the two methods produce different cluster memberships. This is because the two algorithms make different assumptions about how the data is generated. Here we compare results from the two models on the pokemon dataset to see how they differ.

```{r}
# Apply cutree() to hclust.pokemon: cut.pokemon
cut.pokemon <- cutree(hclust.pokemon, k = 3)

# Compare methods
table(km.pokemon$cluster, cut.pokemon)
```

Looking at the table, it looks like the hierarchical clustering model assigns most of the observations to cluster 1, while the k-means algorithm distributes the observations more among all clusters. It's important to note that there's no consensus on which method produces better clusters. The job of the analyst in unsupervised clustering is to observe the cluster assignments and make a judgment call as to which method provides more insights into the data.

## Data/Dimension Reduction with PCA

Dimension reduction helps to find structure in features and aids in visualisation.  A popular method is principal component analysis (PCA)
Three goals when finding lower dimensional representation of features:

* Find linear combination of variables to create principal components
* Maintain most variance in the data
* Principal components are uncorrelated (i.e. orthogonal to each other)

However, once we get past three dimensional PCA it becomes hard to visualise the results and it makes the results harder to understand.  

PCA is relatively easy to implement in R using the prcomp function, which also gives the ability to scale the variable (to +/- 1 s.d.) and centere (around 0) the data first.  Hans suggests centering is done each time by leaving it = TRUE.  

Here we will be creating a PCA on the Pokemon data.

```{r}
# Perform scaled PCA: pr.out
 pr.out <- prcomp(x = pokemon, scale = TRUE, center = TRUE) 

# Inspect model output
summary(pr.out)
```

PCA models in R produce additional diagnostic and output components:

* center: the column means used to center to the data, or FALSE if the data weren't centered
* scale: the column standard deviations used to scale the data, or FALSE if the data weren't scaled
* rotation: the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components
* x: the value of each observation in the original dataset projected to the principal components

You can access these the same as other model components. For example, use pr.out$rotation to access the rotation component.

x has different dimensions as the original - calling dim() on pr.out$rotation and pokemon, you can see they have different dimensions.

### Visualising results

We typically use biplots to show the relationship between principle components or we use scree plots to understand how much variance is explained by our components.   The biplot() function plots both the principal components loadings and the mapping of the observations to their first two principal component values.  

```{r}
biplot(pr.out)
```

Here we see HitPoints and Attack are  related as they have similar loadings in the first two principle components.  If we had a smaller number of observations we could see how individual items are related or unrelated.  For instance, we can see items 430 and 231 are not related - they are the least similar in terms of 2 principle components.  

The second common plot type for understanding PCA models is a scree plot. A scree plot shows the variance explained as the number of principal components increases. Sometimes the cumulative variance explained is plotted as well.

First we will prepare data from the pr.out model, preparing the data for plotting is required because there is not a built-in function in R to create this type of plot.

```{r}
# Variability of each principal component: pr.var
pr.var <- pr.out$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
```

Now we create a scree plot showing the proportion of variance explained by each principal component, as well as the cumulative proportion of variance explained.

These plots can help to determine the number of principal components to retain. One way to determine the number of principal components to retain is by looking for an elbow in the scree plot showing that as the number of principal components increases, the rate at which variance is explained decreases substantially. In the absence of a clear elbow, you can use the scree plot as a guide for setting a threshold.

When the number of principal components is equal to the number of original features in the data, the cumulative proportion of variance explained is 1.

```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
```

When using PCA we should consider that scaling the data may be neccessary.  In addition, we should decide how we deal with missing data - are we going to impute the data or drop observations with missing items?  How are we going to handle categorical features - not include them or encode categorical variables by using dummy vars / one hot encoding?

Sometimes scaling is appropriate when the variances of the variables are substantially different. This is commonly the case when variables have different units of measurement, for example, degrees Fahrenheit (temperature) and miles (distance). Making the decision to use scaling is an important step in performing a principal component analysis.  Here we see the differences between applying the results with and without scaling.

```{r}
# Mean of each variable
colMeans(pokemon)

# Standard deviation of each variable
apply(pokemon, 2, sd)

# PCA model with scaling: pr.with.scaling
pr.with.scaling <- prcomp(x = pokemon, scale = TRUE) 

# PCA model without scaling: pr.without.scaling
pr.without.scaling <- prcomp(x = pokemon, scale = FALSE)

# Create biplots of both for comparison
biplot(pr.with.scaling)
biplot(pr.without.scaling)
```

The new Total column contains much more variation, on average, than the other four columns, so it has a disproportionate effect on the PCA model when scaling is not performed. After scaling the data, there's a much more even distribution of the loading vectors.

## Case Study - Breast Cancer

We will be looking at data from K. P. Benne! and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets" which contains data for nuclei measurements for human breast cells.  Each row is a nuclei and each column or feature is a measurement, we also have atarget variable or label in the dataset in the form of whether the cancer is benign or malignant.  

```{r}
# Load the data
wisc.df <- read.csv("D:/CloudStation/Documents/2017/RData/WisconsinCancer.csv")

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[, 3:32])

# Set the row names of wisc.data to help keep track of the data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector - 1 if malignant
diagnosis <- as.numeric(wisc.df$diagnosis == "M")

# View the data
head(wisc.data, n = 10)
str(wisc.data)


```

We can see there are a few variables with mean in the title, but how many?

```{r}
# Load the library
library(stringr)

# Look for all instances of _mean, sum the value as TRUE = 1, removing any NA values
sum(str_detect(dimnames(wisc.data)[[2]],"_mean"), na.rm = TRUE)

```

So we have 569 observations, 30 features in total of which 10 features are mean measurements.  

The next step in the analysis is to perform PCA on wisc.data.

It's important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data:

The input variables use different units of measurement.
The input variables have significantly different variances.

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data, 2, sd)

# Execute PCA, scaling if appropriate: wisc.pr - it is here as the data is orders of magnitude different
wisc.pr <- prcomp(x = wisc.data, scale = TRUE) 

# Look at summary of results
summary(wisc.pr)
```

Next we can chart the PCA results as bi plots and scree plots.


```{r}
# Create a biplot of wisc.pr
biplot(wisc.pr)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")

# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")

# Next we produce scree and cumulative plots by principle components

# Variability of each principal component: pr.var
pr.var <- wisc.pr$sdev^2

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
```

Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.  From the previous table and the cumulative proprtion of variance chart, nearly 80% of the variance is explained by just the first 4 components, and 91% by the first 7, 98.9% by the first 16 meaning the last 14 only explain 1.1% of the variance.

### Clustering

Here our goal is to do hierarchical clustering of the observations. This type of clustering does not assume in advance the number of natural groups that exist in the data.

As part of the preparation for hierarchical clustering, distance between all pairs of observations are computed. Furthermore, there are different ways to link clusters together, with single, complete, and average being the most common linkage methods.

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

# Calculate the (Euclidean) distances: data.dist
data.dist <- dist(data.scaled)

# Create a hierarchical clustering model: wisc.hclust
wisc.hclust <- hclust(data.dist, method = "complete")
```

Next we can visualise the results

```{r}
plot(wisc.hclust)
```

If we cut at 20, this will produce 4 clusters.

Next we compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing unsupervised learning like this, a target variable isn't available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model.

When performing supervised learning — that is, when you're trying to predict some target variable of interest and that target variable is available in the original data — using clustering to create new features may or may not improve the performance of the final model. This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.  

_A confusion matrix would be useful here_

```{r}
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

# Compare cluster membership to actual diagnoses
table(wisc.hclust.clusters, diagnosis)

```

### K-means

As there are two main types of clustering: hierarchical and k-meansm, here we create a k-means clustering model on the Wisconsin breast cancer data and compare the results to the actual diagnoses and the results of your hierarchical clustering model. Take some time to see how each clustering model performs in terms of separating the two diagnoses and how the clustering models compare to each other.

```{r}
# Create a k-means model on wisc.data: wisc.km
wisc.km <- kmeans(scale(wisc.data), centers = 2, nstart = 20)

# Compare k-means to actual diagnoses
table(wisc.km$cluster, diagnosis)

# Compare k-means to hierarchical clustering
table(wisc.hclust.clusters, wisc.km$cluster)
```

Here we put together several steps you used earlier and, in doing so, you will experience some of the creativity that is typical in unsupervised learning.

Recall from earlier exercises that the PCA model required significantly fewer features to describe 80% and 95% of the variability of the data. In addition to normalizing data and potentially avoiding overfitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques.

Let's see if PCA improves or degrades the performance of hierarchical clustering.

```{r}
# Create a hierarchical clustering model: wisc.pr.hclust
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "complete")

# Cut model into 4 clusters: wisc.pr.hclust.clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 4)

# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)

# Compare to k-means and hierarchical
table(diagnosis, wisc.hclust.clusters)
table(diagnosis, wisc.km$cluster)
```

