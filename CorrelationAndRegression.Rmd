# Correlation and Regression
***
Notes taken during/inspired by the Datacamp course 'Correlation and Regression' by Ben Baumer.

## Visualizing two variables

Some common terminology of data includes

* Response variable a.k.a. y, dependent (usually on the vertical axis if using a scatter plot)
* Explanatory variable, something you think might be related to the response a.k.a. x, independent, predictor (usually on the horizontal axis)

```{r, message = FALSE}
library(openintro)
library(ggplot2)
library(dplyr)
library(tidyr)

# load the data
data(ncbirths)

# Scatterplot of weight vs. weeks
ggplot(ncbirths, aes(weeks, weight)) +
  geom_point()

```

If it is helpful, you can think of boxplots as scatterplots for which the variable on the x-axis has been discretized.

The cut() function takes two arguments: the continuous variable you want to discretize and the number of breaks that you want to make in that continuous variable in order to discretize it.

```{r}
# Boxplot of weight vs. weeks
ggplot(data = ncbirths, 
       aes(x = cut(weeks, breaks = 5), y = weight)) + 
  geom_boxplot()
```

### Transformations

Here the relationship is hard to see.

```{r}
data(mammals)
# Mammals scatterplot
ggplot(mammals, aes(BodyWt, BrainWt)) +
  geom_point()
```

The relationship between two variables may not be linear. In these cases we can sometimes see strange and even inscrutable patterns in a scatterplot of the data. Sometimes there really is no meaningful relationship between the two variables. Other times, a careful transformation of one or both of the variables can reveal a clear relationship.

ggplot2 provides several different mechanisms for viewing transformed relationships. The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes.

```{r}
# Scatterplot with coord_trans()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) + 
  geom_point() + 
  coord_trans(x = "log10", y = "log10")

# Scatterplot with scale_x_log10() and scale_y_log10()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() +
  scale_x_log10() + scale_y_log10()
```

### Identifying Outliers

It is clear here, using the Using the mlbBat10 dataset,  a scatterplot illustrates how the slugging percentage (SLG) of a player varies as a function of his on-base percentage (OBP).

```{r}
data("mlbBat10")
# Baseball player scatterplot
ggplot(mlbBat10, aes(OBP, SLG)) + 
  geom_point()
```

Most of the points are clustered in the lower left corner of the plot, making it difficult to see the general pattern of the majority of the data. This difficulty is caused by a few outlying players whose on-base percentages (OBPs) were exceptionally high. These values are present in our dataset only because these players had very few batting opportunities.

Both OBP and SLG are known as rate statistics, since they measure the frequency of certain events (as opposed to their count). In order to compare these rates sensibly, it makes sense to include only players with a reasonable number of opportunities, so that these observed rates have the chance to approach their long-run frequencies.

In Major League Baseball, batters qualify for the batting title only if they have 3.1 plate appearances per game. This translates into roughly 502 plate appearances in a 162-game season. The mlbBat10 dataset does not include plate appearances as a variable, but we can use at-bats (AB) -- which constitute a subset of plate appearances -- as a proxy.

```{r}
# Scatterplot of SLG vs. OBP
mlbBat10 %>%
  filter(AB >= 200) %>%
  ggplot(aes(x = OBP, y = SLG)) +
  geom_point()

# Identify the outlying player
mlbBat10 %>%
  filter(AB >= 200, OBP < 0.2)
```

## Correlation

We typically calculate the Pearsons aka Pearson product-moment correlation. The cor(x, y) function will compute the Pearson product-moment correlation between variables, x and y. Since this quantity is symmetric with respect to x and y, it doesn't matter in which order you put the variables.

At the same time, the cor() function is very conservative when it encounters missing data (e.g. NAs). The use argument allows you to override the default behavior of returning NA whenever any of the values encountered is NA. Setting the use argument to "pairwise.complete.obs" allows cor() to compute the correlation coefficient for those observations where the values of x and y are both not missing.

```{r}
data(ncbirths)
# Compute correlation between the birthweight and mother's age
ncbirths %>%
  summarize(N = n(), r = cor(mage, weight))

# Compute correlation for all non-missing pairs
ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))
```

### Anscombe Dataset

In 1973, Francis Anscombe famously created four synthetic datasets with remarkably similar numerical properties, but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable, set, that distinguishes the quartet.  


```{r}
data("anscombe")

# Tidy the data for plotting
Anscombe <- anscombe %>%
  mutate(id = seq_len(n())) %>%
  gather(key, value, -id) %>%
  separate(key, c("variable", "set"), 1, convert = TRUE) %>%
  mutate(set = c("1", "2", "3", "4")[set]) %>%
  spread(variable, value)

# Plot the four variants
ggplot(data = Anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)

# Compute statistics for the sets
Anscombe %>%
  group_by(set) %>%
  summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x,y))
```

## Linear Regression

The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a "best fit" line that cuts through the data in a way that minimizes the distance between the line and the data points.

We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we'll set this argument to the value "lm".

```{r}
data(bdims)

# Scatterplot with regression line
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = TRUE)
```

Sometimes it is better to think of the model error as 'noise' which we might try to better incorporate by creating a better model.

Two facts enable you to compute the slope b1 and intercept b0 of a simple linear regression model from some basic summary statistics.

First, the slope can be defined as:

b1=rX,Y⋅sYsX

where rX,Y represents the correlation (cor()) of X and Y and sX and sY represent the standard deviation (sd()) of X and Y, respectively.

Second, the point (x¯,y¯)is always on the least squares regression line, where x¯and y¯denote the average of x and y, respectively.

The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (Y) as a function of height (X). 

```{r}
N <- c(1507)
r <- c(0.7173011)
mean_hgt <- c(171.1438)
sd_hgt <- c(9.407205)
mean_wgt <- c(69.14753)
sd_wgt <- c(13.34576)
bdims_summary <- data.frame(N, r, mean_hgt, sd_hgt, mean_wgt, sd_wgt)

# Print bdims_summary
bdims_summary

# Add slope and intercept
bdims_summary %>%
  mutate(slope = r * sd_wgt / sd_hgt, 
         intercept = mean_wgt - slope * mean_hgt)
```

### Regression to the Mean

Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While "regression to the mean" and "linear regression" are not the same thing, we will examine them together in this exercise.

One way to see the effects of regression to the mean is to compare the heights of parents to their children's heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches.

```{r}
# Galton data fromhttp://www.math.uah.edu/stat/data/Galton.html
Galton <- read.csv("Galton.csv")

# Height of children vs. height of father
Galton %>% 
  filter(Gender == "M") %>%
  ggplot(aes(x = Father, y = Height)) +
    geom_point() + 
    geom_abline(slope = 1, intercept = 0) + 
    geom_smooth(method = "lm", se = FALSE)

# Height of children vs. height of mother
Galton %>% 
  filter(Gender == "F") %>%
  ggplot(aes(x = Mother, y = Height)) +
    geom_point() + 
    geom_abline(slope = 1, intercept = 0) + 
    geom_smooth(method = "lm", se = FALSE)


```


