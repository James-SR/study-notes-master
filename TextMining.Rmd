Text Mining in R
***
Notes taken during/inspired by the Datacamp course 'Text Mining: Bag of Words' by Ted Kwartler. 

Course slides:
* [Part 1 - Jumping into text mining with bag of words]()
* [Part 2 - Word clouds and more interesting visuals]()
* [Part 3 - Addining to your skills]()
* [Part 4 - Case Study]()
* [Additional - Dendextend package](https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html)

## Text Mining Intro

Ted mkaes the definition of Text Mining as 'The process of distilling actionable insights from text'.  We can use Text Mining to reduce the information space in to something more manageable and draw out the important features.  The steps we take are:

* 1 - Problem definition and goals
* 2 - Identify text to be collected
* 3 - Text organisation
* 4 - Feature extraction
* 5 - Aanlysis
* 6 - Reach and insight or make a reccomendation

In this course we look at the bag of words approach, we don't care about word type or order.  The alternative is semantic parsing, which does and looks at word type breaks the elements in to nouns and verb elements, but results in many features for analysis.  

Sometimes we can find out the author's intent and main ideas just by looking at the most common words.

At its heart, bag of words text mining represents a way to count terms, or n-grams, across a collection of documents. 

Manually counting words in the sentences is a pain! Fortunately, the qdap package offers a better alternative. You can easily find the top 4 most frequent terms (including ties) in text by calling the freq_terms function and specifying 4.

> frequent_terms <- freq_terms(text, 4)

The frequent_terms object stores all unique words and their count. You can then make a bar chart simply by calling the plot function on the frequent_terms object.

> plot(frequent_terms)

```{r}
new_text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses."

# Load qdap
library(qdap)

# Print new_text to the console
new_text

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 10)

# Plot term_count
plot(term_count)
```

Next we are going to build our first corpus of text using tweets.  A corpus is a collection of documents, specifying it as a text source for the tm package.  

Text mining begins with loading some text data into R, which we'll do with the read.csv() function. By default, read.csv() treats character strings as factor levels like Male/Female. To prevent this from happening, it's very important to use the argument stringsAsFactors = FALSE.

Best practice is to examine the object you read in to make sure you know which column(s) are important. The str() function provides an efficient way of doing this. You can also count the number of documents using the nrow() function on the new object. In this example, it will tell you how many coffee tweets are in the vector.

If the data frame contains columns that are not text, you may want to make a new object using only the correct column of text (e.g. some_object$column_name).

```{r}
# Import text data
tweets <-  read.csv("D:/CloudStation/Documents/2017/RData/coffee.csv", stringsAsFactors = FALSE) 

# View the structure of tweets
str(tweets)

# Print out the number of rows in tweets
nrow(tweets)

# Isolate text from tweets: coffee_tweets
coffee_tweets <- tweets$text
```

Your next step is to convert this vector containing the text data to a corpus, a corpus is a collection of documents, but it's also important to know that in the tm domain, R recognizes it as a data type.

There are two kinds of the corpus data type, the permanent corpus, PCorpus, and the volatile corpus, VCorpus. In essence, the difference between the two has to do with how the collection of documents is stored in your computer. In this course, we will use the volatile corpus, which is held in your computer's RAM rather than saved to disk, just to be more memory efficient.

To make a volatile corpus, R needs to interpret each element in our vector of text, coffee_tweets, as a document. And the tm package provides what are called Source functions to do just that! In this exercise, we'll use a Source function called VectorSource() because our text data is contained in a vector. The output of this function is called a Source object.

```{r}
# Load tm
library(tm)

# Make a vector source: coffee_source
coffee_source <- VectorSource(coffee_tweets)
```

Now that we've converted our vector to a Source object, we pass it to another tm function, VCorpus(), to create our volatile corpus. 

The VCorpus object is a nested list, or list of lists. At each index of the VCorpus object, there is a PlainTextDocument object, which is essentially a list that contains the actual text data (content), as well as some corresponding metadata (meta). It can help to visualize a VCorpus object to conceptualize the whole thing.

For example, to examine the contents of the second tweet in coffee_corpus, you'd subset twice. Once to specify the second PlainTextDocument corresponding to the second tweet and again to extract the first (or content) element of that PlainTextDocument:

> coffee_corpus[[15]][1]

```{r}
# Make a volatile corpus: coffee_corpus
coffee_corpus <- VCorpus(coffee_source)

# Print out coffee_corpus
coffee_corpus

# Print data on the 15th tweet in coffee_corpus
coffee_corpus[[15]]

# Print the content of the 15th tweet in coffee_corpus
coffee_corpus[[15]]$content
```

Because another common text source is a data frame, there is a Source function called DataframeSource(). The DataframeSource() function treats the entire row as a complete document, so be careful you don't pick up non-text data like customer IDs when sourcing a document this way.

```{r}
# Setup the example text
num <- 1:3
Author1 <- c("Text mining is a great time.","Text analysis provides insights" , "qdap and tm are used in text mining")
Author2 <- c("R is a great language" , "R has many uses" , "DataCamp is cool!")
example_text <- cbind(num, Author1, Author2)
example_text <- as.data.frame(example_text, stringsAsFactors = FALSE)

# Print example_text to the console
example_text

# Create a DataframeSource on columns 2 and 3: df_source
df_source <- DataframeSource(example_text[, 2:3])

# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)

# Examine df_corpus
df_corpus

# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text$Author2)

# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)

# Examine vec_corpus
vec_corpus

```

